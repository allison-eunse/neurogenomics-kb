{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\uddec\ud83e\udde0 Neuro-Omics Knowledge Base","text":"<p>Welcome!</p> <p>This knowledge base connects genomics, brain imaging, and behavioral data through foundation models and multimodal integration strategies. Whether you're analyzing UK Biobank data, implementing gene-brain fusion pipelines, or exploring developmental cohorts, you'll find structured documentation, ready-to-use recipes, and reproducible workflows here.</p> <p>Maintained by Allison Eun Se You | Last updated November 19, 2025</p>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"\ud83c\udd95 New to this KB\ud83d\udd2c Planning an analysis\ud83d\udcda Looking for papers <ol> <li>Start with the KB overview to understand the structure</li> <li>Explore Genetics Models or Brain Models</li> <li>Check out a code walkthrough for hands-on examples</li> </ol> <ol> <li>Review Integration Strategy</li> <li>Pick an analysis recipe</li> <li>Clone an experiment config</li> <li>Validate with <code>python scripts/manage_kb.py</code></li> </ol> <ol> <li>Browse paper cards</li> <li>Check decision logs</li> <li>Review integration strategy</li> </ol>"},{"location":"#foundation-model-registry","title":"\ud83c\udfaf Foundation Model Registry","text":""},{"location":"#genetics-models","title":"Genetics Models","text":"Model Best for Context Quick link \ud83e\uddec Caduceus RC-equivariant gene embeddings DNA sequences Walkthrough \ud83e\uddec DNABERT-2 Cross-species transfer BPE tokenization Walkthrough \ud83e\uddec Evo 2 Ultra-long regulatory regions 1M context Walkthrough \ud83e\uddec GENERator Generative modeling 6-mer LM Walkthrough"},{"location":"#brain-models","title":"Brain Models","text":"Model Modality Best for Quick link \ud83e\udde0 BrainLM fMRI Site-robust embeddings Walkthrough \ud83e\udde0 Brain-JEPA fMRI Lower-latency option Walkthrough \ud83e\udde0 Brain Harmony sMRI + fMRI Multi-modal fusion Walkthrough \ud83e\udde0 BrainMT sMRI/fMRI Mamba efficiency Walkthrough \ud83e\udde0 SwiFT fMRI Hierarchical spatiotemporal Walkthrough"},{"location":"#decisions-roadmaps","title":"\ud83d\udccb Decisions &amp; Roadmaps","text":"<ul> <li>Integration baseline plan (Nov 2025) \u2014 Late fusion first, then escalate if fusion wins.</li> </ul>"},{"location":"#integration-stack","title":"\ud83d\udd17 Integration Stack","text":"<ul> <li>Integration strategy: integration/integration_strategy.md</li> <li>Analysis recipes:</li> <li>CCA + permutation</li> <li>Prediction baselines</li> <li>Partial correlations</li> <li>Modality features:</li> <li>Genomics</li> <li>sMRI</li> <li>fMRI</li> <li>Design patterns: integration/design_patterns.md</li> <li>Multimodal architectures: integration/multimodal_architectures.md</li> </ul>"},{"location":"#multimodal-clinical-models","title":"\ud83c\udfe5 Multimodal &amp; Clinical Models","text":"<p>Beyond genetics and brain FMs, the KB documents multimodal architectures that inform Brain-Omics Model (BOM) design:</p> Model Type Key feature Documentation \ud83d\udd17 BAGEL Unified multimodal MoT experts (understanding/generation) Card \ud83d\udd17 MoT Sparse transformer Modality-aware FFNs (~55% FLOPs) Card \ud83c\udfe5 M3FM Medical imaging + text Bilingual CXR reports Model card: <code>kb/model_cards/m3fm.yaml</code> \ud83c\udfe5 Me-LLaMA Medical LLM Continual pretrained LLaMA Model card: <code>kb/model_cards/me_llama.yaml</code> \ud83c\udfe5 TITAN Whole-slide imaging Multi-scale histopathology Model card: <code>kb/model_cards/titan.yaml</code> <p>\ud83d\udcd6 See full multimodal architectures guide \u2192</p>"},{"location":"#research-papers","title":"\ud83d\udcda Research Papers","text":"<p>14 structured paper cards in <code>kb/paper_cards/</code>:</p>"},{"location":"#integration-methods-5-papers","title":"\ud83d\udd17 Integration &amp; Methods (5 papers)","text":"<ul> <li>Ensemble Integration (Li 2022) \u2014 Late fusion rationale</li> <li>Oncology Multimodal (Waqas 2024) \u2014 Confounds &amp; evaluation</li> <li>Yoon et al. BioKDD 2025 \u2014 MDD gene embeddings + LOGO</li> <li>PRS Guide \u2014 Polygenic risk scores</li> <li>GWAS Diverse Populations \u2014 Ancestry control</li> </ul>"},{"location":"#genetics-fms-3-papers","title":"\ud83e\uddec Genetics FMs (3 papers)","text":"<ul> <li>Caduceus (2024) \u2014 RC-equivariant DNA FM</li> <li>Evo2 (2024) \u2014 1M context StripedHyena</li> <li>GENERator (2024) \u2014 6-mer generative DNA LM</li> </ul>"},{"location":"#brain-fms-4-papers","title":"\ud83e\udde0 Brain FMs (4 papers)","text":"<ul> <li>BrainLM (2024) \u2014 ViT-MAE for fMRI</li> <li>Brain-JEPA (2024) \u2014 JEPA for fMRI</li> <li>Brain Harmony (2025) \u2014 sMRI+fMRI with TAPE</li> <li>BrainMT (2025) \u2014 Hybrid Mamba-Transformer</li> </ul>"},{"location":"#multimodal-architectures-2-papers","title":"\ud83c\udfe5 Multimodal Architectures (2 papers)","text":"<ul> <li>BAGEL (2025) \u2014 Unified multimodal pretraining</li> <li>MoT (2025) \u2014 Mixture-of-Transformers</li> </ul> <p>\ud83d\udccb View all paper card YAMLs \u2192</p>"},{"location":"#data-references","title":"\ud83d\udcca Data References","text":"<ul> <li>Governance &amp; QC: data/governance_qc.md</li> <li>UKB data map &amp; schemas: data/ukb_data_map.md</li> <li>Dataset manifest: <code>kb/datasets/ukb_manifest_stub.yaml</code> (in-repo path; not published)</li> <li>Curated external catalogs: <code>kb/datasets/fms_medical_catalog.yaml</code> for the Awesome Foundation Models roundup</li> <li>(Planned) Developmental / neurodevelopmental cohort cards (e.g., Cha Hospital longitudinal cohort)</li> </ul>"},{"location":"#kb-assets-yaml-curated-sources","title":"\ud83d\uddc2\ufe0f KB Assets (YAML + Curated Sources)","text":"<ul> <li>Model cards: kb/model_cards/</li> <li>Paper cards: kb/paper_cards/</li> <li>Dataset cards: kb/datasets/</li> <li>Integration cards: kb/integration_cards/</li> </ul>"},{"location":"#experiment-configs","title":"\u2699\ufe0f Experiment Configs","text":"<p>Templates in <code>configs/experiments/</code>:</p> <ul> <li>01_cca_gene_smri.yaml \u2014 CCA + permutation baseline</li> <li>02_prediction_baselines.yaml \u2014 Gene vs sMRI vs Fusion</li> <li>03_logo_gene_attribution.yaml \u2014 LOGO \u0394AUC protocol</li> </ul>"},{"location":"#quick-start-guide","title":"\ud83d\ude80 Quick Start Guide","text":""},{"location":"#baselines","title":"Baselines","text":"<ul> <li>Z-score \u2192 residualize \u2192 per-modality 512-D projection \u2192 CCA + permutations \u2192 LR/GBDT (Gene, Brain, Fusion) \u2192 DeLong/bootstrap</li> </ul>"},{"location":"#confounds","title":"Confounds","text":"<ul> <li>Age, sex, site/scanner, motion (FD), SES, genetic PCs</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>Late fusion first \u2192 two-tower contrastive \u2192 EI stacking \u2192 hub tokens/TAPE if needed</li> </ul>"},{"location":"#quick-test","title":"Quick Test","text":"<ul> <li>Run the CCA + permutation template first; it surfaces cross-modal structure before heavier prediction/fusion experiments.</li> </ul> <p>Why CCA + Permutation?</p> <p>CCA alone will always return non-zero canonical correlations, even on shuffled data. The permutation loop builds a null distribution (re-fitting CCA after shuffling one modality within the train fold) so we can report p-values and avoid over-interpreting noise\u2014critical when cohorts share confounds like site or ancestry.</p>"},{"location":"#kb-workflow","title":"\ud83d\udee0\ufe0f KB Workflow","text":""},{"location":"#usage","title":"Usage","text":"<ol> <li>Read paper card YAML for context</li> <li>Check linked code walkthrough for implementation</li> <li>Clone experiment config template</li> <li>Fill dataset paths and parameters</li> <li>Run and log outputs back to KB</li> </ol>"},{"location":"code_walkthroughs/","title":"Code Walkthrough Hub","text":"<p>Each walkthrough now surfaces the KB scaffolding you need to turn narrative notes into living cards and experiment configs.</p>"},{"location":"code_walkthroughs/#quick-links","title":"Quick links","text":"<ul> <li>Integration baseline plan</li> <li>Integration strategy</li> <li>Modality features \u2014 Genomics</li> <li>Modality features \u2014 sMRI</li> <li>Modality features \u2014 fMRI</li> <li>KB templates (model/integration/method/dataset): <code>kb/templates/</code></li> <li>Experiment config stub: <code>kb/templates/experiment_config_stub.md</code></li> </ul>"},{"location":"code_walkthroughs/#walkthrough-roster","title":"Walkthrough roster","text":"Walkthrough KB Model Card Modality Spec BrainLM BrainLM card fMRI spec Brain-JEPA Brain-JEPA card fMRI spec Brain Harmony Brain Harmony card fMRI spec, sMRI spec BrainMT BrainMT card fMRI spec SwiFT SwiFT card fMRI spec Caduceus Caduceus card Genomics spec DNABERT-2 DNABERT-2 card Genomics spec Evo 2 Evo2 card Genomics spec GENERATOR GENERator card Genomics spec M3FM \u2014 \u2014 Me-LLaMA \u2014 \u2014 TITAN \u2014 \u2014 FMS-Medical \u2014 \u2014 BAGEL \u2014 \u2014 MoT \u2014 \u2014"},{"location":"code_walkthroughs/bagel_walkthrough/","title":"BAGEL Code Walkthrough","text":"<p>KB references: BAGEL paper note</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#overview","title":"Overview","text":"<p>BAGEL couples a Qwen2-style Mixture-of-Transformer decoder, a SigLIP NaViT encoder, and a latent VAE so a single 7B active-parameter model can interleave text reasoning, visual understanding, and diffusion-style image synthesis. The public release ships with checkpoints, quantized inference paths, training scripts, and evaluation kits spanning understanding, text-to-image, and editing.^[<code>50:188:external_repos/bagel/README.md</code>][<code>153:198:external_repos/bagel/README.md</code>]</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params / Scale Context Inputs Key capabilities Repo Qwen2 MoT decoder (packed attention, NaiveCache) + SigLIP-NaViT encoder + VAE; modality connectors align latent patches and ViT tokens with the LLM space.^[<code>27:229:external_repos/bagel/modeling/bagel/bagel.py</code>] 7B active / 14B total parameters, trained on trillions of interleaved multimodal tokens; outperforms Qwen2.5-VL and rivals SD3 on benchmarks.^[<code>50:188:external_repos/bagel/README.md</code>] Unified understanding, text-to-image, image editing, and \u201cworld-modeling\u201d tasks surfaced through Gradio, CLI scripts, and evaluation benches.^[<code>50:200:external_repos/bagel/README.md</code>][<code>85:151:external_repos/bagel/app.py</code>] Packed batches contain text token ids, ViT patches, VAE latents, per-token positions, attention masks, and per-modality loss selectors built by <code>PackedDataset</code>.^[<code>45:305:external_repos/bagel/data/dataset_base.py</code>] Training entrypoint wires configurable branches (visual_gen / visual_und), FSDP wrapping, EMA, dataset mixing, and MFU logging.^[<code>98:870:external_repos/bagel/train/pretrain_unified_navit.py</code>] <code>external_repos/bagel</code>"},{"location":"code_walkthroughs/bagel_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Follow the Quick Start: Python\u202f3.10 env, <code>pip install -r requirements.txt</code>, then <code>pip install flash_attn==2.5.8</code> before downloading checkpoints via <code>huggingface_hub.snapshot_download</code>. Modes 1\u20133 in <code>app.py</code> toggle full-precision, NF4, or INT8 pipelines for 12\u201380\u202fGB GPUs.^[<code>107:151:external_repos/bagel/README.md</code>][<code>25:151:external_repos/bagel/app.py</code>]</li> <li>Training relies on CUDA + NCCL with FSDP; <code>pretrain_unified_navit.py</code> auto-detects device TFLOPs for MFU calculation and exposes switches for freezing LLM/ViT/VAE weights, enabling FLEX packing, or running EMA-only resumes.^[<code>98:418:external_repos/bagel/train/pretrain_unified_navit.py</code>]</li> <li>Inference hyperparameters (<code>cfg_text_scale</code>, <code>cfg_img_scale</code>, <code>cfg_interval</code>, <code>timestep_shift</code>, renorm mode, steps) are surfaced both in the README and the Gradio UI so you can script KB experiments consistently.^[<code>90:151:external_repos/bagel/README.md</code>][<code>160:357:external_repos/bagel/app.py</code>]</li> </ul>"},{"location":"code_walkthroughs/bagel_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/bagel_walkthrough/#unified-forward-pass-modelingbagelbagelpy","title":"Unified Forward Pass (<code>modeling/bagel/bagel.py</code>)","text":"<p><code>Bagel</code> hosts the three branches: (1) language tokens (always on), (2) ViT patches for understanding, and (3) VAE latent patches for generation. It projects modality features into the LLM embedding space, injects learned positional/timestep embeddings, and multiplexes MoT experts via packed index tensors. Losses are computed per-branch (CE for text, Smooth L1/MSE for latents) and returned side-by-side.</p> <p>```101:229:external_repos/bagel/modeling/bagel/bagel.py     def forward(..., packed_text_ids, packed_text_indexes, sample_lens, packed_position_ids,                 ..., packed_vit_tokens=None, ..., padded_latent=None, ..., packed_timesteps=None,                 mse_loss_indexes=None):         packed_text_embedding = self.language_model.model.embed_tokens(packed_text_ids)         packed_sequence = packed_text_embedding.new_zeros((sequence_length, self.hidden_size))         packed_sequence[packed_text_indexes] = packed_text_embedding         ...         if self.config.visual_und:             packed_vit_token_embed = self.vit_model(... )             packed_vit_token_embed = self.connector(packed_vit_token_embed)             packed_sequence[packed_vit_token_indexes] = packed_vit_token_embed + vit_pos_embed         if self.config.visual_gen:             ... # patchify VAE latents, inject timestep + position, place into packed sequence             packed_sequence[packed_vae_token_indexes] = packed_latent         last_hidden_state = self.language_model(..., packed_sequence=packed_sequence, ...)         if self.config.visual_gen:             packed_mse_preds = self.llm2vae(last_hidden_state[mse_loss_indexes])             mse = (packed_mse_preds - target[has_mse]) ** 2         if ce_loss_indexes is not None:             packed_ce_preds = self.language_model.lm_head(last_hidden_state[ce_loss_indexes])             ce = F.cross_entropy(packed_ce_preds, packed_label_ids, reduction=\"none\")         return dict(mse=mse, ce=ce) <pre><code>The same class also defines cache-friendly helpers (`prepare_prompts`, `prepare_vit_images`, `prepare_vae_latent`, `generate_image`, `generate_text`) so both training and inference reuse identical packing rules.^[```232:907:external_repos/bagel/modeling/bagel/bagel.py```]\n\n### PackedDataset &amp; Sequence Plans (`data/dataset_base.py`)\n`PackedDataset` streams heterogenous samples, applies conditional dropout (`text_cond_dropout_prob`, etc.), and emits a single packed tensor blob per batch. Each `sequence_plan` step can insert text spans, ViT patches, or VAE tensors, automatically managing BOS/EOS vision tokens, per-split attention modes, and modality-specific losses.^[```45:400:external_repos/bagel/data/dataset_base.py```]\n\n```187:305:external_repos/bagel/data/dataset_base.py\n    def to_tensor(self, sequence_status):\n        data = dict(\n            sequence_length=sum(sequence_status['sample_lens']),\n            sample_lens=sequence_status['sample_lens'],\n            packed_text_ids=torch.tensor(sequence_status['packed_text_ids']),\n            ...\n        )\n        if len(sequence_status['vae_image_tensors']) &gt; 0:\n            data['padded_images'] = padded_images\n            data['patchified_vae_latent_shapes'] = sequence_status['vae_latent_shapes']\n            data['packed_latent_position_ids'] = torch.cat(sequence_status['packed_latent_position_ids'], dim=0)\n        if len(sequence_status['packed_vit_tokens']) &gt; 0:\n            data['packed_vit_tokens'] = torch.cat(sequence_status['packed_vit_tokens'], dim=0)\n            data['packed_vit_position_ids'] = torch.cat(sequence_status['packed_vit_position_ids'], dim=0)\n            data['vit_token_seqlens'] = torch.tensor(sequence_status['vit_token_seqlens'])\n</code></pre></p> <p>The <code>pack_sequence</code> routine adds <code>&lt;|im_start|&gt; / &lt;|im_end|&gt;</code> sentinels, calls <code>patchify</code> for ViT patches, records <code>packed_timesteps</code> for diffusion supervision, and scales CE loss weights by token length so batches with different numbers of captions remain balanced.^[<code>306:724:external_repos/bagel/data/dataset_base.py</code>]</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#training-entry-point-trainpretrain_unified_navitpy","title":"Training Entry Point (<code>train/pretrain_unified_navit.py</code>)","text":"<p>Three dataclasses (<code>ModelArguments</code>, <code>DataArguments</code>, <code>TrainingArguments</code>) expose practically every toggle: source checkpoints, positional interpolation, dropout per modality, packed-data limits, sharding strategy, EMA decay, LR schedule, and loss weights.^[<code>98:405:external_repos/bagel/train/pretrain_unified_navit.py</code>] The <code>main()</code> routine then: - Parses args, initializes NCCL, seeds, and W&amp;B logging. - Loads or restores Qwen2/SigLIP/AE weights (optionally HF checkpoints) and wires them into <code>BagelConfig</code>. - Builds <code>PackedDataset</code> via YAML-specified groups, enabling FLEX packing or resume-friendly overflow buffers. - Wraps the model in FSDP + activation checkpointing, sets up EMA mirrors, optimizer, scheduler, gradient clipping, and MFU telemetry.^[<code>408:775:external_repos/bagel/train/pretrain_unified_navit.py</code>] - Periodically logs CE/MSE/token throughput, tracks dataset sampling state for deterministic resumes, and checkpoints both base + EMA weights alongside optimizer/scheduler state.^[<code>658:867:external_repos/bagel/train/pretrain_unified_navit.py</code>]</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#inference-stack-apppy-inferencerpy","title":"Inference Stack (<code>app.py</code> + <code>inferencer.py</code>)","text":"<p><code>app.py</code> bootstraps configs, shares layers across devices, and lets you choose full precision, NF4, or INT8 quantization before launching the Gradio UI. It wires UI sliders directly to CFG/timestep parameters so experiments match README defaults.^[<code>25:357:external_repos/bagel/app.py</code>]</p> <p><code>InterleaveInferencer</code> encapsulates the streaming generation algorithm: it grows <code>NaiveCache</code> instances as you interleave prompts/images, clones contexts for classifier-free guidance, and alternates between textual \u201cthinking\u201d chains and latent diffusion steps.</p> <p>```22:284:external_repos/bagel/inferencer.py class InterleaveInferencer:     def init_gen_context(self):         return {'kv_lens': [0], 'ropes': [0], 'past_key_values': NaiveCache(...)}</p> <pre><code>def update_context_text(...):\n    generation_input, kv_lens, ropes = self.model.prepare_prompts(...)\n    past_key_values = self.model.forward_cache_update_text(past_key_values, **generation_input)\n\ndef update_context_image(...):\n    if vae:\n        generation_input = self.model.prepare_vae_images(...)\n        past_key_values = self.model.forward_cache_update_vae(self.vae_model, past_key_values, **generation_input)\n    if vit:\n        generation_input = self.model.prepare_vit_images(...)\n        past_key_values = self.model.forward_cache_update_vit(past_key_values, **generation_input)\n\ndef gen_image(...):\n    generation_input = self.model.prepare_vae_latent(...)\n    generation_input_cfg_text = self.model.prepare_vae_latent_cfg(...)\n    unpacked_latent = self.model.generate_image(..., cfg_text_scale=cfg_text_scale, cfg_img_scale=cfg_img_scale, ...)\n    return self.decode_image(unpacked_latent[0], image_shape)\n</code></pre> <p>```</p> <p>Understanding vs. generation differ only in whether you keep emitting text (<code>understanding_output=True</code>) or call <code>gen_image</code> with CFG contexts cloned before the last prompt.^[<code>207:314:external_repos/bagel/inferencer.py</code>]</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#packed-qwen2-navit-layers-modelingbagelqwen2_navitpy","title":"Packed Qwen2-NaViT Layers (<code>modeling/bagel/qwen2_navit.py</code>)","text":"<p><code>PackedAttention</code> and <code>PackedAttentionMoT</code> extend Hugging Face\u2019s Qwen2 attention with flash-attention varlen kernels, optional flex-attention (for packed sparse masks), and modality-aware expert routing. <code>NaiveCache</code> stores per-layer KV tensors so inference can stream text/image blocks without re-encoding past context.^[<code>207:379:external_repos/bagel/modeling/bagel/qwen2_navit.py</code>]</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Dataset alignment. <code>PackedDataset</code> already surfaces conditional dropout flags and CE-weight scalars; reuse them when aligning neuro-omics modality mixes (e.g., drop imaging tokens to train text-only adapters without rewriting loss code).</li> <li>Modality toggles. Training arguments <code>visual_gen</code>/<code>visual_und</code> plus freeze switches make it easy to run ablations (e.g., ViT-only understanding on KB datasets) while reusing the same packed loader.^[<code>212:405:external_repos/bagel/train/pretrain_unified_navit.py</code>]</li> <li>CFG introspection. The inferencer\u2019s CFG contexts are plain dicts holding cloned caches (<code>cfg_text_precontext</code>, <code>cfg_img_precontext</code>), which means you can intercept them to log per-modality contributions or plug your own KB-guided conditioning signals.^[<code>120:172:external_repos/bagel/inferencer.py</code>]</li> </ul>"},{"location":"code_walkthroughs/brainharmony_walkthrough/","title":"BrainHarmony Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 sMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#overview","title":"Overview","text":"<p>BrainHarmony (a.k.a. BrainHarmonix) is a three-stage pipeline that first extracts modality-specific embeddings from fMRI ROI time-series and structural T1 volumes, then performs JEPA-style token-space pretraining, and finally fine-tunes classification heads on downstream cohorts (e.g., ABIDE). Stage\u202f0 runs fused encoders with anatomical+functional positional priors, Stage\u202f1 trains a latent-token predictor with smooth L1 loss between student and EMA targets, and Stage\u202f2 attaches lightweight heads for clinical prediction.^[<code>1:94:external_repos/brainharmony/README.md</code>][<code>32:138:external_repos/brainharmony/configs/harmonizer/stage0_embed/conf_embed_pretrain.py</code>]</p>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Dual FlexVisionTransformer encoders (fMRI + 3D T1) feeding a JEPA predictor with latent tokens, FlashAttention-2 blocks, and mask-conditioned regressors.^[<code>482:610:external_repos/brainharmony/libs/flex_transformer.py</code>][<code>22:260:external_repos/brainharmony/modules/harmonizer/stage1_pretrain/models.py</code>][<code>23:112:external_repos/brainharmony/libs/ssl_models/jepa_flex.py</code>] Default \u201cbase\u201d uses 768-d embeddings, 12 encoder blocks, 6-layer predictor, mask ratio\u202f0.75, and 128 latent tokens (configurable via scripts).^[<code>48:138:external_repos/brainharmony/configs/harmonizer/stage0_embed/conf_embed_pretrain.py</code>][<code>38:49:external_repos/brainharmony/scripts/harmonizer/stage1_pretrain/run_pretrain.sh</code>] 400 cortical ROIs \u00d7\u202f490 TRs are chunked into 18 patches (48-step windows) plus optional 50 subcortical channels; structural MRI cubes are normalized to 160\u202f\u00d7\u202f192\u202f\u00d7\u202f160 voxels.^[<code>317:465:external_repos/brainharmony/datasets/datasets.py</code>][<code>499:561:external_repos/brainharmony/datasets/datasets.py</code>] Stage\u202f0 ingests <code>(fMRI, T1)</code> pairs via <code>UKB_FusionDataset</code>, Stage\u202f1/2 read <code>.npz</code> embeddings with attention masks/labels using <code>GenerateEmbedDataset(_downstream)</code>.^[<code>566:581:external_repos/brainharmony/datasets/datasets.py</code>][<code>803:857:external_repos/brainharmony/datasets/datasets.py</code>] Provided scripts wrap embedding extraction (Accelerate), JEPA pretraining, and downstream finetuning for reproducibility.^[<code>1:23:external_repos/brainharmony/scripts/harmonizer/stage0_embed/run_embed_pretrain.sh</code>][<code>1:49:external_repos/brainharmony/scripts/harmonizer/stage1_pretrain/run_pretrain.sh</code>][<code>1:59:external_repos/brainharmony/scripts/harmonizer/stage2_finetune/run_finetune.sh</code>] Repo checkout: <code>external_repos/brainharmony</code>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda + pip workflow. Create <code>brainharmonix</code> (Python\u202f3.10), install CUDA\u202f12.4 wheels for PyTorch\u202f2.6, then <code>pip install -r requirements.txt</code> and <code>pip install -e .</code>.^[<code>40:56:external_repos/brainharmony/README.md</code>]</li> <li>Checkpoint placement. Download pretrained encoders (harmonix-f/s) plus harmonizer checkpoints and drop them under <code>checkpoints/{harmonix-f,harmonix-s,harmonizer}</code> before running Stage\u202f0/1/2.^[<code>58:71:external_repos/brainharmony/README.md</code>]</li> <li>FlashAttention 2 expectation. <code>FlexVisionTransformer</code> selects FlashAttention\u202f2 when installed (see <code>is_flash_attn_2_available</code>) so ensure compatible GPU builds or fall back to \u201ceager\u201d attention.^[<code>8:52:external_repos/brainharmony/libs/attn_utils/fa2_utils.py</code>][<code>138:214:external_repos/brainharmony/libs/flex_transformer.py</code>]</li> </ul>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/brainharmony_walkthrough/#stage-0-embedding-extraction-modulesharmonizerstage0_embed","title":"Stage\u202f0: Embedding Extraction (<code>modules/harmonizer/stage0_embed</code>)","text":"<p>Accelerate launches (<code>run_embed_pretrain.sh</code>) call <code>embedding_pretrain.py</code>, which loads configurable datasets (default <code>UKB_FusionDataset</code>) and wraps pretrained fMRI/T1 encoders specified in <code>conf_embed_pretrain.py</code>. The fmri encoder receives gradient+geometric-harmonic positional embeddings, while the MAE-style T1 encoder reuses volumetric patches. Each batch returns fmri tokens, T1 tokens, attention masks, and subject IDs; Stage\u202f0 runs both encoders, concatenates their representations, and persists <code>.npz</code> files along with the attention mask for later stages.^[<code>1:23:external_repos/brainharmony/scripts/harmonizer/stage0_embed/run_embed_pretrain.sh</code>][<code>48:138:external_repos/brainharmony/configs/harmonizer/stage0_embed/conf_embed_pretrain.py</code>][<code>81:185:external_repos/brainharmony/modules/harmonizer/stage0_embed/embedding_pretrain.py</code>]</p> <p>```566:581:external_repos/brainharmony/datasets/datasets.py class UKB_FusionDataset(UKBDataset, UKB_T1_Dataset):     def init(self, kwargs):         UKBDataset.init(self, kwargs)         UKB_T1_Dataset.init(self, **kwargs)</p> <pre><code>def __getitem__(self, index):\n    ts, _ = self.load_fmri(index)\n    attn_mask = self.signal_attn_mask()\n    t1 = self.load_t1(index)\n    return ts, t1, self.patch_size, attn_mask, self.ids[index]\n</code></pre> <p>``` </p>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#stage-1-token-space-jepa-pretraining-modulesharmonizerstage1_pretrain","title":"Stage\u202f1: Token-Space JEPA Pretraining (<code>modules/harmonizer/stage1_pretrain</code>)","text":"<p>The generated <code>.npz</code> files are streamed by <code>GenerateEmbedDataset</code>, which yields concatenated embeddings and their attention masks; distributed loaders feed <code>OneTokRegViT</code>, a latent-token ViT that appends learnable latent vectors and mask tokens before passing through decoder blocks. <code>train_one_epoch</code> applies Smooth\u202fL1 loss between the predictor output and EMA targets from the frozen teacher encoder, mirroring the JEPA objective.^[<code>803:825:external_repos/brainharmony/datasets/datasets.py</code>][<code>22:260:external_repos/brainharmony/modules/harmonizer/stage1_pretrain/models.py</code>][<code>13:84:external_repos/brainharmony/modules/harmonizer/stage1_pretrain/engine_pretrain.py</code>][<code>38:49:external_repos/brainharmony/scripts/harmonizer/stage1_pretrain/run_pretrain.sh</code>]  <code>803:825:external_repos/brainharmony/datasets/datasets.py class GenerateEmbedDataset(Dataset):     def __init__(self, root_dir, portion=1.0, seed=42):         pattern = os.path.join(root_dir, \"*.npz\")         all_files = sorted(glob.glob(pattern))         self.files = all_files         if len(all_files) == 0:             raise RuntimeError(f\"No .npz files found in {root_dir}\")      def __len__(self):         return len(self.files)      def __getitem__(self, idx):         filepath = self.files[idx]         arr = np.load(filepath)         tensor = torch.from_numpy(arr[\"data\"])         if tensor.dtype != torch.float32:             tensor = tensor.float()         return tensor.squeeze(), arr[\"attn_mask\"].squeeze()</code></p> <p>```180:223:external_repos/brainharmony/modules/harmonizer/stage1_pretrain/models.py     def forward_encoder(self, x, attn_mask):         target = x         if self.add_pre_mapping:             x = self.pre_map(x)         x = x + self.pos_embed[:, 1:, :]         cls_token = self.cls_token + self.pos_embed[:, :1, :]         cls_tokens = cls_token.expand(x.shape[0], -1, -1)         x = torch.cat((cls_tokens, x), dim=1)         latent_tokens = self.latent_tokens.expand(x.shape[0], -1, -1)         latent_tokens = latent_tokens + self.enc_latent_token_positional_embedding         x = torch.cat([x, latent_tokens], dim=1)         pad = torch.ones(attn_mask.shape[0], 1200 + latent_tokens.shape[1], dtype=attn_mask.dtype, device=attn_mask.device)         pad_0 = torch.ones(attn_mask.shape[0], 1, dtype=attn_mask.dtype, device=attn_mask.device)         attn_mask = torch.cat([pad_0, attn_mask, pad], dim=1)         for blk in self.blocks:             x = blk(x, attention_mask=attn_mask)         x = self.norm(x)         latent_tokens = torch.cat([x[:, :1, :], x[:, -self.num_latent_tokens :]], dim=1)         return latent_tokens, target <pre><code>### Stage\u202f2: Downstream Harmonizer Heads (`modules/harmonizer/stage2_finetune`)\nFor tasks like ABIDE diagnosis, `GenerateEmbedDataset_downstream` reads the saved embeddings plus labels, and `VisionTransformer` attaches either a CLS-token head or global pooler atop the latent-token expanded sequence. Training mixes standard augmentation knobs (mixup/cutmix) with layer-wise LR decay, and evaluation logs accuracy + F1.^[```828:857:external_repos/brainharmony/datasets/datasets.py```][```1:350:external_repos/brainharmony/modules/harmonizer/stage2_finetune/main_finetune.py```][```16:166:external_repos/brainharmony/modules/harmonizer/stage2_finetune/engine_finetune.py```][```1:59:external_repos/brainharmony/scripts/harmonizer/stage2_finetune/run_finetune.sh```]\n\n```117:170:external_repos/brainharmony/modules/harmonizer/stage2_finetune/models.py\n    def forward_features(self, x, attn_mask):\n        if self.add_pre_mapping:\n            x = self.pre_map(x)\n        B = x.shape[0]\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        latent_tokens = self.latent_tokens.expand(x.shape[0], -1, -1)\n        latent_tokens = latent_tokens + self.enc_latent_token_positional_embedding\n        x = torch.cat([x, latent_tokens], dim=1)\n        pad = torch.ones(attn_mask.shape[0], 1200 + latent_tokens.shape[1], dtype=attn_mask.dtype, device=attn_mask.device)\n        pad_0 = torch.ones(attn_mask.shape[0], 1, dtype=attn_mask.dtype, device=attn_mask.device)\n        attn_mask = torch.cat([pad_0, attn_mask, pad], dim=1)\n        for blk in self.blocks:\n            x = blk(x, attention_mask=attn_mask)\n        if self.global_pool:\n            x = torch.cat([x[:, :1, :], x[:, -self.num_latent_tokens :]], dim=1)\n            x = x[:, 1:, :].mean(dim=1)\n            outcome = self.fc_norm(x)\n        else:\n            x = self.norm(x)\n            x = torch.cat([x[:, :1, :], x[:, -self.num_latent_tokens :]], dim=1)\n            outcome = x[:, 0]\n        return outcome\n</code></pre></p>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#flexvisiontransformer-masked-predictor-libsflex_transformerpy","title":"FlexVisionTransformer &amp; Masked Predictor (<code>libs/flex_transformer.py</code>)","text":"<p><code>FlexVisionTransformer</code> supports flexible patch sizes via <code>FlexiPatchEmbed</code>, optional gradient checkpointing, and either FlashAttention\u202f2 or eager attention blocks. Predictor heads (<code>VisionTransformerPredictor</code>) project encoder outputs into predictor space, tile positional embeddings for masked regions, append learnable mask tokens, and regress back to encoder dimensionality; they reuse <code>apply_masks</code> to select context/target indices.^[<code>482:610:external_repos/brainharmony/libs/flex_transformer.py</code>][<code>322:463:external_repos/brainharmony/libs/flex_transformer.py</code>]</p> <p>```403:463:external_repos/brainharmony/libs/flex_transformer.py     def forward(self, x, masks_x, masks, attention_masks=None, return_attention=False):         assert (masks is not None) and (masks_x is not None)         if not isinstance(masks_x, list):             masks_x = [masks_x]         if not isinstance(masks, list):             masks = [masks]         B = len(x) // len(masks_x)         x = self.predictor_embed(x)         predictor_pos_embed = self.predictor_pos_embed()[1]         if self.cls_token is not None:             x_pos_embed = predictor_pos_embed.repeat(B, 1, 1)             x_pos_embed = apply_masks(x_pos_embed, masks_x, cls_token=True)             x += x_pos_embed             , N_ctxt, D = x.shape             pos_embs = predictor_pos_embed.repeat(B, 1, 1)             pos_embs = apply_masks(pos_embs[:, 1:, :], masks)             pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))             pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)             pred_tokens += pos_embs         else:             x_pos_embed = predictor_pos_embed.repeat(B, 1, 1)             x += apply_masks(x_pos_embed, masks_x)             , N_ctxt, D = x.shape             pos_embs = predictor_pos_embed.repeat(B, 1, 1)             pos_embs = apply_masks(pos_embs, masks)             pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))             pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)             pred_tokens += pos_embs         x = x.repeat(len(masks), 1, 1)         x = torch.cat([x, pred_tokens], dim=1)         for blk in self.predictor_blocks:             x = blk(x, attention_masks)         x = self.predictor_norm(x)         x = x[:, N_ctxt:]         x = self.predictor_proj(x)         return x <pre><code>### Positional Embeddings &amp; Mask Utilities\nBrainHarmony blends anatomical gradients with geometric harmonics to produce shared positional priors across encoder/predictor stacks, and the same module can emit decoder embeddings for Stage\u202f1. Mask helpers expose gather-style APIs for re-indexing context/target tokens.^[```137:209:external_repos/brainharmony/libs/position_embedding.py```][```11:31:external_repos/brainharmony/libs/masks/utils.py```]\n\n```167:209:external_repos/brainharmony/libs/position_embedding.py\n        geo_harm_pos_embed = self.geo_harm_proj(self.geo_harm)\n        gradient_pos_embed = self.grad_proj(self.gradient)\n        pos_embed = (gradient_pos_embed + geo_harm_pos_embed) * 0.5\n        emb_w = pos_embed.squeeze().repeat_interleave(self.repeat_time, dim=0)\n        emb_w = (emb_w - emb_w.min()) / (emb_w.max() - emb_w.min()) * 2 - 1\n        emb_encoder = torch.cat([self.emb_h_encoder, emb_w], dim=1).unsqueeze(0)\n        if self.cls_token:\n            pos_embed_encoder = torch.concat(\n                [torch.zeros([1, 1, emb_encoder.shape[2]], requires_grad=False).to(self.device), emb_encoder],\n                dim=1,\n            )\n        else:\n            pos_embed_encoder = emb_encoder\n        if self.use_pos_embed_decoder:\n            emb_w_decoder = self.decoder_pos_embed_proj(emb_w.detach())\n            emb_decoder = torch.cat([self.emb_h_decoder, emb_w_decoder], dim=1).unsqueeze(0)\n            if self.cls_token:\n                pos_embed_decoder = torch.concat(\n                    [torch.zeros([1, 1, emb_decoder.shape[2]], requires_grad=False).to(self.device), emb_decoder],\n                    dim=1,\n                )\n            else:\n                pos_embed_decoder = emb_decoder\n            return pos_embed_encoder, pos_embed_decoder\n        return pos_embed_encoder, None\n</code></pre></p> <p>```11:31:external_repos/brainharmony/libs/masks/utils.py def apply_masks(x, masks, cls_token=False):     all_x = []     if cls_token:         cls_t = x[:, :1, :]         for m in masks:             mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))             all_x += [torch.cat((cls_t, torch.gather(x[:, 1:, :], dim=1, index=mask_keep)), dim=1)]     else:         for m in masks:             mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))             all_x += [torch.gather(x, dim=1, index=mask_keep)]     return torch.cat(all_x, dim=0) <pre><code>## Integration Hooks (Brain \u2194 Genetics)\n- **Token shapes.** `FlexVisionTransformer.forward` outputs `[B, N_tokens, embed_dim]` (CLS + patches [+ latent tokens]); Stage\u202f2 heads either take the CLS vector or mean-pool latent tokens, so downstream genetics encoders should expect 768-d (base) or 1024-d (large) vectors per sample.^[```563:610:external_repos/brainharmony/libs/flex_transformer.py```][```117:170:external_repos/brainharmony/modules/harmonizer/stage2_finetune/models.py```]\n- **Attention masks.** Both `GenerateEmbedDataset` variants surface per-sample masks; reusing them when aligning with long genomic sequences preserves which ROI/time windows were padded vs. observed.^[```803:857:external_repos/brainharmony/datasets/datasets.py```]\n- **Stage bridging.** Stage\u202f0 writes `.npz` files with `data` and `attn_mask` arrays; you can append additional modalities (e.g., gene-expression embeddings) into the same `data` vector before Stage\u202f1 as long as the downstream models\u2019 positional encoders are updated accordingly.^[```138:184:external_repos/brainharmony/modules/harmonizer/stage0_embed/embedding_pretrain.py```]\n- **Projecting to shared latent spaces.** A lightweight projector keeps BrainHarmony tokens compatible with genetics embeddings:\n\n```python\nimport torch.nn as nn\n\nclass BrainHarmonyProjector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre></p> <p>Map mean-pooled latent tokens through this projector, concatenate with genetics embeddings, and fine-tune a fusion head while reusing BrainHarmony\u2019s attention masks for masking-aware losses.</p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/","title":"Brain-JEPA Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#overview","title":"Overview","text":"<p>Brain-JEPA extends Image/Joint-Embedding Predictive Architecture ideas to 4D fMRI tensors: a Vision Transformer encoder ingests masked spatiotemporal blocks, a predictor Transformer reconstructs masked targets with gradient-informed positional encodings, and masking policies operate jointly across space and time.^[<code>1:200:external_repos/brainjepa/src/models/vision_transformer.py</code>][<code>18:282:external_repos/brainjepa/src/masks/spatialtemporal_multiblock.py</code>]</p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo 4D Swin/ViT encoder + predictor head with gradient positional embeddings^[<code>22:400:external_repos/brainjepa/src/models/vision_transformer.py</code>] Configurable (base uses ViT-Base + predictor depth 12)^[<code>400:565:external_repos/brainjepa/src/models/vision_transformer.py</code>] 450 ROIs \u00d7 160 time frames (default)^[<code>19:210:external_repos/brainjepa/src/masks/spatialtemporal_multiblock.py</code>] Preprocessed fMRI tensors from <code>fMRIDataset</code> (UKB/S1200)^[<code>14:205:external_repos/brainjepa/src/datasets/ukbiobank_scale.py</code>] Spatiotemporal JEPA pretraining, downstream fine-tuning &amp; linear probing scripts^[<code>67:360:external_repos/brainjepa/src/train.py</code>][<code>15:94:external_repos/brainjepa/downstream_eval.py</code>] github.com/janklees/brainjepa"},{"location":"code_walkthroughs/brainjepa_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda + pip install. Long-context masking requires the project\u2019s Python\u202f3.8 environment: <code>conda create -n brain-jepa python=3.8</code> followed by <code>pip install -r requirement.txt</code>.^[<code>80:82:external_repos/brainjepa/README.md</code>]</li> <li>Hardware guidance. Official docs note that pretraining ran on four A100\u202f(40\u202fGB) GPUs and provide the multi-GPU launch command <code>python main.py --fname configs/ukb_vitb_ep300.yaml --devices cuda:0 cuda:1 cuda:2 cuda:3</code>.^[<code>84:92:external_repos/brainjepa/README.md</code>]</li> <li>Gradient checkpoint flag. The encoder exposes a <code>gradient_checkpointing</code> argument and wraps each block with <code>torch.utils.checkpoint.checkpoint(...)</code> whenever the flag is set, so you can trade compute for memory on large ROI \u00d7 time grids.^[<code>422:504:external_repos/brainjepa/src/models/vision_transformer.py</code>]</li> </ul>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/brainjepa_walkthrough/#dataset-preprocessing-srcdatasetsukbiobank_scalepy","title":"Dataset &amp; Preprocessing (<code>src/datasets/ukbiobank_scale.py</code>)","text":"<p>The dataset class loads ROI-wise time series, applies robust scaling, optional downsampling, and returns tensors shaped <code>[channels, depth, height, width, time]</code> wrapped in dicts (<code>{'fmri': tensor}</code>) for the mask collator.</p> <p>```14:186:external_repos/brainjepa/src/datasets/ukbiobank_scale.py class fMRIDataset(Dataset):     def getitem(self, idx):         ts_cortical = self._load_ts(id, self.cortical_file)         ts_subcortical = self._load_ts(id, self.subcortical_file)         ts_array = np.concatenate((ts_subcortical, ts_cortical), axis=0).astype(np.float32)         if self.downsample:             ts_array = self._temporal_sampling(...)         ts = torch.unsqueeze(torch.from_numpy(ts_array), 0).to(torch.float32)         return {'fmri': ts} <pre><code>### Mask Collator (`src/masks/spatialtemporal_multiblock.py`)\n`MaskCollator_fmri` samples encoder/predictor windows over ROIs \u00d7 time, enforcing non-overlapping context/target regions and returning boolean masks for each batch.\n\n```18:282:external_repos/brainjepa/src/masks/spatialtemporal_multiblock.py\nclass MaskCollator_fmri(object):\n    def __call__(self, batch):\n        mask_e, _ = self._sample_block_mask_e(e_size)\n        masks_p.append(self._sample_block_mask_p_roi(p_size_roi)[0])\n        mask, mask_C = self._sample_block_mask_p_ts(...)\n        mask_e = self.constrain_e_mask(mask_e, acceptable_regions=masks_C)\n        collated_masks_pred.append([mask_p_final])\n        collated_masks_enc.append([mask_e])\n        return collated_batch, collated_masks_enc, collated_masks_pred\n</code></pre></p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#positional-embeddings-encoder-srcmodelsvision_transformerpy","title":"Positional Embeddings &amp; Encoder (<code>src/models/vision_transformer.py</code>)","text":"<p>Gradient-informed positional encoding (<code>GradTs_2dPE</code>) injects atlas gradients, while the encoder (<code>VisionTransformer</code>) patchifies <code>[B, C, D, H, W, T]</code> tensors, adds position encodings, and runs stacked Swin-like blocks.</p> <p>```22:100:external_repos/brainjepa/src/models/vision_transformer.py class GradTs_2dPE(nn.Module):     def init(...):         self.emb_h = nn.Parameter(...)         self.emb_w = ... if add_w == 'origin' else predictor_pos_embed_proj(gradient) <pre><code>```430:514:external_repos/brainjepa/src/models/vision_transformer.py\nx = self.patch_embed(x)\npos_embed = self.pos_embed_proj(self.gradient_pos_embed)\nx = x + pos_embed\nif masks is not None:\n    x = apply_masks(x, masks)\nfor blk in self.blocks:\n    x = blk(x)\n</code></pre></p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#predictor-head-srcmodelsvision_transformerpy","title":"Predictor Head (<code>src/models/vision_transformer.py</code>)","text":"<p>The predictor maps context tokens to a lower-dimensional space, concatenates learnable mask tokens (with their own positional embeddings), and runs Transformer blocks to regress target embeddings.</p> <p>```280:396:external_repos/brainjepa/src/models/vision_transformer.py class VisionTransformerPredictor(nn.Module):     x = self.predictor_embed(x)     predictor_pos_embed = self.predictor_2dpe_proj(self.gradient_pos_embed)     pos_embs = apply_masks(predictor_pos_embed.repeat(B, 1, 1), masks)     pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)     x = torch.cat([x, pred_tokens + pos_embs], dim=1)     for blk in self.predictor_blocks:         x = blk(x)     x = self.predictor_proj(x[:, N_ctxt:]) <pre><code>### Training Loop (`src/train.py`)\nThe training script builds data loaders, mask collators, encoder/predictor pairs, and optimizers; the loss is Smooth L1 between predictor outputs and target encoder features.\n\n```215:360:external_repos/brainjepa/src/train.py\ndef train_step():\n    def forward_target():\n        with torch.no_grad():\n            h = target_encoder(imgs)\n            h = F.layer_norm(h, (h.size(-1),))\n            h = apply_masks(h, masks_pred)\n            h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n    def forward_context():\n        z = encoder(imgs, masks_enc, return_attention=False)\n        z = predictor(z, masks_enc, masks_pred, return_attention=False)\n    def loss_fn(z, h):\n        return F.smooth_l1_loss(z, h)\n</code></pre></p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#downstream-evaluation-downstream_tasksmodels_vitpy","title":"Downstream Evaluation (<code>downstream_tasks/models_vit.py</code>)","text":"<p>Linear-probe and fine-tuning scripts load pretrained encoders, optionally apply gradient checkpointing, and return global-pooled embeddings for classification/regression heads.</p> <p><code>15:74:external_repos/brainjepa/downstream_tasks/models_vit.py self.encoder, _ = init_model(...) if self.global_pool:     outcome = self.fc_norm(self.encoder(x)[:, :, :].mean(dim=1)) else:     outcome = self.encoder(x)[:, 0] x = self.head(outcome) <pre><code>## Integration Hooks (Brain \u2194 Genetics)\n\n- **Embedding shape.** Encoder outputs `[B, N_tokens, embed_dim]` (after flattening 4D patches). Downstream heads either take the CLS token or mean pool spatial tokens; JEPA predictors output only masked-token predictions shaped `[num_masks, embed_dim]`.^[```280:396:external_repos/brainjepa/src/models/vision_transformer.py```]\n- **Pooling choices.** For multimodal fusion, use the downstream `VisionTransformer` global pool (`mean(dim=1)`) or compute mean pooling across context tokens to mirror predictor inputs.\n- **Projection to shared latent.** Map `[B, embed_dim]` vectors (384/768 for small/base) into 512-D shared space via a lightweight projector:\n\n```python\nimport torch.nn as nn\n\nclass BrainJEPAProjector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> - **Mask-aware embeddings.** When extracting representations for multimodal tasks, disable masking (feed identity masks) or average multiple masked views to reduce noise; the same mask collator can generate augmented views for contrastive objectives. - **Gradient positional alignment.** Because `GradTs_2dPE` injects atlas gradients, keep those embeddings when aligning with genetics\u2014do not strip them off\u2014so the spatial axes remain consistent across modalities.^[</code>22:100:external_repos/brainjepa/src/models/vision_transformer.py```]</p> <p>Following these hooks yields <code>[B, 512]</code> Brain-JEPA embeddings compatible with projected DNA embeddings (Evo\u202f2, GENERator, Caduceus) for multimodal representation learning.</p>"},{"location":"code_walkthroughs/brainlm_walkthrough/","title":"BrainLM Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/brainlm_walkthrough/#overview","title":"Overview","text":"<p>BrainLM is a ViT-MAE\u2013style masked autoencoder: it slices each voxel\u2019s time course into short windows, randomly masks most of them, and reconstructs the missing segments with Nystromformer encoder layers and a lightweight decoder trained on UK Biobank fMRI.^[<code>1:48:external_repos/brainlm/README.md</code>][<code>63:205:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>]</p>"},{"location":"code_walkthroughs/brainlm_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo ViT-MAE encoder (Nystromformer) + MAE decoder^[<code>227:515:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>] 111\u202fM / 650\u202fM checkpoints^[<code>39:48:external_repos/brainlm/README.md</code>] 424 parcels \u00d7 490 timepoints patched into windows^[<code>63:200:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>] Arrow datasets of <code>[B, voxels, time]</code> tensors + XYZ coordinates^[<code>43:205:external_repos/brainlm/train.py</code>] Masked reconstruction, downstream probes/fine-tuning via <code>BrainLMTrainer</code>^[<code>351:470:external_repos/brainlm/train.py</code>] github.com/vandijklab/BrainLM"},{"location":"code_walkthroughs/brainlm_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Setup script. The README directs users to run <code>sh setup.sh</code> to create the conda environment (with FlashAttention for the 111\u202fM/650\u202fM checkpoints) and verify PyTorch/HuggingFace installs via the provided sanity commands.^[<code>16:26:external_repos/brainlm/README.md</code>][<code>50:52:external_repos/brainlm/README.md</code>]</li> <li>Gradient checkpointing toggle. Both the encoder and decoder wrap their Nystromformer layers with <code>if self.gradient_checkpointing and self.training: torch.utils.checkpoint.checkpoint(...)</code>, so you can enable <code>model.gradient_checkpointing_enable()</code> before large runs to keep memory in check.^[<code>245:269:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>][<code>453:500:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>]</li> </ul>"},{"location":"code_walkthroughs/brainlm_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/brainlm_walkthrough/#data-collation-brainlmtrainpy","title":"Data &amp; Collation (<code>brainlm/train.py</code>)","text":"<p>Hydra dataclasses declare dataset paths, voxel/time dimensions, and labels; <code>collate_fn</code> stacks tensors into the format expected by the MAE model.</p> <p>```43:210:external_repos/brainlm/train.py @dataclass class DataTrainingArguments:     train_dataset_path: str     val_dataset_path: str     coords_dataset_path: str     num_timepoints_per_voxel: int = 490     timepoint_patching_size: int = 49 ... def collate_fn(examples):     signal_vectors = torch.stack([example[\"signal_vectors\"] for example in examples], dim=0)     xyz_vectors = torch.stack([example[\"xyz_vectors\"] for example in examples])     labels = torch.stack([example[\"label\"] for example in examples])     return {\"signal_vectors\": signal_vectors, \"xyz_vectors\": xyz_vectors, \"input_ids\": signal_vectors, \"labels\": labels} <pre><code>### Embeddings &amp; Masking (`brainlm_mae/modeling_brainlm.py`)\n`BrainLMEmbeddings` reshapes time signals into patches, projects signals and spatial coordinates, injects positional encoding, and randomly masks patches before appending a CLS token.\n\n```63:160:external_repos/brainlm/brainlm_mae/modeling_brainlm.py\nreshaped_signal_vectors = torch.reshape(signal_vectors, (batch, num_voxels, -1, self.timepoint_patching_size))\nsignal_projection = self.signal_embedding_projection(reshaped_signal_vectors)\nxyz_projection = self.xyz_embedding_projection(xyz_vectors).unsqueeze(2).repeat(1, 1, num_patch_tokens, 1)\nx = self.pos_embedding(signal_projection + xyz_projection)\nembeddings, mask, ids_restore = self.random_masking(x, noise=noise)\ncls_tokens = self.cls_token.expand(embeddings.shape[0], -1, -1)\nembeddings = torch.cat((cls_tokens, embeddings), dim=1)\n</code></pre></p>"},{"location":"code_walkthroughs/brainlm_walkthrough/#encoder-decoder-brainlm_maemodeling_brainlmpy","title":"Encoder &amp; Decoder (<code>brainlm_mae/modeling_brainlm.py</code>)","text":"<p>The encoder stacks Nystromformer layers, while the decoder reintroduces mask tokens, adds spatial/time encodings again, and predicts the missing time windows.</p> <p>```227:340:external_repos/brainlm/brainlm_mae/modeling_brainlm.py class BrainLMModel(ViTMAEModel):     self.embeddings = BrainLMEmbeddings(config)     self.encoder = BrainLMEncoder(config)     encoder_outputs = self.encoder(embedding_output, ...)     sequence_output = self.layernorm(encoder_outputs[0]) <pre><code>```355:515:external_repos/brainlm/brainlm_mae/modeling_brainlm.py\nmask_tokens = self.mask_token.repeat(batch_size, num_mask_tokens, 1)\nx_ = torch.reshape(x_, (batch_size, self.num_brain_voxels, num_patch_tokens, hidden_dim))\nx_ = x_ + self.decoder_xyz_projection(xyz_vectors)\nx_ = self.pos_embedding(x_)\nlogits = self.decoder_pred2(self.decoder_pred_nonlinearity(self.decoder_pred1(hidden_states)))\nlogits = torch.reshape(logits, (batch_size, self.num_brain_voxels, ..., self.timepoint_patching_size))\n</code></pre></p>"},{"location":"code_walkthroughs/brainlm_walkthrough/#loss-brainlm_maemodeling_brainlmpy","title":"Loss (<code>brainlm_mae/modeling_brainlm.py</code>)","text":"<p>Masked reconstruction loss is computed only on the masked tokens (MSE or MAE).</p> <p>```562:584:external_repos/brainlm/brainlm_mae/modeling_brainlm.py mask = mask.unsqueeze(-1).repeat(1, 1, 1, pred_values.shape[-1]) if self.config.loss_fn == \"mse\":     loss = (((pred_values - signal_values) ** 2) * mask).sum() / mask.sum() elif self.config.loss_fn == \"mae\":     loss = abs((pred_values - signal_values) * mask).sum() / mask.sum() <pre><code>### Training Driver (`brainlm/train.py`)\n`BrainLMTrainer` glues everything together\u2014optimizer, scheduler, metrics, evaluation.\n\n```351:470:external_repos/brainlm/train.py\ntrainer = BrainLMTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=None,\n    data_collator=collate_fn,\n    compute_metrics=metrics.compute_metrics,\n)\ntrainer.train()\n</code></pre></p>"},{"location":"code_walkthroughs/brainlm_walkthrough/#integration-hooks-brain-genetics","title":"Integration Hooks (Brain \u2194 Genetics)","text":"<ul> <li>Embedding shape. Encoder outputs <code>[B, (num_voxels * kept_tokens) + 1, hidden]</code>. Index\u202f0 is CLS; the rest represent unmasked voxel windows sorted deterministically.^[<code>329:350:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>]</li> <li>Pooling choices. CLS pooling mirrors the MAE training objective; mean pooling across tokens smooths noise; reshaping tokens back to <code>[B, voxels, windows, hidden]</code> lets you average over time first, then voxels.</li> <li>Projection to shared latent. Map pooled <code>[B, hidden]</code> vectors (hidden\u2248768 on the 111\u202fM model) into a 512-D shared space:</li> </ul> <pre><code>import torch.nn as nn\n\nclass BrainLMProjector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <ul> <li>Masking control. When extracting embeddings, set <code>mask_ratio=0.0</code> so every patch contributes; enable masking only for pretraining/augmentation.</li> <li>Alignment with genetics. After projection, normalize (LayerNorm or z-score) before concatenating with genetic embeddings (Evo\u202f2, GENERator, Caduceus) or using contrastive loss.</li> </ul> <p>This workflow delivers <code>[B, 512]</code> fMRI embeddings that align with projected DNA representations for multimodal analyses.</p>"},{"location":"code_walkthroughs/brainmt_walkthrough/","title":"BrainMT Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/brainmt_walkthrough/#overview","title":"Overview","text":"<p>BrainMT pairs bidirectional Mamba mixers (temporal-first scanning) with MHSA transformer blocks to model long-range fMRI dynamics, delivering state-of-the-art regression/classification on UKB and HCP phenotypes.^[<code>3:170:external_repos/brainmt/README.md</code>][<code>294:462:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] The architecture is now described in an official conference paper (SpringerLink, Lecture Notes in Computer Science, pp. 150\u2013160; first online 19 September 2025), so reference the proceedings PDF in <code>docs/generated/kb_curated/papers-pdf/brainmt_2025.pdf</code> when citing.</p>"},{"location":"code_walkthroughs/brainmt_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo 3D Conv patch embed \u2192 bidirectional Mamba blocks \u2192 Transformer attention blocks^[<code>202:462:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] Configurable (default hidden 512, depth <code>[12,8]</code>)^[<code>293:375:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] 91\u00d7109\u00d791 voxels \u00d7 200 frames (default)^[<code>294:339:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] Preprocessed <code>.pt</code> tensors from <code>data/datasets.py</code>^[<code>15:80:external_repos/brainmt/src/brainmt/data/datasets.py</code>] DDP training with regression/classification heads, inference utilities^[<code>1:330:external_repos/brainmt/src/brainmt/train.py</code>][<code>1:390:external_repos/brainmt/src/brainmt/inference.py</code>] github.com/arunkumar-kannan/brainmt-fmri"},{"location":"code_walkthroughs/brainmt_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Exact environment commands. The README targets Python\u202f3.9.18 + PyTorch\u202f2.6/CUDA\u202f12.4, created via <code>python -m venv brainmt_env</code>, <code>source brainmt_env/bin/activate</code>, and <code>pip install -r requirements.txt</code>.^[<code>44:60:external_repos/brainmt/README.md</code>]</li> <li>Gradient checkpoint flag. Every Mamba block accepts <code>use_checkpoint</code> and conditionally calls <code>checkpoint.checkpoint(...)</code>, so you can instantiate <code>BrainMT(..., use_checkpoint=True)</code> to reduce memory usage on long temporal contexts.^[<code>95:125:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>][<code>293:334:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>]</li> </ul>"},{"location":"code_walkthroughs/brainmt_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/brainmt_walkthrough/#dataset-loader-srcbrainmtdatadatasetspy","title":"Dataset Loader (<code>src/brainmt/data/datasets.py</code>)","text":"<p>The dataset stores fMRI volumes as fp16 tensors (<code>func_data_MNI_fp16.pt</code>), slices contiguous time segments, permutes them into <code>[frames, channel, depth, height, width]</code>, and returns <code>(tensor, target)</code> pairs.</p> <p>```15:80:external_repos/brainmt/src/brainmt/data/datasets.py class fMRIDataset(Dataset):     def getitem(self, idx):         data = torch.load(img_file)         start_index = torch.randint(0, total_frames - num_frames + 1, (1,)).item()         data_sliced = data[:, :, :, start_index:end_index]         data_global = data_sliced.unsqueeze(0).permute(4, 0, 2, 1, 3)         target = self.target_dict[subject_dir]         return data_global, torch.tensor(target, dtype=torch.float16) <pre><code>### Patch Embed &amp; Conv Blocks (`src/brainmt/models/brain_mt.py`)\n`PatchEmbed` downsamples the 4D tensor with strided 3D convolutions before two ConvBlocks + Downsample layers reduce spatial resolution while keeping temporal length.\n\n```202:263:external_repos/brainmt/src/brainmt/models/brain_mt.py\nclass PatchEmbed(nn.Module):\n    self.conv_down = nn.Sequential(\n        nn.Conv3d(in_chans, in_dim, 3, 2, 1, bias=False),\n        nn.ReLU(),\n        nn.Conv3d(in_dim, dim, 3, 2, 1, bias=False),\n        nn.ReLU()\n    )\n</code></pre></p>"},{"location":"code_walkthroughs/brainmt_walkthrough/#hybrid-mamba-transformer-backbone-srcbrainmtmodelsbrain_mtpy","title":"Hybrid Mamba + Transformer Backbone (<code>src/brainmt/models/brain_mt.py</code>)","text":"<p>Temporal-first processing reshapes tokens, feeds them through <code>create_block</code> (bidirectional Mamba) and then through transformer attention + MLP to capture residual spatial dependencies.</p> <p>```331:462:external_repos/brainmt/src/brainmt/models/brain_mt.py self.layers = nn.ModuleList([     create_block(embed_dim, ssm_cfg=ssm_cfg, ..., drop_path=inter_dpr[i], ...)     for i in range(depth[0]) ]) self.blocks = nn.ModuleList([     Attention(embed_dim, num_heads=num_heads, ...)     for i in range(depth[1]) ]) ... def forward_features(self, x, ...):     x = self.patch_embed(x)     x = self.conv_block0(x); x = self.downsample0(x)     x = self.conv_block1(x); x = self.downsample1(x)     x = rearrange(x, '(b t) n m -&gt; (b n) t m', b=B, t=T)     x = x + self.temporal_pos_embedding     for layer in self.layers:         hidden_states, residual = layer(hidden_states, residual, ...)     for block in self.blocks:         hidden_states = hidden_states + drop_path_attn(block(self.norm(hidden_states))) <pre><code>### Forward &amp; Head (`src/brainmt/models/brain_mt.py`)\nCLS token is prepended before Mamba blocks; `forward` returns final MLP head output for regression/classification.\n\n```400:461:external_repos/brainmt/src/brainmt/models/brain_mt.py\ncls_token = self.cls_token.expand(x.shape[0], -1, -1)\nx = torch.cat((cls_token, x), dim=1)\n...\nreturn hidden_states[:, 0, :]\n</code></pre></p>"},{"location":"code_walkthroughs/brainmt_walkthrough/#training-loop-srcbrainmttrainpy","title":"Training Loop (<code>src/brainmt/train.py</code>)","text":"<p>Hydra config builds datasets, wraps the model in DDP, selects loss (MSE or BCEWithLogits), constructs layer-wise LR decay groups, and trains with <code>GradScaler</code> + cosine warm restarts.</p> <p>```132:234:external_repos/brainmt/src/brainmt/train.py model = BrainMT(**model_config).to(device) model = nn.parallel.DistributedDataParallel(model, device_ids=[device_id], ...) if cfg.task.loss_fn == \"mse\":     criteria = nn.MSELoss() ... train_loss, train_outputs, train_targets = train_one_epoch(model, criteria, train_loader, optimizer, scaler, device, epoch, cfg) val_loss, val_outputs, val_targets = evaluate(model, criteria, val_loader, device) <pre><code>### Inference (`src/brainmt/inference.py`)\nThe inference script mirrors dataset splits, loads checkpoints, and computes detailed metrics (accuracy/AUROC for classification, MSE/MAE/R\u00b2/Pearson for regression), plus optional plots.\n\n```26:210:external_repos/brainmt/src/brainmt/inference.py\nmodel = BrainMT(**model_config).to(device)\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nif cfg.task.name == 'classification':\n    metrics = calculate_classification_metrics(test_outputs, test_targets)\nelse:\n    metrics = calculate_regression_metrics(test_outputs, test_targets)\n</code></pre></p>"},{"location":"code_walkthroughs/brainmt_walkthrough/#integration-hooks-brain-genetics","title":"Integration Hooks (Brain \u2194 Genetics)","text":"<ul> <li>Embedding shape. <code>forward_features</code> returns <code>[B, hidden]</code> CLS vectors (hidden default 512). To access intermediate token embeddings, tap <code>hidden_states[:, 1:, :]</code> before the final average/MLP.^[<code>400:462:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>]</li> <li>Pooling choices. CLS token encodes temporal-first, globally attentive context. For voxel-conditioned embeddings, reshape post-Mamba tensor back to <code>[B, voxels, hidden]</code> prior to the transformer block and average along the voxel axis.</li> <li>Projection to shared latent. Map <code>[B, 512]</code> BrainMT vectors into a 512-D multimodal space with a lightweight projector:</li> </ul> <p><pre><code>import torch.nn as nn\n\nclass BrainMTProjector(nn.Module):\n    def __init__(self, input_dim=512, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> - Normalization. Because BrainMT ends with LayerNorm (<code>self.norm_f</code>), additional LayerNorm in the projector keeps the scale comparable to genetic embeddings. - Temporal handling. The temporal-first scan (<code>rearrange(..., b n) t m -&gt; b (n t) m</code>) is crucial for long-range modeling\u2014preserve this ordering if you export intermediate features for contrastive alignment with DNA sequences.^[<code>421:444:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>]</p> <p>Projected BrainMT features can then be concatenated or contrastively aligned with Evo\u202f2/GENERator/Caduceus embeddings to study genetics\u2194fMRI correspondences.</p>"},{"location":"code_walkthroughs/caduceus_walkthrough/","title":"Caduceus Code Walkthrough","text":"<p>KB references: Model card \u00b7 Genomics feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/caduceus_walkthrough/#overview","title":"Overview","text":"<p>Caduceus couples Mamba sequence mixers with reverse-complement parameter sharing so every layer sees both DNA strands simultaneously, yielding 131\u202fkbp masked-language-model checkpoints that remain equivariant to strand flips across the published HuggingFace collection.^[<code>6:141:external_repos/caduceus/README.md</code>]</p>"},{"location":"code_walkthroughs/caduceus_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo RC-equivariant BiMamba blocks inside <code>CaduceusMixerModel</code>^[<code>33:389:external_repos/caduceus/caduceus/modeling_caduceus.py</code>] ~150\u202fM (e.g., 256\u202f\u00d7\u202f16 layers in HF releases)^[<code>15:22:external_repos/caduceus/README.md</code>] 131\u202fkbp pretraining windows^[<code>15:22:external_repos/caduceus/README.md</code>] Character tokenizer w/ explicit complement map and BOS/PAD logic^[<code>15:158:external_repos/caduceus/src/dataloaders/datasets/hg38_char_tokenizer.py</code>] Lightning training for pretraining/downstream tasks, RC embeddings for VEP^[<code>1:400:external_repos/caduceus/train.py</code>][<code>30:399:external_repos/caduceus/vep_embeddings.py</code>] github.com/kuleshov-group/caduceus"},{"location":"code_walkthroughs/caduceus_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda bootstrap. Long-context experiments rely on the repo\u2019s environment file; create it with <code>conda env create -f caduceus_env.yml</code> and activate via <code>conda activate caduceus_env</code> before running the Hydra configs.^[<code>53:63:external_repos/caduceus/README.md</code>]</li> <li>Gradient checkpointing status. The backbone still has a TODO for checkpointing and the HF wrapper sets <code>supports_gradient_checkpointing = False</code>, so memory savings must come from RCPS channel splitting or ZeRO rather than built-in checkpoint flags.^[<code>228:231:external_repos/caduceus/caduceus/modeling_caduceus.py</code>][<code>297:302:external_repos/caduceus/caduceus/modeling_caduceus.py</code>]</li> </ul>"},{"location":"code_walkthroughs/caduceus_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/caduceus_walkthrough/#tokenizer-preprocessing-hg38_char_tokenizerpy-rcpy","title":"Tokenizer &amp; Preprocessing (<code>hg38_char_tokenizer.py</code>, <code>rc.py</code>)","text":"<p>Tokenization is strictly character-based, enumerating all specials and precomputing a complement map so RCPS layers can look up complements without re-tokenizing. String-level utilities supply reverse complements for augmentation or evaluation.</p> <p>```15:74:external_repos/caduceus/src/dataloaders/datasets/hg38_char_tokenizer.py class CharacterTokenizer(PreTrainedTokenizer):     self._vocab_str_to_int = {         \"[CLS]\": 0,         ...         \"[UNK]\": 6,         **{ch: i + 7 for i, ch in enumerate(characters)},     }     ...     complement_map = {\"A\": \"T\", \"C\": \"G\", \"G\": \"C\", \"T\": \"A\"}     self.complement_map[self._vocab_str_to_int[k]] = complement_id <pre><code>```7:27:external_repos/caduceus/src/dataloaders/utils/rc.py\nSTRING_COMPLEMENT_MAP = {\"A\": \"T\", ...}\ndef string_reverse_complement(seq):\n    rev_comp = \"\"\n    for base in seq[::-1]:\n        rev_comp += STRING_COMPLEMENT_MAP.get(base, base)\n    return rev_comp\n</code></pre></p>"},{"location":"code_walkthroughs/caduceus_walkthrough/#positional-rc-handling-modeling_caduceuspy","title":"Positional &amp; RC Handling (<code>modeling_caduceus.py</code>)","text":"<p>Bidirectional Mamba wrappers run forward and reverse streams (optionally weight tied) and merge them, while RCPS-aware embeddings split channel dimensions so hidden states encode forward and RC halves that can be combined later.</p> <p>```87:141:external_repos/caduceus/caduceus/modeling_caduceus.py class BiMambaWrapper(nn.Module):     self.mamba_fwd = Mamba(...)     if bidirectional:         self.mamba_rev = Mamba(...)         if bidirectional_weight_tie:             self.mamba_rev.in_proj.weight = self.mamba_fwd.in_proj.weight     def forward(...):         out = self.mamba_fwd(hidden_states, ...)         if self.bidirectional:             out_rev = self.mamba_rev(hidden_states.flip(dims=(1,))).flip(dims=(1,))             out = out + out_rev if self.bidirectional_strategy == \"add\" else out * out_rev <pre><code>```152:214:external_repos/caduceus/caduceus/modeling_caduceus.py\nclass CaduceusEmbeddings(nn.Module):\n    if config.rcps:\n        self.word_embeddings = RCPSEmbedding(...)\n...\nclass CaduceusMixerModel(nn.Module):\n    self.layers = nn.ModuleList([\n        create_block(..., rcps=config.rcps, ...) for i in range(config.n_layer)\n    ])\n    self.norm_f = norm_f if (config.fused_add_norm or not config.rcps) else RCPSAddNormWrapper(norm_f)\n</code></pre></p>"},{"location":"code_walkthroughs/caduceus_walkthrough/#backbone-embedding-wrapper-dna_embeddingpy","title":"Backbone &amp; Embedding Wrapper (<code>dna_embedding.py</code>)","text":"<p><code>DNAEmbeddingModelCaduceus</code> strips the LM head and exposes hidden states shaped <code>[B, L, d]</code> for standard mode or <code>[B, L, d, 2]</code> when RCPS/conjoined inference is enabled.</p> <p>```156:195:external_repos/caduceus/src/models/sequence/dna_embedding.py class DNAEmbeddingModelCaduceus(DNAEmbeddingModel):     def forward(...):         if self.config.rcps:             hidden_states = self.caduceus(input_ids, return_dict=False)             num_chan = hidden_states.shape[-1]             return torch.stack(                 [hidden_states[..., :num_chan // 2], torch.flip(hidden_states[..., num_chan // 2:], dims=[1, 2])],                 dim=-1             ), None         if self.conjoin_train or (self.conjoin_test and not self.training):             hidden_states = self.caduceus(input_ids[..., 0], return_dict=False)             hidden_states_rc = self.caduceus(input_ids[..., 1], return_dict=False)             return torch.stack([hidden_states, hidden_states_rc], dim=-1), None         return self.caduceus(input_ids, return_dict=False), None <pre><code>### Training Loop (`train.py`)\nThe Lightning `SequenceLightningModule` builds datasets/encoders from Hydra configs, forwards batches through encoder/decoder stacks, logs losses/metrics, and supports distributed strategies plus gradient accumulation.\n\n```126:377:external_repos/caduceus/train.py\nclass SequenceLightningModule(pl.LightningModule):\n    def setup(...):\n        self.dataset = SequenceDataset.registry[self.hparams.dataset._name_](**self.hparams.dataset)\n        ...\n        self.encoder = U.PassthroughSequential(self.task.encoder, encoder)\n        self.decoder = U.PassthroughSequential(decoder, self.task.decoder)\n        self.loss = self.task.loss\n    def _shared_step(...):\n        x, y, w = self.forward(batch)\n        loss = self.loss(x, y, **w)\n        metrics = self.metrics(x, y, **w)\n        self.log_dict({f\"{prefix}/{k}\": v for k, v in metrics.items()}, ...)\n</code></pre></p>"},{"location":"code_walkthroughs/caduceus_walkthrough/#inference-vep-embeddings-vep_embeddingspy","title":"Inference &amp; VEP Embeddings (<code>vep_embeddings.py</code>)","text":"<p>The VEP helper loads any HF model (Caduceus by default), tokenizes ref/alt + RC windows, averages variant-centered windows, and writes <code>.pt</code> tensors per split\u2014handy for multimodal or downstream regression.</p> <p>```30:392:external_repos/caduceus/vep_embeddings.py class DNAEmbeddingModel(nn.Module):     def forward(self, input_ids):         return self.backbone(input_ids).last_hidden_state ... tokens_window_ref = torch.gather(     item_ref, 1,     expanded_indices.unsqueeze(-1).expand(-1, -1, item_ref.size(2)) ).mean(dim=1) storage_dict[\"concat_avg_ws\"] = torch.cat([tokens_window_ref, tokens_window_alt], dim=-1) <pre><code>### Sequence Constraints (`configuration_caduceus.py`)\nAll RC/bidirectional behavior is driven by config: enabling RCPS, picking merge strategy (`add` vs `ew_multiply`), and passing complement maps extracted from the tokenizer.\n\n```10:55:external_repos/caduceus/caduceus/configuration_caduceus.py\nclass CaduceusConfig(PretrainedConfig):\n    def __init__(..., bidirectional: bool = True, bidirectional_strategy: Union[str, None] = \"add\",\n                 bidirectional_weight_tie: bool = True, rcps: bool = False, complement_map: Optional[dict] = None, ...):\n        self.bidirectional = bidirectional\n        self.bidirectional_strategy = bidirectional_strategy\n        self.bidirectional_weight_tie = bidirectional_weight_tie\n        self.rcps = rcps\n        self.complement_map = complement_map\n</code></pre></p>"},{"location":"code_walkthroughs/caduceus_walkthrough/#integration-hooks-genetics-brain","title":"Integration Hooks (Genetics \u2194 Brain)","text":"<ul> <li>Embedding shapes. Outputs are <code>[B, L, d]</code> by default, <code>[B, L, d, 2]</code> when strands are stacked, or <code>[B, K, 2d]</code> after VEP window pooling\u2014mean over tokens to get <code>[B, d]</code> per strand before any fusion.^[<code>156:195:external_repos/caduceus/src/models/sequence/dna_embedding.py</code>][<code>275:385:external_repos/caduceus/vep_embeddings.py</code>]</li> <li>Pooling choices. Mean pooling across tokens mirrors the masked-LM objective; to keep strand info, average forward and RC halves separately, then concatenate them to <code>[B, 2d]</code>.</li> <li>Projection to shared latent. Map pooled vectors into a 512-D multimodal space with a lightweight projector:</li> </ul> <pre><code>import torch.nn as nn\n\nclass CaduceusProjector(nn.Module):\n    def __init__(self, input_dim=512, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <ul> <li>Normalization &amp; RC alignment. When RCPS doubles channels, follow the model\u2019s own <code>RCPSAddNormWrapper</code> convention: flip the RC half, sum/average with the forward half, and only then project\u2014this keeps embeddings invariant to strand order.^[<code>210:274:external_repos/caduceus/caduceus/modeling_caduceus.py</code>]</li> <li>Batch &amp; memory tips. The Lightning module instantiates DDP-aware samplers and can rehydrate checkpoints via <code>utils.instantiate</code>, so mirror that pattern (especially <code>_shared_step</code>) when adding multimodal heads to avoid redundant forward passes.^[<code>291:377:external_repos/caduceus/train.py</code>]</li> </ul> <p>Using this flow yields <code>[B, 512]</code> Caduceus embeddings ready to align with BrainLM CLS tokens, average BrainMT CLS outputs, or SwiFT pooled features for cross-modal genetics\u2194fMRI experiments.</p>"},{"location":"code_walkthroughs/dnabert2_walkthrough/","title":"DNABERT-2 Code Walkthrough","text":"<p>KB references: Model card \u00b7 Genomics feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#overview","title":"Overview","text":"<p>DNABERT\u20112 swaps classic k-mer vocabularies for a DNA BPE tokenizer backed by ALiBi positional bias in a 117\u202fM parameter BERT encoder, and the repo focuses on supervised fine-tuning utilities for the Genome Understanding Evaluation benchmark.^[<code>30:110:external_repos/dnabert2/README.md</code>]</p>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo BERT encoder with ALiBi bias + BPE tokenizer^[<code>30:110:external_repos/dnabert2/README.md</code>] 117\u202fM (<code>zhihan1996/DNABERT-2-117M</code>)^[<code>30:94:external_repos/dnabert2/README.md</code>] 512 tokens (default <code>model_max_length</code>)^[<code>43:99:external_repos/dnabert2/finetune/train.py</code>] Optional k-mer preprocessing plus HF tokenizer/padding^[<code>79:185:external_repos/dnabert2/finetune/train.py</code>] Supervised fine-tuning, LoRA adapters, k-mer augmentation, evaluation^[<code>33:304:external_repos/dnabert2/finetune/train.py</code>] github.com/Zhihan1996/DNABERT2"},{"location":"code_walkthroughs/dnabert2_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Exact install command. The README instructs you to run <code>python3 -m pip install -r requirements.txt</code> (after cloning) to pull in the exact transformer/torch stack used for the GUE benchmark; no additional kernels are required because contexts stay at 512 tokens.^[<code>54:68:external_repos/dnabert2/README.md</code>]</li> </ul>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/dnabert2_walkthrough/#tokenizer-dataset-pipeline-finetunetrainpy","title":"Tokenizer &amp; Dataset Pipeline (<code>finetune/train.py</code>)","text":"<p><code>SupervisedDataset</code> ingests CSVs, optionally swaps sequences for cached k-mer strings, and tokenizes them with the HF tokenizer, honoring <code>model_max_length</code> and storing label counts for dynamic classifier heads.</p> <p>```79:160:external_repos/dnabert2/finetune/train.py class SupervisedDataset(Dataset):     def init(..., kmer: int = -1):         with open(data_path, \"r\") as f:             data = list(csv.reader(f))[1:]         ...         if kmer != -1:             texts = load_or_generate_kmer(data_path, texts, kmer)         output = tokenizer(             texts,             return_tensors=\"pt\",             padding=\"longest\",             max_length=tokenizer.model_max_length,             truncation=True,         )         self.input_ids = output[\"input_ids\"]         self.labels = labels <pre><code>### K-mer Utilities (`finetune/train.py`)\nHelper functions create or cache k-mer corpora for experiments that compare raw BPE tokens vs explicit k-mer inputs.\n\n```79:109:external_repos/dnabert2/finetune/train.py\ndef generate_kmer_str(sequence: str, k: int) -&gt; str:\n    return \" \".join([sequence[i:i+k] for i in range(len(sequence) - k + 1)])\n...\ndef load_or_generate_kmer(...):\n    if os.path.exists(kmer_path):\n        with open(kmer_path, \"r\") as f:\n            kmer = json.load(f)\n    else:\n        kmer = [generate_kmer_str(text, k) for text in texts]\n        with open(kmer_path, \"w\") as f:\n            json.dump(kmer, f)\n</code></pre></p>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#backbone-lora-finetunetrainpy","title":"Backbone &amp; LoRA (<code>finetune/train.py</code>)","text":"<p>The trainer script loads <code>AutoModelForSequenceClassification</code>, optionally wraps LoRA adapters (with user-specified target modules), and defers the loss/eval loop to <code>transformers.Trainer</code>.</p> <p>```235:304:external_repos/dnabert2/finetune/train.py tokenizer = transformers.AutoTokenizer.from_pretrained(..., model_max_length=training_args.model_max_length, ...) model = transformers.AutoModelForSequenceClassification.from_pretrained(     model_args.model_name_or_path,     cache_dir=training_args.cache_dir,     num_labels=train_dataset.num_labels,     trust_remote_code=True, ) if model_args.use_lora:     lora_config = LoraConfig(...)     model = get_peft_model(model, lora_config) trainer = transformers.Trainer(     model=model,     tokenizer=tokenizer,     args=training_args,     preprocess_logits_for_metrics=preprocess_logits_for_metrics,     compute_metrics=compute_metrics,     train_dataset=train_dataset,     eval_dataset=val_dataset,     data_collator=data_collator) trainer.train() <pre><code>### Embedding Access (README)\nYou can grab `[CLS]` embeddings straight from `AutoModel`, as shown in the README example, to feed into multimodal projectors.\n\n```98:110:external_repos/dnabert2/README.md\ninputs = tokenizer(dna, return_tensors = 'pt')[\"input_ids\"]\nhidden_states = model(inputs)[0] # [1, sequence_length, 768]\ncls_embedding = hidden_states[:, 0, :]\n</code></pre></p>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#integration-hooks-genetics-brain","title":"Integration Hooks (Genetics \u2194 Brain)","text":"<ul> <li>Embedding shape. HF models return <code>last_hidden_state</code> shaped <code>[B, L_tokens, 768]</code>. Use the <code>[:, 0, :]</code> CLS token for classification-style features or mean-pool across tokens for regression-style features.^[<code>98:110:external_repos/dnabert2/README.md</code>]</li> <li>Pooling choices. CLS pooling mirrors the pretraining objective; mean pooling smooths noise, and max pooling highlights motifs. You can concatenate these pooled views before projection if you need richer features.</li> <li>Projection to shared latent. Map <code>[B, 768]</code> vectors into a 512-D brain space:</li> </ul> <pre><code>import torch.nn as nn\n\nclass DNABERT2Projector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> <ul> <li>Normalization. LayerNorm (as above) or z-scoring per batch keeps embeddings compatible with BrainLM/BrainMT outputs that also end in norm layers.</li> <li>Sequence hygiene. Stick to uppercase A/C/G/T before tokenization; when enabling k-mer augmentation, remember the transformation shortens the sequence by <code>k-1</code>, so adjust <code>model_max_length</code> in <code>TrainingArguments</code> to avoid truncation.^[<code>79:109:external_repos/dnabert2/finetune/train.py</code>][<code>43:52:external_repos/dnabert2/finetune/train.py</code>]</li> </ul> <p>With these steps you can turn DNABERT\u20112 outputs into <code>[B, 512]</code> embeddings ready for concatenation or contrastive alignment with BrainLM CLS tokens, BrainMT CLS vectors, or SwiFT pooled representations.</p>"},{"location":"code_walkthroughs/evo2_walkthrough/","title":"Evo 2 Code Walkthrough","text":"<p>KB references: Model card \u00b7 Genomics feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/evo2_walkthrough/#overview","title":"Overview","text":"<p>Evo 2 packages StripedHyena\u202f2 checkpoints behind a lightweight Python API so you can run 1\u202fMbp autoregressive DNA modeling, scoring, and generation without touching the underlying Vortex stack. The repo exposes the 1B/7B/40B parameter checkpoints described in the bioRxiv preprint and HuggingFace collection, all of which share the same tokenizer (single\u2010nucleotide CharLevelTokenizer with a 512-symbol vocab) and reverse-complement aware inference utilities.^[<code>5:118:external_repos/evo2/README.md</code>]</p>"},{"location":"code_walkthroughs/evo2_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo StripedHyena\u202f2 mixer loaded via Vortex (<code>StripedHyena</code> backbone)^[<code>171:258:external_repos/evo2/evo2/models.py</code>] 1B / 7B / 40B checkpoints (<code>MODEL_NAMES</code>)^[<code>1:33:external_repos/evo2/evo2/utils.py</code>] <code>max_seqlen: 1048576</code> (\u22481\u202fMbp)^[<code>3:60:external_repos/evo2/evo2/configs/evo2-7b-1m.yml</code>] Char-level tokenizer with 512 symbols; padding handled in <code>prepare_batch</code>^[<code>19:51:external_repos/evo2/evo2/models.py</code>][<code>10:34:external_repos/evo2/evo2/scoring.py</code>] Autoregressive scoring, reverse-complement averaging, cached generation^[<code>109:169:external_repos/evo2/evo2/models.py</code>][<code>127:170:external_repos/evo2/evo2/scoring.py</code>] github.com/ArcInstitute/evo2"},{"location":"code_walkthroughs/evo2_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Exact pip/conda setup for FP8 + long context. The README prescribes installing Transformer Engine and FlashAttention before running million-token inference: <code>conda install -c nvidia cuda-nvcc cuda-cudart-dev</code> <code>conda install -c conda-forge transformer-engine-torch=2.3.0</code> <code>pip install flash-attn==2.8.0.post2 --no-build-isolation</code>^[<code>54:58:external_repos/evo2/README.md</code>]</li> <li>Hardware guidance. Official instructions call for Linux/WSL2 with CUDA\u202f12.1+, FP8-capable GPUs (compute capability \u22658.9) and note that the 40\u202fB checkpoints require multi-GPU partitioning via Vortex; Python\u202f3.12 is required for the packaged binaries.^[<code>34:52:external_repos/evo2/README.md</code>]</li> </ul>"},{"location":"code_walkthroughs/evo2_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/evo2_walkthrough/#tokenizer-preprocessing-evo2evo2modelspy-evo2evo2scoringpy","title":"Tokenizer &amp; Preprocessing (<code>evo2/evo2/models.py</code>, <code>evo2/evo2/scoring.py</code>)","text":"<p>The API always instantiates a Vortex <code>CharLevelTokenizer(512)</code> and pairs it with pad/BOS conventions inside <code>prepare_batch</code>, which right-pads sequences to the longest length in the batch while optionally prepending an EOD token for language modeling reductions.</p> <p>```19:34:external_repos/evo2/evo2/models.py class Evo2:     def init(self, model_name: str = MODEL_NAMES[1], local_path: str = None):         ...         self.tokenizer = CharLevelTokenizer(512) <pre><code>```10:34:external_repos/evo2/evo2/scoring.py\ndef prepare_batch(\n        seqs: List[str],\n        tokenizer: object,\n        prepend_bos: bool = False,\n        device: str = 'cuda:0'\n) -&gt; Tuple[torch.Tensor, List[int]]:\n    ...\n    padding = [tokenizer.pad_id] * (max_seq_length - len(seq))\n    input_ids.append(\n        torch.tensor(\n            ([tokenizer.eod_id] * int(prepend_bos)) + tokenizer.tokenize(seq) + padding,\n            dtype=torch.long,\n        ).to(device).unsqueeze(0)\n    )\n</code></pre></p>"},{"location":"code_walkthroughs/evo2_walkthrough/#positional-reverse-complement-handling-evo2evo2scoringpy","title":"Positional &amp; Reverse-Complement Handling (<code>evo2/evo2/scoring.py</code>)","text":"<p>Reverse-complement scoring duplicates the batch, flips it via Biopython, and averages the two likelihood traces so downstream metrics remain strand invariant.</p> <p>```127:170:external_repos/evo2/evo2/scoring.py def score_sequences_rc(...):     batch_seqs_rc = [ str(Seq(seq).reverse_complement()) for seq in batch_seqs ]     ...     batch_scores = _score_sequences(...)     batch_scores_rc = _score_sequences(...)     batch_scores = (np.array(batch_scores) + np.array(batch_scores_rc)) * 0.5 <pre><code>### Backbone Loader (`evo2/evo2/models.py`, `evo2/evo2/configs/evo2-7b-1m.yml`)\n`load_evo2_model` resolves the YAML config, instantiates `StripedHyena` with the Hyena/Mamba layer schedule, and merges HF shards (removing them afterward) before loading the `.pt` checkpoint.\n\n```171:258:external_repos/evo2/evo2/models.py\ndef load_evo2_model(...):\n    config = dotdict(yaml.load(open(config_path), Loader=yaml.FullLoader))\n    model = StripedHyena(config)\n    load_checkpoint(model, weights_path)\n    return model\n</code></pre></p> <p>```3:65:external_repos/evo2/evo2/configs/evo2-7b-1m.yml hidden_size: 4096 num_layers: 32 max_seqlen: 1048576 tokenizer_type: CharLevelTokenizer use_flash_attn: True <pre><code>### Objective &amp; Diagnostics (`evo2/evo2/test/test_evo2.py`)\nThe supplied test harness tokenizes CSV prompts, runs a forward pass, and reports cross-entropy plus next-token accuracy so you can validate checkpoints against the published numbers.\n\n```34:124:external_repos/evo2/evo2/test/test_evo2.py\nwith torch.inference_mode():\n    logits, _ = model.model.forward(input_ids.unsqueeze(0))\n    loss = F.cross_entropy(pred_logits, target_ids.long())\n    accuracy = (target_ids == pred_tokens).float().mean().item()\n...\nexpected_metrics = {\n    'evo2_40b': {'loss': 0.2159424, 'acc': 91.673},\n    ...\n}\n</code></pre></p>"},{"location":"code_walkthroughs/evo2_walkthrough/#inference-helpers-evo2evo2modelspy","title":"Inference Helpers (<code>evo2/evo2/models.py</code>)","text":"<p><code>score_sequences</code> and <code>generate</code> wrap the Hyena forward pass with batching, tokenizer reuse, and memory-aware knobs such as cached generation and <code>force_prompt_threshold</code> so you can guard against OOMs on long prompts.</p> <p>```109:169:external_repos/evo2/evo2/models.py def score_sequences(...):     scoring_func = partial(score_sequences_rc if average_reverse_complement else score_sequences, ...)     with torch.no_grad():         scores = scoring_func(seqs) ... def generate(...):     output = vortex_generate(         prompt_seqs=prompt_seqs,         model=self.model,         tokenizer=self.tokenizer,         n_tokens=n_tokens,         temperature=temperature,         top_k=top_k,         cached_generation=cached_generation,         force_prompt_threshold=force_prompt_threshold,     ) <pre><code>### Embedding Extraction Hooks (`evo2/evo2/models.py`)\nForward hooks can be registered on any named submodule (e.g., `blocks.28.mlp`) so you can capture intermediate representations without modifying Vortex internals.\n\n```52:105:external_repos/evo2/evo2/models.py\ndef forward(..., return_embeddings: bool = False, layer_names=None):\n    if return_embeddings:\n        layer = self.model.get_submodule(name)\n        handles.append(layer.register_forward_hook(hook_fn(name)))\n    logits = self.model.forward(input_ids)\n    if return_embeddings:\n        return logits, embeddings\n</code></pre></p>"},{"location":"code_walkthroughs/evo2_walkthrough/#sequence-constraints-evo2evo2configsevo2-7b-1myml-evo2evo2scoringpy","title":"Sequence Constraints (<code>evo2/evo2/configs/evo2-7b-1m.yml</code>, <code>evo2/evo2/scoring.py</code>)","text":"<p>Hyena checkpoints assume 1\u202fM tokens and the char tokenizer emits IDs per nucleotide, so batching must pad or truncate to keep tensors rectangular; the scorer enforces this padding and strips BOS tokens before computing positional log-likelihoods.</p> <p>```52:88:external_repos/evo2/evo2/scoring.py softmax_logprobs = torch.log_softmax(logits, dim=-1) softmax_logprobs = softmax_logprobs[:, :-1] input_ids = input_ids[:, 1:] logprobs = torch.gather(softmax_logprobs, 2, input_ids.unsqueeze(-1)).squeeze(-1) <pre><code>## Integration Hooks (Genetics \u2194 Brain)\n\n- **Embedding shape.** The Hyena forward returns logits shaped `[B, L, vocab]` and any hooked layer keeps `[B, L, hidden]`, matching the token dimension used in `_score_sequences`. Mean pooling along the token axis reproduces the default `reduce_method='mean'` used for log-likelihoods.^[```70:89:external_repos/evo2/evo2/scoring.py```]\n- **Pooling choices.** Use mean pooling for global sequence summaries, max pooling to emphasize motifs, or last-token pooling when you want autoregressive state; all three are equivalent to selecting how you reduce `logprobs[idx][:seq_len]` inside the scorer.^[```79:88:external_repos/evo2/evo2/scoring.py```]\n- **Reverse-complement normalization.** For strand-agnostic features, reuse `score_sequences_rc` logic: tokenize both strands, run the same hook set, and average activations before pooling so the resulting `[B, H]` encodes orientation-invariant context.^[```127:170:external_repos/evo2/evo2/scoring.py```]\n- **Projection to shared space.** Once you have pooled `[B, H]` embeddings (H\u22484096 for the 7B model), feed them through a lightweight projector to align with fMRI representations. A drop-in block that mirrors common multimodal heads is:\n\n```python\nimport torch.nn as nn\n\nclass MultimodalProjector(nn.Module):\n    def __init__(self, input_dim=4096, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.proj(x)\n</code></pre></p> <ul> <li>Batch &amp; memory tips. Long prompts can trigger Hyena\u2019s FFT prefill path; set <code>force_prompt_threshold</code> or <code>cached_generation=False</code> when working on 24\u202fGB GPUs, and ensure you shard batches via the <code>batch_size</code> argument in <code>score_sequences</code> to keep <code>prepare_batch</code> from materializing multi-megabase tensors at once.^[<code>109:169:external_repos/evo2/evo2/models.py</code>][<code>10:34:external_repos/evo2/evo2/scoring.py</code>]</li> </ul> <p>With these hooks, Evo\u202f2 embeddings (<code>[B, H]</code> after pooling) can be projected into the same 512-D latent space used by your fMRI encoder, normalized (LayerNorm) for stability, and concatenated or contrastively aligned with brain-derived features.</p>"},{"location":"code_walkthroughs/fms_medical_walkthrough/","title":"FMS-Medical Curation Walkthrough","text":"<p>KB references: Survey digest (pending) \u00b7 Integration strategy \u00b7 KB overview \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#overview","title":"Overview","text":"<p><code>external_repos/fms-medical</code> is an \u201cawesome list\u201d style knowledge base that tracks foundation-model research across healthcare modalities\u2014language (LFM), vision (VFM), bioinformatics (BFM), and multimodal (MFM)\u2014plus dataset catalogs, tutorials, and bilingual survey PDFs. The maintainers keep the README current with publication news (e.g., IEEE Reviews acceptance) and provide both English and Chinese summaries (<code>files/HFM_Chinese.pdf</code>), making it a convenient seed for KB model/dataset cards.^[<code>1:38:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>]</p>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Markdown knowledge graph: NEWS \u2192 Survey references \u2192 modality-specific method lists + dataset tables.^[<code>39:399:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>] Organized by year (2020\u20132024) and modality; each entry stores venue, short description, and code/paper links\u2014ready for transformation into KB YAML cards. Highlights IEEE Reviews 2024 survey + arXiv companions; hosts bilingual PDF in <code>files/</code> for regional teams.^[<code>5:38:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>] Pure Markdown + embedded images; contributions occur via pull requests (no runtime code). Quick lookup for LFM/VFM/BFM/MFM models, dataset tables (text, imaging, omics, multimodal), lectures, blogs, and related awesome lists.^[<code>39:400:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>] github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare"},{"location":"code_walkthroughs/fms_medical_walkthrough/#repository-notes","title":"Repository Notes","text":"<ul> <li>Documentation-only. No Python modules or requirements\u2014syncing the README (and optional PDF) is sufficient to integrate the content into KB templates.</li> <li>Bilingual assets. <code>files/HFM_Chinese.pdf</code> mirrors the survey for Mandarin readers; cite it when translating KB summaries.^[<code>17:34:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>]</li> <li>Citation-first. Each entry lists papers/code, so KB automation scripts can parse the tables to populate <code>kb/model_cards</code>, <code>kb/datasets</code>, or <code>docs/generated</code> references.</li> </ul>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/fms_medical_walkthrough/#survey-metadata-news-banner","title":"Survey Metadata &amp; NEWS Banner","text":"<p>Top-of-file announcements capture publication milestones, acceptance venues, and contact information. These lines can drive KB changelogs or curated timelines.</p> <p>```5:34:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md </p> <p>[NEWS.20241115] Our survey paper has been accepted by IEEE Reviews in Biomedical Engineering (IF: 17.2).</p> <p>[NEWS.20240405] The related survey paper has been released.</p> <p>[NOTE] If you have any questions, please don't hesitate to contact us. <pre><code>### Modality Method Registries (LFM/VFM/BFM/MFM)\nEach section groups methods by year, venue, and modality. Capturing these rows lets the KB auto-generate candidate model cards or integration experiments.\n\n```82:210:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md\n## LFM methods\n**2024**\n- [AAAI] Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and realworld multi-turn dialogue. [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/29907) [[Code]](https://github.com/SupritYoung/Zhongjing)\n- [NeurIPS] MDAgents: An adaptive collaboration of LLMs for medical decision-making. [[Paper]](https://arxiv.org/abs/2404.15155) [[Code]](https://github.com/mitmedialab/MDAgents)\n...\n## VFM methods\n**2024**\n- [arXiv] USFM: A universal ultrasound foundation model generalized to tasks and organs towards label efficient image analysis. [[paper]](https://arxiv.org/html/2401.00153v2) \n</code></pre></p>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#dataset-catalogs","title":"Dataset Catalogs","text":"<p>Separate tables detail datasets per modality (text, imaging, multimodal). These rows map neatly onto <code>kb/datasets/*.yaml</code> and help ensure coverage across integrative experiments.</p> <p>```339:399:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</p>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#datasets","title":"Datasets","text":""},{"location":"code_walkthroughs/fms_medical_walkthrough/#lfm-datasets","title":"LFM datasets","text":"Dataset  Name Text Types Scale Task Link PubMed Literature 18B tokens Language modeling * MedC-I Literature 79.2B tokens Dialogue * ... CMeKG-8K Dialogue 8K instances Dialogue * <pre><code>### Other Resources (Lectures/Blogs/Awesome lists)\nThe README also aggregates tutorials, blogs, and related awesome repositories under \u201cOther Resources,\u201d which can seed KB \u201cFurther reading\u201d sections or onboarding material.\n\n```53:74:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md\n- [Other Resources](#other-resources)\n  - [Lectures and tutorials](#lectures-and-tutorials)\n  - [Blogs](#blogs)\n  - [Related awesome repositories](#related-awesome-repositories)\n</code></pre>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#integration-hooks-curation-kb","title":"Integration Hooks (Curation \u2194 KB)","text":"<ul> <li>Automate card creation. Parse the Markdown tables (e.g., via pandoc or custom scripts) to prefill <code>kb/model_cards</code> with metadata (venue, year, links), ensuring coverage parity across modalities.</li> <li>Align dataset registries. Map the README dataset entries to <code>kb/datasets/*.yaml</code> and tag each with modality + task so integration plans can quickly reference scale/availability.</li> <li>Leverage bilingual PDFs. When publishing KB summaries for non-English audiences, cite <code>files/HFM_Chinese.pdf</code> to keep translations aligned with the upstream survey and avoid redundant localization work.</li> </ul>"},{"location":"code_walkthroughs/generator_walkthrough/","title":"GENERator Code Walkthrough","text":"<p>KB references: Model card \u00b7 Genomics feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/generator_walkthrough/#overview","title":"Overview","text":"<p>GENERator wraps GPT-style causal decoders (1.2\u202fB and 3\u202fB parameters for both eukaryote and prokaryote checkpoints) with a strict 6-mer tokenizer and long-context optimizations\u2014FlashAttention, Liger kernels, sliding-window decoding\u2014so you can score or generate up to one million base pairs per prompt.^[<code>5:125:external_repos/generator/README.md</code>]</p>"},{"location":"code_walkthroughs/generator_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo HuggingFace <code>AutoModelForCausalLM</code> decoder w/ optional ChunkEnsemble Llama heads^[<code>257:349:external_repos/generator/src/tasks/downstream/fine_tuning.py</code>][<code>508:688:external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>] 1.2\u202fB &amp; 3\u202fB checkpoints for euk/prok.^[<code>52:118:external_repos/generator/README.md</code>] 1\u202fMbp prompts via sliding windows + FlashAttention^[<code>84:99:external_repos/generator/README.md</code>][<code>612:667:external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>] 6-mer tokenizer; sequences must be multiples of 6, enforced in preprocessing^[<code>118:125:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py</code>][<code>115:235:external_repos/generator/src/tasks/downstream/fine_tuning.py</code>] Variant effect scoring, sequence recovery, classification/regression fine-tuning^[<code>141:406:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py</code>][<code>400:687:external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>] github.com/GenerTeam/GENERator"},{"location":"code_walkthroughs/generator_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Long-context dependencies. For million-base contexts the README recommends installing the custom kernels explicitly: <code>pip install liger-kernel</code> <code>pip install flash-attn --no-build-isolation</code>^[<code>84:89:external_repos/generator/README.md</code>]</li> <li>Gradient checkpointing flag. When operating on &gt;10\u202fkbp sequences, the authors enable <code>model.gradient_checkpointing_enable()</code> to trade compute for memory.^[<code>420:424:external_repos/generator/README.md</code>]</li> </ul>"},{"location":"code_walkthroughs/generator_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/generator_walkthrough/#tokenizer-preprocessing-variant_effect_predictionpy-fine_tuningpy","title":"Tokenizer &amp; Preprocessing (<code>variant_effect_prediction.py</code>, <code>fine_tuning.py</code>)","text":"<p>The downstream scripts consistently load the HF tokenizer with <code>trust_remote_code=True</code>, force pad tokens to EOS if missing, and either truncate or pad every sequence to the nearest 6-mer boundary (<code>pad_to_multiple_of_six</code> flag) so the 6-mer BPE never emits <code>&lt;oov&gt;</code> tokens.</p> <p>```151:176:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) ... inputs = tokenizer(batch_sequences, return_tensors=\"pt\", padding=True) <pre><code>```118:125:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py\ntruncate_length = len(sequence) % 6\nif truncate_length &gt; 0:\n    sequence = sequence[truncate_length:]\n</code></pre></p> <p>```208:243:external_repos/generator/src/tasks/downstream/fine_tuning.py if pad_to_multiple_of_six:     remainder = len(seq) % 6     if remainder != 0:         pad_len = 6 - remainder         seq = seq + \"A\" * pad_len tokenized = tokenizer(     sequences,     truncation=True,     max_length=max_length,     add_special_tokens=True,     padding=False, ) <pre><code>### Positional &amp; Long-Context Handling (`sequence_understanding.py`)\n`sequence_understanding.py` either scales RoPE via YaRN or injects sliding-window attention patches so you can extend Llama-based classifiers to &gt;1\u202fM tokens while staying numerically stable.\n\n```596:666:external_repos/generator/src/tasks/downstream/sequence_understanding.py\nelif length_extension_mode == \"sliding_window\":\n    config.sliding_window = int(original_model_max_length_for_scaling)\n    ...\n    def _sliding_llama_forward(...):\n        kwargs[\"sliding_window\"] = self.config.sliding_window\n        return _orig_forward(...)\n    LlamaAttention.forward = _sliding_llama_forward\n    attn_implementation = \"flash_attention_2\"\n</code></pre></p>"},{"location":"code_walkthroughs/generator_walkthrough/#backbone-instantiation-fine_tuningpy-sequence_understandingpy","title":"Backbone Instantiation (<code>fine_tuning.py</code>, <code>sequence_understanding.py</code>)","text":"<p>Fine-tuning uses <code>AutoModelForCausalLM</code> with optional pad ID fixes, while sequence-understanding swaps in <code>AutoModelForSequenceClassification</code> or the ChunkEnsemble wrapper to keep a rolling window over million-token sequences.</p> <p>```257:285:external_repos/generator/src/tasks/downstream/fine_tuning.py model = AutoModelForCausalLM.from_pretrained(     model_name,     trust_remote_code=True, ) if model.config.pad_token_id is None and hasattr(model.config, \"eos_token_id\"):     model.config.pad_token_id = model.config.eos_token_id <pre><code>```508:593:external_repos/generator/src/tasks/downstream/sequence_understanding.py\nclass ChunkEnsembleLlamaForSequenceClassification(LlamaPreTrainedModel):\n    def forward(...):\n        input_ids_chunks = input_ids.unfold(dimension=1, size=self.chunk_size, step=self.stride)\n        ...\n        chunk_eos_embedding = hidden_states[\n            torch.arange(batch_size, device=hidden_states.device),\n            sequence_lengths,\n        ]\n        stacked_embeddings = torch.stack(all_chunk_eos_embeddings, dim=1)\n        final_representation = padded_embeddings.view(batch_size, -1)\n        logits = self.classifier(final_representation)\n</code></pre></p>"},{"location":"code_walkthroughs/generator_walkthrough/#objective-training-loop-fine_tuningpy","title":"Objective &amp; Training Loop (<code>fine_tuning.py</code>)","text":"<p>The script wraps everything in <code>transformers.Trainer</code> with <code>DataCollatorForLanguageModeling</code> (<code>mlm=False</code>) so causal LM losses line up with the HF training stack and distributed options (DeepSpeed, FSDP) set via CLI.</p> <p>```351:390:external_repos/generator/src/tasks/downstream/fine_tuning.py trainer = Trainer(     model=model,     args=training_args,     train_dataset=dataset,     tokenizer=tokenizer,     data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False), ) trainer.train() <pre><code>### Inference Helpers (`variant_effect_prediction.py`)\nVariant effect prediction shards ClinVar sequences across GPUs, caches logits, and computes per-base probabilities by summing over all tokens starting with ref/alt characters. This utility powers the headline ClinVar AUROC numbers.\n\n```201:290:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py\ndef compute_logits_parallel(...):\n    num_gpus = torch.cuda.device_count()\n    shards.append({'shard_id': i, 'sequences_data': sequences_data[start_idx:end_idx], ...})\n    with ctx.Pool(processes=num_gpus) as pool:\n        results = list(pool.imap(compute_logits_shard, args_list))\n</code></pre></p> <p>```292:383:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py def parallel_compute_probabilities(...):     vocab = tokenizer.get_vocab()     char_indices = get_char_indices(vocab)     results = list(pool.imap(compute_prob, args_list, chunksize=chunksize))     p_ref, p_alt = zip(*results) <pre><code>### Embedding Extraction (`sequence_understanding.py`)\nChunkEnsemble accumulates the EOS vector from each sliding chunk, pads/truncates them to a fixed count, and flattens into a `[B, max_chunks * hidden]` representation before the classifier head\u2014exactly what you can reuse for downstream alignment.\n\n```446:505:external_repos/generator/src/tasks/downstream/sequence_understanding.py\nstacked_embeddings = torch.stack(all_chunk_eos_embeddings, dim=1)\nnum_padding_chunks = self.max_chunks - stacked_embeddings.shape[1]\n...\nfinal_representation = padded_embeddings.view(batch_size, -1)\nlogits = self.classifier(final_representation)\n</code></pre></p>"},{"location":"code_walkthroughs/generator_walkthrough/#sequence-constraints-variant_effect_predictionpy-fine_tuningpy","title":"Sequence Constraints (<code>variant_effect_prediction.py</code>, <code>fine_tuning.py</code>)","text":"<p>Both inference and training enforce the 6-mer constraint by trimming or padding raw strings and, for dataset preprocessing, only accepting columns named <code>sequence</code>, <code>seq</code>, <code>dna_sequence</code>, etc., so you cannot silently feed invalid tokens.</p> <p>```208:244:external_repos/generator/src/tasks/downstream/fine_tuning.py if \"sequence\" in examples:     sequences = examples[\"sequence\"] elif \"seq\" in examples:     sequences = examples[\"seq\"] ... else:     raise ValueError(\"No sequence column found in dataset.\") <pre><code>## Integration Hooks (Genetics \u2194 Brain)\n\n- **Embedding shapes.** GENERator decoders yield `[B, L_tokens, hidden]` tensors; ChunkEnsemble condenses them into `[B, max_chunks, hidden]` before flattening to `[B, max_chunks * hidden]`. You can stop just before the final classifier to grab the stacked embeddings for pooling.^[```446:505:external_repos/generator/src/tasks/downstream/sequence_understanding.py```]\n- **Pooling strategies.** Use mean pooling along the chunk dimension for overall sequence summaries, max pooling for motif emphasis, or take the final chunk (equivalent to autoregressive \u201clast token\u201d). Because chunk embeddings correspond to non-overlapping windows, pooling behaves like low-resolution downsampling.\n- **Projection to shared latent.** After pooling to `[B, H]` (H\u22481536 for the 1.2\u202fB model), apply a projector to map into the same 512-D space used by your brain encoder:\n\n```python\nimport torch.nn as nn\n\nclass GeneratorProjector(nn.Module):\n    def __init__(self, input_dim=1536, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre></p> <ul> <li>Normalization. LayerNorm (as in the projector above) keeps token-averaged embeddings comparable to fMRI CLS tokens (BrainLM/SwiFT), especially before cosine-similarity objectives.</li> <li>Sequence hygiene. Reuse the <code>pad_to_multiple_of_six</code> logic or <code>ensure_6mer_compatible</code> helper whenever you extract embeddings outside the packaged scripts; otherwise, HF will inject <code>&lt;oov&gt;</code> tokens that shift chunk boundaries and misalign pooling.^[<code>118:125:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py</code>]</li> <li>Memory tips. For million-token prompts, lean on ChunkEnsemble (<code>length_extension_mode=\"chunk_ensemble\"</code>) or sliding-window RoPE to avoid editing HF internals; both paths keep per-chunk lengths manageable and let FlashAttention v2 handle the heavy lifting.^[<code>566:666:external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>]</li> </ul> <p>Following these steps yields <code>[B, 512]</code> genetic embeddings that can be concatenated with or contrastively aligned against brain-model outputs such as BrainLM CLS vectors or BrainMT/SwiFT pooled features.</p>"},{"location":"code_walkthroughs/m3fm_walkthrough/","title":"M3FM Code Walkthrough","text":"<p>KB references: Model card (pending) \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#overview","title":"Overview","text":"<p>M3FM couples multilingual CLIP text embeddings with the original R2Gen relational-memory Transformer decoder to generate bilingual COVID-era chest X-ray reports. The entrypoint <code>M3FM.py</code> wires tokenization, dataset splits, optimizer/scheduler, and the trainer while <code>modules/text_extractor.py</code> handles medical text preprocessing and embedding, and <code>modules/encoder_decoder.py</code> implements the Transformer + RelationalMemory decoder that outputs report logits for teacher-forced training; inference routes beam/greedy decoding through English or Chinese heads via CLI flags.^[<code>1:72:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/README.md</code>][<code>15:126:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/M3FM.py</code>][<code>16:53:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/text_extractor.py</code>][<code>227:355:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/encoder_decoder.py</code>][<code>130:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>]</p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Multilingual CLIP text embeddings \u2192 relational-memory Transformer decoder (beam/greedy) for bilingual CXRs.^[<code>16:53:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/text_extractor.py</code>][<code>227:355:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/encoder_decoder.py</code>][<code>130:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] Defaults: <code>d_model=512</code>, FFN 2048, 3 decoder layers, 8 heads, <code>rm_num_slots=3</code>, <code>beam_size=3</code>, <code>epochs=15</code>.^[<code>29:81:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/M3FM.py</code>][<code>34:76:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] 224\u00d7224 CXRs with max 100 tokens; BOS token <code>1</code> (English) or <code>2</code> (Chinese) selects the generation language.^[<code>18:75:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/dataloaders.py</code>][<code>20:115:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/datasets.py</code>][<code>162:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] <code>annotation.json</code> + image roots streamed by <code>R2DataLoader</code>, yielding <code>(reports_ids, reports_ids_use)</code> tensors for teacher forcing.^[<code>18:75:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/dataloaders.py</code>][<code>20:115:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/datasets.py</code>] Trainer wraps SGD + StepLR, gradient clipping, multilingual greedy decoding, and BLEU/SPICE-compatible evaluation utilities.^[<code>91:124:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/M3FM.py</code>][<code>203:221:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/trainer.py</code>][<code>130:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] github.com/ai-in-health/M3FM"},{"location":"code_walkthroughs/m3fm_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda + pip workflow. Create <code>conda create -n M3FM python==3.9</code>, activate, install CUDA 11.8-compatible PyTorch (<code>torch&gt;=1.10.1</code>, <code>torchvision&gt;=0.11.2</code>, <code>pytorch-cuda==11.8</code>) followed by <code>pip install -r requirements.txt</code>; repo validated on <code>torch==2.2.1</code>.^[<code>4:21:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/README.md</code>]</li> <li>Metric prerequisites. Java, <code>pycocoevalcap</code>, <code>pycocotools</code>, and Stanford CoreNLP jars are required for SPICE; README documents manual download/placement steps to avoid firewalls.^[<code>46:71:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/README.md</code>]</li> <li>Language evaluation assets. Place <code>stanford-corenlp-4.5.2</code> under <code>data/</code> and keep <code>corenlp_root</code> in <code>configs/__init__.py</code> synchronized when switching between English and Chinese inference.^[<code>61:71:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/README.md</code>]</li> </ul>"},{"location":"code_walkthroughs/m3fm_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/m3fm_walkthrough/#tokenizer-data-interface-modulesdataloaderspy-modulesdatasetspy","title":"Tokenizer + Data Interface (<code>modules/dataloaders.py</code>, <code>modules/datasets.py</code>)","text":"<p><code>R2DataLoader</code> centralizes resizing/normalization, dataset selection (IU X-Ray vs. MIMIC/COV), and a collate function that pads both the teacher-forced <code>reports_ids</code> (targets) and decoder inputs (<code>reports_ids_use</code>). The dataset class uses cleaned strings to build token IDs, tracks language label via the leading token, and emits both full targets and shifted inputs.</p> <p>```8:45:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/dataloaders.py class R2DataLoader(DataLoader):     def init(self, args, tokenizer, split, shuffle):         self.args = args         self.dataset_name = args.dataset_name         self.batch_size = args.batch_size         self.shuffle = shuffle         self.num_workers = args.num_workers         self.tokenizer = tokenizer         self.split = split</p> <pre><code>    if split == 'train':\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.RandomCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406),\n                                 (0.229, 0.224, 0.225))])\n    else:\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406),\n                                 (0.229, 0.224, 0.225))])\n\n    if self.dataset_name == 'iu_xray':\n        self.dataset = IuxrayMultiImageDataset(self.args, self.tokenizer, self.split, transform=self.transform)\n    else:\n        self.dataset = MimiccxrSingleImageDataset(self.args, self.tokenizer, self.split, transform=self.transform)\n</code></pre> <p><code></code>48:74:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/dataloaders.py     @staticmethod     def collate_fn(data):         images_id, images, reports_ids, report, seq_lengths, seq_length1,image_path_all, reports_ids_use = zip(*data)          images = torch.stack(images, 0)         max_seq_length = max(seq_lengths)          targets = np.zeros((len(reports_ids), max_seq_length), dtype=int)         targets_masks = np.zeros((len(reports_ids), max_seq_length), dtype=int)          for i, report_ids in enumerate(reports_ids):             targets[i, :len(report_ids)] = report_ids          max_seq_length_us = max(seq_length1)         targets_us = np.zeros((len(reports_ids_use),  max_seq_length_us), dtype=int)         targets_masks1 = np.zeros((len(reports_ids_use), max_seq_length_us ), dtype=int)          for i,reports_ids_use1 in enumerate(reports_ids_use):             targets_us[i, :len(reports_ids_use1)] = reports_ids_use1          return images_id, images, torch.LongTensor(targets), report,image_path_all ,torch.LongTensor(targets_us) ```</p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#multilingual-textextractor-modulestext_extractorpy","title":"Multilingual TextExtractor (<code>modules/text_extractor.py</code>)","text":"<p>The <code>TextExtractor</code> loads <code>M-CLIP/XLM-Roberta-Large-Vit-L-14</code>, averages contextual token embeddings with attention masking, projects them through CLIP\u2019s linear head, then applies a learnable affine + ReLU to map the 768-d output to the 512-d hidden size expected by the decoder. Reports are cleaned per language before tokenization, enabling bilingual support without retraining the encoder.</p> <p>```16:53:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/text_extractor.py class TextExtractor(nn.Module):     def init(self, args):         super(TextExtractor, self).init()         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"</p> <pre><code>    self.model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'\n    self.model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(self.model_name, device=self.device)\n    self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name, device=self.device)\n    self.clean_report = self.clean_report_cov\n\n    self.transformer = self.model.transformer\n    self.LinearTransformation = self.model.LinearTransformation\n\n    self.affine_aa = nn.Linear(768, 512).cuda()\n\ndef forward(self, reports):\n    if isinstance(reports, tuple):\n        texts=[]\n        for example in reports:\n            example=self.clean_report(example)\n            texts.append(example)\n    else:\n        texts=self.clean_report(reports)\n\n\n    with torch.no_grad():\n        txt_tok = self.tokenizer(texts, padding=True, return_tensors='pt').to(self.device)\n        embs = self.transformer(**txt_tok)[0]\n        att = txt_tok['attention_mask']\n        embs = (embs * att.unsqueeze(2)).sum(dim=1) / att.sum(dim=1)[:, None]\n        embeddings = self.LinearTransformation(embs).cuda()\n\n\n\n\n    embeddings = F.relu(self.affine_aa(embeddings)).cuda() #batch*768--\u300bbatch*512\n    return embeddings #batch*512\n</code></pre> <p>``` </p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#relational-memory-transformer-decoder-modulesencoder_decoderpy","title":"Relational-Memory Transformer Decoder (<code>modules/encoder_decoder.py</code>)","text":"<p><code>Transformer</code> wraps a Decoder-only stack augmented with conditional layer norm controlled by a relational memory module. Before projection, the model reshapes logits to match token vocab (default 464). Memory slots capture long-range dependencies from the previous tokens, improving report fluency over vanilla Transformer decoders.  <code>228:252:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/encoder_decoder.py class Transformer(nn.Module):     def __init__(self):         super(Transformer, self).__init__()         #self.encoder = Encoder().cuda()         self.decoder = Decoder().cuda()         self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()         self.rm=RelationalMemory()         self.tgt_emb = Embeddings().cuda()      def forward(self, enc_outputs, dec_inputs):         '''         enc_inputs: [batch_size, src_len]         dec_inputs: [batch_size, tgt_len]         '''         # tensor to store decoder outputs         dec_outputs = self.decode(dec_inputs,  enc_outputs)         dec_logits = self.projection(dec_outputs)  # dec_logits: [batch_size, tgt_len, tgt_vocab_size]          return dec_logits.reshape(-1, dec_logits.size(-1))      def decode(self,  dec_inputs, enc_outputs):         memory = self.rm.init_memory(enc_outputs.size(0)).to( enc_outputs)         memory = self.rm(self.tgt_emb(dec_inputs), memory)         return self.decoder(dec_inputs, enc_outputs, memory)</code></p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#trainer-scheduler-modulestrainerpy","title":"Trainer &amp; Scheduler (<code>modules/trainer.py</code>)","text":"<p><code>Trainer._train_epoch</code> streams teacher-forced batches, clips gradients to <code>0.1</code>, steps SGD and the StepLR schedule every iteration, and records average loss per epoch. Mixed precision isn\u2019t enabled here, so plan GPU memory accordingly.</p> <p>```203:221:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/trainer.py     def _train_epoch(self, epoch):</p> <pre><code>    train_loss = 0\n    self.model.cuda().train()\n    for batch_idx, (images_id, images, reports_ids, report,image_path_all,reports_ids_use) in enumerate(self.train_dataloader):\n        images, reports_ids,reports_ids_use= images.to(self.device), reports_ids.to(self.device),reports_ids_use.to(self.device)\n\n        output = self.model(report, reports_ids_use)\n\n        loss = self.criterion(output, reports_ids[:, 1:].reshape(-1))\n        train_loss += loss.item()\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_value_(self.model.parameters(), 0.1)\n        self.optimizer.step()\n    log = {'train_loss': train_loss / len(self.train_dataloader)}\n\n    self.lr_scheduler.step()\n    return log\n</code></pre> <p>``` </p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#bilingual-inference-script-inferencepy","title":"Bilingual Inference Script (<code>inference.py</code>)","text":"<p><code>inference.py</code> mirrors the training CLI, loads both English and Chinese <code>R2GenModel</code> variants, performs greedy decoding conditioned on the BOS token, and prints generated reports. Changing <code>--language</code> toggles which head runs and when the search halts.  <code>162:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py if args.language=='English' or args.language=='All':     model_en.eval()     output = False     with torch.no_grad():         for batch_idx, (images_id, images, reports_ids, report, image_path_all, reports_ids_use) in enumerate(                 test_dataloader):             images, reports_ids, reports_ids_use = images.to(device), reports_ids.to(                 device), reports_ids_use.to(device)              for i in range(len(images_id)):                 if reports_ids[i][0] == 1:                     greedy_dec_input = greedy_decoder(model_en, image_path_all[i], reports_ids[i], start_symbol=1)                     predict = model_en(image_path_all[i], greedy_dec_input)                     predict = predict.data.max(1, keepdim=True)[1]                     output = True                     predict = predict.squeeze()                     report = model_en.tokenizer.decode(predict.cpu().numpy())                     print(\"----------------------------------------------------------------------------------------\")                     print(\"Generated English Report:\")                     print(report)                     print(\"----------------------------------------------------------------------------------------\")                     break             if output:                 break</code></p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#integration-hooks-vision-clinical-language","title":"Integration Hooks (Vision \u2194 Clinical Language)","text":"<ul> <li>Tap 512-d text embeddings. <code>TextExtractor</code> already outputs normalized 512-d vectors before relational memory; export them for multimodal alignment (e.g., with genetic embeddings) or to seed cross-modal contrastive losses.^[<code>16:53:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/text_extractor.py</code>]</li> <li>Language-aware batching. The dataset prefixes each sequence with <code>1</code> (English) or <code>2</code> (Chinese); filtering on the first token lets you run per-language evaluators or remap tokens to KB-friendly ontologies without retraining.^[<code>20:75:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/datasets.py</code>]</li> <li>Reuse greedy decoder outputs. <code>greedy_decoder</code> yields raw token IDs before detokenization, making it straightforward to log intermediate logits or route them into KB experiment trackers for BLEU/SPICE comparisons across modalities.^[<code>130:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>]</li> </ul>"},{"location":"code_walkthroughs/melamma_walkthrough/","title":"Me-LLaMA Code Walkthrough","text":"<p>KB references: Model card (pending) \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/melamma_walkthrough/#overview","title":"Overview","text":"<p>Me-LLaMA extends LLaMA-2/3 checkpoints through 129B-token continual pre-training (biomedical + clinical + general corpora) followed by 214K-instruction LoRA-based tuning, then evaluates models with a custom <code>lm_eval</code> harness covering 12 medical QA/NLP benchmarks. The README documents data composition, optimizer settings, and HPC footprint, while the <code>src/</code> tree contains a prompt-aware evaluation CLI, task registry, and optional OpenAI-backed <code>ChatLM</code> for API comparisons.^[<code>1:188:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/README.md</code>][<code>1:97:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/eval.py</code>][<code>1:210:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/evaluator.py</code>]</p>"},{"location":"code_walkthroughs/melamma_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Continual-pretrained LLaMA2/3 (13B/70B/8B) + instruction tuning + prompt wrapper evaluation harness.^[<code>33:134:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/README.md</code>][<code>1:97:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/eval.py</code>] Uses AdamW (lr 8e-6), cosine schedule (5% warmup), bf16, DeepSpeed model parallelism, 8\u00d7H100 for LoRA instruction tuning.^[<code>80:90:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/README.md</code>] 129B tokens with 15\u202f:\u202f1\u202f:\u202f4 biomedical\u202f:\u202fclinical\u202f:\u202fgeneral ratio plus 214K instruction samples; guidelines + PubMed + general corpora.^[<code>68:77:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/README.md</code>] <code>poetry run python src/eval.py --model hf-causal-vllm --tasks PUBMEDQA,...</code> with optional OpenAI API keys; default PYTHONPATH extends <code>src/</code> and <code>medical-evaluation</code> for metrics.^[<code>143:188:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/README.md</code>][<code>1:14:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/scripts/run_evaluation.sh</code>] Custom <code>lm_eval</code> fork with medical tasks (<code>tasks/vital_measure.py</code>), prompt templating, JSON output, caching, and OpenAI-compatible <code>ChatLM</code> for API baselines.^[<code>1:210:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/evaluator.py</code>][<code>1:120:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/tasks/vital_measure.py</code>][<code>12:165:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/chatlm.py</code>] github.com/BIDS-Xu-Lab/Me-LLaMA"},{"location":"code_walkthroughs/melamma_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Poetry-first install. Clone the repo, run <code>poetry install</code>, export <code>PYTHONPATH=\"$repo/src:$repo/src/medical-evaluation:$repo/src/metrics/BARTScore\"</code>, and optionally set <code>CUDA_VISIBLE_DEVICES</code> before running evaluation scripts.^[<code>1:14:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/scripts/run_evaluation.sh</code>]</li> <li>Metric assets. Download <code>bart_score.pth</code> (BARTScore), install <code>en_core_web_lg</code>, and keep Stanford CoreNLP + multilingual extras for evaluation tasks that rely on syntax or multilingual scoring.^[<code>143:188:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/README.md</code>]</li> <li>API comparisons. Set <code>OPENAI_API_SECRET_KEY</code> when using <code>--model gpt-4</code> (see README instructions) so <code>ChatLM</code> can enqueue HTTPX requests with exponential backoff.^[<code>178:188:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/README.md</code>][<code>12:165:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/chatlm.py</code>]</li> </ul>"},{"location":"code_walkthroughs/melamma_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/melamma_walkthrough/#evaluation-entrypoint-srcevalpy","title":"Evaluation Entrypoint (<code>src/eval.py</code>)","text":"<p><code>parse_args</code> exposes the same knobs as upstream <code>lm_eval</code> (model/model_args/tasks/few-shot/batching/output). <code>main()</code> resolves task patterns, optional description dicts, and forwards everything to <code>evaluator.simple_evaluate</code>, writing JSON to <code>--output_path</code> when provided.</p> <p>```13:94:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/eval.py def parse_args():     parser = argparse.ArgumentParser()     parser.add_argument(\"--model\", required=True)     parser.add_argument(\"--model_args\", default=\"\")     parser.add_argument(\"--tasks\", default=None, choices=utils.MultiChoice(tasks.ALL_TASKS))     parser.add_argument(\"--model_prompt\", default=\"no_prompt\", choices=list(MODEL_PROMPT_MAP.keys()))     parser.add_argument(\"--provide_description\", action=\"store_true\")     parser.add_argument(\"--num_fewshot\", type=int, default=0)     parser.add_argument(\"--batch_size\", type=str, default=None)     parser.add_argument(\"--max_batch_size\", type=int, default=None,                         help=\"Maximal batch size to try with --batch_size auto\")     parser.add_argument(\"--device\", type=str, default=None)     parser.add_argument(\"--output_path\", default=None)     parser.add_argument(\"--limit\", type=float, default=None,                         help=\"Limit the number of examples per task. \"                              \"If &lt;1, limit is a percentage of the total number of examples.\")     parser.add_argument(\"--data_sampling\", type=float, default=None)     parser.add_argument(\"--no_cache\", action=\"store_true\")     parser.add_argument(\"--decontamination_ngrams_path\", default=None)     parser.add_argument(\"--description_dict_path\", default=None)     parser.add_argument(\"--check_integrity\", action=\"store_true\")     parser.add_argument(\"--write_out\", action=\"store_true\", default=False)     parser.add_argument(\"--output_base_path\", type=str, default=None)</p> <pre><code>return parser.parse_args()\n</code></pre> <p>...     results = evaluator.simple_evaluate(         model=args.model,         model_args=args.model_args,         tasks=task_names,         num_fewshot=args.num_fewshot,         batch_size=args.batch_size,         max_batch_size=args.max_batch_size,         device=args.device,         no_cache=args.no_cache,         limit=args.limit,         description_dict=description_dict,         decontamination_ngrams_path=args.decontamination_ngrams_path,         check_integrity=args.check_integrity,         write_out=args.write_out,         output_base_path=args.output_base_path,         model_prompt=args.model_prompt     ) ``` </p>"},{"location":"code_walkthroughs/melamma_walkthrough/#evaluator-task-dispatch-srcevaluatorpy","title":"Evaluator &amp; Task Dispatch (<code>src/evaluator.py</code>)","text":"<p><code>simple_evaluate</code> instantiates a Hugging Face (or OpenAI) LM, wraps it with a caching layer, builds the medical task dictionary, and runs <code>evaluate</code> to orchestrate prompts, few-shot contexts, turn-based conversations, and metric aggregation. Bootstrap statistics, JSON logging, and caching directories mirror upstream <code>lm_eval</code> APIs for drop-in adoption.  <code>19:133:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/evaluator.py @positional_deprecated def simple_evaluate(     model,     model_args=None,     tasks=[],     num_fewshot=0,     batch_size=None,     max_batch_size=None,     device=None,     no_cache=False,     limit=None,     bootstrap_iters=100,     description_dict=None,     check_integrity=False,     decontamination_ngrams_path=None,     write_out=False,     output_base_path=None,     model_prompt=None ):     random.seed(1234)     np.random.seed(1234)      assert len(tasks) != 0, \"No tasks specified\"      if isinstance(model, str):         if model_args is None:             model_args = \"\"         if model[:3] != \"gpt\":             lm = lm_eval.models.get_model(model).create_from_arg_string(                 model_args, {\"batch_size\": batch_size, \"max_batch_size\": max_batch_size, \"device\": device}             )         else:             lm = ChatLM(model)     else:         assert isinstance(model, lm_eval.base.LM)         lm = model      if not no_cache:         lm = lm_eval.base.CachingLM(             lm,             \"lm_cache/\"             + (model if isinstance(model, str) else model.model.config._name_or_path)             + \"_\"             + model_args.replace(\"=\", \"-\").replace(\",\", \"_\").replace(\"/\", \"-\")             + \".db\",         )      task_dict = ta.get_task_dict(tasks) ...     results[\"config\"] = {         \"model\": (model if isinstance(model, str) else model.model.config._name_or_path),         \"model_args\": model_args,         \"num_fewshot\": num_fewshot,         \"batch_size\": batch_size,         \"batch_sizes\": list(lm.batch_sizes.values()) if hasattr(lm, \"batch_sizes\") else [],         \"device\": device,         \"no_cache\": no_cache,         \"limit\": limit,         \"bootstrap_iters\": bootstrap_iters,         \"description_dict\": description_dict,     }</code></p>"},{"location":"code_walkthroughs/melamma_walkthrough/#medical-task-registry-srctasks__init__py","title":"Medical Task Registry (<code>src/tasks/__init__.py</code>)","text":"<p><code>TASK_REGISTRY</code> maps human-readable task names to custom task classes (PubMedQA, MedQA, MedMCQA, etc.). Pattern-matched CLI arguments expand into this registry, so adding a new dataset is a matter of appending to <code>TASK_REGISTRY</code> or creating a JSON-backed task via <code>add_json_task</code>.</p> <p>```9:34:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/tasks/init.py TASK_REGISTRY = {     \"PUBMEDQA\": vital_measure.PUBMEDQA,     \"MedQA\": vital_measure.MedQA,     \"MedMCQA\": vital_measure.MedMCQA,     \"EmrQA\": vital_measure.EmrQA,     \"i2b2\": vital_measure.I2B2,     \"DDI2013\": vital_measure.DDI2013,     \"hoc\": vital_measure.HoC,     \"MTSample\": vital_measure.MTSample,     \"PUBMEDSUM\": vital_measure.PubmedSum,     \"MimicSum\": vital_measure.MimicSum,     \"BioNLI\": vital_measure.BioNLI,     \"MedNLI\": vital_measure.MedNLI, } <pre><code>### Task Definitions &amp; Metrics (`src/tasks/vital_measure.py`)\n`Classification` (and its subclasses) provides language-cleaning, response parsing, accuracy/F1/MCC aggregation, and sequence labeling utilities. `SequentialLabeling`/`NER` extend this to HTML BIO alignment, while summarization tasks use Rouge/BARTScore (loaded via Poetry extras).\n\n```50:181:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/tasks/vital_measure.py\nclass Classification(Task):\n    CALCULATE_MCC = False\n    LOWER_CASE = True\n    FIRST_LETTER = False\n    VERSION = 1\n    EVAL_LAST_TURN = True\n\n    def reformulate_turn_req(self, req, turn_request, turn):\n        return req\n\n    def has_training_docs(self):\n        return True\n\n    def has_validation_docs(self):\n        return True\n\n    def has_test_docs(self):\n        return True\n\n    def training_docs(self):\n        return self.dataset[\"train\"]\n\n    def validation_docs(self):\n        return self.dataset[\"validation\"]\n\n    def test_docs(self):\n        return self.dataset[\"test\"]\n\n    def construct_requests(self, doc, ctx):\n        cont_request = rf.greedy_until(ctx, {\"until\": None})\n        return cont_request\n...\n    def aggregation(self):\n        metrics = {\n            \"acc\": mean,\n            \"missing\": mean,\n            \"f1\": self.weighted_f1,\n            \"macro_f1\": self.macro_f1,\n        }\n        if self.CALCULATE_MCC:\n            metrics[\"mcc\"] = self.matthews_corrcoef\n        return metrics\n</code></pre></p>"},{"location":"code_walkthroughs/melamma_walkthrough/#chatlm-prompt-templates-srcchatlmpy-srcmodel_promptpy","title":"ChatLM + Prompt Templates (<code>src/chatlm.py</code>, <code>src/model_prompt.py</code>)","text":"<p>For API-based baselines, <code>ChatLM</code> batches requests with asyncio + HTTPX, uses exponential backoff, and overrides <code>greedy_until</code>. Prompt wrappers in <code>model_prompt.py</code> add <code>Human/Assistant</code> prefixes when <code>--model_prompt mellama_prompt</code> is provided.</p> <p>```12:154:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/chatlm.py async def single_chat(client, **kwargs):     ...</p> <p>class ChatLM(BaseLM):     REQ_CHUNK_SIZE = 10</p> <pre><code>def __init__(self, model, truncate=False):\n    super().__init__()\n\n    import openai\n\n    self.model = model\n    self.truncate = truncate\n    api_key = os.environ[\"OPENAI_API_SECRET_KEY\"]\n    self.tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n    self.headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n</code></pre> <p>...     def greedy_until(self, requests):         if not requests:             return []         res = []         ...             responses = asyncio.run(oa_completion(                 url=\"https://api.openai.com/v1/chat/completions\",                 headers=self.headers,                 model=self.model,                 messages=[{\"role\": \"user\", \"content\": inp} for inp in inps],                 max_tokens=self.max_gen_toks,                 temperature=0.0,             )) <code></code>1:12:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/model_prompt.py def no_prompt(ctx):     return ctx   def mellama_prompt(ctx):     return f'Human: \\n{ctx}\\n\\nAssistant: \\n'   MODEL_PROMPT_MAP = {     \"no_prompt\": no_prompt,     \"mellama_prompt\": mellama_prompt, } ```</p>"},{"location":"code_walkthroughs/melamma_walkthrough/#integration-hooks-language-kb","title":"Integration Hooks (Language \u2194 KB)","text":"<ul> <li>Extend the task roster. Add a new <code>Task</code> subclass in <code>tasks/vital_measure.py</code>, register it via <code>TASK_REGISTRY</code>, and it becomes instantly available to KB experiment configs via <code>--tasks</code> filters.^[<code>9:34:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/tasks/__init__.py</code>]</li> <li>Standardize prompts. Use <code>MODEL_PROMPT_MAP</code> to ensure generated transcripts align with KB annotation guidelines (e.g., always wrap with <code>Human/Assistant</code> before ingesting outputs into evaluation cards).^[<code>1:12:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/model_prompt.py</code>]</li> <li>Capture evaluation metadata. <code>simple_evaluate</code> returns JSON with config, bootstrap stats, and per-task metrics; store these blobs under <code>kb/results/</code> to track longitudinal performance as you fine-tune domain-specific adapters.^[<code>19:133:/Users/allison/Projects/neuro-omics-kb/external_repos/me-lamma/src/evaluator.py</code>]</li> </ul>"},{"location":"code_walkthroughs/mot_walkthrough/","title":"MoT Code Walkthrough","text":"<p>KB references: MoT paper note</p>"},{"location":"code_walkthroughs/mot_walkthrough/#overview","title":"Overview","text":"<p>Mixture-of-Transformers (MoT) introduces modality-aware sparsity to every non-embedding block so that each modality owns its feed-forward, attention, and normalization routes while still sharing global self-attention. In practice this lets a 7B-text+image MoT hit dense-model quality with only 55.8\u202f% of the FLOPs, extend to speech with 37.2\u202f% of the dense compute, and run multi-branch generation faster on commodity A100s.^[<code>6:30:external_repos/MoT/README.md</code>]</p>"},{"location":"code_walkthroughs/mot_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params / FLOPs Context Inputs Key capabilities Repo Attach modality-untied feed-forward + attention experts to an existing transformer, using binary <code>modality_masks</code> to route tokens yet keeping shared global attention.^[<code>75:151:external_repos/MoT/README.md</code>][<code>16:151:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>] 7B MoT (text+image) matches dense baselines at 55.8\u202f% FLOPs; 443\u202fM MoT (text+image+speech) hits dense speech quality at 37.2\u202f% FLOPs.^[<code>15:23:external_repos/MoT/README.md</code>] Chameleon (autoregressive text + raster image), Transfusion (text autoregressive + image diffusion), and broader \u201cnative multimodal\u201d projects.^[<code>15:30:external_repos/MoT/README.md</code>] Any packed token sequence as long as each token is tagged in <code>modality_masks</code>; examples show text/image/speech masks and detoured normalization rules.^[<code>129:137:external_repos/MoT/README.md</code>][<code>87:137:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>] Step-by-step tutorial covering FFN experts, attention experts, and normalization placement so you can graft MoT onto proprietary stacks.^[<code>75:330:external_repos/MoT/README.md</code>] <code>external_repos/MoT</code>"},{"location":"code_walkthroughs/mot_walkthrough/#environment-integration-notes","title":"Environment &amp; Integration Notes","text":"<ul> <li>Designed as a playbook on top of your transformer\u2014start from any stack that exposes attention/FFN modules and thread through MoT\u2019s modality-specific replacements.^[<code>40:71:external_repos/MoT/README.md</code>]</li> <li>Efficient gains hinge on accurate routing masks; the README demonstrates simple boolean lists and emphasises deterministic routing per modality.^[<code>129:137:external_repos/MoT/README.md</code>]</li> <li>Norm placement matters: either keep residual norms inside the expert modules (preferred) or refactor your <code>TransformerBlock</code> to avoid double-normalizing.^[<code>307:330:external_repos/MoT/README.md</code>]</li> </ul>"},{"location":"code_walkthroughs/mot_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/mot_walkthrough/#modality-untied-feed-forward-srcsimple_modalityuntiedfeedforwardpy","title":"Modality-Untied Feed-Forward (<code>src/simple_ModalityUntiedFeedForward.py</code>)","text":"<p><code>SimpleModalityUntiedFeedForward</code> replicates a SiLU-gated MLP per modality, normalizes each expert\u2019s output, then stitches results back into the original token order via <code>merge_modalities</code>. Swapping only this block already covers \u224867\u202f% of non-embedding parameters, so most FLOP savings arrive after this step.^[<code>75:119:external_repos/MoT/README.md</code>]</p> <p>```17:60:external_repos/MoT/src/simple_ModalityUntiedFeedForward.py class SimpleModalityUntiedFeedForward(torch.nn.Module):     def init(..., n_modalities: int = 2):         ...         self.local_experts = torch.nn.ModuleList([             SimpleFeedForward(...)             for _ in range(self.n_modalities)         ])         self.local_experts_ffn_norm = torch.nn.ModuleList(             [SimpleRMSNorm(dim, eps=1e-5) for _ in range(self.n_modalities)]         )</p> <pre><code>def forward(self, x, modality_masks):\n    expert_outputs = []\n    for i in range(self.n_modalities):\n        expert_input = x[modality_masks[i]]\n        expert_output = self.local_experts[i](expert_input)\n        expert_output = self.local_experts_ffn_norm[i](expert_output)\n        expert_outputs.append(expert_output)\n    return merge_modalities(expert_outputs, modality_masks)\n</code></pre> <p><code>Because experts only see their modality tokens, you can scale specialization (e.g., text-heavy vs. image-heavy hidden sizes) without perturbing other branches. `SimpleFeedForward` itself is the Lingua-style gated MLP that preserves tensor-parallel friendliness.^[</code>64:107:external_repos/MoT/src/simple_ModalityUntiedFeedForward.py```] </p>"},{"location":"code_walkthroughs/mot_walkthrough/#modality-untied-attention-srcsimple_modalityuntiedattentionpy","title":"Modality-Untied Attention (<code>src/simple_ModalityUntiedAttention.py</code>)","text":"<p>The attention module mirrors the FFN pattern: per-modality projections and RMSNorms for Q/K/V/outputs, shared global attention via <code>torch.nn.MultiheadAttention</code>, and a final per-modality projection back to the model dimension.^[<code>141:301:external_repos/MoT/README.md</code>][<code>16:151:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>]  <code>16:151:external_repos/MoT/src/simple_ModalityUntiedAttention.py class SimpleModalityUntiedAttention(torch.nn.Module):     def __init__(...):         self.local_experts_wq = self._create_experts(dim, n_heads * head_dim)         self.local_experts_wk = self._create_experts(dim, n_heads * head_dim)         self.local_experts_wv = self._create_experts(dim, n_heads * head_dim)         self.local_experts_wo = self._create_experts(n_heads * head_dim, dim)         ...         self.attention_comp = torch.nn.MultiheadAttention(             head_dim=head_dim,             n_heads=n_heads,             dropout=dropout,         )</code></p> <p>During <code>forward</code>, tokens are first split by mask, projected/normed per modality, concatenated back for standard attention, and finally projected/normed per modality again.^[<code>86:151:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>] Optional QK normalization reshapes tensors to <code>[*, num_heads, head_dim]</code> before applying <code>SimpleRMSNorm</code>, which keeps the rotary-scaled statistics stable.^[<code>112:174:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>]</p>"},{"location":"code_walkthroughs/mot_walkthrough/#utility-primitives-srcutilspy","title":"Utility Primitives (<code>src/utils.py</code>)","text":"<p><code>merge_modalities</code> reconstructs the packed sequence according to mask order, so expert outputs can be arbitrarily sharded while still producing a contiguous tensor for the residual path. <code>SimpleRMSNorm</code> is the Lingua-derived RMSNorm variant used consistently across experts.^[<code>14:66:external_repos/MoT/src/utils.py</code>]</p> <p><code>14:34:external_repos/MoT/src/utils.py def merge_modalities(expert_outputs, modality_masks):     merged = torch.empty_like(expert_outputs[0])     for i in range(len(expert_outputs) - 1, -1, -1):         merged[modality_masks[i]] = expert_outputs[i]     return merged</code></p>"},{"location":"code_walkthroughs/mot_walkthrough/#implementation-checklist","title":"Implementation Checklist","text":"<ul> <li>Start from your baseline <code>TransformerBlock</code>, then replace FFN/attention classes with the modality-untied versions, ensuring the residual structure still performs <code>x + module(x)</code> as shown in the README.^[<code>307:330:external_repos/MoT/README.md</code>]</li> <li>Provide <code>modality_masks</code> for every forward pass. The README\u2019s three-modality example demonstrates boolean masks; in production you can precompute these from tokenizer metadata or image/video region plans.^[<code>129:137:external_repos/MoT/README.md</code>]</li> <li>Keep norm layers inside the modality-specific modules to avoid double-scaling outputs; only keep block-level norms if your baseline requires them.^[<code>307:330:external_repos/MoT/README.md</code>]</li> </ul>"},{"location":"code_walkthroughs/mot_walkthrough/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Routing data from KB assets. When generating multimodal batches (e.g., fMRI tokens + gene tokens) build boolean masks once per modality and pass them into MoT blocks\u2014only the masks need awareness of modality boundaries.</li> <li>Progressive specialization. Because experts are independent <code>nn.ModuleList</code> entries, you can freeze or reinitialize select modalities while fine-tuning others (useful when only one KB modality changes).</li> <li>FLOP budgeting. The FLOP savings callouts (55.8\u202f% / 37.2\u202f% / one-third) provide targets for profiling when adapting MoT to new neuro-omics settings.^[<code>15:30:external_repos/MoT/README.md</code>]</li> </ul>"},{"location":"code_walkthroughs/swift_walkthrough/","title":"SwiFT Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/swift_walkthrough/#overview","title":"Overview","text":"<p>SwiFT (Swin 4D fMRI Transformer) tokenizes 4D fMRI volumes with 3D convolutions, processes them with windowed 4D self-attention (spatial + temporal windows), and trains contrastive or supervised heads via PyTorch Lightning.^[<code>1:400:external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>][<code>1:188:external_repos/swift/project/main.py</code>]</p>"},{"location":"code_walkthroughs/swift_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Swin-inspired 4D transformer w/ window attention &amp; patch merging^[<code>21:400:external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>] Configurable (e.g., embed_dim=96, depths from config)^[<code>402:565:external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>] 96\u00d796\u00d796 voxels \u00d7 20 frames (default)^[<code>250:300:external_repos/swift/project/module/utils/data_module.py</code>] Preprocessed volumes from <code>fMRIDataModule</code> (UKB/HCP/etc.)^[<code>13:260:external_repos/swift/project/module/utils/data_module.py</code>] Lightning training with contrastive or supervised heads, downstream evaluation scripts^[<code>21:187:external_repos/swift/project/main.py</code>][<code>32:395:external_repos/swift/project/module/pl_classifier.py</code>] github.com/Transconnectome/SwiFT"},{"location":"code_walkthroughs/swift_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda environment. The README tells you to run <code>conda env create -f envs/py39.yaml</code> followed by <code>conda activate py39</code> to pull in the exact PyTorch/Lightning versions used for the released checkpoints.^[<code>45:55:external_repos/swift/README.md</code>]</li> <li>Gradient checkpoint knobs. Every Swin4D stage accepts <code>use_checkpoint</code> and executes <code>torch.utils.checkpoint.checkpoint(...)</code> when set, so add <code>use_checkpoint=True</code> in your model config to extend contexts without exceeding GPU memory.^[<code>224:312:external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>][<code>507:744:external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>]</li> </ul>"},{"location":"code_walkthroughs/swift_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/swift_walkthrough/#data-module-projectmoduleutilsdata_modulepy","title":"Data Module (<code>project/module/utils/data_module.py</code>)","text":"<p><code>fMRIDataModule</code> loads datasets (UKB, HCP, etc.), splits subjects, and returns PyTorch <code>DataLoader</code>s. Augmentations (affine/noise) are applied in the Lightning module.</p> <p>```13:230:external_repos/swift/project/module/utils/data_module.py class fMRIDataModule(pl.LightningDataModule):     def get_dataset(self):         if self.hparams.dataset_name == \"S1200\": return S1200         ...     def setup(self, stage=None):         Dataset = self.get_dataset()         params = {\"root\": self.hparams.image_path, \"sequence_length\": self.hparams.sequence_length, ...}         self.train_dataset = Dataset(**params, subject_dict=train_dict, ...)         self.train_loader = DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, ...) <pre><code>### Patch Embedding &amp; Window Attention (`swin4d_transformer_ver7.py`)\n`PatchEmbed` downsamples volumes with strided 3D convs, `WindowAttention4D` computes attention inside local 4D windows, and `SwinTransformerBlock4D` applies shifted windows for better coverage. `PatchMergingV2` reduces spatial resolution while keeping temporal size.\n\n```202:399:external_repos/swift/project/module/models/swin4d_transformer_ver7.py\nclass PatchEmbed(nn.Module):\n    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=(1, patch_size), stride=(1, patch_size))\n...\nclass WindowAttention4D(nn.Module):\n    def forward(self, x, mask):\n        qkv = self.qkv(x).reshape(...)\n        attn = self.softmax((q @ k.transpose(-2, -1)) * self.scale)\n        x = (attn @ v)\n</code></pre></p>"},{"location":"code_walkthroughs/swift_walkthrough/#swin4d-backbone-swin4d_transformer_ver7py","title":"Swin4D Backbone (<code>swin4d_transformer_ver7.py</code>)","text":"<p><code>BasicLayer</code> stacks windowed blocks, handles padding, applies attention masks, and optionally downsamples. The main <code>SwinTransformer4D</code> builds multiple stages with positional embeddings, patch merging, and normalization.</p> <p>```400:796:external_repos/swift/project/module/models/swin4d_transformer_ver7.py class BasicLayer(nn.Module):     for blk in self.blocks:         x = blk(x, attn_mask)         x = x + self.drop_path(self.mlp(self.norm2(x)))     if self.downsample is not None:         x = self.downsample(x) ... class SwinTransformer4D(nn.Module):     self.patch_embed = PatchEmbed(...)     self.layers = nn.ModuleList([...])     def forward(self, x):         x = self.patch_embed(x)         x = self.pos_drop(x)         for layer in self.layers:             x = self.pos_embeds[i] (x)             x = self.layers[i] (x.contiguous())         return x <pre><code>### Lightning Module (`project/module/pl_classifier.py`)\n`LitClassifier` wraps the encoder, applies augmentations if requested, and attaches task-specific heads (classification/regression/contrastive). `_calculate_loss` routes to BCE, MSE, or contrastive losses.\n\n```32:205:external_repos/swift/project/module/pl_classifier.py\nself.model = load_model(self.hparams.model, self.hparams)\nif self.hparams.downstream_task == 'sex':\n    self.output_head = load_model(\"clf_mlp\", self.hparams)\nelif self.hparams.downstream_task == 'age':\n    self.output_head = load_model(\"reg_mlp\", self.hparams)\n...\ndef _calculate_loss(self, batch, mode):\n    if self.hparams.pretraining:\n        # contrastive losses (NT-Xent)\n    else:\n        subj, logits, target = self._compute_logits(batch)\n        if classification:\n            loss = F.binary_cross_entropy_with_logits(logits, target)\n        else:\n            loss = F.mse_loss(logits.squeeze(), target.squeeze())\n</code></pre></p>"},{"location":"code_walkthroughs/swift_walkthrough/#training-entry-point-projectmainpy","title":"Training Entry Point (<code>project/main.py</code>)","text":"<p>CLI parses dataset/model/task args, instantiates the Lightning module + data module, and launches PyTorch Lightning <code>Trainer</code> with callbacks (checkpointing, LR monitor).</p> <p><code>18:187:external_repos/swift/project/main.py parser = ArgumentParser(...) parser = Classifier.add_model_specific_args(parser) parser = Dataset.add_data_specific_args(parser) parser = pl.Trainer.add_argparse_args(parser) args = parser.parse_args() data_module = Dataset(**vars(args)) model = Classifier(data_module=data_module, **vars(args)) trainer = pl.Trainer.from_argparse_args(args, logger=logger, callbacks=callbacks) trainer.fit(model, datamodule=data_module) <pre><code>## Integration Hooks (Brain \u2194 Genetics)\n\n- **Embedding shape.** Encoder outputs `[B, N_tokens, embed_dim]`. Downstream heads either global-average tokens (`mean(dim=[2,3,4])`) or use CLS-like features (depending on head). Use `_compute_logits` to capture the tensor before the head for multimodal projection.^[```108:205:external_repos/swift/project/module/pl_classifier.py```]\n- **Pooling choices.** Mean pooling across spatial dimensions (`features.mean(dim=[2,3,4])`) produces `[B, embed_dim]`; temporal pooling can be added if you keep time as a separate axis prior to patch merging.\n- **Projection to shared latent.** Apply a lightweight projector to map `[B, embed_dim]` into a 512-D shared space:\n\n```python\nimport torch.nn as nn\n\nclass SwiFTProjector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> - **Augmentation awareness.** When extracting embeddings for alignment, disable augmentations (`augment_during_training=False`) to avoid random affine/noise perturbations that would misalign with genetic features.^[</code>108:205:external_repos/swift/project/module/pl_classifier.py<code>] - **Window constraints.** Ensure inference volumes match training window sizes (`img_size`, `window_size`)\u2014`get_window_size` shrinks windows when needed, but you lose attention overlap if sizes are too small.^[</code>110:200:external_repos/swift/project/module/models/swin4d_transformer_ver7.py```]</p> <p>After projection, SwiFT embeddings (global pooled or CLS) can be concatenated or contrastively aligned with Evo\u202f2/GENERator/Caduceus projections for multimodal neurogenomics.</p>"},{"location":"code_walkthroughs/titan_walkthrough/","title":"TITAN Code Walkthrough","text":"<p>KB references: Model card (pending) \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/titan_walkthrough/#overview","title":"Overview","text":"<p>TITAN (Transformer-based pathology Image and Text Alignment Network) aggregates CONCH\u202fv1.5 patch embeddings into slide-level representations using a Transformer encoder aligned with pathology-report text via a CoCa-style captioning loss. The public release focuses on the slide &amp; text encoders (decoder weights removed) and ships Hugging Face <code>trust_remote_code</code> modules for <code>encode_slide_from_patch_features</code>, plus fine-tuning and evaluation scaffolding for tasks like TCGA-OT linear probes and zero-shot slide retrieval.^[<code>1:110:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>][<code>25:111:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>]</p>"},{"location":"code_walkthroughs/titan_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo CONCH\u202fv1.5 patch encoder \u2192 TITAN slide Transformer with learned spatial grids + vision-language alignment (CoCa-inspired).^[<code>9:110:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>] Hugging Face <code>MahmoodLab/TITAN</code> exposes 768-d slide embeddings; fine-tuning head typically 768\u2192num_classes MLP; training loop enables FP16/bfloat16 autocast + cosine sched.^[<code>25:50:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>][<code>137:199:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>] Pretrained on 335,645 WSIs + 182K real reports + 423K synthetic captions; patch grids derived from Level-0 coordinates with <code>patch_size_lv0</code> (512 @20\u00d7, 1024 @40\u00d7).^[<code>14:99:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>] <code>.h5</code> files containing <code>features</code> (N\u00d7768) and <code>coords</code> (N\u00d72) arrays, plus <code>patch_size_level0</code> attribute; sample downloads provided via Hugging Face hub.^[<code>82:99:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>] <code>finetune.py</code> (CustomSequential + cosine LR + GradScaler), <code>eval_linear_probe.py</code> (logistic regression), <code>titan/utils.py</code> (metrics/bootstrap), and TCGA config/prompts for zero-shot classification.^[<code>25:339:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>][<code>2:75:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/eval_linear_probe.py</code>][<code>13:154:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/utils.py</code>][<code>1:120:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/datasets/config_tcga-ot.yaml</code>] github.com/mahmoodlab/TITAN"},{"location":"code_walkthroughs/titan_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Install &amp; deps. Clone the repo, <code>conda create -n titan python=3.9</code>, activate, upgrade pip, then <code>pip install -e .</code> (installs PyTorch\u202f2.0.1, timm\u202f1.0.3, h5py, sklearn, transformers\u202f4.46).^[<code>25:42:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>][<code>5:18:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/setup.py</code>]</li> <li>Model access. Run <code>huggingface_hub.login()</code> before calling <code>AutoModel.from_pretrained(\"MahmoodLab/TITAN\", trust_remote_code=True)</code>; this downloads slide + text encoders as well as CONCH\u202fv1.5 patch encoder helpers.^[<code>43:59:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>]</li> <li>Feature extraction options. Either (a) use TRIDENT/CLAM pipelines for CONCH features or (b) load shared <code>.h5</code> demo files and call <code>model.encode_slide_from_patch_features(features, coords, patch_size_lv0)</code> directly; set <code>patch_size_lv0</code> to 1024 (40\u00d7) or 512 (20\u00d7) per slide metadata.^[<code>63:98:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>]</li> </ul>"},{"location":"code_walkthroughs/titan_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/titan_walkthrough/#custom-sequential-wrapper-titanfinetunepy","title":"Custom Sequential Wrapper (<code>titan/finetune.py</code>)","text":"<p>Fine-tuning wraps the frozen TITAN backbone with a lightweight MLP head. <code>CustomSequential</code> simply forwards tensor tuples into <code>encode_slide_from_patch_features</code>, then feeds slide embeddings into the classification head. <code>create_mlp</code> helps instantiate arbitrary hidden stacks.</p> <p>```25:50:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py class CustomSequential(nn.Module):     def init(self, model, mlp):         super(CustomSequential, self).init()         self.model = model         self.mlp = mlp</p> <pre><code>def forward(self, *args, **kwargs):\n    x = self.model.encode_slide_from_patch_features(*args, **kwargs)\n    x = self.mlp(x)\n    return x\n</code></pre> <p>def create_mlp(in_dim=None, hid_dims=[], act=nn.ReLU(), dropout=0.0, out_dim=None, end_with_fc=True):     layers = []     if len(hid_dims) &gt; 0:         for hid_dim in hid_dims:             layers.append(nn.Linear(in_dim, hid_dim))             layers.append(act)             layers.append(nn.Dropout(dropout))             in_dim = hid_dim     layers.append(nn.Linear(in_dim, out_dim))     if not end_with_fc:         layers.append(act)         layers.append(nn.Dropout(dropout))     mlp = nn.Sequential(*layers)     return mlp <pre><code>### Training Loop &amp; Scheduler (`titan/finetune.py`)\n`train` constructs two optimizer parameter groups (bias/LayerNorm vs. rest), applies cosine LR with warmup, leverages `torch.cuda.amp.GradScaler`, and evaluates on a validation loader with early stopping that tracks the best weights.\n\n```137:199:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py\n    model.train()\n    fp16_scaler = torch.cuda.amp.GradScaler()\n    step = 0\n    early_stopping = EarlyStopping(patience=2, verbose=True)\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        preds_all = []\n        targets_all = []\n        total_train_loss = 0\n        for features, coords, patch_size_lv0, label in tqdm(train_loader):\n            lr_scheduler(step)\n            features = features.to(device)\n            coords = coords.to(device)\n            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n                logits = model(features, coords, patch_size_lv0.to(device), **kwargs)\n                loss = loss_fn(logits, label.to(device))\n            fp16_scaler.scale(loss).backward()\n            fp16_scaler.step(optimizer)\n            fp16_scaler.update()\n            optimizer.zero_grad()\n\n            preds_all.append(logits.argmax(1).cpu().numpy())\n            targets_all.append(label.numpy())\n            step += 1\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        ...\n            tqdm.write(f\"epoch {epoch}, bacc: {np.round(bacc, 4):.4f}, bacc_val: {np.round(bacc_val, 4):.4f}, loss: {avg_train_loss:.4f}, val_loss: {avg_val_loss:.4f}\")\n            early_stopping(avg_val_loss, model)\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n</code></pre></p>"},{"location":"code_walkthroughs/titan_walkthrough/#linear-probe-evaluation-titaneval_linear_probepy","title":"Linear Probe Evaluation (<code>titan/eval_linear_probe.py</code>)","text":"<p>For frozen-feature experiments, <code>train_and_evaluate_logistic_regression_with_val</code> sweeps log-spaced <code>C</code>, fits <code>LogisticRegression</code>, and reports metrics (balanced accuracy, Cohen\u2019s \u03ba, AUROC) via shared utilities.</p> <p>```2:75:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/eval_linear_probe.py def train_and_evaluate_logistic_regression_with_val(train_data, train_labels, val_data, val_labels, test_data, test_labels, log_spaced_values=None, max_iter=500):     seed_torch(torch.device('cpu'), 0)</p> <pre><code>metric_dict = {\n    'bacc': 'balanced_accuracy',\n    'kappa': 'cohen_kappa_score',\n    'auroc': 'roc_auc_score',\n}\n\nif log_spaced_values is None:\n    log_spaced_values = np.logspace(np.log10(10e-6), np.log10(10e5), num=45)\n\nbest_score = -float('inf')\nbest_C = None\nlogistic_reg_final = None\nfor log2_coeff in tqdm(log_spaced_values, desc=\"Finding best C\"):\n    ...\n    logistic_reg = LogisticRegression(\n        C=1/log2_coeff,\n        fit_intercept=True,\n        max_iter=max_iter,\n        random_state=0,\n        solver=\"lbfgs\",\n    )\n    logistic_reg.fit(train_data, train_labels)\n    ...\neval_metrics = get_eval_metrics(test_labels, test_preds, test_probs, roc_kwargs=roc_kwargs)\n</code></pre> <p>``` </p>"},{"location":"code_walkthroughs/titan_walkthrough/#metrics-bootstrap-zero-shot-templates-titanutilspy","title":"Metrics, Bootstrap &amp; Zero-Shot Templates (<code>titan/utils.py</code>)","text":"<p><code>get_eval_metrics</code> reports accuracy/balanced accuracy/kappa/weighted F1 (+ AUROC/log-loss when probs are provided). The module also seeds reproducibility, merges dictionaries, and defines zero-shot text templates for class prompts.  ```13:89:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/utils.py</p>"},{"location":"code_walkthroughs/titan_walkthrough/#zeroshot-prompt-templates","title":"zeroshot prompt templates","text":"<p>TEMPLATES = [     \"CLASSNAME.\",     \"an image of CLASSNAME.\",     ...     \"CLASSNAME is identified.\", ]  def get_eval_metrics(     targets_all: Union[List[int], np.ndarray],     preds_all: Union[List[int], np.ndarray],     probs_all: Optional[Union[List[float], np.ndarray]] = None,     unique_classes: Optional[List[int]] = None,     get_report: bool = True,     prefix: str = \"\",     roc_kwargs: Dict[str, Any] = {}, ) -&gt; Dict[str, Any]:     unique_classes = unique_classes if unique_classes is not None else np.unique(targets_all)     bacc = balanced_accuracy_score(targets_all, preds_all) if len(targets_all) &gt; 1 else 0     kappa = cohen_kappa_score(targets_all, preds_all, weights=\"quadratic\")     nw_kappa = cohen_kappa_score(targets_all, preds_all, weights=\"linear\")     acc = accuracy_score(targets_all, preds_all)     cls_rep = classification_report(targets_all, preds_all, output_dict=True, zero_division=0, labels=unique_classes)      eval_metrics = {         f\"{prefix}/acc\": acc,         f\"{prefix}/bacc\": bacc,         f\"{prefix}/kappa\": kappa,         f\"{prefix}/nw_kappa\": nw_kappa,         f\"{prefix}/weighted_f1\": cls_rep[\"weighted avg\"][\"f1-score\"],     } ```</p>"},{"location":"code_walkthroughs/titan_walkthrough/#tcga-ot-configuration-prompts-datasetsconfig_tcga-otyaml","title":"TCGA-OT Configuration &amp; Prompts (<code>datasets/config_tcga-ot.yaml</code>)","text":"<p>The YAML describes label counts, OncoTree codes, class-specific textual prompts (supporting zero-shot CLIP-like scoring), and dataset metadata. Integrate these prompts with TITAN\u2019s text encoder or other VLMs for retrieval tasks.</p> <p><code>1:60:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/datasets/config_tcga-ot.yaml aggregation: slide cohorts: - TCGA ext_cohorts: [] folds: 1 label_dict:   AASTR: 0   ACC: 1   AOAST: 2   ASTR: 3   BLCA: 4   CCRCC: 5   CESC: 6   CHRCC: 7   COAD: 8   DDLS: 9   DSTAD: 10   ESCA: 11   ESCC: 12   GBM: 13 ... prompts:   AASTR:   - anaplastic astrocytoma   - astrocytoma, anaplastic   - grade III astrocytoma   - AASTR   ACC:   - adrenocortical carcinoma   - adrenal cortical carcinoma   - adrenal cortex carcinoma   - ACC</code></p>"},{"location":"code_walkthroughs/titan_walkthrough/#integration-hooks-pathology-multimodal-kb","title":"Integration Hooks (Pathology \u2194 Multimodal KB)","text":"<ul> <li>Slide embeddings as shared latent vectors. <code>encode_slide_from_patch_features</code> returns 768-d tensors suitable for concatenation with genetic or clinical embeddings before populating KB integration cards; store patch grids alongside metadata for reproducibility.^[<code>25:99:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>]</li> <li>Prompt templates for zero-shot mapping. The <code>TEMPLATES</code> list and TCGA label prompts can seed cross-modal retrieval experiments or provide natural-language anchors for other models (e.g., LLaVA-Med) to align with TITAN outputs.^[<code>13:39:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/utils.py</code>][<code>1:120:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/datasets/config_tcga-ot.yaml</code>]</li> <li>Metrics + bootstrap interoperability. Use <code>get_eval_metrics</code> / <code>bootstrap</code> outputs to populate KB evaluation summaries, ensuring consistent confidence intervals across modalities when comparing TITAN features against alternative encoders.^[<code>13:154:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/utils.py</code>]</li> </ul>"},{"location":"data/governance_qc/","title":"Governance &amp; QC","text":"<ul> <li>Eligibility criteria (UKB consent, imaging + genetics availability)</li> <li>Exclusion rules (withdrawn, major QC flags, incomplete covariates)</li> <li>Missingness tracking per table + imputation policy</li> <li>Harmonization notes for imaging, genetics, and covariates</li> <li>PII handling and secure storage (encrypted buckets, audit logging)</li> <li>Data security (role-based access, key rotation, least privilege)</li> </ul>"},{"location":"data/schemas/","title":"Data schemas","text":""},{"location":"data/schemas/#genetics_embeddingsparquet","title":"genetics_embeddings.parquet","text":"<ul> <li><code>eid</code></li> <li><code>embedding_dim</code></li> <li><code>source_model</code></li> <li><code>layer</code></li> <li><code>vector</code></li> </ul>"},{"location":"data/schemas/#brain_idpsparquet","title":"brain_idps.parquet","text":"<ul> <li><code>eid</code></li> <li><code>site</code></li> <li><code>modality</code> (sMRI or fMRI)</li> <li>Selected IDPs:</li> <li>sMRI: FreeSurfer 7.x <code>aparc.stats</code> (cortical thickness, ~68 regions) + <code>aseg.stats</code> (subcortical volumes, ~40 structures) + surface area \u2192 ~176 features</li> <li>fMRI: Parcel-wise BOLD statistics (mean, variance), connectivity matrices (optional), or direct FM embeddings (BrainLM, Brain-JEPA)</li> <li>Confounds:</li> <li><code>intracranial_volume</code> (sMRI)</li> <li><code>mean_fd</code> (mean framewise displacement, fMRI)</li> <li><code>tsnr</code> (temporal SNR, fMRI)</li> <li><code>euler_number</code> (FreeSurfer QC metric, sMRI)</li> </ul>"},{"location":"data/schemas/#participantsparquet","title":"participants.parquet","text":"<ul> <li><code>eid</code></li> <li><code>age</code></li> <li><code>sex</code></li> <li><code>income_bin</code></li> <li><code>pcs_1</code>..<code>pcs_10</code></li> <li><code>site</code></li> <li><code>mdd_label</code></li> </ul>"},{"location":"data/schemas/#splitsjson","title":"splits.json","text":"<ul> <li><code>fold_id</code></li> <li><code>train</code> / <code>val</code> / <code>test</code> EID lists</li> <li><code>seed</code></li> <li><code>created_at</code></li> </ul> <p>Validation: reference <code>scripts/validate_schemas.py</code>.</p>"},{"location":"data/subject_keys/","title":"Subject key schemas","text":""},{"location":"data/subject_keys/#overview","title":"Overview","text":"<p>This page documents how subject IDs are handled across cohorts (UKB, GARD, CHA, etc.), and clarifies that raw identifiers and mapping tables never live in this repository. Only schemas and conventions are stored here.</p>"},{"location":"data/subject_keys/#uk-biobank-ukb","title":"UK Biobank (UKB)","text":"<ul> <li>Primary ID: <code>eid</code></li> <li>Used as the join key across:<ul> <li>genetics/WES tables (<code>ukb_wes.yaml</code>)</li> <li>sMRI features (<code>ukb_smri_freesurfer.yaml</code>)</li> <li>fMRI tensors (<code>ukb_fmri_tensor.yaml</code>)</li> <li>manifest (<code>ukb_manifest_stub.yaml</code>)</li> </ul> </li> <li>Derived IDs:</li> <li>Analysis repos may define hashed IDs (e.g., <code>ukb_hash_id = SHA256(eid + salt)</code>), but the mapping from <code>eid</code> to     hash must remain in secure infrastructure, not in this KB.</li> </ul>"},{"location":"data/subject_keys/#cha-developmental-cohort","title":"CHA developmental cohort","text":"<ul> <li>Primary ID: <code>subject_id</code> (local, non-PHI)</li> <li>See <code>cha_dev_longitudinal.yaml</code> \u2192 <code>keys_and_linkage.subject_id_schema</code>.</li> <li>Raw hospital IDs (<code>cha_ehr_id</code>) are stored only in secure mapping tables.</li> <li>Time axis:</li> <li>Represented as <code>months_from_birth</code> (or equivalent) in analysis code; DOB remains PHI and is not stored here.</li> </ul>"},{"location":"data/subject_keys/#cross-cohort-links-conceptual-only","title":"Cross-cohort links (conceptual only)","text":"<ul> <li>GARD / CRIS / other cohorts:</li> <li>If future projects link UKB, GARD, CHA, etc., the actual link tables must remain in a governed environment.</li> <li>This KB can store:<ul> <li>field names (e.g., <code>gard_id</code>, <code>cha_id</code>),</li> <li>high-level descriptions of linkage methods (e.g., trusted third-party linkage),</li> <li>but never the mapping content itself.</li> </ul> </li> </ul>"},{"location":"data/subject_keys/#policy","title":"Policy","text":"<ul> <li>Subject identifiers in this repo must be:</li> <li>non-PHI (no direct MRNs, names, or obvious re-identifiers),</li> <li>schema-level only (field names and usage, not values),</li> <li>and, where hashing is used, only the hash schema is documented, not salts or mapping tables.</li> <li>All actual mapping tables live in:</li> <li>secure institutional storage (e.g., hospital servers, controlled lab disks),</li> <li>under DRB/IRB-approved data governance,</li> <li>and are referenced in this KB only by description.</li> </ul>"},{"location":"data/ukb_data_map/","title":"UKB data map","text":"<p>Field ID placeholders (verify in extract): - Age: 21022 / 21003 - Sex: 31 - Income: 738 - Assessment center / site: 21054 or 54 - Genetics PCs: 22009 (pcs_1..10) - Imaging-derived phenotypes + confounds: FMRIB tables</p> <p>Note: Instances follow UKB suffixes (e.g., <code>-0.0</code> baseline vs <code>-2.0</code> repeat imaging). Capture instance + visit metadata in schemas.</p>"},{"location":"decisions/2025-11-integration-plan/","title":"Integration Baseline Plan","text":"<p>Where each step came from (paper \u2192 inference \u2192 our plan)</p> <ul> <li>Principle: Prefer late integration first under heterogeneous semantics.</li> <li>Sources: Ensemble Integration (Li et al. 2022), Oncology multimodal review (2024).</li> <li>Inference: Preserve modality-specific signal; avoid premature joint spaces.</li> <li> <p>Plan: Concatenate compact per-modality features; train LR and GBDT baselines.</p> </li> <li> <p>Robustness and evaluation discipline.</p> </li> <li>Sources: Oncology review; BrainLM/Brain-JEPA/Harmony practices.</li> <li> <p>Plan: Z-score + residualize per feature vs covariates; same CV folds; AUROC/AUPRC with CIs; DeLong/bootstrap for differences.</p> </li> <li> <p>CCA + permutation and partial correlations before heavy fusion.</p> </li> <li>Sources: Review guidance; neuro CCA tradition.</li> <li> <p>Plan: CCA on residualized, standardized inputs; 1,000 permutations; partial correlations/logistic with covariates.</p> </li> <li> <p>Modality sequencing.</p> </li> <li>Sources: Harmony, SwiFT, BrainLM/JEPA.</li> <li> <p>Plan: Start with sMRI ROIs; add fMRI as FC vectors; later consider brain FMs.</p> </li> <li> <p>Genetics embedding hygiene and attribution.</p> </li> <li>Sources: Caduceus (RC-equivariance), DNABERT-2, GENERator; BIOKDD'25 LOGO.</li> <li>Plan: RC-average; deterministic tokenization; LOGO \u0394AUC with Wilcoxon + FDR.</li> </ul> <p>Escalation</p> <ul> <li>If late fusion adds value: two-tower contrastive (frozen encoders, small projectors, InfoNCE), EI stacking, hub tokens/TAPE if TR/site heterogeneity dominates.</li> </ul>"},{"location":"decisions/dev_validation_plan/","title":"Developmental cohort validation plan (CHA)","text":""},{"location":"decisions/dev_validation_plan/#goal","title":"Goal","text":"<p>Define a prospective validation strategy for the CHA developmental cohort so that future models (brain-only, gene\u2013brain\u2013behaviour, BOM-aligned LLM/VLM) are evaluated on truly held-out data.</p>"},{"location":"decisions/dev_validation_plan/#high-level-plan","title":"High-level plan","text":"<ul> <li>Retrospective training window: initial waves (e.g., years 1\u2013N of data collection).</li> <li>Prospective validation window: later-enrolled participants and/or later waves (e.g., year N+1 onward).</li> <li>Constraints:</li> <li>Preserve age and diagnosis distributions between train and validation where possible.</li> <li>Avoid leakage across siblings or repeated visits when defining subject-level splits.</li> </ul>"},{"location":"decisions/dev_validation_plan/#suggested-splits-to-refine-once-data-are-available","title":"Suggested splits (to refine once data are available)","text":"<ol> <li>Temporal split</li> <li>Train:<ul> <li>All subjects enrolled up to a cutoff date (e.g., end of 2027).</li> </ul> </li> <li>Prospective test:<ul> <li>Subjects enrolled after the cutoff date (e.g., 2028+).</li> </ul> </li> <li> <p>Rationale:</p> <ul> <li>Mimics real-world deployment where models trained on early waves are applied to future patients.</li> </ul> </li> <li> <p>Wave-aware cross-validation</p> </li> <li>Within the training period, use wave-aware CV (e.g., group by subject ID) so that:<ul> <li>Multiple visits for the same child never appear in both train and validation folds.</li> </ul> </li> <li> <p>Target variables:</p> <ul> <li>diagnostic status (ASD/ADHD/TD/etc.),</li> <li>cognitive and adaptive trajectories.</li> </ul> </li> <li> <p>Documentation in configs</p> </li> <li>Prospective split metadata should be referenced from:<ul> <li><code>kb/datasets/cha_dev_longitudinal.yaml</code> (manifest fields once defined).</li> <li><code>configs/experiments/dev_*.yaml</code> (explicit <code>train_period</code> / <code>test_period</code> notes).</li> </ul> </li> </ol>"},{"location":"decisions/dev_validation_plan/#what-we-can-do-now-without-data","title":"What we can do now (without data)","text":"<ul> <li>Fix the conceptual split strategy in this document.</li> <li>Ensure all CHA experiment templates:</li> <li>use non-leaky CV (<code>group_by: subject_id</code> where applicable),</li> <li>are written assuming a future prospective test set.</li> <li>Once the cohort is finalized:</li> <li>add concrete sample sizes and calendar cutoffs to this file,</li> <li>and register a canonical manifest strategy in <code>ukb_manifest_stub.yaml</code>-style metadata for CHA.</li> </ul>"},{"location":"generated/kb_curated/","title":"AI Summaries \u2192 KB Cards","text":"<p>Use the PDF\u21c4MD repo (<code>~/Projects/pdf&lt;-&gt;md;ai-summaries/</code>) for extraction + summarization, then land the curated assets back in this KB using the three-layer structure below.</p>"},{"location":"generated/kb_curated/#storage-layout","title":"Storage layout","text":"<pre><code>docs/generated/kb_curated/\n\u251c\u2500\u2500 integration_cards/        # short actionable guidance (already in use)\n\u251c\u2500\u2500 papers-pdf/               # Layer 2 source PDFs copied in for reference\n\u251c\u2500\u2500 papers-md/                # Layer 2 technical notebooks (medium MD)\n\u2514\u2500\u2500 templates/                # helper templates (shared)\n</code></pre> <ul> <li>Layer 1 (Citation &amp; context, short) \u2192 <code>kb/paper_cards/&lt;slug&gt;_YYYY.yaml</code></li> <li>Layer 2 (Technical notebook, medium) \u2192 <code>docs/generated/kb_curated/papers-md/&lt;slug&gt;.md</code></li> <li>Layer 2 assets (PDFs) \u2192 <code>docs/generated/kb_curated/papers-pdf/&lt;slug&gt;.pdf</code></li> <li>Layer 3 (Hooks into KB) \u2192 bottom section of each Layer 2 MD + existing <code>integration_cards/*.md</code></li> </ul>"},{"location":"generated/kb_curated/#workflow","title":"Workflow","text":"<ol> <li>Convert &amp; summarize outside the KB</li> <li><code>pdf_to_markdown.py input/*.pdf \u2192 build/&lt;slug&gt;.md</code></li> <li><code>summary_generator.py build/&lt;slug&gt;.md \u2192 build/&lt;slug&gt;_summary.md</code></li> <li>Copy curated assets into this repo</li> <li><code>cp ~/Projects/pdf&lt;-&gt;md;ai-summaries/input/&lt;slug&gt;.pdf docs/generated/kb_curated/papers-pdf/</code></li> <li><code>cp ~/Projects/pdf&lt;-&gt;md;ai-summaries/build/&lt;slug&gt;_summary.md docs/generated/kb_curated/papers-md/&lt;slug&gt;.md</code></li> <li>Fill the Layer 2 template</li> <li>Follow <code>docs/generated/kb_curated/papers-md/template.md</code> for sections (problem/tasks, datasets, methods, results, limitations, hooks).</li> <li>Update the Layer 1 YAML card</li> <li>Keep summaries to 3\u20135 sentences, set <code>summary_md_path</code> and <code>local_pdf_path</code>, and list every KB doc/config the paper informs.</li> <li>Regenerate/extend guidance cards if needed</li> <li>For integration principles (e.g., Li 2022, Waqas 2024) update <code>integration_cards/*.md</code> with any new practices captured in the Layer 2 notes.</li> </ol>"},{"location":"generated/kb_curated/#why-this-matters","title":"Why this matters","text":"<ul> <li>Keeps PDFs + long notes co-located with the KB for future RAG/indexing.</li> <li>Ensures YAML cards stay concise and machine-friendly while MDs hold the richer narrative + explicit \u201chooks into our KB\u201d.</li> <li>Makes it trivial to trace a config (e.g., <code>configs/experiments/02_prediction_baselines.yaml</code>) back to the exact papers + notes that motivated each requirement.</li> </ul>"},{"location":"generated/kb_curated/#original-source-index","title":"Original source index","text":"<p>All PDFs/MDs under <code>docs/generated/kb_curated/</code> mirror the outputs generated in <code>../pdf&lt;-&gt;md;ai-summaries/outputs</code>. Use the table below to jump from a KB summary back to the published source.</p> Reference KB assets Original publication TabPFN (Nature 2025) \u2014 (summary pending) Nature article \u00b7 PriorLabs/TabPFN Brain Harmony (2025) PDF \u00b7 MD arXiv:2509.24693 Brain-JEPA (2024) PDF \u00b7 MD arXiv:2409.19407 BrainLM (2024) PDF \u00b7 MD OpenReview RwI7ZEfR27 BrainMT (2025) PDF \u00b7 MD LNCS DOI 10.1007/978-3-032-05162-2_15 Caduceus (2024) PDF \u00b7 MD arXiv:2403.03234 Evo 2 (2025) PDF \u00b7 MD bioRxiv 10.1101/2025.02.18.638918 GENERaTOR (2024) PDF \u00b7 MD arXiv:2502.07272 Me-LLaMA medical LLMs (2025) PDF \u00b7 MD npj Digital Medicine 8, 141 (2025) Multimodal LLMs in radiology (KJR 2025) PDF \u00b7 MD Korean Journal of Radiology, DOI 10.3348/kjr.2025.0599 Medical MMFMs in diagnosis &amp; treatment (2025) PDF \u00b7 MD Artificial Intelligence in Medicine, ISSN S0933365725002003 Foundation models for advancing healthcare (2024) PDF \u00b7 MD npj Digital Medicine (2024) TITAN pathology FM (2025) PDF \u00b7 MD Nature Medicine 31, 1\u201313 (2025) Ensemble Integration (Li 2022) PDF \u00b7 MD Bioinformatics Advances 2022 (late-integration EI framework) Oncology multimodal review (2024) PDF \u00b7 MD PubMed 39118787 GWAS in diverse populations (2019) PDF \u00b7 MD PubMed 36158455 PRS guide (2019) PDF \u00b7 MD PubMed 31607513 Yoon et al. BIOKDD'25 (MDD embeddings) PDF \u00b7 MD bioRxiv 10.1101/2025.02.18.638918 Mixture-of-Transformers (2025) PDF \u00b7 MD arXiv:2411.04996 BAGEL unified multimodal FM (2025) PDF \u00b7 MD arXiv:2505.14683"},{"location":"generated/kb_curated/integration_cards/ensemble_integration/","title":"Ensemble Integration (EI)","text":"<p>Problem it solves - Robust multimodal integration under heterogeneous semantics with interpretable ensembles.</p> <p>Core mechanics - Train strong per-modality learners with diverse inductive biases (LR, trees, SVM). - Late fusion via stacking/ensemble selection; rank-based interpretation available.</p> <p>When to use - Heterogeneous feature spaces; small-to-medium N; missing data.</p> <p>Adoption in our pipeline - LR + LightGBM/CatBoost per modality and concatenated; optional stacking after baselines succeed.</p> <p>Caveats - Stacking must be fold-proper to avoid leakage; prefer simple meta-learners.</p> <p>References - Li et al., 2022 (Bioinformatics Advances).</p>","tags":["integration","ensembles"]},{"location":"generated/kb_curated/integration_cards/oncology_multimodal_review/","title":"Oncology Multimodal Review \u2014 Principles","text":"<p>Key taxonomy - Early / intermediate / late fusion (trade-offs)</p> <p>Practical cautions - Heterogeneous semantics, alignment, missingness, over-smoothing (GNNs), confounds, leakage.</p> <p>Practices we adopt - Residualization, harmonization, fold sharing, AUROC/AUPRC, significance testing.</p> <p>Implications for our project - Start with late integration; deconfound per modality; only escalate when fusion gains are established.</p> <p>References - Waqas et al., 2024.</p>","tags":["integration","principles"]},{"location":"generated/kb_curated/integration_cards/template/","title":"Template","text":"<p>title: \"{{CARD_TITLE}}\" status: draft updated: {{DATE}} tags: [integration]</p>"},{"location":"generated/kb_curated/integration_cards/template/#card_title","title":"{{CARD_TITLE}}","text":"<p>Context and scope</p> <p>Key taxonomy (early/intermediate/late)</p> <p>Cautions (heterogeneous semantics, alignment, missingness, over-smoothing, leakage)</p> <p>Practices we adopt</p> <p>What we defer (and why)</p> <p>Direct implications for our project</p> <p>References</p>"},{"location":"generated/kb_curated/papers-md/bagel_2025/","title":"BAGEL: Emerging Properties in Unified Multimodal Pretraining","text":"<p>Authors: Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan Year: 2025 Venue: arXiv preprint</p>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Vision / VLM / Multimodal FM  </li> <li> <p>BAGEL is a unified multimodal foundation model that jointly supports multimodal understanding and generation over text, images, video, and web data.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Text (natural language).  </li> <li>Images (single and multi\u2011image).  </li> <li>Video clips.  </li> <li>Web and interleaved multimodal content (mixed text\u2013image\u2013video sequences).</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>BAGEL is an open\u2011source unified multimodal foundation model that learns from trillions of interleaved text, image, video, and web tokens to support both understanding and generation in a single architecture. The model builds on a decoder\u2011only transformer (initialized from Qwen2.5) and uses a Mixture\u2011of\u2011Transformer\u2011Experts (MoT) design with two experts: one specialized for multimodal understanding and the other for multimodal generation, both operating on a shared token sequence through common self\u2011attention. Visual inputs are handled by separate understanding\u2011 and generation\u2011oriented encoders (SigLIP\u2011style ViT and a FLUX\u2011derived VAE), while visual outputs are produced via diffusion\u2011style rectified flow conditioned on the transformer states. The authors curate large\u2011scale, reasoning\u2011oriented, interleaved multimodal data and introduce IntelligentBench, a new benchmark suite that better reveals emerging capabilities such as free\u2011form visual manipulation, 3D manipulation, and world navigation. BAGEL outperforms prior open\u2011source unified models on standard multimodal leaderboards and delivers image generation quality competitive with state\u2011of\u2011the\u2011art public generators. For a new grad student, BAGEL illustrates how large\u2011scale interleaved pretraining and unified architectures can close part of the gap between academic models and proprietary systems like GPT\u20114o.</p>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Develop an open\u2011source unified multimodal model that can both understand and generate across many modalities (text, images, video) and tasks (VQA, captioning, editing, navigation, etc.), approaching the capabilities of proprietary systems.  </li> <li> <p>Study the emerging properties that arise when scaling interleaved multimodal pretraining, especially complex compositional reasoning and world\u2011modeling abilities.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Data complexity: <ul> <li>High\u2011quality multimodal interleaved data is hard to source, clean, and structure; it must include conversations, instructions, generation tasks, and reasoning traces.  </li> <li>Video data is large but crucial for temporal and physical continuity; integrating it at scale is expensive.  </li> </ul> </li> <li>Architectural challenges: <ul> <li>Unified models must balance strong language reasoning with high\u2011fidelity visual generation, avoiding bottlenecks between understanding and generation.  </li> <li>Na\u00efvely combining autoregressive text and diffusion\u2011style image generation can be compute\u2011intensive and tricky to optimize.  </li> </ul> </li> <li>Evaluation gaps: <ul> <li>Standard benchmarks capture basic understanding and generation, but not more advanced capabilities like free\u2011form manipulation, multiview synthesis, or world navigation.  </li> </ul> </li> <li>Open\u2011source competitiveness: <ul> <li>Academic models historically trail proprietary systems (GPT\u20114o, Gemini 2.0) by a wide margin in unified multimodal settings.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data: </li> <li>Large\u2011scale interleaved multimodal corpus combining text, images, video, and web content.  </li> <li>Data format emphasizes sequences that mix modalities naturally (e.g., conversations with inline images or video frames, web pages with embedded media).  </li> <li> <p>Includes reasoning\u2011oriented content inspired by DeepSeek\u2011R1: explicit <code>&lt;think&gt;</code> segments, chain\u2011of\u2011thought style explanations, and multimodal reasoning traces.</p> </li> <li> <p>Modalities: </p> </li> <li>Text: questions, instructions, descriptions, and dialog.  </li> <li>Images: high\u2011resolution images for understanding and generation.  </li> <li>Video: multi\u2011frame clips providing temporal continuity.  </li> <li> <p>Web / interleaved content: structured pages and documents with embedded media.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Text is tokenized with the Qwen2.5 tokenizer.  </li> <li>Visual understanding uses a SigLIP2\u2011style ViT encoder (SigLIP2\u2011so400m/14) with NaViT support for native aspect ratios; outputs image tokens.  </li> <li>Visual generation uses a pretrained FLUX VAE to convert images to latent tokens (downsampled by 8\u00d7, 16 channels), followed by patch embedding to match the transformer\u2019s hidden size.  </li> <li>2D positional encodings and timestep embeddings are applied to vision tokens for diffusion\u2011style generation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Unified multimodal decoder\u2011only transformer with a Mixture\u2011of\u2011Transformer\u2011Experts (MoT) architecture: one expert for understanding, one for generation.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>BAGEL is a new open\u2011source unified multimodal foundation model that combines MoT, interleaved data, and diffusion\u2011style generation.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Backbone Qwen2.5 decoder\u2011only transformer with RMSNorm, SwiGLU, RoPE, GQA, QK\u2011Norm Experts Understanding expert + generation expert; both operate on the same token sequence via shared self\u2011attention Visual encoders SigLIP2\u2011based ViT (understanding) + FLUX VAE (generation) with patch embeddings Objectives Next\u2011token prediction for text tokens; rectified\u2011flow diffusion objectives for visual tokens Parameter scale 7B active parameters (14B total), with MoT structure over shared attention Data design Carefully curated, reasoning\u2011oriented, interleaved multimodal data, including video and web <ul> <li>Training setup (high level):</li> <li>Initialize from Qwen2.5 and train on trillions of interleaved multimodal tokens.  </li> <li>Use unified decoding: both understanding and generation tasks operate within the same transformer, with expert specialization rather than separate modules.  </li> <li>Visual generation uses Rectified Flow, following best practice in large diffusion transformers.  </li> <li>Training emphasizes scaling length (context) and variety (modalities, tasks) rather than narrow single\u2011task optimization.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Modalities integrated: </li> <li> <p>Text, images, and video, with web content as an additional multimodal source.</p> </li> <li> <p>How integration works: </p> </li> <li>All tokens\u2014text and visual\u2014are placed into a single long token sequence and processed with shared self\u2011attention in each transformer block.  </li> <li>Two experts (understanding and generation) share this sequence but have distinct parameters; both see the same context, enabling tight coupling between reasoning and generation.  </li> <li>Vision tokens come from either the SigLIP\u2011based ViT (for understanding) or the FLUX\u2011based VAE (for generation), but they are mapped into the same hidden space.  </li> <li> <p>The architecture is bottleneck\u2011free: there is no small connector layer compressing context between understanding and generation modules.</p> </li> <li> <p>Why this integration is useful / new capabilities: </p> </li> <li>Enables seamless transfer between understanding and generation: reasoning about input images/videos can directly inform generation within the same context.  </li> <li>Long\u2011context self\u2011attention over interleaved sequences supports complex tasks such as world navigation, future\u2011frame prediction, and multi\u2011step visual manipulation.  </li> <li>The MoT expert split allows specialization without losing the benefits of a unified transformer.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Benchmarks: </li> <li>Standard multimodal understanding benchmarks (e.g., VQA, captioning, visual reasoning tasks).  </li> <li>Image and video generation quality benchmarks (e.g., FID, CLIP\u2011score, human or preference evaluations) compared with open\u2011source generators like SD3 and FLUX.  </li> <li> <p>New IntelligentBench suite introduced by the authors, focusing on complex multimodal reasoning, free\u2011form manipulation, multiview synthesis, and world navigation.</p> </li> <li> <p>Baselines: </p> </li> <li>Open\u2011source unified multimodal models and strong VLMs.  </li> <li> <p>Public image and video generators (e.g., SD3, FLUX) for generative quality comparisons.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>BAGEL outperforms prior open\u2011source unified models on a broad range of multimodal understanding and generation benchmarks.  </li> <li>Image generation quality is competitive with leading public generators, particularly when conditioned on rich prompts and reasoning traces.  </li> <li>The model exhibits emerging abilities: free\u2011form visual manipulation guided by text and image context, multiview and 3D manipulation, and navigation\u2011style tasks involving world modeling.  </li> <li>Scaling interleaved multimodal pretraining reveals a progression: basic understanding/generation \u2192 advanced editing and manipulation \u2192 long\u2011context reasoning and compositional abilities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Provides a unified, open\u2011source alternative to proprietary multimodal systems, with strong performance on both understanding and generation.  </li> <li>Uses a bottleneck\u2011free MoT architecture that allows rich interaction between reasoning and generation within a single transformer.  </li> <li>Emphasizes interleaved multimodal data and reasoning\u2011oriented content, which appear crucial for emergent abilities.  </li> <li>Introduces IntelligentBench, which better surfaces advanced multimodal reasoning and manipulation capabilities.</li> </ul> <p>Limitations:</p> <ul> <li>Training requires huge amounts of data and compute, limiting reproducibility and accessibility for smaller labs.  </li> <li>Data curation details\u2014especially for web and video sources\u2014may affect biases and coverage; not all sources are fully public.  </li> <li>While open\u2011source, deployment may still require substantial GPU resources, particularly for long\u2011context, video\u2011heavy tasks.  </li> <li>The paper focuses more on performance and qualitative capabilities than on detailed safety, robustness, or bias analyses.</li> </ul> <p>Open questions and future directions:</p> <ol> <li>How can we make BAGEL\u2011style unified models more compute\u2011efficient, especially for real\u2011time or edge deployments?  </li> <li>What are the best ways to control and align such powerful multimodal generators, especially for safety\u2011critical or high\u2011stakes tasks?  </li> <li>Can similar MoT\u2011based unified architectures be extended to additional modalities (e.g., audio waveforms, 3D scenes, sensor data) without major redesign?  </li> <li>How should benchmarks evolve to better measure world\u2011modeling and navigation capabilities beyond current tasks?  </li> <li>What data governance and licensing practices are needed to responsibly scale interleaved multimodal corpora?</li> </ol>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>BAGEL is part of a new wave of unified multimodal foundation models that aim to close the performance gap with proprietary systems while remaining open\u2011source.  </li> <li>It extends integrated transformer\u2011diffusion architectures by adding expert specialization and large\u2011scale interleaved data.  </li> <li>Relation to well-known ideas: </li> <li>Combines ideas from decoder\u2011only LLMs, diffusion transformers, and MoT/MoE\u2011style expert architectures within a single framework.  </li> <li>The use of reasoning\u2011oriented pretraining echoes DeepSeek\u2011R1\u2011style chain\u2011of\u2011thought data, but extended to multimodal sequences.  </li> <li>Why this paper is a useful reference: </li> <li>For researchers building VLMs and multimodal FMs, BAGEL provides a blueprint for scaling unified models and for designing interleaved training data.  </li> <li>It also illustrates the importance of designing new benchmarks (IntelligentBench) that reveal capabilities not captured by traditional tasks.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Academic unified multimodal models lag far behind proprietary systems in both understanding and generation, especially for complex reasoning and world\u2011modeling tasks.</p> </li> <li> <p>Method / model: </p> </li> <li>BAGEL is a 7B\u2011active-parameter, MoT\u2011based unified multimodal model initialized from Qwen2.5, with separate encoders for visual understanding and generation and rectified\u2011flow diffusion for visual outputs.  </li> <li> <p>It is trained on large\u2011scale, reasoning\u2011oriented, interleaved multimodal data covering text, images, video, and web content.</p> </li> <li> <p>Results: </p> </li> <li>Outperforms state\u2011of\u2011the\u2011art open\u2011source unified models on standard multimodal benchmarks and delivers image quality competitive with leading public generators.  </li> <li> <p>Exhibits emerging abilities such as free\u2011form visual manipulation, multiview synthesis, and navigation\u2011style tasks, especially as pretraining scale increases.</p> </li> <li> <p>Why it matters: </p> </li> <li>Shows that open\u2011source unified multimodal FMs can approach proprietary systems when trained on rich interleaved data with carefully designed architectures.  </li> <li>Highlights the role of reasoning\u2011oriented multimodal data and bottleneck\u2011free architectures in enabling complex, world\u2011modeling capabilities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/","title":"Brain Harmony (2025)","text":""},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#brain-harmony-a-multimodal-foundation-model-unifying-morphology-and-function-into-1d-tokens","title":"Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens","text":"<p>Authors: Zijian Dong, Ruilin Li, Joanna Su Xian Chong, Niousha Dehestani, Yinghui Teng, Yi Lin, Zhizhou Li, Yichi Zhang, Yapei Xie, Leon Qi Rong Ooi, B.T. Thomas Yeo, Juan Helen Zhou Year: 2025 Venue: NeurIPS 2025</p>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM + Multimodal / Integration   Brain Harmony (BrainHarmonix) is a brain foundation model that jointly models structural MRI and functional MRI, explicitly targeting multimodal integration of brain morphology and dynamics.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration   The paper introduces a new multimodal brain FM architecture and training scheme, rather than applying an existing FM.</p> </li> <li> <p>Key Modalities: </p> </li> <li>T1-weighted structural MRI (cortical morphology).  </li> <li>Resting-state fMRI time series (functional dynamics, heterogeneous TRs).  </li> <li>Derived: geometric harmonics on cortical surface meshes, functional gradients, ROI-level (Schaefer-400) time series.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>Brain Harmony (BrainHarmonix) is a multimodal brain foundation model that learns a compact sequence of 1D \u201cbrain hub\u201d tokens summarizing both structural MRI and functional MRI of the human brain. The model first trains separate encoders for T1 anatomy and fMRI dynamics, then fuses their latent representations through shared hub tokens that are optimized to reconstruct both modalities, embodying the neuroscience idea that structure constrains function. To handle real-world fMRI data collected with different temporal resolutions (TRs), the authors introduce Temporal Adaptive Patch Embedding (TAPE), which adapts patch sizes and embedding filters so that each token always corresponds to a consistent time duration, and uses multi-TR downsampling as an effective data augmentation. They also incorporate geometric harmonics derived from cortical surfaces as position embeddings for fMRI, aligning functional representations with cortical geometry. Pretrained on large UK Biobank and ABCD datasets, BrainHarmonix is evaluated on six benchmarks (ASD, ADHD, PD, MCI, cognition) plus an Asian clinical cohort and consistently outperforms strong structure-only and function-only baselines, including prior brain FMs like BrainLM, Brain-JEPA, BrainMVP, and BrainMass. For a new grad student, this work is a concrete example of how to design a multimodal, large-scale brain FM that respects neuroscience principles, deals with heterogeneous acquisition, and yields versatile representations usable for many downstream tasks.</p>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<p>Scientific / practical problem: </p> <ul> <li>Learn a unified representation of brain structure and function that supports diverse downstream tasks (disease classification, cognition prediction) across datasets and sites.  </li> <li>Existing brain FMs either model only T1 structure or only fMRI dynamics, missing complementary information and known structure\u2013function relationships.  </li> <li>fMRI FMs usually assume a single TR, limiting their ability to leverage multiple datasets and protocols and ignoring important temporal details when forced to downsample.</li> </ul> <p>Why this is hard: </p> <ul> <li>High-dimensional, heterogeneous inputs: 3D T1 volumes and long fMRI time series are large and noisy, and come from multiple scanners, sites, and TRs.  </li> <li>Multimodal alignment: Structural and functional images differ in resolution and sampling; mapping them into a shared latent space requires careful design to avoid one modality dominating.  </li> <li>Physics and biology constraints: Neuroscience suggests functional waves are constrained by cortical geometry, but most models do not encode this.  </li> <li>Generalization and robustness: The model must work across age ranges (children vs adults), disorders, and acquisition protocols, while being resistant to motion and site artifacts.  </li> <li>Interpretability and clinical relevance: Representations should connect to known networks and clinical markers, not just achieve raw predictive performance.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>Pretraining data:</p> <ul> <li>UK Biobank (UKB): </li> <li>43,112 participants (ages 44\u201383).  </li> <li>46,455 T1-weighted MRI scans.  </li> <li>40,162 resting-state fMRI time series (TR = 0.735 s).  </li> <li> <p>fMRI downsampled to additional TRs of 1.47, 2.205, 2.94 s for multi-TR augmentation.</p> </li> <li> <p>ABCD: </p> </li> <li>11,221 children (8\u201311 years, baseline + 2-year follow-up).  </li> <li>18,139 T1-weighted images.  </li> <li> <p>30,771 resting-state fMRI time series (TR = 0.8 s) downsampled to TRs 1.6 and 2.4 s.</p> </li> <li> <p>Totals: </p> </li> <li>T1 pretraining: 64,594 images.  </li> <li>fMRI pretraining (after augmentation): 252,961 time series.  </li> <li>Multimodal fusion (T1\u2013fMRI pairs): 69,360 paired sessions.</li> </ul> <p>Downstream benchmarks:</p> <ul> <li>Neurodevelopmental disorders: </li> <li>ABIDE-I &amp; ABIDE-II: Autism Spectrum Disorder (ASD) vs controls (multi-site, heterogeneous TRs).  </li> <li> <p>ADHD-200: ADHD vs controls (multi-site, heterogeneous TRs).</p> </li> <li> <p>Neurodegenerative disorders and cognition: </p> </li> <li>PPMI: 4-way classification (controls, SWEDD, prodromal, PD).  </li> <li>ADNI: CN vs MCI classification.  </li> <li> <p>HCP-A: Regression of executive function (Flanker task).</p> </li> <li> <p>Additional cohort: </p> </li> <li>MACC (Asian clinical cohort): amyloid-positive vs negative classification.</li> </ul> <p>Preprocessing / representation:</p> <ul> <li>T1: skull-stripping (FreeSurfer), reorientation (FSL), registration to MNI152 (FLIRT), cropping, intensity normalization.  </li> <li>fMRI: motion correction, slice-timing correction, nuisance regression (global, white matter, CSF, motion), censoring high-motion frames, band-pass filtering, mapping to standard space or surface, then parcellation to 400 ROIs (Schaefer-400).  </li> <li>Final representation: T1 patch tokens; ROI-wise fMRI time series; geometric harmonics computed on a population-average cortical mesh.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>Model type: </p> <ul> <li>Transformer-based multimodal foundation model with:  </li> <li>A 3D Masked Autoencoder (MAE) for T1 (BrainHarmonix-S).  </li> <li>A JEPA-style fMRI dynamics encoder (BrainHarmonix-F) with geometric harmonics and TAPE.  </li> <li>A multimodal \u201cHarmonizer\u201d transformer using learnable 1D hub tokens as a shared bottleneck.</li> </ul> <p>New FM vs existing: </p> <ul> <li>BrainHarmonix is a new FM that:  </li> <li>Encodes structure\u2013function coupling via geometric harmonics.  </li> <li>Handles arbitrary TRs with TAPE.  </li> <li>Learns unified 1D hub-token representations that can reconstruct both modalities.</li> </ul> <p>Key components and innovations:</p> <ul> <li>BrainHarmonix-S (structure encoder): </li> <li>Backbone: ViT-B MAE on T1 volumes (patch size 16, ~1200 tokens).  </li> <li> <p>Objective: reconstruct masked patches, capturing rich cortical morphology.</p> </li> <li> <p>BrainHarmonix-F (functional encoder): </p> </li> <li>Backbone: ViT-B with JEPA-style masked prediction over fMRI tokens.  </li> <li>Geometric harmonics positional encoding: eigenmodes of Laplace\u2013Beltrami operator on a population-average cortical surface, downsampled to ROIs and linearly projected to positional embeddings.  </li> <li> <p>TAPE: define a canonical temporal window \u03c4, adjust patch size (k \\approx \\tau / \\text{TR}), resize embedding filters via pseudoinverse operations, pad shorter sequences with attention masks\u2014allowing consistent token semantics across TRs and enabling multi-TR augmentation.</p> </li> <li> <p>Multimodal fusion (Harmonizer + hub tokens): </p> </li> <li>Introduce NH learnable hub tokens, shared across all T1\u2013fMRI pairs.  </li> <li>Concatenate hub tokens with modality-specific latents and feed through a transformer; self-attention lets hub tokens aggregate cross-modal information.  </li> <li>Lightweight decoders map hub tokens back to structural and functional latent spaces; training minimizes reconstruction error for both modalities, yielding a compact, shared latent space.  </li> <li>For downstream tasks, average-pool hub tokens and pass through a simple projection head.</li> </ul> <p>Training setup:</p> <ul> <li>Encoders and Harmonizer use ViT-B backbones with FlashAttention; optimization via AdamW with cosine learning-rate and weight-decay schedules.  </li> <li>Pretraining runs on 8\u00d7H100 GPUs (80 GB), with fusion training (Harmonizer) taking ~10 hours for NH=128 tokens.  </li> <li>Downstream: encoders are frozen; only Harmonizer and a linear head are tuned or even just the linear head for linear probing.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>Multimodal integration in BrainHarmonix:</p> <ul> <li>Modalities integrated: T1 morphology, fMRI dynamics (multi-TR), and geometry-derived harmonic modes.  </li> <li>Integration strategy: </li> <li>Late fusion of latent representations: T1 and fMRI encoders are trained separately and then frozen, preserving modality-specific features.  </li> <li>Structure-informed functional encoding: geometric harmonics anchor fMRI ROI embeddings to cortical geometry, embedding structure\u2013function constraints directly into functional latents.  </li> <li>Hub-token bottleneck: learnable 1D tokens sit between modalities and are optimized to reconstruct both structural and functional latents, forming a compact joint embedding space.</li> </ul> <p>Why this integration is useful:</p> <ul> <li>Encodes the neuroscience principle that function follows structure, helping cross-subject and cross-dataset alignment.  </li> <li>Exploits complementary strengths: stable anatomical variation (atrophy, cortical thickness) from T1 plus dynamic network organization from fMRI.  </li> <li>TAPE + multi-TR augmentation make it practical to fuse data from many scanners and protocols, which is essential for large-scale multimodal foundation models.</li> </ul> <p>Relation to the integration baseline plan:</p> <ul> <li>The plan emphasizes late integration and preserving modality-specific signal; BrainHarmonix follows this by training separate unimodal encoders and fusing only at the level of latents via hub tokens, with reconstruction losses that discourage modality collapse.  </li> <li>The Harmonizer\u2019s learned joint embedding is analogous to a nonlinear CCA-style latent space, but constrained by physics-informed geometry, echoing the plan\u2019s suggestion to use CCA/structured methods before heavy fusion.  </li> <li>Evaluation practice (multiple datasets, consistent splits, reporting variance and significance, ablations on fusion components) aligns with the plan\u2019s focus on robustness and disciplined comparison.  </li> <li>Attention analyses reveal modality-specific and cross-modal hub tokens, consistent with the plan\u2019s goal of preserving heterogeneous modality-specific features while enabling cross-modal interactions.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>Tasks / benchmarks:</p> <ul> <li>ASD vs controls (ABIDE-I, ABIDE-II), ADHD vs controls (ADHD-200). +- 4-way PD-related classification (PPMI), CN vs MCI (ADNI), executive function regression (HCP-A).  </li> <li>Amyloid-positive vs negative classification in an Asian clinical cohort (MACC).</li> </ul> <p>Baselines:</p> <ul> <li>Structure-only: BrainMVP variants, BrainHarmonix-S.  </li> <li>Function-only: BrainNetCNN, BrainGNN, BrainNetTF, BrainMass, BrainLM, Brain-JEPA.  </li> <li>Ablations: BrainHarmonix-F, variants without geometric pre-alignment or without multi-TR augmentation, and models without multimodal fusion.</li> </ul> <p>Key findings (trends):</p> <ul> <li>Multimodal BrainHarmonix consistently matches or outperforms all baselines across neurodevelopmental and neurodegenerative tasks, often with statistically significant gains in accuracy and F1.  </li> <li>BrainHarmonix-F (functional-only) outperforms prior fMRI FMs (BrainLM, Brain-JEPA, BrainMass) and task-specific models, showing the value of multi-TR dynamics modeling and geometry-informed position embeddings.  </li> <li>BrainHarmonix-S is competitive with or better than BrainMVP despite not using multi-parametric MRI, thanks to larger T1 pretraining.  </li> <li>Multi-TR augmentation and geometric pre-alignment both yield consistent performance improvements; ablations confirm that removing either degrades results.  </li> <li>Scaling the number of hub tokens improves performance up to ~128\u2013256 tokens, after which gains plateau; even linear probing of BrainHarmonix yields state-of-the-art or competitive results.  </li> <li>On the MACC cohort, BrainHarmonix achieves the highest accuracy and F1, demonstrating promising cross-population generalization.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>First brain FM to jointly model structure and function with a principled multimodal architecture and physics-informed inductive biases.  </li> <li>Addresses a major practical barrier\u2014heterogeneous TRs\u2014via TAPE and multi-TR augmentation, enabling large-scale functional pretraining.  </li> <li>Learns compact, versatile hub-token representations that support strong linear probing and efficient fine-tuning across many tasks.  </li> <li>Thorough empirical validation with multiple public datasets, an independent clinical cohort, ablations, scaling studies, and interpretability analyses linking tokens to known networks and ASD-related patterns.</li> </ul> <p>Limitations:</p> <ul> <li>Pretraining demography is still limited (children and middle/older adults) and mostly Western datasets; infancy, early adulthood, and broader global populations are underrepresented.  </li> <li>Training requires substantial compute (8\u00d7H100 GPUs), making replication and extension difficult for small labs.  </li> <li>Only T1 and resting-state fMRI are modeled; other modalities (diffusion MRI, task fMRI, EEG/MEG, genetics, clinical variables) are not yet integrated.  </li> <li>Multimodal fusion can be sensitive to low-quality structural data (e.g., motion artifacts in ADHD-200 T1), which can degrade performance.  </li> <li>Despite some interpretability analyses, the mapping from token-level patterns to clinically actionable insights remains preliminary.</li> </ul> <p>Open questions / future directions:</p> <ol> <li>How to extend the hub-token framework to incorporate additional modalities (diffusion, task fMRI, genomic features) while preserving modality-specific signals in line with the integration baseline plan?  </li> <li>Can joint fine-tuning of unimodal encoders and Harmonizer (possibly via parameter-efficient adapters or prompts) further improve performance without prohibitive compute?  </li> <li>How does BrainHarmonix behave in low-data or few-shot clinical settings, and can light-weight adaptation strategies close the gap?  </li> <li>Can the learned structure\u2013function tokens support more mechanistic analyses, e.g., probing causal pathways, simulating interventions, or building \u201cdigital twin\u201d models of individual brains?  </li> <li>What safeguards and evaluation protocols are needed before using such multimodal FMs in clinical decision support, especially concerning bias, robustness, and privacy?</li> </ol>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Within brain FMs: BrainHarmonix extends the line of fMRI FMs (BrainLM, Brain-JEPA) and structural FMs (BrainMVP) by explicitly fusing structure and function, showing that multimodal foundation models can better capture brain organization and disease-relevant signals than unimodal models.  </li> <li>Analogy to general FMs: Conceptually, BrainHarmonix is like a CLIP-style or multimodal FM for the brain: it learns a shared token-based representation over multiple modalities (anatomy + dynamics), but with strong physics-informed priors (geometric harmonics) and careful handling of temporal sampling.  </li> <li>Relevance to multimodal integration generally: The architecture and training strategy\u2014separate encoders, late fusion via a compact bottleneck, physics-informed embeddings, robust multi-dataset evaluation\u2014provide a concrete template for integrating other modalities (e.g., genomics, cell imaging, clinical data) in line with the integration baseline plan.  </li> <li>For a grad student: This paper is a valuable reference on how to (1) scale brain FMs, (2) encode domain knowledge (geometry, TR constraints) into model design, and (3) evaluate multimodal models across heterogeneous datasets and populations.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: Existing brain foundation models typically handle either structural MRI or functional MRI, and struggle with heterogeneous TRs and multimodal integration, limiting their ability to capture holistic brain organization.  </li> <li>Model: BrainHarmonix introduces a multimodal brain FM with separate T1 and fMRI encoders, geometric harmonics-based positional encoding, TAPE for arbitrary TRs, and a Harmonizer transformer with learnable 1D hub tokens that form a compact joint representation.  </li> <li>Data: The model is pretrained on large UKB and ABCD datasets (tens of thousands of T1 and fMRI scans) and evaluated on six benchmark datasets plus an independent Asian clinical cohort.  </li> <li>Results: BrainHarmonix consistently outperforms structure-only and function-only baselines (including BrainLM, Brain-JEPA, BrainMVP, BrainMass) across ASD, ADHD, PD, MCI, and cognition tasks, with ablations confirming the benefits of geometric pre-alignment, multi-TR augmentation, and multimodal fusion.  </li> <li>Integration: The fusion strategy aligns with late integration principles in the integration baseline plan\u2014preserving modality-specific encoders, using a compact bottleneck for cross-modal alignment, and enforcing reconstruction of both modalities from shared tokens.  </li> <li>Significance: BrainHarmonix provides a strong template for future multimodal brain FMs and suggests how to design scalable, physics-informed integration architectures that are robust across datasets and acquisition protocols.  </li> <li>For future work: Extending the model to more modalities, broader populations, and low-resource settings, and deepening interpretability and clinical pathways, are key directions for the next generation of multimodal brain foundation models.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/","title":"Brain-JEPA (2024)","text":""},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#brain-jepa-brain-dynamics-foundation-model-with-gradient-positioning-and-spatiotemporal-masking","title":"Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking","text":"<p>Authors: Zijian Dong, Ruilin Li, Yilei Wu, Thuan Tinh Nguyen, Joanna Su Xian Chong, Fang Ji, Nathanael Ren Jie Tong, Christopher Li Hsian Chen, Juan Helen Zhou Year: 2024 (approx., based on arXiv:2409.19407 and NeurIPS 2024) Venue: NeurIPS 2024 (Brain-JEPA: Brain Dynamics Foundation Model)</p>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM. The paper develops a large self-supervised foundation model for fMRI time-series (resting-state fMRI across multiple cohorts) to support many downstream neuro-related tasks (demographics, traits, and disease prediction). It focuses entirely on brain activity recordings and their functional organization rather than genomics or generic multimodal integration.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. Brain-JEPA is a new brain dynamics foundation model that adapts the Joint-Embedding Predictive Architecture (JEPA) framework to fMRI, introduces a novel functional positional encoding (Brain Gradient Positioning), and a customized pretraining mask (Spatiotemporal Masking). The paper primarily proposes this new architecture and training scheme, then applies it to a broad suite of downstream tasks.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Resting-state fMRI BOLD time series (450 ROI-level time series, including cortical and subcortical regions).  </li> <li>Associated clinical / behavioral labels for downstream tasks (age, sex, cognitive scores, disease status), but the core model itself is pretrained purely on fMRI time series.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces Brain-JEPA, a large-scale brain dynamics foundation model that learns from resting-state fMRI time series using a Joint-Embedding Predictive Architecture (JEPA) instead of reconstructing raw signals. The authors argue that fMRI BOLD signals are noisy and sparsely informative, making direct reconstruction (as in the earlier BrainLM model) suboptimal, especially for off-the-shelf evaluations like linear probing. Brain-JEPA instead predicts latent representations of masked fMRI patches using a Vision Transformer (ViT) encoder, a JEPA-style predictor, and an exponential moving average (EMA) target encoder. Two core innovations tailor JEPA to brain data: Brain Gradient Positioning, which uses functional connectivity gradients to define a functional coordinate system for regions of interest (ROIs), and Spatiotemporal Masking, which structures the self-supervised prediction task across ROIs and timesteps to provide a strong inductive bias. Pretrained on large-scale UK Biobank fMRI, Brain-JEPA achieves state-of-the-art performance across a wide range of downstream tasks (demographics, cognitive traits, and disease classification) and generalizes well across ethnic groups and external datasets. For a new grad student, this paper is important as a blueprint for how to construct and pretrain foundation models for brain time series, and as a demonstration that latent prediction architectures plus principled positional encodings can yield robust, generalizable brain representations.</p>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Build a foundation model for brain dynamics that learns general-purpose representations of resting-state fMRI time series, which can be adapted to many downstream tasks: demographic prediction (age, sex), cognitive and personality traits (e.g., Neuroticism, Flanker score), and disease diagnosis/prognosis (e.g., Alzheimer\u2019s, mild cognitive impairment, amyloid positivity).  </li> <li>Move beyond task-specific fMRI models (e.g., BrainNetCNN, BrainGNN, Brain Network Transformer, SwiFT) that must be trained separately for each dataset/task and often fail to exploit large unlabeled fMRI repositories.  </li> <li> <p>Improve on BrainLM, the first fMRI foundation model that uses a masked autoencoder (MAE) to reconstruct masked BOLD time series patches, which performs well under heavy fine-tuning but has weaker off-the-shelf representations and limited evaluation across ethnicities.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Noisy, low-SNR data: fMRI BOLD signals are an indirect proxy for neural activity, influenced by physiological noise and scanner artifacts, with relatively low signal-to-noise ratio. Directly reconstructing all masked voxels or ROI time series can encourage modeling noise rather than meaningful structure.  </li> <li>Sparse, complex spatiotemporal structure: Unlike images (with local edges and dense spatial information), fMRI signals are sparse and distributed across ROIs and time, without clear local edges; reconstructive MAE losses can struggle to learn subtle patterns in such data.  </li> <li>Lack of natural spatial ordering: Transformers rely on positional embeddings, but there is no simple 1D order for ROIs across the 3D brain. Anatomical coordinates do not necessarily correspond to functional organization; spatially adjacent ROIs can have very different activity profiles.  </li> <li>Heterogeneous time-series patches: fMRI patches differ both in space (ROI location in functional networks) and time (brain states, tasks, intrinsic fluctuations). Masking and prediction strategies must reflect this heterogeneity to avoid trivial shortcuts and encourage robust learning.  </li> <li>Generalization across cohorts and ethnicities: Foundation models for clinical use must generalize beyond the pretraining cohort (e.g., UK Biobank, mostly Caucasian) to external datasets and different ethnic groups, which is challenging given domain shift in acquisition protocols and demographics.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used: </li> <li>UK Biobank (UKB): <ul> <li>Large-scale public dataset (resting-state fMRI) with 40,162 participants aged 44\u201383.  </li> <li>Used for self-supervised pretraining and internal downstream tasks (age and sex prediction); 80% for pretraining, 20% held-out for evaluation.  </li> </ul> </li> <li>HCP-Aging (Human Connectome Project \u2013 Aging): <ul> <li>656 healthy elderly participants with resting-state fMRI.  </li> <li>Used for external evaluation of demographics (age, sex) and behavioral traits (Neuroticism, Flanker score).  </li> </ul> </li> <li>ADNI (Alzheimer\u2019s Disease Neuroimaging Initiative): <ul> <li>Resting-state fMRI for 189 participants (NC vs. MCI classification) and 100 cognitively normal participants (amyloid positive vs. negative classification).  </li> </ul> </li> <li>MACC (Memory, Ageing and Cognition Centre; Asian cohort): <ul> <li>Resting-state fMRI for 539 participants, used for NC vs. MCI classification in Asian participants to test cross-ethnic generalization.  </li> </ul> </li> <li> <p>Additional external datasets (Appendix): </p> <ul> <li>OASIS-3: AD conversion prediction in MCI participants.  </li> <li>CamCAN: Depression diagnosis.</li> </ul> </li> <li> <p>Modalities: </p> </li> <li>Primary modality: Resting-state fMRI BOLD time series.  </li> <li> <p>Labels or metadata include age, sex, cognitive and personality scores (e.g., Neuroticism, Flanker), and disease/prognosis labels (NC vs. MCI, amyloid status, AD conversion, depression).</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Parcellation: All fMRI data is parcellated into n = 450 ROIs, using:  <ul> <li>Schaefer-400 atlas for cortical regions.  </li> <li>Tian-Scale III atlas for subcortical regions.  </li> </ul> </li> <li>Scaling: Robust scaling per ROI (subtract median, divide by interquartile range) across participants.  </li> <li>Temporal resolution alignment: <ul> <li>UKB and HCP-Aging: multiband acquisition (TR \u2248 0.735s).  </li> <li>ADNI and MACC: single-band acquisition (TR \u2248 2s).  </li> <li>Multi-band data are downsampled (stride 3) to align all datasets to TR \u2248 2s.  </li> </ul> </li> <li>Input size: Default model input is 450 \u00d7 160 (ROIs \u00d7 timesteps).  </li> <li>Patchify and shuffle: For JEPA pretraining, time series are patchified into temporal patches per ROI (e.g., 10 patches per ROI), with ROI shuffling and spatiotemporal partitioning into observation and target regions.</li> </ul> <p>If any fine-grained preprocessing details are missing in the main text, they are specified in Appendix B and table summaries, but the above captures the key design choices.</p>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li>Vision Transformer (ViT)-based JEPA model for fMRI time series, using a latent predictive architecture rather than reconstruction.  </li> <li>Core components:  <ul> <li>Observation encoder: ViT (ViT-Small, ViT-Base, or ViT-Large).  </li> <li>Target encoder: ViT with EMA-updated parameters (JEPA-style).  </li> <li>Predictor network: a narrower ViT that maps the observation representation to predicted target representations.  </li> </ul> </li> <li> <p>Architecturally, this is analogous to I-JEPA for images, but adapted to fMRI time series with specific choices for positional encoding and masking.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>New foundation model. Brain-JEPA is not simply an application of an existing FM; it adapts the JEPA framework to brain dynamics and introduces Brain Gradient Positioning and Spatiotemporal Masking as domain-specific innovations.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Component Description Observation encoder (f_\\theta) ViT backbone (ViT-S, ViT-B, or ViT-L), processes an observation block (subset of ROIs \u00d7 timesteps). Target encoder (f_\\theta) (EMA) Same architecture as observation encoder, parameters updated via Exponential Moving Average of (f_\\theta). Predictor (g_\\phi) Narrower ViT that takes observation representations and positional embeddings to predict target block embeddings. Brain Gradient Positioning Functional connectivity gradient-based spatial positional embedding that defines a functional coordinate system for ROIs. Temporal Positioning Standard sine\u2013cosine positional encoding applied along the time dimension for each ROI. Spatiotemporal Masking Partitioning of the input into Cross-ROI (\u03b1), Cross-Time (\u03b2), and Double-Cross (\u03b3) regions, plus overlapped sampling to control masking ratios. <ul> <li> <p>Brain Gradient Positioning (spatial positional encoding): </p> <ul> <li>Compute a non-negative affinity matrix (A(i,j)) from functional connectivity features (c_i, c_j) across ROIs using a cosine-based similarity.  </li> <li>Use diffusion maps to compute gradients (eigenvectors) that capture macroscale functional organization; each gradient is a dimension of a latent manifold.  </li> <li>Stack eigenvectors into a gradient matrix (G \\in \\mathbb{R}^{n \\times m}) (n ROIs, m gradient components; default m = 30).  </li> <li>Map (G) through a trainable linear layer to (\\hat{G} \\in \\mathbb{R}^{n \\times d/2}), where (d) is the ViT embedding dimension.  </li> <li>Combine (\\hat{G}) with temporal positional encoding (T \\in \\mathbb{R}^{n \\times d/2}) to get final positional embeddings (P = [T, \\hat{G}] \\in \\mathbb{R}^{n \\times d}).  </li> <li>This yields a functional coordinate system where distances reflect connectivity similarity, enabling the model to respect functional rather than purely anatomical proximity.</li> </ul> </li> <li> <p>Spatiotemporal Masking and targets: </p> <ul> <li>The fMRI input (after patchifying and ROI shuffling) is divided into an observation block and three non-overlapping regions for targets:  </li> <li>Cross-ROI (\u03b1): Same time range as observation but different ROIs \u2192 forces spatial generalization across ROIs.  </li> <li>Cross-Time (\u03b2): Same ROIs but different timestep patches \u2192 forces temporal forecasting/generalization.  </li> <li>Double-Cross (\u03b3): Different ROIs and timesteps \u2192 most challenging region, requiring generalization across space and time.  </li> <li>Sample K target blocks from each region (K = 1 in experiments), giving multiple prediction sub-tasks per sample.  </li> <li>Use overlapped sampling to flexibly adjust observation-to-input ratio and encourage diverse masking patterns.  </li> <li>Remove overlaps between observation and \u03b1/\u03b2 targets to ensure non-trivial prediction.  </li> <li>Loss is mean squared error in latent space between predicted target embeddings (\\hat{s}^r_y) and target encoder outputs (s^r_y), averaged over regions and blocks.</li> </ul> </li> <li> <p>Training setup (as far as available):</p> </li> <li> <p>Pretraining objective: </p> <ul> <li>Joint-Embedding Predictive Architecture (JEPA)-style latent prediction: </li> <li>Given observation encoding (s_x) and positional embeddings (P), predictor (g_\\phi) outputs (\\hat{s}^r_y = g_\\phi(s_x | P)) for each target region r; minimize (| \\hat{s}^r_y - s^r_y |_2^2) averaged over r and targets.  </li> <li>Crucially, the model predicts representations, not raw fMRI signals, which helps ignore noise and focus on more abstract features.</li> </ul> </li> <li> <p>Model scale: </p> <ul> <li>ViT-S (~22M parameters), ViT-B (~86M), ViT-L (~307M) for the observation encoder.  </li> <li>Predictors have matching architectures but shallower depth and smaller embedding dimensions (e.g., 6 layers with dim 192/384, etc.).  </li> <li>No [CLS] token in pretraining; for downstream evaluation, they use the target encoder and average pooling over patches to obtain global fMRI embeddings.</li> </ul> </li> <li> <p>Optimization and hyperparameters (pretraining): </p> <ul> <li>Optimizer: AdamW with carefully tuned weight decay and cosine scheduling.  </li> <li>Learning rate: warmup cosine schedule, peak LR around 1e-3 with warmup and final LR 1e-6; weight decay schedule from 0.04 to 0.4.  </li> <li>EMA momentum increases from 0.996 to 1.0.  </li> <li>Batch size: effective 4 GPUs \u00d7 8 gradient accumulation steps \u00d7 16 batch size.  </li> <li>Training epochs: 300 epochs for main ViT-B model.  </li> <li>Patch size p = 16 time points; gradient vector dimension m = 30.</li> </ul> </li> <li> <p>Downstream evaluation: </p> <ul> <li>Fine-tuning and linear probing use AdamW or LARS with cosine decay, 50 training epochs, and dataset-specific settings.  </li> <li>For linear probing, they add a BatchNorm layer before the linear head, following MAE practice, to stabilize learning.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Is the paper multimodal? </li> <li>Not in the sense of integrating multiple different data modalities. Brain-JEPA operates on a single modality\u2014resting-state fMRI time series. The \u201cmultimodality\u201d here is spatiotemporal within the fMRI itself (ROIs \u00d7 time), not multiple distinct biological modalities (like EEG + MRI or fMRI + genetics).  </li> <li> <p>The paper does, however, span multiple datasets and cohorts (UKB, HCP-Aging, ADNI, MACC, OASIS-3, CamCAN), which could be viewed as multi-site integration, but there is no explicit multimodal fusion mechanism.</p> </li> <li> <p>Relation to integration baseline plan: </p> </li> <li>The Integration Baseline Plan emphasizes late fusion of modality-specific features, careful covariate adjustment, CCA-based cross-modality exploration, and robustness-focused evaluation (e.g., standardized preprocessing, consistent CV folds, significance testing).  </li> <li>Brain-JEPA\u2019s design is conceptually aligned with the plan\u2019s priority to preserve modality-specific signals (here, fMRI) by learning a strong fMRI foundation model before any cross-modal integration. The model produces high-quality, compressed fMRI embeddings that could be plugged into late fusion schemes (e.g., concatenation with genomic or clinical features followed by logistic regression or GBDT).  </li> <li>The evaluation practices\u2014multiple independent runs, reporting means and standard deviations, and significance markers ()\u2014mirror the robustness and evaluation discipline* recommended in the plan (e.g., repeated runs, clear metrics, statistical testing).  </li> <li>Although the paper does not explicitly use CCA, partial correlations, or complex multimodal fusion, it provides the fMRI tower that future work could combine with genomic, behavioral, or structural MRI towers using late fusion or more advanced contrastive frameworks, directly aligning with the \u201cmodality sequencing\u201d and \u201cescalation\u201d steps of the plan.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Internal tasks on UKB (20% held-out): <ul> <li>Age prediction (regression; MSE and Pearson correlation).  </li> <li>Sex prediction (binary classification; accuracy and F1).  </li> </ul> </li> <li>External tasks on HCP-Aging: <ul> <li>Age prediction.  </li> <li>Sex prediction.  </li> <li>Neuroticism score prediction (personality trait).  </li> <li>Flanker task performance prediction (attention / inhibitory control).  </li> </ul> </li> <li>External tasks on ADNI and MACC: <ul> <li>NC vs. MCI classification (ADNI; Caucasian cohort).  </li> <li>Amyloid-positive vs. negative classification (ADNI).  </li> <li>NC vs. MCI classification in Asian participants (MACC).  </li> </ul> </li> <li> <p>Additional tasks (Appendix C): </p> <ul> <li>AD conversion prediction (OASIS-3; MCI participants).  </li> <li>Depression diagnosis (CamCAN).</li> </ul> </li> <li> <p>Baselines: </p> </li> <li>Task-specific fMRI models: <ul> <li>BrainNetCNN (CNN-based on connectivity matrices).  </li> <li>BrainGNN (graph neural network on ROI-level graphs).  </li> <li>Brain Network Transformer (BNT; transformer-based on connectivity).  </li> <li>SwiFT (Swin Transformer-based model for raw fMRI).  </li> </ul> </li> <li>Non-deep baselines: <ul> <li>SVM/SVR trained on connectivity features.  </li> </ul> </li> <li>fMRI foundation models and self-supervised baselines: <ul> <li>BrainLM (MAE-based fMRI foundation model, ViT-B backbone).  </li> <li>BrainMass (graph-based large-scale self-supervised model for brain networks).  </li> <li>CSM (text-like representation model for brain data).  </li> </ul> </li> <li> <p>Variants and ablations of Brain-JEPA:  </p> <ul> <li>Using anatomical locations or sine\u2013cosine spatial embeddings instead of Brain Gradient Positioning.  </li> <li>Using standard multi-block sampling instead of Spatiotemporal Masking.  </li> <li>Changing number of gradient components (3 vs. 30).  </li> <li>Removing the JEPA framework (i.e., MAE/BrainLM-like framework with contributions).</li> </ul> </li> <li> <p>Key findings (high-level trends):</p> </li> <li> <p>Overall performance: </p> <ul> <li>Brain-JEPA achieves state-of-the-art performance on almost all downstream tasks compared with task-specific models (BrainNetCNN, BrainGNN, BNT, SwiFT), non-deep baselines (SVM/SVR), and prior foundation models (BrainLM, BrainMass, CSM).  </li> <li>On UKB, Brain-JEPA substantially reduces age prediction error and improves correlation and sex classification metrics compared to BrainLM and others.  </li> <li>On HCP-Aging, Brain-JEPA shows higher Pearson correlation for age, higher accuracy/F1 for sex, and strong improvements for Neuroticism and Flanker, especially in personality and cognition, where it surpasses BrainLM by a notable margin.</li> </ul> </li> <li> <p>Disease diagnosis and prognosis: </p> <ul> <li>On ADNI and MACC, Brain-JEPA achieves top or near-top performance in NC vs. MCI, amyloid positivity, and NC vs. MCI (Asian) tasks.  </li> <li>Particularly, Brain-JEPA shows superior performance in NC vs. MCI classification for Asian participants, even though it was pretrained only on Caucasian UKB data, indicating strong cross-ethnic generalization.  </li> <li>On OASIS-3 and CamCAN, Brain-JEPA again outperforms many baselines for AD conversion and depression diagnosis tasks.</li> </ul> </li> <li> <p>Scaling with model size and dataset size: </p> <ul> <li>Performance increases consistently as model size grows from ViT-S to ViT-B to ViT-L, mirroring scaling laws seen in vision and language FMs.  </li> <li>Likewise, using larger fractions of the UKB pretraining data (25% \u2192 50% \u2192 75% \u2192 100%) yields monotonic improvements in downstream metrics, indicating that Brain-JEPA benefits from more data.</li> </ul> </li> <li> <p>Fine-tuning vs. linear probing: </p> <ul> <li>Brain-JEPA achieves strong performance under fine-tuning, but also shows excellent linear probing results, outperforming BrainLM in both regimes.  </li> <li>The performance drop from fine-tuning to linear probing is smaller for Brain-JEPA than for BrainLM, suggesting that Brain-JEPA learns more robust and semantically rich representations during pretraining.</li> </ul> </li> <li> <p>Ablation: Brain Gradient Positioning &amp; Spatiotemporal Masking: </p> <ul> <li>Replacing Brain Gradient Positioning with sine\u2013cosine or anatomical location embeddings hurts performance across age, sex, and NC vs. MCI tasks.  </li> <li>This indicates that gradient-based functional coordinates better capture the brain\u2019s functional architecture and benefit representation learning.  </li> <li>Replacing Spatiotemporal Masking with standard multi-block sampling leads to slower pretraining and lower peak performance; Brain-JEPA with Spatiotemporal Masking reaches or exceeds the ablated model\u2019s best performance in fewer epochs, emphasizing its efficient inductive bias.</li> </ul> </li> <li> <p>Interpretability: </p> <ul> <li>Analysis of self-attention patterns across canonical brain networks (DMN, control network, salience/ventral attention, limbic, etc.) in NC vs. MCI tasks shows that Brain-JEPA focuses on networks known to be implicated in cognitive impairment.  </li> <li>Attention distributions are consistent across Caucasian and Asian cohorts, suggesting that the model has learned robust, neurobiologically meaningful patterns.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths:</li> <li>Tailored JEPA architecture for brain dynamics: The model combines JEPA-style latent prediction with fMRI-specific positional encoding and masking, addressing the limitations of MAE-style reconstruction for noisy, low-SNR BOLD signals.  </li> <li>Principled functional positional encoding: Brain Gradient Positioning offers a compelling way to incorporate functional connectivity gradients into transformer positional embeddings, grounding the model in known macroscale brain organization.  </li> <li>Strong and broad empirical performance: Brain-JEPA achieves state-of-the-art results across many tasks, datasets, and ethnic groups, demonstrating both versatility (demographics, traits, multiple clinical tasks) and generalization.  </li> <li>Improved off-the-shelf representations: Superior linear probing performance and smaller fine-tuning\u2013to\u2013linear-probe gaps suggest that Brain-JEPA learns more semantic and robust representations than prior MAE-based fMRI FMs.  </li> <li> <p>Interpretable attention patterns: Network-level attention aligns with known neurobiological findings regarding cognitive impairment, providing a bridge between deep models and neuroscientific insights.</p> </li> <li> <p>Limitations:</p> </li> <li>Model and compute scale: The largest model used is ViT-L (~307M parameters); larger models (e.g., ViT-H) are not explored due to compute limits, leaving open how far scaling could push performance.  </li> <li>Dataset diversity for pretraining: Pretraining is primarily on UKB (mostly Caucasian), with external datasets used only for downstream evaluation. A more diverse, multi-ethnic, multi-site pretraining corpus might further improve robustness and fairness.  </li> <li>Single-modality focus: While the paper hints at future multimodal integration (e.g., MEG, EEG, structural MRI), the current model only handles resting-state fMRI, so it does not directly address multimodal fusion challenges.  </li> <li>Complexity of gradients and masking: Gradient computation, diffusion maps, and spatiotemporal masking introduce additional complexity and hyperparameters that may be non-trivial to implement and tune for new labs.  </li> <li> <p>Interpretability depth: Although some attention analyses are provided, rich causal or mechanistic interpretation of what the model has learned (e.g., specific ROIs and pathways) remains limited and could be extended.</p> </li> <li> <p>Open Questions and Future Directions:</p> </li> <li>How does scaling up the model size (e.g., ViT-H or mixture-of-experts) and pretraining data diversity (multi-site, multi-ethnic, multi-task) affect generalization and fairness across populations?  </li> <li>Can Brain-JEPA embeddings serve as the fMRI tower in a multimodal FM that integrates genetics, structural MRI, behavior, or clinical data via late fusion, contrastive objectives, or cross-attention, aligning with the integration baseline plan?  </li> <li>How robust are Brain Gradient Positioning and Spatiotemporal Masking to changes in parcellation schemes (e.g., different atlases, voxel-level modeling) or to task-based fMRI instead of resting-state data?  </li> <li>Can we design more interpretable JEPA objectives or probing methods that link latent representations to specific cognitive processes or disease mechanisms, beyond network-level attention summaries?  </li> <li>What are the implications of using JEPA-style latent prediction (rather than generative reconstruction) for identifying causal relationships or performing counterfactual reasoning in brain data?</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the FM landscape: </li> <li>Brain-JEPA sits in the emerging class of brain foundation models, analogous to how GPT-like models function for text and ViT/MAE/Jepa-like models for images. It extends the JEPA paradigm\u2014originally developed for 2D images\u2014to spatiotemporal fMRI dynamics, emphasizing latent prediction over pixel/voxel reconstruction.  </li> <li> <p>It can be seen as a counterpart to BrainLM (MAE-based fMRI FM), providing an alternative pretraining objective that prioritizes robust, abstract representations suitable for off-the-shelf use in diverse tasks.</p> </li> <li> <p>Relation to well-known ideas: </p> </li> <li>Conceptually, Brain-JEPA is like \u201cI-JEPA but for fMRI time series\u201d, with key adaptations for brain data. Instead of predicting missing pixels, it predicts representations of masked ROI-time patches.  </li> <li>Brain Gradient Positioning draws on the functional gradient literature in neuroscience, which uses diffusion maps to uncover macroscale cortical organization. This connects the model\u2019s positional encoding to a well-established neuro-scientific framework.  </li> <li> <p>Spatiotemporal Masking is akin to structured masking strategies in self-supervised learning (e.g., masked patches in MAE), but tailored to enforce meaningful fMRI forecasting and generalization across ROIs and timesteps.</p> </li> <li> <p>Why this paper is a useful reference: </p> </li> <li>For a new grad student interested in AI for neuroscience, this paper is a strong example of how to adapt modern self-supervised architectures (JEPA, ViT) to the constraints and opportunities of brain data.  </li> <li>It provides a concrete recipe for building a foundation model for fMRI, including data preprocessing, model architecture, pretraining objective, and extensive evaluation across multiple datasets and tasks.  </li> <li>It also showcases how to connect architectural choices to neuroscience concepts (e.g., functional gradients, network-level attention), which is crucial for interdisciplinary work and for gaining acceptance in the neuroscience community.  </li> <li>Finally, it provides a robust fMRI representation that fits naturally into the integration baseline plan as the brain modality component to be later fused with other modalities using late fusion, CCA, or contrastive learning.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li>Brain-JEPA aims to build a general-purpose foundation model for brain dynamics that can be adapted to many downstream tasks (demographics, traits, disease diagnosis/prognosis) using resting-state fMRI data.  </li> <li> <p>Existing task-specific models and MAE-based fMRI foundation models (like BrainLM) struggle with noisy BOLD signals, limited generalizability, and weaker off-the-shelf representations.</p> </li> <li> <p>Method / Model: </p> </li> <li>Brain-JEPA adopts a Joint-Embedding Predictive Architecture (JEPA): instead of reconstructing raw fMRI signals, it predicts latent representations of masked fMRI patches in a ViT-based architecture.  </li> <li>It introduces Brain Gradient Positioning, a functional embedding of ROIs derived from diffusion maps on functional connectivity, providing a brain-specific positional encoding that reflects macroscale functional organization.  </li> <li>It proposes Spatiotemporal Masking, which partitions fMRI patches into cross-ROI, cross-time, and double-cross regions and uses overlapped sampling to encourage generalization across space and time with a strong inductive bias.  </li> <li> <p>The model is pretrained on large UK Biobank data using ViT-S/B/L backbones, with EMA target encoder and predictor networks, and evaluated via fine-tuning and linear probing on multiple external datasets.</p> </li> <li> <p>Results: </p> </li> <li>Across UKB, HCP-Aging, ADNI, MACC, OASIS-3, and CamCAN, Brain-JEPA achieves state-of-the-art performance on age, sex, trait prediction, and multiple disease diagnosis/prognosis tasks, outperforming task-specific models, SVM/SVR, and prior fMRI FMs like BrainLM and BrainMass.  </li> <li>It scales well with model size and data size, with larger ViT backbones and more pretraining data yielding better performance.  </li> <li>Brain-JEPA shows strong linear probing performance and smaller gaps between fine-tuning and linear probing than BrainLM, indicating more robust and transferable representations.  </li> <li> <p>Ablation studies confirm that Brain Gradient Positioning and Spatiotemporal Masking are crucial for performance and training efficiency.</p> </li> <li> <p>Why it matters: </p> </li> <li>Brain-JEPA demonstrates that latent prediction architectures with neuroscience-informed positional encodings can produce powerful, generalizable representations of brain activity.  </li> <li>It provides a solid foundation for future multimodal integration where fMRI embeddings are combined with genetics, structural imaging, or behavioral data using late fusion or contrastive methods, aligning well with broader integration plans.  </li> <li>For students and researchers, Brain-JEPA is an influential reference on how to design, train, and evaluate brain foundation models at scale, bridging modern self-supervised ML techniques and contemporary neuroscience.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/","title":"BrainLM (2024)","text":""},{"location":"generated/kb_curated/papers-md/brainlm_2024/#brainlm-a-foundation-model-for-brain-activity-recordings","title":"BrainLM: A Foundation Model For Brain Activity Recordings","text":"<p>Authors: Josue Ortega Caro, Antonio H. de O. Fonseca, Syed A. Rizvi, Matteo Rosati, Christopher Averill, James L. Cross, Prateek Mittal, Emanuele Zappala, Rahul M. Dhodapkar, Chadi G. Abdallah, David van Dijk, et al. Year: 2024 Venue: ICLR (International Conference on Learning Representations)</p>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM. The paper develops a large foundation model specifically for functional MRI (fMRI) recordings, learning spatiotemporal representations of brain activity dynamics across the whole brain.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. BrainLM is introduced as a new foundation model architecture and pretraining scheme for fMRI, with subsequent fine-tuning and zero-shot applications.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Task-based and resting-state fMRI (BOLD time series) from large population cohorts (UK Biobank and Human Connectome Project).</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces BrainLM, a large transformer-based foundation model trained on 6,700 hours of fMRI recordings from over 77,000 scans. Instead of training separate models for each narrow decoding task, BrainLM is pretrained in a self-supervised masked-reconstruction fashion to learn general-purpose representations of whole-brain activity over time. After pretraining, the same model can be fine-tuned to predict clinical variables (age, neuroticism, PTSD, anxiety), forecast future brain states, and perform zero-shot inference such as discovering functional brain networks directly from attention patterns. The authors show that BrainLM generalizes across datasets, performing well both on held-out UK Biobank scans and on the independent Human Connectome Project cohort. They also demonstrate interpretable attention maps that align with known brain networks and clinical differences (e.g., depression severity). For a new grad student, this paper is a key example of how foundation model ideas from language and vision (e.g., masked autoencoders) can be adapted to neuroimaging, creating a versatile model that unifies many downstream tasks on fMRI data.</p>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>Build a single, general-purpose model of brain activity dynamics that can support many downstream tasks: predicting clinical variables, modeling brain networks, and forecasting future activity.</li> <li>Learn unsupervised representations from large-scale fMRI repositories rather than training separate, task-specific models on small datasets.</li> <li> <p>Capture the full spatiotemporal structure of fMRI signals across the brain, not just limited regions like the visual cortex.</p> </li> <li> <p>Why this is hard</p> </li> <li>High dimensionality and complexity: fMRI produces thousands of voxel or parcel time series, with complex dependencies across brain regions and time.</li> <li>Indirect and noisy signal: BOLD signals are an indirect measure of neural activity and can be hard to interpret.</li> <li>Limited labels: Many large fMRI datasets have rich time series but relatively few labels for specific tasks, making supervised training challenging.</li> <li>Task-specific models do not generalize well: Traditional supervised models (e.g., SVMs, small neural nets) are tuned to narrow tasks and do not transfer well to new datasets or objectives.</li> <li>Need for scalable training: To benefit from large repositories like UK Biobank and HCP, models must handle massive data and learn representations that scale with model size and data size.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>UK Biobank (UKB):<ul> <li>~76,296 task-based and resting-state fMRI recordings with associated medical records.</li> <li>Ages approximately 40\u201369; scanned on a Siemens 3T scanner at ~0.735 s temporal resolution.</li> <li>80% (61,038 recordings) used for training; 20% held out for testing.</li> </ul> </li> <li>Human Connectome Project (HCP):<ul> <li>1,002 high-quality fMRI recordings from healthy adults.</li> <li>~0.72 s temporal resolution; used entirely as an external evaluation cohort.</li> </ul> </li> <li> <p>In total, the training corpus spans 77,298 recordings and 6,700 hours of preprocessed fMRI.</p> </li> <li> <p>Modalities</p> </li> <li>Single modality: functional MRI (fMRI), representing whole-brain BOLD time series.</li> <li> <p>Both task and resting-state recordings are included.</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>Standard preprocessing: motion correction, normalization, temporal filtering, and ICA-based denoising.</li> <li>Brain parcellation into 424 regions (AAL-424 atlas), yielding 424-dimensional time series per scan.</li> <li>Time series sampled at ~1 Hz after preprocessing.</li> <li>Robust scaling: per-parcel median subtraction and division by interquartile range across subjects.</li> <li> <p>For model input:</p> <ul> <li>Random 200-timestep subsequences are extracted from each recording.</li> <li>Each parcel\u2019s 200-timestep sequence is split into patches of 20 time points, giving 10 patches per parcel.</li> <li>The resulting patches (conceptually 424 \u00d7 10) are treated as \"tokens\" via a learnable linear projection into 512-dimensional embeddings.</li> <li>The 424 \u00d7 200 window is also viewed as a 2D image (parcels \u00d7 time) with parcels ordered by Y-coordinate to preserve spatial locality.</li> </ul> </li> <li> <p>Missing details</p> </li> <li>Exact number of subjects, hardware details beyond scanner type, and some hyperparameters are referenced but not fully spelled out in the main extracted text (likely given in supplementary material).</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Masked autoencoder (MAE) based on a Transformer architecture.</li> <li> <p>Encoder\u2013decoder structure with self-attention blocks that operate on spatiotemporal tokens derived from parcel-time patches.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>New foundation model. The authors design BrainLM specifically for fMRI data, inspired by BERT and Vision Transformer (ViT) style masked modeling but adapted to 2D parcel\u00d7time structure.</p> </li> <li> <p>Key components and innovations</p> </li> </ul> Component Description Tokenization of fMRI patches 200-timestep windows split into 20-timestep patches per parcel; patches projected into 512-d embeddings. Spatiotemporal masking Random and future-timepoint masking at rates of 20%, 75%, or 90%, making the model reconstruct masked tokens. 2D \"image-like\" formulation Treats the 424-parcel \u00d7 200-timestep window as a 2D grid; parcels ordered by Y-coordinate to preserve spatial adjacency, enabling multi-parcel tokens and scalable encoding. Transformer encoder Processes only unmasked tokens, with 4 self-attention layers and 4 attention heads. Transformer decoder 2-layer decoder that takes both encoded visible tokens and masked tokens, then reconstructs the full input. Positional embeddings Learnable spatial and temporal embeddings added to token representations to encode parcel location and time. Latent CLS token A special token summarizing each sequence, later used for clinical variable prediction and visualization. <ul> <li>Training setup</li> <li>Objective: Minimize mean squared error (MSE) between original and reconstructed fMRI signals for masked patches (self-supervised reconstruction).</li> <li>Pretraining data: 6,700 hours of fMRI from UKB and HCP, using random 200-timestep subsequences.</li> <li>Optimization: Adam optimizer; 100 epochs; batch size 512.</li> <li>Scaling: Multiple model sizes (e.g., 13M, 111M, 650M parameters), with performance improving as both model size and dataset size increase.</li> <li>Downstream adaptation:<ul> <li>Add a 3-layer MLP head to the pretrained encoder for regression of clinical variables.</li> <li>Fine-tune on subsets of UKB data withheld from pretraining.</li> <li>For future state prediction, fine-tune the model to forecast the next 20 timesteps given 180 observed timesteps.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Is the paper multimodal?</li> <li>Not in the main experiments. BrainLM is trained and evaluated on single-modality fMRI data (task and rest). The core contribution is a unimodal foundation model for brain activity recordings.</li> <li> <p>However, the discussion explicitly points to multimodal extensions as future work, suggesting integration with EEG, MEG, and other brain-wise or even genomic information.</p> </li> <li> <p>Relation to integration baseline plan</p> </li> <li>The paper itself does not implement late fusion, CCA, or contrastive cross-modal alignment. It is focused on learning a strong single-modality encoder for fMRI.</li> <li>In terms of the integration baseline plan:<ul> <li>BrainLM can be seen as a per-modality encoder that could feed into a late fusion or stacking approach when combined with other modalities (e.g., structural MRI, genetics, clinical variables).</li> <li>Its robust self-supervised representations align with the plan\u2019s emphasis on preserving modality-specific signal before fusion.</li> <li>The evaluation on multiple tasks and datasets is compatible with the plan\u2019s emphasis on robustness and disciplined evaluation, although the paper does not explicitly follow the full AUROC/AUPRC + confidence-interval protocol discussed in the plan.</li> </ul> </li> <li> <p>Future multimodal systems could:</p> <ul> <li>Use BrainLM\u2019s embeddings as one tower in a two-tower contrastive model, with another tower encoding, for example, genetics or behavioral data.</li> <li>Perform late fusion of BrainLM features with those from other FMs (e.g., for combined clinical prediction).</li> </ul> </li> <li> <p>Summary</p> </li> <li>For now, BrainLM is best viewed as a strong building block for multimodal integration, rather than a multimodal FM itself.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks</li> <li>Masked reconstruction / generalization:<ul> <li>Evaluate reconstruction accuracy (e.g., R\u00b2 on masked patches) on held-out UKB test data and independent HCP data.</li> </ul> </li> <li>Clinical variable prediction:<ul> <li>Fine-tune BrainLM to regress age, neuroticism, PTSD (PCL-5), and general anxiety (GAD-7) scores from fMRI recordings.</li> </ul> </li> <li>Future brain state prediction:<ul> <li>Given 180 observed timesteps, predict the next 20 timesteps of parcel activity, evaluated on UKB and HCP.</li> </ul> </li> <li>Interpretability via attention analysis:<ul> <li>Analyze self-attention weights (especially from the CLS token) to study how attention changes across tasks and clinical groups (e.g., depression severity).</li> </ul> </li> <li> <p>Functional network prediction (zero-shot-like):</p> <ul> <li>Use attention-derived features to classify parcels into 7 intrinsic functional networks without network-specific supervision.</li> </ul> </li> <li> <p>Baselines</p> </li> <li>For clinical variable regression:<ul> <li>SVR and MLP on correlation matrices.</li> <li>LSTM and GCN models that directly use fMRI recordings.</li> <li>Comparisons both to models trained on raw data and models on pretrained embeddings.</li> </ul> </li> <li>For future state prediction:<ul> <li>LSTM.</li> <li>Neural ODE and Latent ODE models.</li> <li>A Transformer model without pretraining (same architecture but trained only on the forecasting task).</li> </ul> </li> <li> <p>For functional network identification:</p> <ul> <li>k-NN classifiers using:</li> <li>Raw parcel time series,</li> <li>Variational Autoencoder (VAE) embeddings,</li> <li>GCN embeddings,</li> <li>BrainLM attention weights.</li> </ul> </li> <li> <p>Key findings</p> </li> <li>Generalization and reconstruction:<ul> <li>BrainLM achieves strong R\u00b2 on UKB test data and generalizes well to HCP, despite domain differences, showing that pretraining learns robust, dataset-agnostic representations.</li> <li>Performance improves with larger models and more data, demonstrating scaling laws similar to those seen in language and vision FMs.</li> </ul> </li> <li>Clinical variable prediction:<ul> <li>BrainLM-based regressors achieve lower mean squared error than baselines (SVR, MLP, LSTM, GCN, raw data) across age, PTSD, anxiety, and neuroticism.</li> <li>Fine-tuning further improves over using frozen embeddings, indicating that pretrained representations are rich but still adaptable.</li> <li>Even zero-shot regression (no fine-tuning) shows non-trivial predictive power, and performance scales with model size.</li> </ul> </li> <li>Future brain state prediction:<ul> <li>Fine-tuned BrainLM significantly outperforms LSTM, Neural ODE, Latent ODE, and the non-pretrained Transformer on both UKB and HCP.</li> <li>The benefit of pretraining is clear: the model without pretraining performs noticeably worse.</li> <li>Larger BrainLM variants maintain better forecasting performance over multiple timesteps.</li> </ul> </li> <li>Interpretability and networks:<ul> <li>Attention maps distinguish task vs rest (e.g., stronger attention to visual cortex during task states).</li> <li>Differences in attention for high vs low depression emphasize frontal and limbic regions, consistent with clinical literature.</li> <li>Using attention-based features, BrainLM achieves ~58.8% accuracy in classifying parcels into 7 functional networks, outperforming VAE, GCN, and raw data baselines.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths</li> <li>Introduces a true foundation model for fMRI, trained at a scale (6,700 hours, 77k recordings) much larger than prior work.</li> <li>Uses self-supervised masked modeling to efficiently exploit unlabeled fMRI data, enabling versatile downstream applications.</li> <li>Demonstrates strong generalization across cohorts (UKB \u2192 HCP) and across diverse tasks (reconstruction, forecasting, clinical prediction, network identification).</li> <li>Provides interpretable attention maps that align with known brain networks and clinical patterns, offering neuroscientific insights.</li> <li> <p>Shows clear scaling behavior, suggesting that larger models and datasets can further improve performance.</p> </li> <li> <p>Limitations</p> </li> <li>Currently unimodal: only fMRI is modeled; multimodal integration with EEG, structural MRI, genetics, and behavior is left for future work.</li> <li>Despite interpretability via attention, the latent representations are still complex, and a full mechanistic understanding of what is encoded remains challenging.</li> <li>The approach is computationally heavy, requiring large-scale pretraining with transformers on big imaging datasets.</li> <li>Some preprocessing choices (e.g., parcellation scheme, scaling, window length) may influence results, but not all ablations are detailed in the main text.</li> <li> <p>Real-world clinical deployment requires careful validation, robustness checks, and fairness analyses that go beyond the current experiments.</p> </li> <li> <p>Open Questions and Future Directions:</p> </li> <li>How does BrainLM compare to alternative pretraining objectives (contrastive, masked prediction on different views, generative modeling) for fMRI?</li> <li>Can BrainLM embeddings be effectively combined with other modalities (EEG, MEG, structural MRI, genetics) using late fusion or contrastive two-tower setups, and does this improve clinical prediction?</li> <li>What neuroscientific structure is captured in the CLS token and internal layers\u2014can we relate specific attention patterns or latent dimensions to known circuits or cognitive processes?</li> <li>How robust is BrainLM to distribution shifts such as different scanners, acquisition protocols, or clinical populations (e.g., pediatric, elderly, or specific disorders)?</li> <li>Can smaller, distilled versions of BrainLM retain most performance while being practical for clinical or real-time applications?</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the FM landscape</li> <li>BrainLM is to fMRI what large language models (like GPT) are to text: a general, pretrained backbone that can be adapted to many tasks rather than a task-specific model.</li> <li>Within brain/neuro FMs, it extends prior work that focused on visual cortex or small datasets to whole-brain modeling at population scale.</li> <li> <p>It shows that the masked autoencoder paradigm from vision and language transfers well to spatiotemporal brain data.</p> </li> <li> <p>Relation to well-known ideas</p> </li> <li>Conceptually, BrainLM behaves like a BERT-style or ViT-style masked model applied to a 2D grid where one dimension is space (brain parcels) and the other is time (fMRI timesteps).</li> <li>The attention-based interpretability parallels how we interpret attention maps in NLP and vision, but here the \"tokens\" are brain parcels over time.</li> <li> <p>The clinical prediction and network discovery tasks illustrate how foundation models can support both prediction and scientific discovery.</p> </li> <li> <p>Why it matters and how it links to integration plans</p> </li> <li>For a grad student interested in computational neuroscience, BrainLM is a blueprint for building large, reusable models of brain activity.</li> <li>It provides a ready-made encoder that can plug into multimodal integration pipelines\u2014consistent with the integration baseline plan\u2019s idea of learning strong, modality-specific representations before fusion.</li> <li>The work signals a general trend: foundation models are moving into neuroimaging, opening paths to richer multimodal systems that combine brain signals with genetic, behavioral, and clinical data.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem</li> <li>There is a need for a single, scalable model that can learn from massive fMRI repositories and support many downstream neuroscience and clinical tasks.</li> <li> <p>Traditional task-specific models struggle with generalization, data scale, and transfer across cohorts.</p> </li> <li> <p>Method / model</p> </li> <li>BrainLM is a transformer-based masked autoencoder that treats fMRI parcel\u00d7time windows as a 2D grid of tokens.</li> <li>It uses spatiotemporal masking and reconstruction to learn representations from 6,700 hours of fMRI without task labels.</li> <li>The architecture includes 4-layer encoder and 2-layer decoder transformers, with learned spatial and temporal embeddings and a summary CLS token.</li> <li> <p>Multiple model sizes (13M\u2013650M parameters) are trained, showing improved performance with scale.</p> </li> <li> <p>Results</p> </li> <li>BrainLM shows strong reconstruction and generalization performance on both UKB and HCP datasets.</li> <li>It outperforms baselines (SVR, MLP, LSTM, GCN, Neural ODE, non-pretrained Transformer) in clinical variable prediction and future brain state forecasting.</li> <li> <p>Attention-based analyses reveal meaningful functional networks and clinical differences, and attention-derived features outperform other representations in classifying parcels into known networks.</p> </li> <li> <p>Why it matters</p> </li> <li>BrainLM establishes a foundation model paradigm for fMRI, demonstrating that large, self-supervised models can unify diverse tasks in brain dynamics modeling.</li> <li>It provides a flexible, interpretable, and extensible backbone that future work can extend to multimodal settings and more ambitious clinical and neuroscientific applications.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/","title":"BrainMT (2025)","text":""},{"location":"generated/kb_curated/papers-md/brainmt_2025/#brainmt-a-hybrid-mamba-transformer-architecture-for-modeling-long-range-dependencies-in-functional-mri-data","title":"BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data","text":"<p>Authors: Arunkumar Kannan, Martin A. Lindquist, Brian Caffo Year: 2025 Venue: Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2025</p>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM. The paper develops a deep architecture specifically for resting-state functional MRI (rs-fMRI) and uses it to predict subject-level phenotypes (sex and cognitive intelligence) from whole-brain 4D volumes.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development (brain-specific sequence model). The work introduces a new hybrid architecture, BrainMT, that combines Mamba state-space models and transformers to handle long 4D fMRI sequences end-to-end. It is not a huge general-purpose foundation model in the GPT/CLIP sense, but it does propose a new, reusable backbone for a broad class of fMRI prediction tasks.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Resting-state fMRI (4D volumetric time series: 3D brain volumes over time).  </li> <li>Phenotypic labels: sex (classification) and cognitive intelligence scores (regression).</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#2-one-paragraph-high-level-summary-for-a-new-grad-student","title":"2. One-Paragraph High-Level Summary (For a New Grad Student)","text":"<p>This paper proposes BrainMT, a deep learning architecture designed to model long-range spatial and temporal dependencies in resting-state fMRI data for subject-level phenotypic prediction. Instead of compressing fMRI into connectivity matrices or using short temporal windows, BrainMT operates directly on 4D voxel-wise volumes and can process much longer temporal sequences efficiently. The model uses a hierarchical convolutional block to extract local spatial features, a bidirectional spatiotemporal Mamba block (a modern state-space model) to capture long-range temporal and spatiotemporal patterns, and a lightweight transformer block to model global spatial relationships. The authors evaluate BrainMT on large UK Biobank and Human Connectome Project datasets, predicting both sex and cognitive intelligence, and show it outperforms strong baselines including graph neural networks, transformer-based models, and the recent SwiFT voxel-wise transformer. Quantitative and ablation studies demonstrate that BrainMT achieves better accuracy with lower memory usage while benefiting from longer temporal context. Finally, interpretability analysis using Integrated Gradients highlights brain regions in default mode and frontoparietal networks that align with known neuroscience findings. Overall, the work is valuable to a new grad student because it showcases how modern sequence modeling ideas (Mamba + transformers) can be adapted to challenging 4D neuroimaging data and why long temporal context matters for fMRI-based prediction.</p>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>The goal is to predict subject-level phenotypes (sex and cognitive intelligence) from resting-state fMRI data.  </li> <li>Each subject has a 4D fMRI scan: a sequence of 3D brain volumes over time (volumes \u00d7 height \u00d7 width \u00d7 depth).  </li> <li> <p>The model should learn functional connectivity and spatiotemporal patterns directly from these volumetric time series, without relying on hand-crafted parcellations or connectivity matrices.</p> </li> <li> <p>Why existing approaches are limited: </p> </li> <li>Correlation-based pipelines: <ul> <li>They typically parcellate the brain into regions of interest (ROIs) and compute a region-by-region functional connectivity matrix (e.g., Pearson correlation).  </li> <li>This reduces dimensionality but can discard fine-grained spatial information, especially with coarse parcellations.  </li> <li>Performance can vary widely depending on the chosen parcellation and connectivity measure, leading to instability and lack of consensus.</li> </ul> </li> <li> <p>Voxel-based deep models: </p> <ul> <li>Recent transformer or CNN-based methods operate directly on voxel-level fMRI, but transformer-based models face quadratic complexity in sequence length.  </li> <li>As a result, they are often restricted to short sequences (e.g., 10\u201320 time frames), then aggregate predictions across multiple windows.  </li> <li>Because fMRI signals evolve relatively slowly (hemodynamics), limiting context to a few frames can miss important long-range temporal dynamics.</li> </ul> </li> <li> <p>Why this is hard technically: </p> </li> <li>fMRI is high-dimensional: 3D volumes with tens of thousands of voxels repeated hundreds to thousands of times per scan.  </li> <li>Spatiotemporal patterns are long-range and structured: activity in distant brain regions can be correlated across long time scales.  </li> <li>Direct transformer modeling over all voxels and time points is computationally heavy (quadratic in sequence length).  </li> <li>Models must balance expressivity (capturing complex patterns) with efficiency (memory and compute) to be practical on large datasets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used: </li> <li>UK Biobank (UKB): <ul> <li>Resting-state fMRI data from about 6,000 participants.  </li> <li>Scan length: approximately 490 volumes per subject.  </li> <li>Spatial dimensions: 91 \u00d7 109 \u00d7 91 in MNI space.  </li> <li>Sex distribution: ~50.86% female.  </li> </ul> </li> <li> <p>Human Connectome Project (HCP, S1200 release): </p> <ul> <li>Resting-state fMRI data from 1,075 participants.  </li> <li>Scan length: approximately 1,200 volumes per subject.  </li> <li>Spatial dimensions: 91 \u00d7 109 \u00d7 91 in MNI space.  </li> <li>Sex distribution: ~51.16% female.</li> </ul> </li> <li> <p>Modalities: </p> </li> <li>Primary modality: resting-state fMRI volumes (4D: time \u00d7 3D brain).  </li> <li> <p>Targets:  </p> <ul> <li>Cognitive intelligence scores: </li> <li>\u201cCognitive function\u201d composite scores from HCP.  </li> <li>\u201cFluid intelligence/reasoning\u201d scores from UKB.  </li> <li>Sex: binary labels for sex classification.</li> </ul> </li> <li> <p>Preprocessing / representation: </p> </li> <li>The authors use existing preprocessed fMRI data from both datasets, following \u201cfMRI volume\u201d pipelines that include:  <ul> <li>Bias field reduction, skull stripping, cross-modality registration, and spatial normalization.  </li> </ul> </li> <li>For modeling:  <ul> <li>Global Z-score normalization is applied across brain voxels, excluding background regions; background voxels are filled with minimum Z-score intensity.  </li> <li>Each 3D volume is partitioned into partially overlapping 3D patches.  </li> <li>Patches are embedded via convolutional layers into a lower-dimensional feature space before being passed into the Mamba and transformer blocks.  </li> </ul> </li> <li>For comparison with correlation-based approaches, the HCP multimodal atlas is used to parcellate the data and compute ROI\u2013ROI Pearson correlation matrices.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>BrainMT is a hybrid architecture combining convolutional neural networks, Mamba state-space models, and transformers to capture local, long-range temporal, and global spatial dependencies in fMRI.</p> <ul> <li>Model type: </li> <li>A hybrid deep sequence model for spatiotemporal fMRI data, composed of:  <ul> <li>3D convolutional blocks (hierarchical feature extractor).  </li> <li>Bidirectional Vision Mamba blocks (selective state-space models) arranged with a temporal-first scanning mechanism.  </li> <li>A multi-head self-attention transformer block for global spatial modeling.  </li> </ul> </li> <li> <p>Overall, it can be viewed as a brain-specific backbone that blends linear-time sequence modeling (Mamba) with transformer attention.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li>The architecture itself, BrainMT, is new.  </li> <li>It builds on existing ideas: Vision Mamba / Mamba-based state-space models and standard transformers.  </li> <li>The paper does not present large-scale general-purpose pretraining; instead, BrainMT is trained directly on the task objectives (sex and intelligence prediction) on large datasets.  </li> <li> <p>So in FM terms, it is closer to a new, reusable backbone than a fully generic foundation model.</p> </li> <li> <p>Key architectural components and innovations:</p> </li> <li> <p>1. Convolution block (local spatial feature extractor): </p> <ul> <li>Takes fMRI volumes (X \\in \\mathbb{R}^{T \\times H \\times W \\times D}).  </li> <li>Each volume is split into partially overlapping 3D patches (downsampling the spatial resolution).  </li> <li>Two convolutional layers project patches into a C-dimensional embedding.  </li> <li>A two-stage convolutional encoder with downsampling forms multi-scale feature maps, capturing coarse and fine spatial details.  </li> <li>Residual convolutional units with GELU activations and layer normalization provide stable feature extraction.</li> </ul> </li> <li> <p>2. Positional embeddings and sequence construction: </p> <ul> <li>The output of the convolution block is a sequence of patch tokens per time step.  </li> <li>Learnable spatial positional embeddings and temporal positional embeddings are added to encode positions.  </li> <li>A learnable classification token (X_{\\text{cls}}) is prepended as a global aggregator.  </li> <li>Tokens are reshaped into a long 1D sequence of length (L = T \\times K) (where (K) is the number of patches), preparing them for spatiotemporal modeling.</li> </ul> </li> <li> <p>3. Spatiotemporal Mamba block (long-range context): </p> <ul> <li>Based on selective state-space models (SSMs), originally inspired by Kalman-like systems.  </li> <li>Uses Vision Mamba with bidirectional selective SSMs to handle spatiotemporal context.  </li> <li>Mamba introduces input-dependent parameters (e.g., B, C, and time scales), enabling adaptive context modeling.  </li> <li>The temporal-first scanning mechanism arranges tokens so that time is the leading dimension, followed by spatial dimensions, which:  </li> <li>Emphasizes temporal continuity and long-range temporal correlations critical for fMRI.  </li> <li>Helps the SSM capture long sequences with linear-time complexity in sequence length.  </li> <li>Forward and backward selective SSMs process the sequence in both directions, and their outputs are gated and combined, enabling rich bidirectional context.</li> </ul> </li> <li> <p>4. Transformer block (global spatial relationships): </p> <ul> <li>After Mamba processing, a multi-head self-attention transformer operates on the sequence.  </li> <li>Attention is defined as usual ( \\text{Attention}(Q,K,V) = \\text{Softmax}(QK^\\top / \\sqrt{d_\\text{head}}) V ).  </li> <li>Because convolution and downsampling have already reduced the spatial resolution, the sequence length is shorter, making quadratic transformer attention tractable.  </li> <li>This block focuses on global spatial interactions among feature tokens, complementing the temporally oriented Mamba block.</li> </ul> </li> <li> <p>5. Output head: </p> <ul> <li>The final representation is taken from the normalized classification token (X_{\\text{cls}}).  </li> <li>A multilayer perceptron (MLP) head is applied for downstream tasks:  </li> <li>Regression head for cognitive intelligence prediction.  </li> <li>Classification head for sex prediction.</li> </ul> </li> <li> <p>Training setup (as described):</p> </li> </ul> Aspect Description Implementation PyTorch; trained on NVIDIA L40S GPUs (48GB RAM) Architecture depth 2 convolution blocks, 12 Mamba blocks, 8 transformer blocks Input frames 200 frames per subject (chosen via ablation; trade-off between SNR and overfitting) Mamba hyperparameters State dimension 16, expansion ratio 2 (default Mamba settings) Optimization AdamW, cosine learning rate schedule over 20 epochs Warm-up First 5 epochs used for linear warm-up Learning rate 2e-4 (default in experiments) Weight decay 0.05 Batch size 2 Training strategy Distributed data-parallel training Early stopping Based on validation loss Regression objective Mean squared error (MSE), evaluated with MSE, MAE, Pearson\u2019s R Classification objective Binary cross-entropy, evaluated with accuracy, balanced accuracy, AUROC"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Is this a multimodal integration paper? </li> <li>Not primarily. The core focus is on single-modality resting-state fMRI data.  </li> <li> <p>The model does not fuse different data types (e.g., structural MRI, behavior, genetics) within the architecture; instead, it learns from volumetric fMRI alone and predicts phenotypes.</p> </li> <li> <p>Integration aspects (within fMRI): </p> </li> <li>BrainMT integrates spatial and temporal information from fMRI in a unified framework:  <ul> <li>Convolution handles local spatial integration.  </li> <li>Mamba handles long-range temporal and spatiotemporal dependencies.  </li> <li>Transformers handle global spatial relationships across the entire brain.  </li> </ul> </li> <li> <p>This can be viewed as intra-modality integration (across time and space) rather than multimodal integration.</p> </li> <li> <p>Relation to the integration baseline plan: </p> </li> <li>The integration baseline plan emphasizes late fusion across heterogeneous modalities, CCA-based analysis, and disciplined evaluation.  </li> <li>BrainMT does not explicitly adopt late fusion or CCA-style strategies, as it operates on a single modality.  </li> <li>However, its design aligns with the principle of preserving rich modality-specific signal (here, fMRI) by avoiding aggressive parcellation and instead modeling voxel-level dynamics directly.  </li> <li>The evaluation uses solid metrics (MSE/MAE/R for regression; accuracy, balanced accuracy, AUROC for classification), which resonates with the plan\u2019s emphasis on robust, properly reported metrics.</li> </ul> <p>Because the paper is not truly multimodal, the integration baseline plan is more of a conceptual backdrop here than a direct methodological influence.</p>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Cognitive intelligence prediction (regression): <ul> <li>Predicts continuous intelligence scores for subjects in HCP and UKB.  </li> <li>Evaluated with MSE, MAE, and Pearson\u2019s correlation R.  </li> </ul> </li> <li>Sex classification: <ul> <li>Binary classification of sex for HCP and UKB participants.  </li> <li>Evaluated with accuracy, balanced accuracy, and AUROC.  </li> </ul> </li> <li> <p>Additional analysis: </p> <ul> <li>Predicting functional connectivity correlations, comparing BrainMT with SwiFT on subject-level Pearson correlations.  </li> <li>Ablation studies on number of frames, architecture components, numbers of layers, and alternative Mamba variants.  </li> <li>Interpretability analyses to identify brain regions contributing to predictions.</li> </ul> </li> <li> <p>Baselines: </p> </li> <li>Correlation-based methods: <ul> <li>XGBoost on connectivity features.  </li> <li>BrainNetCNN (CNN for brain networks).  </li> <li>BrainGNN (graph neural network for brain graphs).  </li> <li>BrainNetTF (transformer applied to brain networks).  </li> </ul> </li> <li>Voxel-based methods: <ul> <li>TFF: self-supervised transformers for fMRI representation.  </li> <li>SwiFT: 4D Swin transformer for fMRI (strong recent voxel-based baseline).  </li> </ul> </li> <li> <p>All baselines are implemented following their original papers and tuned on the validation sets.</p> </li> <li> <p>Key quantitative findings (trends, not exact numbers): </p> </li> <li>Intelligence prediction: <ul> <li>On both HCP and UKB, BrainMT achieves lower MSE and MAE and higher Pearson\u2019s R than all baselines.  </li> <li>Many baselines achieve MSE close to 1.0 (given targets are normalized to unit variance), suggesting they mostly predict the mean.  </li> <li>BrainMT notably reduces MSE by around 6\u20139% relative to the best baselines across datasets, indicating meaningful improvement.  </li> </ul> </li> <li>Sex classification: <ul> <li>On HCP, BrainMT achieves the best accuracy, balanced accuracy, and AUROC among all methods.  </li> <li>On UKB, BrainMT closely matches or slightly exceeds SwiFT, maintaining state-of-the-art performance.  </li> </ul> </li> <li>Memory efficiency and scalability: <ul> <li>BrainMT is reported to be about 35.8% more memory-efficient than SwiFT.  </li> <li>Its memory usage grows linearly with the number of time frames, enabling longer sequence modeling than standard transformers.  </li> </ul> </li> <li> <p>Ablation studies: </p> <ul> <li>Number of frames: Using around 200 frames provides the best trade-off; fewer frames reduce signal-to-noise, while substantially more frames risk overfitting.  </li> <li>Component ablations: Removing either the transformer or convolution block degrades performance, confirming the importance of the hybrid design.  </li> <li>Depth variations: Larger or smaller numbers of Mamba/transformer layers change performance; the chosen configuration (12 Mamba, 8 transformer) is near optimal.  </li> <li>Alternative Mamba variants: Replacing the bidirectional Vision Mamba block with alternatives like VMamba or MambaVision worsens results, suggesting the specific temporal-first, bidirectional setup is crucial.  </li> <li>Functional connectivity prediction: BrainMT surpasses SwiFT in subject-level Pearson correlations, indicating better capture of dynamics that underlie functional connectivity.</li> </ul> </li> <li> <p>Qualitative / interpretability findings: </p> </li> <li>Integrated Gradients (IG) maps for cognitive intelligence highlight regions in:  <ul> <li>Default Mode Network (DMN) and Frontoparietal Network (FPN), including posterior cingulate cortex (PCC), anterior cingulate cortex (ACC), precuneus (PCu), and cuneus (Cu).  </li> <li>These are known to be involved in working memory, attention, decision-making, and visuospatial processing.  </li> </ul> </li> <li>For sex prediction, IG maps consistently emphasize regions such as the superior temporal gyrus (STG), middle frontal gyrus (MFG), and precuneus (PCu), aligning with prior studies on sex differences in functional brain organization.  </li> <li>These maps suggest that BrainMT\u2019s predictive patterns are consistent with established neuroscientific knowledge, not just arbitrary features.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths: </li> <li>End-to-end voxel-level modeling: Operates directly on 4D fMRI volumes, avoiding parcellation-induced information loss and inconsistencies.  </li> <li>Long-range temporal modeling: The Mamba-based temporal-first design enables efficient handling of long sequences (hundreds of frames), which is crucial for capturing slow hemodynamic dynamics.  </li> <li>Hybrid architecture: Combining convolution, Mamba, and transformers gives a balanced treatment of local spatial, long-range temporal, and global spatial dependencies.  </li> <li>Strong empirical results: Consistently outperforms state-of-the-art correlation-based and voxel-based models on large, well-known datasets (HCP and UKB).  </li> <li>Interpretability: Integrated Gradients provide biologically plausible importance maps, grounding model predictions in known functional networks.  </li> <li> <p>Memory efficiency: More memory-efficient than a strong voxel-based transformer baseline (SwiFT), which is important for practical large-scale neuroimaging.</p> </li> <li> <p>Limitations: </p> </li> <li>Single-modality focus: The model only uses resting-state fMRI; it does not yet integrate structural MRI, behavioral data, genetics, or other modalities that could aid prediction.  </li> <li>Task-specific training: BrainMT is trained directly on supervised targets rather than via large-scale self-supervised pretraining; this limits its status as a general-purpose foundation model.  </li> <li>Compute requirements: Despite efficiency improvements, training on 4D fMRI with deep Mamba and transformer stacks on large datasets still requires substantial compute (L40S GPUs, distributed training).  </li> <li>Generalization beyond studied tasks: The paper evaluates on sex and cognitive intelligence; it remains unclear how well BrainMT transfers to other phenotypes or clinical conditions without substantial retraining.  </li> <li> <p>Interpretability scope: Integrated Gradients focus on voxel importance but do not fully explain temporal dynamics or causal relationships.</p> </li> <li> <p>Open Questions and Future Directions: </p> </li> <li>Multimodal extensions: How would BrainMT perform if extended to jointly model structural MRI, diffusion MRI, or behavioral measures along with fMRI? Could late fusion or shared embedding approaches from the integration baseline plan improve performance?  </li> <li>Self-supervised or foundation-style pretraining: Can we pretrain BrainMT on large-scale unlabeled rs-fMRI datasets using self-supervised objectives (e.g., masked volume prediction, temporal contrastive tasks) and then fine-tune for many downstream tasks?  </li> <li>Clinical translation: How well does BrainMT generalize to clinical populations (e.g., psychiatric or neurodegenerative disorders), and what adaptations are needed for imbalanced or smaller datasets?  </li> <li>Temporal modeling variants: Would alternative state-space architectures, bidirectionality schemes, or temporal pooling strategies further improve performance or robustness?  </li> <li>Uncertainty and reliability: How can we estimate prediction uncertainty and assess reliability across sites, scanners, and preprocessing pipelines?  </li> <li>Integration with graph-based representations: Could learned voxel-level representations be aggregated into dynamic functional graphs and combined with GNNs for more interpretable connectivity analysis?</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#9-how-this-connects-to-the-bigger-picture-for-a-new-grad-student","title":"9. How This Connects to the Bigger Picture (For a New Grad Student)","text":"<ul> <li>Position in the landscape of brain foundation models: </li> <li>BrainMT sits in the growing space of deep backbones for fMRI, alongside transformer-based models (e.g., TFF, SwiFT) and graph-based methods (BrainGNN).  </li> <li>It emphasizes efficient long-sequence modeling, borrowing ideas from modern sequence models like Mamba, and adapting them to 4D neuroimaging.  </li> <li> <p>While not a traditional \u201cfoundation model\u201d with massive pretraining, it can serve as a strong architectural template for future brain FMs.</p> </li> <li> <p>Connections to well-known ideas: </p> </li> <li>Conceptually, BrainMT is like \u201ca video model for brain volumes\u201d: it treats fMRI as a spatiotemporal sequence, similar to video transformers but with neurobiological constraints.  </li> <li>The Mamba block provides linear-time sequence modeling, akin to efficient alternatives to transformers in NLP and vision; the transformer layer then adds global relational reasoning.  </li> <li> <p>Compared to traditional connectivity pipelines, it replaces hand-designed ROI-based features with end-to-end learned representations from raw data.</p> </li> <li> <p>Relation to integration and broader research programs: </p> </li> <li>For multimodal integration projects (e.g., integrating brain imaging with genomics or clinical data), BrainMT can act as a strong fMRI encoder, producing subject-level representations that can be fused with other modalities via late fusion or joint embedding methods.  </li> <li>Its success supports the integration baseline plan\u2019s principle of preserving modality-specific signal (learning rich fMRI representations before combining with other data types).  </li> <li> <p>The evaluation practices (consistent splits, multiple metrics, ablations) echo the plan\u2019s emphasis on careful, robust benchmarking.</p> </li> <li> <p>Why this paper is a useful reference: </p> </li> <li>It provides a concrete, well-validated example of how to design and train a modern deep architecture for 4D fMRI at scale.  </li> <li>It offers design patterns (temporal-first scanning, hybrid Mamba\u2013transformer stacks, integrated interpretability) that can inform future brain FMs and multimodal models.  </li> <li>For a new grad student, it is a good entry point into understanding how sequence modeling ideas from NLP/vision can be translated into neuroimaging.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: Resting-state fMRI contains complex, long-range spatiotemporal patterns that can predict subject-level phenotypes such as sex and cognitive intelligence, but existing approaches either lose spatial detail (via parcellation) or are limited to short time windows (due to transformer complexity).  </li> <li> <p>Problem: There is a need for architectures that can handle long 4D fMRI sequences efficiently while capturing both local and global dependencies in brain activity.</p> </li> <li> <p>Method / model: BrainMT is a hybrid deep architecture combining convolutional blocks, a bidirectional Vision Mamba state-space module with temporal-first scanning, and a transformer block for global spatial attention.  </p> </li> <li>Method / model: The model processes 200 fMRI frames per subject at a reduced spatial resolution, builds a long token sequence with positional embeddings and a classification token, and uses Mamba to capture long-range temporal context before applying transformer attention.  </li> <li> <p>Method / model: Training uses large-scale resting-state fMRI datasets (UKB and HCP) with supervision for sex classification and cognitive intelligence regression, optimized with AdamW and standard loss functions.</p> </li> <li> <p>Results: BrainMT outperforms strong correlation-based (XGBoost, BrainNetCNN, BrainGNN, BrainNetTF) and voxel-based (TFF, SwiFT) baselines in both intelligence prediction and sex classification across HCP and UKB.  </p> </li> <li>Results: The model is more memory-efficient than SwiFT, scales linearly with the number of frames, and benefits from longer temporal context; ablations confirm the importance of its hybrid design and the specific Mamba variant.  </li> <li> <p>Results: Integrated Gradients reveal that BrainMT\u2019s predictions rely on default mode and frontoparietal regions for intelligence, and regions like STG, MFG, and precuneus for sex differences, aligning with known neuroscience.</p> </li> <li> <p>Why it matters: BrainMT demonstrates that modern sequence models like Mamba, combined with transformers, can efficiently model long 4D fMRI sequences and yield state-of-the-art predictive performance.  </p> </li> <li>Why it matters: The architecture is a promising backbone for future brain foundation models and a strong fMRI encoder that could be integrated into larger multimodal systems (e.g., combining brain imaging with genetics or clinical data).</li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/","title":"Caduceus (2024)","text":""},{"location":"generated/kb_curated/papers-md/caduceus_2024/#caduceus-bi-directional-equivariant-long-range-dna-sequence-modeling","title":"Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling","text":"<p>Authors: Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, Volodymyr Kuleshov Year: 2024 Venue: 41st International Conference on Machine Learning (ICML), PMLR 235</p>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. The paper develops long-range sequence models specifically for DNA and evaluates them on genomics tasks such as regulatory element classification and variant effect prediction.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. The main contribution is a new family of DNA foundation models (Caduceus) and underlying architectural blocks (BiMamba and MambaDNA), together with pretraining and fine-tuning strategies.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Single-modality DNA sequence (human reference genome; nucleotide-level modeling).</li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces Caduceus, a family of long-range DNA language models designed to capture two key biological symmetries: bi-directional context and reverse complement (RC) equivariance of DNA. Standard sequence models either struggle with very long genomic contexts or ignore that DNA\u2019s two strands carry equivalent information in opposite directions, which wastes data and harms generalization. Building on the Mamba structured state space model, the authors propose BiMamba for efficient bi-directional sequence modeling and MambaDNA for RC-equivariant processing, then assemble these into Caduceus variants that operate as DNA foundation models trained with masked language modeling. They pretrain on the human genome and fine-tune on a wide range of genomics benchmarks, showing that Caduceus matches or outperforms both HyenaDNA (another long-range SSM-based model) and much larger Transformer-based models such as Nucleotide Transformer v2 and Enformer, especially for tasks requiring long-range sequence context and symmetry handling. A highlight is variant effect prediction, where Caduceus competes with or exceeds 10\u00d7 larger attention-based models at long genomic distances from genes. For a new grad student, this paper is a concrete example of how to incorporate biological inductive biases (symmetries) into modern foundation-model-style architectures to unlock better performance at manageable scale.</p>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>How to build foundation-scale models that understand long DNA sequences, especially non-coding regions that regulate gene expression.</li> <li>Specifically, the model should:<ul> <li>Use bi-directional context: regulatory effects can depend on bases both upstream and downstream of a position.</li> <li>Be reverse complement (RC) aware: the two strands of DNA encode the same information in opposite directions (A\u2194T, C\u2194G), and biological assays may sequence either strand.</li> <li>Capture long-range interactions: variants up to hundreds of kilobases (or more) away from a gene can affect expression.</li> </ul> </li> <li> <p>Downstream, they particularly care about variant effect prediction (VEP)\u2014predicting whether a single nucleotide polymorphism (SNP) causally affects gene expression.</p> </li> <li> <p>Why this is hard</p> </li> <li>Long-range dependencies:<ul> <li>Transformers scale quadratically with sequence length, making it costly to model context lengths of 100k\u20131M base pairs.</li> <li>Biological regulatory effects can span these very long distances, so truncating context harms performance.</li> </ul> </li> <li>Bi-directionality:<ul> <li>Many language models are causal (left-to-right) and only see past context; for DNA, both \u201cpast\u201d and \u201cfuture\u201d bases matter symmetrically.</li> </ul> </li> <li>Reverse complement symmetry:<ul> <li>A DNA sequence and its reverse complement should yield equivalent predictions for most tasks.</li> <li>Standard models treat RC pairs as unrelated sequences, requiring explicit data augmentation and still not enforcing strict symmetry.</li> </ul> </li> <li>Data and modeling complexity:<ul> <li>Non-coding DNA is vast and noisy; many relevant signals are subtle (e.g., conservation, motif patterns).</li> <li>Tokenization choices (e.g., k-mers) can make small sequence changes look large in token space.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>Pretraining:<ul> <li>Human reference genome HG38 / GRCh37 (from the Genome Reference Consortium), split into ~34,021 segments extended to length (2^{20} \\approx 1,048,576) base pairs.</li> <li>Total scale \u2248 35 billion nucleotide tokens.</li> </ul> </li> <li> <p>Downstream benchmarks:</p> <ul> <li>Genomics Benchmarks suite (Gre\u0161ov\u00e1 et al., 2023): eight regulatory element classification tasks (e.g., mouse enhancers, human enhancers, open chromatin regions, promoters).</li> <li>Nucleotide Transformer benchmark (Dalla-Torre et al., 2023): 18 datasets, including histone mark prediction, enhancer and promoter annotations, and splice site prediction.</li> <li>Variant effect prediction (VEP) dataset derived from Enformer (Avsec et al., 2021) and Trop et al. (2023), with SNPs labeled as causal vs non-causal for gene expression using SuSiE fine-mapping.</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Single modality: DNA sequence at single-nucleotide resolution (A/C/G/T).</li> <li> <p>Outputs are task-specific labels (e.g., regulatory class, histone mark presence, variant effect).</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>Character-level tokenization: each nucleotide (A, C, G, T) is a token, avoiding k-mer tokenization.<ul> <li>Motive: k-mers make small base changes cause large token changes, which complicates learning; single-nucleotide tokens preserve locality.</li> </ul> </li> <li>For downstream tasks:<ul> <li>Sequences are trimmed or padded to task-specific lengths (e.g., 200\u20132,000 bps for Genomics Benchmarks).</li> <li>Final hidden states are pooled (e.g., mean pooling over positions) to obtain fixed-size embeddings.</li> </ul> </li> <li>For VEP:<ul> <li>They extract features from windows centered at SNP positions (e.g., 1,536 bp windows for SSM-based models) and use these embeddings with an external classifier (SVM).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Based on Structured State Space Models (SSMs)\u2014specifically the Mamba architecture (Gu &amp; Dao, 2023), which models sequences via linear state-space dynamics plus input-dependent \u201cselection\u201d mechanisms.</li> <li> <p>Mamba operates in linear time in sequence length, enabling long contexts (up to hundreds of thousands of base pairs) without quadratic cost.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li>The paper extends Mamba with new architectural components tailored to DNA:<ul> <li>BiMamba: a bi-directional variant that processes sequences in both forward and reversed order with shared parameters.</li> <li>MambaDNA: an RC-equivariant block that enforces reverse complement symmetry.</li> </ul> </li> <li> <p>These blocks are used to build Caduceus, a new family of DNA foundation models pretrained with a masked language modeling objective.</p> </li> <li> <p>Key components and innovations</p> </li> </ul> Component Role / Idea Mamba Base SSM block combining selective state space layers with a gated MLP; originally causal (uni-directional). BiMamba Applies Mamba to the sequence and its reversed version, then flips and adds outputs; uses shared projections to keep parameter count low while modeling bi-directional context. MambaDNA RC-equivariant module: splits channels, applies Mamba (or BiMamba) to forward and reverse-complement sequences with shared parameters, and recombines to guarantee RC equivariance. Caduceus-PS Fully RC-equivariant LM via parameter sharing: RC-equivariant embeddings, MambaDNA blocks, and LM head; predictions for RC inputs transform appropriately. Caduceus-Ph Uses BiMamba with RC data augmentation during pretraining and post-hoc conjoining at inference (averaging predictions on forward and RC sequences) to enforce RC invariance downstream. <ul> <li>Architectural details (intuitive)</li> <li>BiMamba:<ul> <li>Take a sequence (X) and a reversed copy.</li> <li>Run the Mamba block on both with shared in/out projections.</li> <li>Flip the reversed output back along the sequence length and add it to the forward output.</li> <li>This yields a representation that incorporates information from both directions without doubling parameters.</li> </ul> </li> <li>MambaDNA:<ul> <li>Split the channel dimension into two halves.</li> <li>Apply Mamba (or BiMamba) to the forward half and to the reverse complement of the other half, with shared parameters.</li> <li>Apply the RC transform again to the reversed output and concatenate the two halves back, resulting in a representation that is provably RC-equivariant.</li> </ul> </li> <li>Caduceus-PS:<ul> <li>Uses RC-equivariant embeddings, stacks of MambaDNA blocks with BiMamba inside, and an RC-equivariant LM head that combines channel-flipped outputs.</li> <li>Enforces RC-equivariant behavior already during pretraining, so RC data augmentation is not needed.</li> </ul> </li> <li> <p>Caduceus-Ph:</p> <ul> <li>Uses BiMamba without built-in RC equivariance in the LM.</li> <li>Relies on RC data augmentation during pretraining and post-hoc conjoining (averaging outputs for forward and RC inputs) at downstream inference to enforce RC invariance.</li> </ul> </li> <li> <p>Training setup</p> </li> <li>Pretraining objective:<ul> <li>Masked Language Modeling (MLM), similar to BERT:</li> <li>Mask 15% of tokens; among these, 80% replaced with [MASK], 10% replaced with random bases, 10% unchanged.</li> <li>Causal next-token prediction is used for some baseline comparisons (e.g., Mamba vs HyenaDNA), but Caduceus FM training is primarily MLM-based and bi-directional.</li> </ul> </li> <li>Pretraining data &amp; scale:<ul> <li>Human reference genome with long sequence segments (1k, 32k, 131k bps), keeping number of tokens per batch approximately constant across lengths.</li> <li>Several model variants with different depths and hidden dimensions; small (~hundreds of thousands to a few million parameters) compared to Transformers like Nucleotide Transformer and Enformer.</li> </ul> </li> <li>Optimization details (high level):<ul> <li>ADAM optimizer, cosine learning rate decay, learning rate around (8 \\times 10^{-3}) for Mamba-based models.</li> <li>RC data augmentation used for non-RC-equivariant models (including HyenaDNA and Caduceus-Ph during pretraining).</li> </ul> </li> <li>Fine-tuning:<ul> <li>For downstream tasks, they pool final hidden states (often mean pooling), then train task-specific heads.</li> <li>Hyperparameters (learning rate, batch size) are tuned per task using cross-validation (5-fold for Genomics Benchmarks, 10-fold for Nucleotide Transformer tasks).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This paper does not perform multimodal integration: it focuses on a single modality (DNA sequence) and does not combine genomics with other data types such as expression, imaging, or clinical variables.</p> <ul> <li>Relation to integration and the baseline plan</li> <li>Within the broader Integration Baseline Plan, Caduceus informs the \u201cGenetics embedding hygiene and attribution\u201d principle:<ul> <li>It emphasizes reverse-complement handling (RC equivariance or RC averaging) as a core inductive bias for DNA encoders.</li> <li>It uses deterministic, nucleotide-level tokenization rather than k-mers, aligning with the plan\u2019s caution about unstable tokenization for variant-level analyses.</li> </ul> </li> <li>If Caduceus embeddings are later integrated with other modalities (e.g., brain imaging, clinical phenotypes), the plan would advocate:<ul> <li>Preserving modality-specific representations (i.e., using Caduceus as a frozen or carefully fine-tuned encoder, then fusing at a later stage).</li> <li>Applying the robustness practices (standardization, residualization, consistent CV, calibrated metrics) discussed in the plan.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Pretraining analyses</li> <li>Mamba vs HyenaDNA (next-token prediction):<ul> <li>On human-genome pretraining with different sequence lengths (1k, 32k, 131k bps), Mamba achieves lower cross-entropy loss than HyenaDNA at similar model sizes.</li> <li>Mamba also appears more robust to higher learning rates, supporting its use as the core building block.</li> </ul> </li> <li>Effect of BiMamba\u2019s parameter sharing:<ul> <li>BiMamba with projection weight tying enables deeper bi-directional models for the same parameter budget.</li> <li>These deeper, tied models achieve better MLM loss than naive bi-directional Mamba (without weight tying, shallower models).</li> </ul> </li> <li> <p>Effect of RC equivariance:</p> <ul> <li>RC-equivariant language modeling (as in Caduceus-PS) leads to improved MLM loss across sequence lengths compared to non-equvariant models.</li> <li>This suggests that encoding RC symmetry directly into the model improves pretraining quality, not just downstream metrics.</li> </ul> </li> <li> <p>Genomics Benchmarks (8 classification tasks)</p> </li> <li>Baselines:<ul> <li>CNN trained from scratch, HyenaDNA, non-RC-equvariant Mamba and Caduceus backbones.</li> </ul> </li> <li> <p>Key findings:</p> <ul> <li>Across all eight tasks, Caduceus variants attain the best or near-best accuracy.</li> <li>Caduceus-Ph often provides the strongest performance, sometimes slightly surpassing Caduceus-PS and non-equvariant Caduceus.</li> <li>Example trends (qualitative):<ul> <li>Mouse enhancers: Caduceus models outperform CNN and HyenaDNA, with Caduceus-PS performing best among Caduceus variants.</li> <li>Human enhancer and promoter tasks: Caduceus models typically exceed Mamba-only and HyenaDNA baselines.</li> </ul> </li> <li>The results show that combining long-range SSMs with bi-directionality and RC handling is consistently beneficial on regulatory sequence tasks.</li> </ul> </li> <li> <p>Nucleotide Transformer benchmark (18 tasks)</p> </li> <li>Baselines:<ul> <li>Large Transformer-based models: Enformer (~252M parameters), DNABERT-2 (~117M), Nucleotide Transformer v2 (~500M).</li> <li>HyenaDNA (~1.6M parameters).</li> </ul> </li> <li>Metrics: MCC for histone marks/enhancers, F1 for promoters and splice annotation, accuracy for \u201csplice sites all\u201d.</li> <li> <p>Key findings:</p> <ul> <li>Caduceus-Ph and Caduceus-PS (~1.9M parameters) perform competitively with much larger Transformers, particularly on histone marks and regulatory annotation tasks.</li> <li>Caduceus models generally outperform HyenaDNA on most histone and regulatory tasks.</li> <li>HyenaDNA remains strong on some splice site annotation tasks, where Caduceus performance is more mixed.</li> <li>Overall, Caduceus demonstrates that carefully designed SSM-based FMs with biological inductive biases can rival or outperform huge Transformers on many genomics tasks at a fraction of the parameter count.</li> </ul> </li> <li> <p>Variant Effect Prediction (VEP) on gene expression</p> </li> <li>Setup:<ul> <li>Use embeddings from Caduceus, HyenaDNA, Nucleotide Transformer v2, and Enformer.</li> <li>For each SNP, extract embeddings from a window centered at the SNP (e.g., 1,536 bp window for SSM models, shorter effective windows for Transformer baselines), optionally concatenated with tissue information.</li> <li>Train an SVM with RBF kernel to classify whether the SNP is causal for gene expression, stratifying by distance to the nearest transcription start site (TSS): short (0\u201330k bp), medium (30\u2013100k bp), long (&gt;100k bp).</li> </ul> </li> <li>Key findings:<ul> <li>Caduceus models consistently outperform HyenaDNA across distance buckets.</li> <li>Caduceus-PS often matches or exceeds Nucleotide Transformer v2 (500M params), especially at long ranges.</li> <li>For SNPs &gt;100k bp from TSS, Caduceus even surpasses Enformer, which is a strong, supervised baseline with large receptive fields.</li> <li>These trends demonstrate that Caduceus\u2019 long-range, bi-directional, RC-aware representations are especially powerful when long genomic contexts matter most.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths</li> <li>Biologically grounded inductive biases:<ul> <li>Incorporates bi-directionality and RC equivariance directly into the architecture, aligning model invariances with DNA\u2019s physical properties.</li> </ul> </li> <li>Long-context efficiency:<ul> <li>Uses SSM-based Mamba blocks to handle sequences up to ~131k bps and beyond with linear-time scaling, enabling realistic genomic context sizes.</li> </ul> </li> <li>Strong empirical performance:<ul> <li>Outperforms or matches both other SSM-based models (HyenaDNA) and much larger Transformers (Nucleotide Transformer, Enformer) on many benchmarks.</li> <li>Particularly strong on long-range variant effect prediction, a high-impact biological task.</li> </ul> </li> <li> <p>Clear architectural story:</p> <ul> <li>Breaks down innovations into modular components (BiMamba, MambaDNA, Caduceus-PS/Ph), making it easier to reuse or extend in other settings.</li> </ul> </li> <li> <p>Limitations</p> </li> <li>Single-modality focus:<ul> <li>Only models DNA sequence; does not integrate other relevant data (RNA expression, chromatin contact maps, clinical covariates).</li> </ul> </li> <li>Task coverage:<ul> <li>Evaluations focus on classification tasks and VEP; less attention to generative uses (e.g., sequence design) or interpretability tools (e.g., motif discovery pipelines).</li> </ul> </li> <li>Complexity of RC handling:<ul> <li>While the theory is clean, implementing and debugging RC-equivariant models and post-hoc conjoining pipelines may be non-trivial in practice.</li> </ul> </li> <li> <p>Reliance on external classifiers for VEP:</p> <ul> <li>The VEP evaluation uses SVMs on top of embeddings, which might not fully exploit the capacity of the representations compared to end-to-end fine-tuning.</li> </ul> </li> <li> <p>Open Questions and Future Directions:</p> </li> <li>Multimodal integration:<ul> <li>How do Caduceus embeddings combine with other modalities (e.g., expression, epigenomics, imaging) in late-fusion or contrastive frameworks, as suggested in the integration baseline plan?</li> </ul> </li> <li>Interpretability:<ul> <li>Can the RC-equivariant and bi-directional structure be exploited for clearer attribution of variants, motif discovery, or mechanistic interpretation?</li> </ul> </li> <li>Scaling laws:<ul> <li>How do performance and sample efficiency scale with model size and context length for Mamba-based DNA FMs compared to Transformers?</li> </ul> </li> <li>End-to-end VEP:<ul> <li>Does training end-to-end predictors on top of Caduceus (instead of SVMs) further improve performance and robustness?</li> </ul> </li> <li>Generalization across species and assays:<ul> <li>How well do Caduceus models transfer to other species, tissue types, or new assay types (e.g., single-cell, new histone marks)?</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the FM landscape</li> <li>In the genomics FM space, Caduceus is part of a movement from Transformer-based models (DNABERT, Nucleotide Transformer, Enformer) toward efficient long-range architectures like HyenaDNA and SSM-based FMs.</li> <li>Conceptually, you can think of Caduceus as \u201clike BERT but for DNA, built on Mamba instead of a Transformer, and explicitly aware of DNA\u2019s symmetries\u201d.</li> <li> <p>It complements work on large language models in biology (e.g., protein LMs) by focusing on non-coding genomic sequence and variant effects.</p> </li> <li> <p>Relation to well-known ideas</p> </li> <li>Bi-directionality echoes BERT and ELMo, which improved NLP by considering both left and right context during pretraining.</li> <li>Equivariance connects to broader ML trends of embedding symmetry (e.g., rotational equivariance in vision) into architectures to improve data efficiency and generalization.</li> <li> <p>The use of SSMs like Mamba fits into a growing ecosystem of non-attention-based, long-context sequence learners.</p> </li> <li> <p>Connection to the integration baseline plan</p> </li> <li>The integration baseline plan explicitly cites Caduceus as a reference for genetics embedding hygiene:<ul> <li>RC handling (e.g., RC averaging, post-hoc conjoining) and stable tokenization are central to trustworthy variant-level modeling.</li> </ul> </li> <li>For multimodal or clinical integration projects, Caduceus-style encoders can serve as frozen or lightly fine-tuned DNA feature extractors, feeding into late-fusion models or contrastive frameworks as described in the plan.</li> <li>The evaluation discipline in this paper (clear splits, cross-validation, AUROC/AUPRC reporting) is aligned with the plan\u2019s emphasis on robust, statistically careful evaluation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem</li> <li>Long-range DNA sequence modeling needs models that can handle very long contexts, use bi-directional information, and respect reverse complement symmetry.</li> <li> <p>Existing models either struggle with scaling (Transformers) or ignore key biological invariances (many sequence models).</p> </li> <li> <p>Method / Model</p> </li> <li>The authors introduce BiMamba, a parameter-efficient bi-directional extension of the Mamba SSM, and MambaDNA, an RC-equivariant module built around Mamba/BiMamba.</li> <li>They combine these components into Caduceus, a family of DNA foundation models trained with masked language modeling on the human genome.</li> <li> <p>Two main variants, Caduceus-PS (RC-equivariant via parameter sharing) and Caduceus-Ph (post-hoc RC conjoining), provide different trade-offs between strict equivariance and practical performance.</p> </li> <li> <p>Results</p> </li> <li>Mamba-based models achieve better pretraining loss than HyenaDNA and benefit from weight tying and RC equivariance.</li> <li>On Genomics Benchmarks, Caduceus variants consistently outperform CNN, HyenaDNA, and non-RC Mamba baselines across eight regulatory tasks.</li> <li>On the Nucleotide Transformer benchmark, Caduceus (~1.9M params) matches or surpasses much larger Transformer models on many histone and regulatory tasks while beating HyenaDNA on most of them.</li> <li> <p>For variant effect prediction, Caduceus models outperform HyenaDNA and Caduceus-PS exceeds the performance of a 500M-parameter Nucleotide Transformer and even Enformer at long distances to TSS.</p> </li> <li> <p>Why it matters</p> </li> <li>Caduceus shows that carefully designed, symmetry-aware, long-range SSM-based architectures can function as powerful genomic foundation models at modest scale.</li> <li>For a grad student, this paper is a template for combining domain knowledge (DNA symmetries) with modern sequence modeling techniques to achieve strong performance on biologically important tasks like variant effect prediction.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/","title":"Integrating Multimodal Data Through Interpretable Heterogeneous Ensembles","text":"<p>Authors: Yan Chak Li, Linhua Wang, Jeffrey N. Law, T. M. Murali, Gaurav Pandey Year: 2022 Venue: Bioinformatics Advances</p>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Multimodal / Integration. The paper focuses on integrating heterogeneous biomedical data types (STRING multi-omic networks and clinical EHR modalities) to improve prediction of protein function and COVID-19 mortality, and on making these integrations interpretable.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Multimodal FM or cross-modal integration (conceptual). While the work does not introduce a neural \u201cfoundation model\u201d in the modern sense, it develops a general-purpose, reusable late-integration framework (Ensemble Integration, EI) for combining modality-specific models, which fills a similar role for heterogeneous biomedical data.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Protein function prediction: multi-omic STRING network modalities (protein\u2013protein interaction, curated databases, co-expression, genomic neighborhood, co-occurrence, fusion networks).  </li> <li>Clinical prediction: EHR-derived admission features, comorbidities, vital signs, laboratory test measurements for COVID-19 patients.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces Ensemble Integration (EI), a framework for integrating multimodal biomedical data by first learning separate models for each modality and then combining those models using heterogeneous ensembles. The authors argue that common \u201cearly\u201d and \u201cintermediate\u201d integration methods, which merge data into a single representation, often lose modality-specific signals and therefore underperform when modalities have very different structures and semantics. EI instead treats each modality as its own prediction problem, trains strong local classifiers per modality, and then fuses their predictions via ensemble methods such as mean aggregation, ensemble selection, and stacking. The framework is applied to two challenging tasks: predicting protein functions from multimodal STRING network data and predicting COVID-19 mortality from multimodal EHR data. Across both applications, EI consistently outperforms single-modality models and strong early-integration baselines like Mashup, deepNF, and XGBoost. The paper also introduces an interpretation method that ranks features across all modalities by their contribution to the final ensemble, revealing biologically and clinically meaningful predictors. A new grad student should care because EI offers a practical, interpretable baseline strategy for multimodal integration that respects modality-specific structure and can be combined with more modern foundation-model embeddings.</p>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>How to integrate heterogeneous biomedical data modalities to predict important outcomes such as protein function and patient mortality.  </li> <li>For protein function prediction (PFP), the goal is to predict Gene Ontology (GO) annotations for human proteins using multiple STRING-derived networks that capture different biological relationships.  </li> <li> <p>For COVID-19 mortality prediction, the goal is to predict whether a hospitalized patient will die from COVID-19 using multimodal EHR data (admission characteristics, comorbidities, vital signs, lab tests).</p> </li> <li> <p>Why existing approaches are insufficient: </p> </li> <li>Heterogeneous semantics: Different modalities have different structures (e.g., dense gene expression matrices vs. graph-structured protein\u2013protein interactions vs. time-series vital signs), making a single joint representation hard to design.  </li> <li>Early / intermediate integration limitations: Methods that first create a single integrated network or embedding tend to emphasize agreement between modalities but can suppress modality-specific signals that are important for prediction.  </li> <li>Lack of systematic late integration: While late integration (combining predictions from modality-specific models) has been discussed, it has not been systematically developed or evaluated for multimodal biomedical prediction tasks.  </li> <li> <p>Interpretability challenges: Complex integrated models are often \u201cblack boxes,\u201d making it difficult to understand which modalities and features drive predictions, which is especially problematic in biomedical and clinical settings.</p> </li> <li> <p>Motivating idea: </p> </li> <li>Treat each modality as a first-class citizen by training specialized models that best match its structure, and then combine these local models with flexible ensemble methods.  </li> <li>Provide a principled way to interpret the resulting ensembles by quantifying how much each feature and each local model contributes to the final predictions.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used: </li> <li>STRING protein function prediction: <ul> <li>Version 11.5 of STRING, focusing on human proteins and their pairwise functional associations.  </li> <li>Predicts GO Molecular Function and Biological Process annotations for 18,866 human proteins and 2,139 GO terms (each with at least 50 annotated proteins).  </li> </ul> </li> <li> <p>COVID-19 mortality prediction (EHR): </p> <ul> <li>EHR data from 4,783 COVID-19 patients treated at Mount Sinai (March 15\u2013October 1, 2020).  </li> <li>Outcome: in-hospital mortality (27.7% positive cases).</li> </ul> </li> <li> <p>Modalities: </p> </li> <li>STRING multimodal networks (for PFP): <ul> <li>Protein\u2013protein interactions (PPI).  </li> <li>Curated database interactions.  </li> <li>Co-expression networks.  </li> <li>Genomic neighborhood.  </li> <li>Co-occurrence of orthologs across genomes.  </li> <li>Fusion events of orthologs.  </li> </ul> </li> <li> <p>EHR modalities (for COVID-19): </p> <ul> <li>Admission: demographic and baseline clinical variables (e.g., age, sex, race/ethnicity, some vital signs at admission).  </li> <li>Comorbidities: presence of other conditions (e.g., asthma, obesity).  </li> <li>Vital signs: max/min heart rate, body temperature, respiratory rate, blood pressure, oxygen saturation over the first 36 hours.  </li> <li>Laboratory tests: various lab measurements (e.g., BUN, calcium, sodium, white blood cell count), with 44 features retained.</li> </ul> </li> <li> <p>Preprocessing / representation: </p> </li> <li>STRING: Each protein is represented by its adjacency vector in each network modality; missing proteins receive all-zero vectors.  </li> <li>PFP labels: Positive examples are proteins manually annotated (non-IEA evidence) to a GO term; negatives are proteins with other annotations in the same ontology but not to that term or its relatives.  </li> <li>EHR: <ul> <li>Only features with &lt;30% missingness are retained.  </li> <li>Remaining missing values within each modality are imputed using KNNImpute with (K=5).  </li> <li>Categorical variables are one-hot encoded; continuous variables are standardized to z-scores.  </li> <li>For repeated vital signs and labs, the first values within 36 hours of hospitalization are used to enable early prediction.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li>EI is a heterogeneous ensemble framework that combines multiple base classifiers trained on separate modalities.  </li> <li>Base models are standard supervised learning algorithms (e.g., decision trees, random forests, SVMs, logistic regression, k-NN, Naive Bayes, boosting methods).  </li> <li> <p>Ensemble methods used for integration include simple averaging, ensemble selection, and stacking with various meta-learners.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li>The paper does not introduce a large-scale neural foundation model.  </li> <li> <p>Instead, it proposes a general-purpose late integration framework (EI) that can sit \u201con top of\u201d any modality-specific models, including future foundation-model encoders for genomics or clinical data.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Component Role in EI Local models per modality Capture modality-specific structure and signals Heterogeneous ensembles Combine diverse local models into a global predictor Mean aggregation Simple averaging of local predictions Caruana\u2019s ensemble selection Greedy selection of local models to maximize performance Stacking Meta-model trained on local-model predictions Feature-importance interpretation Ranks features via combined local feature and model importance <ul> <li>Training setup (as described): </li> <li>Local models (all modalities): <ul> <li>Trained with 10 standard binary classifiers in Weka: AdaBoost, decision tree (J48), gradient boosting, k-NN, SVM, random forest (RF), logistic regression (LR), PART (rule-based), Naive Bayes, Voted Perceptron.  </li> <li>Class imbalance handled by random undersampling of the majority (negative) class in training; test sets preserve original class ratios.  </li> </ul> </li> <li>Ensemble methods: <ul> <li>Mean aggregation.  </li> <li>Caruana-style ensemble selection (CES) that iteratively adds models that most improve validation performance.  </li> <li>Stacking with meta-predictors from scikit-learn: AdaBoost, decision tree, gradient boosting, k-NN, SVM (linear kernel), RF, LR, Naive Bayes, plus XGBoost as an additional meta-classifier.  </li> </ul> </li> <li>Evaluation protocol: <ul> <li>EI and heterogeneous-ensemble baselines are trained and evaluated using 5-fold nested cross-validation to separate local-model and ensemble training.  </li> <li>Mashup, deepNF, and XGBoost baselines use standard 5-fold cross-validation.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This paper is fundamentally about multimodal data integration, and EI is a concrete implementation of a late integration strategy.</p> <ul> <li>Which modalities are integrated? </li> <li>For PFP, EI integrates multiple STRING network modalities capturing complementary biological evidence (PPIs, curated databases, co-expression, genomic neighborhood, co-occurrence, fusions).  </li> <li> <p>For COVID-19 mortality, EI integrates clinical admission data, comorbidities, vital signs, and lab tests.</p> </li> <li> <p>How are they integrated? </p> </li> <li>EI follows a strict late integration pipeline:  <ol> <li>Train multiple local models per modality using algorithms appropriate for that modality.  </li> <li>Obtain prediction scores from each local model.  </li> <li>Combine these scores across modalities using heterogeneous ensemble methods (mean, ensemble selection, stacking).  </li> </ol> </li> <li> <p>No single \u201cjoint embedding\u201d or early-fusion feature vector is constructed; instead, integration happens purely at the level of model outputs.</p> </li> <li> <p>Why this integration is useful / what new capabilities it gives: </p> </li> <li>Preserves modality-specific signals by allowing each modality to use its most suitable modeling approach, rather than forcing all data into a common structure.  </li> <li>Flexible enough to incorporate arbitrary new modalities (e.g., new omics assays, imaging-derived features, or embeddings from foundation models) by simply adding new local models.  </li> <li> <p>Provides an interpretation mechanism that attributes importance to features across modalities via a unified ranking, which is crucial for biomedical trust and discovery.</p> </li> <li> <p>Relation to the Integration Baseline Plan:</p> </li> <li>The paper operationalizes the principle: \u201cPrefer late integration first under heterogeneous semantics.\u201d EI explicitly avoids premature joint spaces, instead building strong per-modality models and then combining them, which directly aligns with the plan\u2019s recommendation to preserve modality-specific signals and avoid over-aggressive early fusion.  </li> <li>EI\u2019s approach resembles the plan\u2019s suggestion of concatenating compact per-modality features and training robust tabular models\u2014but at a higher level: EI concatenates prediction scores (rather than raw features) and uses ensembles like LR, RF, and boosting as meta-models.  </li> <li>Regarding robustness and evaluation discipline, EI uses nested cross-validation, imbalance-aware metrics (AUPRC, Fmax), and formal statistical tests (Friedman\u2013Nemenyi, Wilcoxon tests), which is very much in the spirit of the plan\u2019s emphasis on disciplined evaluation and uncertainty quantification.  </li> <li>The plan\u2019s emphasis on CCA and permutation tests is conceptually related to EI\u2019s interpretation method: both aim to understand cross-modal relationships and contributions. EI\u2019s permutation-based local model ranks (LMRs) and feature ranks (LFRs) serve a similar diagnostic role for model-level integration.  </li> <li>Overall, EI provides a concrete, well-validated example of late fusion via heterogeneous ensembles, which can be adopted as the default multimodal baseline before moving to more complex contrastive or joint-embedding approaches described in the integration baseline.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Protein function prediction (PFP): Predict GO Molecular Function and Biological Process annotations for 2,139 GO terms using multimodal STRING networks.  </li> <li> <p>COVID-19 mortality prediction: Predict in-hospital death for COVID-19 patients using multimodal EHR features.</p> </li> <li> <p>Baselines: </p> </li> <li>For PFP: <ul> <li>Early integration methods: Mashup and deepNF, which fuse multiple networks into a single integrated network before classification.  </li> <li>Single-modality baselines: heterogeneous ensembles trained separately on each STRING modality.  </li> </ul> </li> <li> <p>For COVID-19 mortality: </p> <ul> <li>Early integration baseline: XGBoost trained on the concatenated feature vector across all EHR modalities (a strong tabular-data baseline).  </li> <li>Single-modality baselines: heterogeneous ensembles trained on each EHR modality individually.</li> </ul> </li> <li> <p>Key findings (PFP): </p> </li> <li>EI achieves significantly higher Fmax scores than Mashup, deepNF, and any single STRING modality across 2,139 GO terms.  </li> <li>This performance advantage persists across GO terms with varying depth, information content, and number of annotated proteins, although performance understandably decreases for terms with very few annotations.  </li> <li>Stacking with RF and LR as meta-learners tends to perform best among EI variants, consistent with previous work on heterogeneous ensembles.  </li> <li> <p>The only setting where EI underperforms is for GO terms with very few annotations (50\u2013100), where Mashup slightly outperforms EI.</p> </li> <li> <p>Key findings (COVID-19 mortality): </p> </li> <li>EI outperforms ensembles trained on individual EHR modalities and slightly surpasses the early-integration XGBoost baseline in Fmax and AUROC.  </li> <li>The best EI variant (stacking with LR) achieves a modest but meaningful improvement in Fmax over XGBoost, with a slightly better balance of precision and recall.  </li> <li> <p>EI-based predictions confirm that laboratory test features are particularly informative, but admission and comorbidity features also contribute.</p> </li> <li> <p>Interpretation results: </p> </li> <li>The proposed interpretation method identifies the top-contributing features for the best EI model in the COVID-19 task.  </li> <li>Top features include age at admission, minimum oxygen saturation, blood urea nitrogen (BUN), calcium, chloride, sodium, venous ( \\text{PCO}_2 ), respiratory rate, and atrial fibrillation\u2014variables known to be clinically relevant to COVID-19 severity and mortality.  </li> <li>There is statistically significant overlap between EI\u2019s top features and those highlighted by XGBoost\u2019s SHAP-based importance, supporting the validity of EI\u2019s explanations.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths: </li> <li>Provides a clear, general framework for late integration that can be applied to many multimodal biomedical problems.  </li> <li>Preserves modality-specific structure and signals by training specialized local models instead of forcing all data into a common representation.  </li> <li>Demonstrates strong empirical performance on two very different tasks (PFP and clinical mortality prediction) relative to established early-integration baselines.  </li> <li>Introduces a model-agnostic interpretation method that yields clinically and biologically meaningful feature rankings, increasing trust and enabling scientific insight.  </li> <li> <p>Uses rigorous evaluation practices (nested CV, appropriate metrics, statistical tests), aligning well with best-practice recommendations like DOME.</p> </li> <li> <p>Limitations: </p> </li> <li>The framework is evaluated mainly on structured data and classical machine learning models; it does not yet incorporate modern deep or foundation-model encoders for unstructured inputs (e.g., sequences, images, free text).  </li> <li>Comparisons focus on early integration (Mashup, deepNF, XGBoost); intermediate integration methods are not systematically evaluated.  </li> <li>Interpretation is only fully explored for the COVID-19 mortality task; PFP interpretations for thousands of GO terms are not examined in depth.  </li> <li>The interpretation method uses AUPRC-based ranks and percentile normalization, which may slightly bias importance toward modalities with more features.  </li> <li> <p>Computational cost may increase with many modalities and large libraries of local models, since both ensemble selection and permutation-based importance are relatively heavy.</p> </li> <li> <p>Open Questions and Future Directions: </p> </li> <li>How does EI perform when local models are foundation-model encoders (e.g., protein language models, EHR transformers, imaging FMs) instead of classical classifiers?  </li> <li>Can we design hybrid integration schemes that combine EI-style late fusion with intermediate or contrastive objectives (e.g., CCA, cross-modal contrastive learning) to better exploit cross-modal structure?  </li> <li>How can the interpretation framework be extended to handle thousands of labels (as in PFP) in a scalable way, perhaps by grouping labels or focusing on biologically meaningful subsets?  </li> <li>Can we mitigate the bias of the current ranking scheme toward feature-rich modalities, for example by normalizing importance across modalities or using multi-task feature selection?  </li> <li>What are the best practices for choosing and tuning the library of local models and meta-learners in EI when applying it to new domains or datasets?</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>Within the broader space of foundation models and multimodal integration, EI provides a strong, interpretable late-integration baseline for combining outputs from many models or modalities.  </li> <li>For genomics and proteomics, it complements work on network integration (Mashup, deepNF) by showing that keeping network modalities separate and integrating predictions can outperform aggressive early fusion.  </li> <li> <p>For clinical applications, EI offers an alternative to monolithic models like XGBoost or single neural networks, emphasizing modularity and interpretability.</p> </li> <li> <p>Relation to well-known ideas: </p> </li> <li>Conceptually, EI is like building a committee of experts, where each expert specializes in one modality, and a supervisor aggregates their opinions via ensemble methods.  </li> <li>It aligns with ensemble learning ideas such as stacking and ensemble selection, and can be viewed as a late-fusion wrapper around any set of modality-specific models, including deep FMs.  </li> <li> <p>From the perspective of modern FMs, you can imagine replacing local Weka classifiers with frozen or fine-tuned foundation models that output per-modality predictions or embeddings, which EI then integrates.</p> </li> <li> <p>Relevance to the integration baseline plan: </p> </li> <li>EI is a concrete instantiation of the plan\u2019s recommendation to start with late fusion under heterogeneous semantics, using robust tabular models and careful evaluation before moving to more complex multimodal architectures.  </li> <li>The paper\u2019s nested CV, imbalance-aware metrics, and statistical comparisons mirror the plan\u2019s emphasis on robustness and evaluation discipline, making it a strong methodological template.  </li> <li>For future multimodal FM work (e.g., combining brain FMs, genomic FMs, and clinical data), EI can serve both as a baseline pipeline and as a reusable integration layer atop pre-trained encoders.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li>Integrating heterogeneous multimodal biomedical data (e.g., STRING networks, EHR features) is crucial for accurate prediction of protein function and clinical outcomes but is challenging due to differing data structures and semantics.  </li> <li> <p>Early and intermediate integration approaches that force data into a single representation can lose modality-specific information and underperform.</p> </li> <li> <p>Method / model: </p> </li> <li>Ensemble Integration (EI) is a late integration framework that first trains multiple local models per modality and then combines their prediction scores using heterogeneous ensemble methods (mean aggregation, ensemble selection, stacking).  </li> <li>EI is model-agnostic and can work with any base classifier, enabling flexible integration of diverse modalities and algorithms.  </li> <li> <p>A novel interpretation method combines local feature ranks and local model ranks to produce a global ranking of features across all modalities.</p> </li> <li> <p>Results: </p> </li> <li>On protein function prediction with multimodal STRING data, EI significantly outperforms early integration methods (Mashup, deepNF) and single-modality baselines across thousands of GO terms.  </li> <li>On COVID-19 mortality prediction from EHR data, EI slightly but consistently outperforms a strong early-integration XGBoost baseline and individual-modality ensembles.  </li> <li> <p>The interpretation framework highlights clinically and biologically meaningful features (e.g., age, oxygen saturation, BUN, calcium), with significant overlap with SHAP-based importances from XGBoost.</p> </li> <li> <p>Why it matters: </p> </li> <li>EI offers a practical, interpretable, and extensible baseline for multimodal integration that respects modality-specific signals and uses rigorous evaluation.  </li> <li>It provides a natural way to integrate outputs from future foundation models for genomics, proteomics, and clinical data, making it highly relevant for students and researchers planning multimodal FM systems.</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/","title":"Genome Modeling And Design Across All Domains Of Life With Evo 2","text":"<p>Authors: Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher R\u00e9, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie Year: 2025 Venue: bioRxiv preprint</p>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. Evo 2 is a large-scale foundation model trained purely on DNA sequences from genomes across all domains of life, used for prediction and generation tasks that span molecular, genomic, and epigenomic levels.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. The paper introduces a new large biological foundation model (Evo 2) together with a new multi-hybrid architecture (StripedHyena 2), training recipe, and evaluation suite, rather than just applying an existing FM.</p> </li> <li> <p>Key Modalities: </p> </li> <li>DNA sequence (genomic, organelle, and metagenomic).  </li> <li>RNA and protein effects (accessed via DNA-centric modeling and downstream assays, not as separate input modalities).  </li> <li>Epigenomic profiles (chromatin accessibility) used as downstream design targets via external predictive models (Enformer, Borzoi).</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper presents Evo 2, a large-scale biological foundation model that learns directly from DNA sequences across all domains of life to support both prediction and generation of genomic functions. The authors train 7B and 40B parameter models on 9.3 trillion nucleotides with context windows up to 1 million base pairs, enabling the model to reason over entire genomes rather than short fragments. Evo 2 is evaluated on a wide range of tasks, including zero-shot prediction of mutational effects on proteins, noncoding RNAs, and organismal fitness, as well as clinical variant effect prediction for human genes like BRCA1/2. They show that Evo 2 achieves competitive or state-of-the-art performance, especially for noncoding and splice variants, without task-specific fine-tuning and while remaining alignment-free (no multiple sequence alignments). Using mechanistic interpretability tools, they reveal that the model\u2019s internal features correspond to biologically meaningful concepts such as exons, introns, transcription factor motifs, protein secondary structures, and prophage regions. Finally, the paper demonstrates genome-scale DNA generation and guided \u201cgenerative epigenomics,\u201d where Evo 2 is steered at inference time by epigenomic predictors to design sequences with specified chromatin accessibility patterns. Overall, Evo 2 is positioned as a generalist genomic language model and design engine that can underpin many downstream tasks in computational biology.</p>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>How to build a single machine learning model that can understand and design genomic DNA across the full tree of life, from bacteria to humans, at scales ranging from short motifs to entire genomes.</li> <li>The model should support zero-shot mutational effect prediction (how mutations affect fitness or function) across proteins, RNAs, and noncoding regions, without relying on multiple sequence alignments or task-specific training.</li> <li> <p>It should also enable genome-scale sequence generation, including organelle genomes and full chromosomes, and support controllable design of higher-level properties such as chromatin accessibility.</p> </li> <li> <p>Why this is hard</p> </li> <li>Data scale and diversity: Genomes vary widely in size and composition (small, compact prokaryotic genomes vs. large, intron-rich eukaryotic genomes with extensive noncoding and repetitive DNA). Capturing useful patterns across this diversity requires massive, carefully curated datasets.</li> <li>Long-range dependencies: Many genomic functions (e.g., regulatory interactions, chromatin organization) depend on relationships spread over tens of kilobases to megabases, far beyond the context lengths of typical sequence models.</li> <li>Multiple biological \u201cmodalities\u201d along the central dogma: DNA variation influences RNA, protein, and organismal phenotypes; a useful model must implicitly encode patterns across these levels while only seeing DNA sequences.</li> <li>Noncoding and regulatory elements: Noncoding variants and regulatory regions (enhancers, splice sites, chromatin features) are complex and \u201cfuzzy,\u201d making them harder to model than coding regions where the genetic code provides a more direct mapping to function.</li> <li>Safety and biosecurity: Large biological foundation models could in principle be misused (e.g., in viral design), so the training data and evaluation strategy must be carefully structured to limit harmful capabilities while retaining broad utility.</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>OpenGenome2: a large, curated genomic dataset totaling 8.84\u20139.3 trillion nucleotides, strongly expanding the earlier OpenGenome dataset.<ul> <li>Representative prokaryotic genomes (bacteria and archaea), expanded from earlier GTDB releases.</li> <li>Eukaryotic reference genomes from NCBI (16,000+ genomes), focusing on primary assemblies and non-nuclear contigs.</li> <li>Organelle genomes (mitochondria and others), plus metagenomic sequencing data.</li> <li>Function-focused subsets around coding genes (windows near genes and other functional regions) to enrich training for downstream tasks.</li> </ul> </li> <li> <p>Evaluation datasets (not all named explicitly in the excerpt, but outlined conceptually):</p> <ul> <li>Deep mutational scanning (DMS) datasets for proteins and noncoding RNAs.</li> <li>Human mRNA stability data (for decay rates).</li> <li>Variant effect prediction datasets: ClinVar, SpliceVarDB, BRCA1/2 saturation mutagenesis.</li> <li>Organismal fitness and gene essentiality datasets for bacteria, phage, and human lncRNAs.</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Primary input modality: DNA sequence, tokenized at single-nucleotide resolution.</li> <li>Implicit biological modalities captured through tasks: protein function, RNA function, noncoding regulatory function, and organismal fitness (all inferred from DNA variation).</li> <li> <p>Epigenomic modality: chromatin accessibility tracks, used via external predictors (Enformer and Borzoi) for guided design, not as training inputs for Evo 2.</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>DNA sequences are byte-tokenized and presented as long contiguous sequences, with a context length up to 1 million tokens during midtraining.</li> <li>Training uses sequence packing and a reweighted cross-entropy loss that downweights repetitive regions to better calibrate model likelihoods between repetitive and non-repetitive DNA.</li> <li>Lowercase annotations for repeats are used early in pretraining, then removed later so the model learns stable representations of repetitive vs non-repetitive regions.</li> <li>Special tokens (e.g., stitch tokens and phylogenetic tags) are used to condition the model and are masked out in the loss.</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Evo 2 is an autoregressive genomic language model built on StripedHyena 2, a multi-hybrid architecture that combines input-dependent convolutions with attention.</li> <li> <p>It is analogous to large language models (LLMs) in NLP but operates on nucleotide sequences and is optimized for extremely long contexts.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>New FM. Evo 2 extends the earlier Evo 1 models with a new architecture, larger parameter counts, vastly more training data, and a much longer context window (up to 1M bases).</p> </li> <li> <p>Key components and innovations</p> </li> <li>StripedHyena 2 architecture:<ul> <li>Uses multiple kinds of input-dependent convolution operators (Hyena-SE, Hyena-MR, Hyena-LI) interleaved with attention, arranged in a \u201cstriped\u201d pattern.</li> <li>Designed to balance quality and efficiency, giving higher throughput than Transformers and earlier StripedHyena variants, especially at long sequence lengths.</li> <li>Achieves up to 3\u00d7 speedup at 1M context compared to optimized Transformers at 40B scale.</li> </ul> </li> <li>Multi-phase training:<ul> <li>Pretraining phase at shorter context lengths (1024\u21928192 tokens), enriched for genic windows and high-information regions to learn core biological features efficiently.</li> <li>Midtraining phase for context extension to 1M tokens using rotary positional embeddings with positional interpolation and base frequency scaling, plus more whole-genome sequences.</li> </ul> </li> <li>Model sizes and configuration:<ul> <li>Evo 2 40B: ~40.3B parameters, 50 layers, hidden size ~8192, trained on 9.3T tokens.</li> <li>Evo 2 7B: ~6.5B parameters, 32 layers, trained on 2.4T tokens.</li> <li>Evo 2 1B base: smaller 1.1B variant trained on 1T tokens (also released).</li> </ul> </li> <li> <p>Loss and training tricks:</p> <ul> <li>Reweighted cross-entropy that reduces emphasis on repetitive DNA.</li> <li>Context-parallel training infrastructure (Savanna) with 3D parallelism (data, tensor, context) and mixed precision (FP8 in some components).</li> <li>A new needle-in-a-haystack evaluation to test long-context retrieval at up to 1M bases using a categorical Jacobian-based retrieval score.</li> </ul> </li> <li> <p>Training setup (as far as available)</p> </li> </ul> Aspect Details Objective Autoregressive next-token prediction on byte-tokenized DNA Parameters 1.1B, 6.5B, and 40.3B variants Tokens seen Up to 9.3T tokens for Evo 2 40B Context length 8,192 in pretraining, up to 1,048,576 (\u22481M) in midtraining Architecture StripedHyena 2 multi-hybrid (convolutions + attention + RoPE) Optimizer AdamW with cosine LR decay Infrastructure Custom stack (Savanna) with DeepSpeed, GPT-NeoX, Transformer Engine Availability Open-source model weights, training and inference code, OpenGenome2"},{"location":"generated/kb_curated/papers-md/evo2_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This paper does not focus on multimodal integration in the sense of combining heterogeneous data modalities like imaging, clinical features, and genomics into a single model or fusion pipeline. Evo 2 operates on DNA sequence alone and learns representations that implicitly capture information relevant to proteins, RNAs, and organismal fitness; epigenomic aspects (chromatin accessibility) enter only through external predictive models used to score generated sequences.</p> <p>That said, Evo 2 is highly relevant to multimodal integration pipelines such as those outlined in the Integration Baseline Plan: - Evo 2 can serve as a genomics encoder that produces compact sequence embeddings for downstream integration with other modalities using late fusion (e.g., concatenating Evo 2-derived features with imaging or clinical features for logistic regression or gradient-boosted trees). - Its strong zero-shot performance and robust variant scores align with the plan\u2019s emphasis on modality-specific modeling first, followed by disciplined late integration and careful evaluation (e.g., AUROC/AUPRC with CIs, bootstrapping or DeLong tests). - Mechanistic interpretability features (e.g., exon, splice, motif features) could act as interpretable genomic covariates in CCA or partial correlation analyses, fitting nicely into integration strategies that emphasize interpretability before moving to heavier fusion models.</p>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks</li> <li>Zero-shot mutational effect prediction across proteins, RNAs, and genomes from multiple domains of life, using changes in Evo 2 likelihood when introducing mutations.</li> <li>Human variant effect prediction (VEP):<ul> <li>Predicting pathogenic vs benign variants in ClinVar for both coding and noncoding variants, including indels and other non-SNVs.</li> <li>Predicting splice-altering variants using SpliceVarDB, split into exonic and intronic variants.</li> <li>Predicting functional consequences of variants in BRCA1 and BRCA2 from saturation mutagenesis/scanning datasets.</li> </ul> </li> <li>Organismal fitness and gene essentiality:<ul> <li>Zero-shot prediction of gene essentiality in bacteria and phage using mutational likelihood of premature stop codons.</li> <li>Prediction of human lncRNA essentiality using scrambled subsequences.</li> </ul> </li> <li>Mechanistic interpretability:<ul> <li>Training sparse autoencoders over Evo 2 representations to identify latent features corresponding to genomic and protein-level concepts.</li> </ul> </li> <li> <p>Generative tasks:</p> <ul> <li>Genome-scale generation of mitochondrial genomes, minimal bacterial genomes (e.g., M. genitalium), and yeast chromosome-scale DNA.</li> <li>Generative epigenomics: designing sequences with specified chromatin accessibility patterns using Enformer/Borzoi-guided inference-time search.</li> </ul> </li> <li> <p>Baselines</p> </li> <li>Sequence and variant models: Nucleotide Transformer, Evo 1, and specialized variant effect models such as AlphaMissense and GPN-MSA.</li> <li>For DMS and fitness tasks: State-of-the-art autoregressive protein LMs and DNA LMs used for mutational effect prediction.</li> <li> <p>For generation and structure: Comparisons to Evo 1 and to natural sequence/structure distributions (e.g., using Pfam, ESMFold, AlphaFold 3).</p> </li> <li> <p>Key findings</p> </li> <li>Zero-shot mutational effects &amp; biological priors<ul> <li>Evo 2 likelihood changes correctly reflect fundamental coding properties: strong penalties for mutations in start/stop codons, triplet codon periodicity, and decreased penalties at wobble positions.</li> <li>The model distinguishes different genetic codes (e.g., standard vs mycoplasma vs ciliate) and requires longer contexts (4\u20138 kb) to correctly infer some species-specific codes, highlighting the benefit of long context.</li> <li>Likelihood changes correlate with known constraints: non-synonymous, frameshift, and stop-gain mutations are more deleterious than synonymous mutations; deletions in essential RNAs (tRNAs, rRNAs) are more damaging than in intergenic regions.</li> </ul> </li> <li>DMS and fitness prediction<ul> <li>Evo 2\u2019s zero-shot likelihoods correlate well with DMS fitness measurements across diverse proteins and noncoding RNAs, matching or exceeding state-of-the-art protein LMs, especially for ncRNAs.</li> <li>For human mRNAs, Evo 2 likelihoods negatively correlate with mRNA decay rates (higher likelihood \u2194 more stable mRNA), and the 40B model performs better than 7B.</li> <li>Evo 2 matches Evo 1 on prokaryotic gene essentiality and extends predictive power to eukaryotic noncoding elements like lncRNAs, outperforming baselines.</li> </ul> </li> <li>Clinical variant prediction<ul> <li>In ClinVar, Evo 2 is competitive for coding SNVs and outperforms baselines for non-SNVs (indels) and noncoding variants, where many specialized models perform poorly or cannot score variants.</li> <li>In SpliceVarDB, Evo 2 achieves top zero-shot performance for both exonic and intronic splice variants.</li> <li>For BRCA1/2 datasets, Evo 2 sets or matches state-of-the-art performance, particularly when combining coding and noncoding variants.</li> <li>A supervised classifier built on Evo 2 embeddings further improves performance, achieving AUROCs up to ~0.95 on BRCA1 SNVs and outperforming AlphaMissense and other baselines.</li> </ul> </li> <li>Mechanistic interpretability<ul> <li>Sparse autoencoders over Evo 2 representations reveal features aligned with:</li> <li>Prophage and mobile genetic elements.</li> <li>ORFs, intergenic regions, tRNAs, rRNAs.</li> <li>Protein secondary structures (\u03b1-helices, \u03b2-sheets).</li> <li>Exon\u2013intron architectures (including features that fire at specific exon boundaries).</li> <li>Promoter motifs and transcription factor binding-like patterns in human genome regions.</li> <li>These features generalize to out-of-training genomes, e.g., annotating exon\u2013intron structure in the woolly mammoth genome.</li> </ul> </li> <li>Genome-scale generation<ul> <li>Evo 2 generates mitochondrial genomes with correct counts and arrangements of coding, tRNA, and rRNA genes, with diverse but structurally plausible proteins.</li> <li>It generates minimal bacterial genomes (M. genitalium scale) where ~70% of predicted genes have significant Pfam hits, a large improvement over Evo 1.</li> <li>Yeast chromosome-scale generation yields sequences with recognizable genes, introns, promoters, and tRNAs, with proteins that resemble natural yeast proteins in sequence and structure.</li> <li>Viral protein generation from human-infecting viruses remains poor by design, reflecting intentional data exclusion and risk mitigation.</li> </ul> </li> <li>Generative epigenomics and inference-time scaling<ul> <li>Using Enformer and Borzoi to score chromatin accessibility, a beam-search-based inference-time procedure produces sequences whose predicted accessibility matches desired patterns (e.g., open vs closed regions).</li> <li>Increasing inference-time compute (wider beam search, more sampled chunks) shows a log-linear improvement in design success (higher AUROC for distinguishing open vs closed regions), demonstrating inference-time scaling laws for a biological design task.</li> <li>The method is expressive enough to encode Morse-code messages in chromatin accessibility peaks, while maintaining natural sequence statistics (e.g., dinucleotide distributions).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths</li> <li>Scale and coverage: Evo 2 is trained on one of the largest curated genomic datasets to date, spanning all domains of life and multiple genomic contexts, and achieves a 1M-token context window.</li> <li>Generalist capabilities: A single model supports diverse tasks: zero-shot mutational effect prediction, clinical variant scoring, genome-scale generation, and guided epigenomic design.</li> <li>Strong performance on hard regimes: Evo 2 is particularly strong for noncoding, splice, and indel variants, where many existing models struggle.</li> <li>Mechanistic interpretability: The use of sparse autoencoders reveals interpretable features that align with biological concepts, improving trust and offering new tools for discovery and annotation.</li> <li>Open release: Model weights, training and inference code, and the OpenGenome2 dataset are released under an open license, enabling reproducibility and community-driven extensions.</li> <li> <p>Safety-aware design: The training corpus deliberately excludes pathogenic viral genomes for eukaryotic hosts, and empirical tests show degraded performance in that domain, demonstrating concrete risk mitigation.</p> </li> <li> <p>Limitations</p> </li> <li>DNA-only input: Despite implicitly touching proteins, RNAs, and epigenomics, Evo 2 only ingests DNA; it does not explicitly integrate multi-omic modalities (e.g., expression, proteomics) or other data types like imaging or clinical variables.</li> <li>Reliance on external models for function-specific design: Generative epigenomics depends on Enformer and Borzoi for scoring; Evo 2 itself is not directly conditioned on chromatin states or other annotations.</li> <li>Dataset and species bias: Although the training set is huge, it is still limited by what genomes are available and curated, and some domains (e.g., viruses infecting humans) are intentionally underrepresented.</li> <li>Compute and infrastructure requirements: Training and inference at 40B scale with 1M context is expensive and requires specialized infrastructure (Savanna, Vortex), limiting who can retrain or significantly modify the model.</li> <li> <p>Interpretability coverage: Even with SAEs, only a subset of latent features is mapped to interpretable biological concepts; many features likely encode higher-order or mixed patterns that remain to be understood.</p> </li> <li> <p>Open Questions and Future Directions:</p> </li> <li>How can Evo 2 embeddings be integrated with population-scale human genetic variation (e.g., biobanks) to further improve pathogenicity prediction and identify subtle regulatory effects?</li> <li>Can mechanistic interpretability tools (e.g., feature steering, activation clamping) be used to control specific biological properties during generation, such as alternative splicing or tissue-specific regulation?</li> <li>What are the best strategies to combine Evo 2 with other modalities (transcriptomics, epigenomics, imaging) in late fusion or two-tower architectures, in line with the integration baseline plan?</li> <li>How robust are Evo 2\u2019s predictions across underrepresented species or genomic contexts, and what additional data curation or training strategies would reduce biases?</li> <li>Can inference-time design frameworks be generalized beyond chromatin accessibility to optimize for other complex properties (e.g., protein\u2013protein interaction networks, gene circuit behavior) while maintaining safety?</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<p>Evo 2 sits at the frontier of foundation models for genomics, analogous to large language models in NLP but operating directly on DNA across the tree of life. Compared to earlier genomic models and protein LMs, Evo 2 pushes three main frontiers at once: scale of data, model size and architecture, and context length, enabling it to capture patterns from local motifs up to genome-scale organization. Its success on zero-shot variant effect prediction and generative tasks shows that alignment-free, DNA-only models can be powerful generalists, complementing or even competing with highly specialized models like AlphaMissense. For students interested in multimodal integration, Evo 2 can be viewed as a genome encoder: its embeddings and interpretable features are natural inputs to late-fusion or contrastive multimodal pipelines that combine genomics with imaging, behavior, or clinical data, consistent with the integration baseline plan\u2019s emphasis on strong per-modality modeling. More broadly, Evo 2 demonstrates how ideas from large-scale LMs (scaling laws, long-context training, inference-time scaling, mechanistic interpretability) can be translated into biology, opening up a path toward \u201cvirtual cell\u201d models that integrate genomics with epigenomic and transcriptomic layers.</p>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Evo 2 is a large-scale genomic foundation model trained on ~9.3T nucleotides from genomes spanning all domains of life, with context windows up to 1M base pairs.</li> <li>The model uses a new StripedHyena 2 multi-hybrid architecture, combining input-dependent convolutions and attention to achieve efficient training and inference at long context lengths.</li> <li>Evo 2 supports zero-shot mutational effect prediction for proteins, RNAs, and noncoding regions, with likelihood changes that align with known biological constraints and genetic codes.</li> <li>On clinical variant tasks (ClinVar, SpliceVarDB, BRCA1/2), Evo 2 is competitive with or better than specialized models, especially for noncoding, splice, and non-SNV variants.</li> <li>Evo 2 embeddings can power supervised classifiers that achieve state-of-the-art performance on clinically important tasks such as BRCA1 variant classification.</li> <li>Mechanistic interpretability with sparse autoencoders reveals latent features corresponding to exons, introns, motifs, protein secondary structure, and prophages, and these features generalize to unseen genomes.</li> <li>The model can generate genome-scale DNA sequences (mitochondrial genomes, minimal bacterial genomes, yeast chromosomes) that resemble natural sequences in structure and function.</li> <li>By combining Evo 2 with epigenomic predictors and beam-search-style inference-time search, the authors demonstrate controllable \u201cgenerative epigenomics\u201d and the first inference-time scaling laws in a biological design setting.</li> <li>Evo 2 is released fully open (weights, training and inference code, data), offering a powerful foundation for downstream genomics, variant interpretation, and design tasks, while incorporating explicit risk mitigation for viral domains.</li> <li>For a new grad student, Evo 2 is a key reference for how to build, evaluate, and interpret DNA-based foundation models, and a natural starting point for projects in variant effect prediction, genome design, and multimodal integration.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/","title":"Foundation Models for Advancing Healthcare: Challenges, Opportunities and Future Directions","text":"<p>Authors: Yuting He, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, Hao Chen Year: 2024 Venue: arXiv (survey)</p>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>General FM survey / theory  </li> <li> <p>This paper surveys healthcare foundation models (HFMs) across language, vision, bioinformatics, and multimodal domains, and discusses their challenges and future directions.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Multimodal FM or cross-modal integration (conceptual survey)  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Text (clinical notes, biomedical literature), medical images (radiology, pathology, ophthalmology, etc.), bioinformatics data (DNA, RNA, protein sequences), and multimodal clinical data (EHR, physiology).</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This survey provides a broad overview of how foundation models are transforming healthcare, framing them as healthcare foundation models (HFMs) that can be adapted to many clinical tasks. The authors first define HFMs and trace their roots from early transfer learning to modern large language models (LLMs), vision foundation models (VFMs), bioinformatics foundation models (BFMs), and multimodal foundation models (MFMs). They then review representative models and datasets in each subfield, highlighting successes such as BioBERT, AlphaFold2, SAM\u2011style medical VFMs, and multimodal models integrating images, text, and omics. The paper devotes substantial discussion to the challenges of data, algorithms, and computing infrastructure\u2014ethics, heterogeneity, cost, and environmental impact\u2014as well as open questions around fairness, robustness, and deployment. Finally, it outlines promising research directions, arguing that HFMs can drive the next generation of precision medicine if developed and governed responsibly. For a new grad student, this article is a high\u2011level map of the healthcare FM landscape.</p>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Understand how foundation models\u2014pretrained on broad healthcare data and adaptable to many tasks\u2014can advance diagnosis, prognosis, treatment, and research.  </li> <li> <p>Identify current capabilities, limitations, and research directions across language, vision, bioinformatics, and multimodal healthcare AI.  </p> </li> <li> <p>Why this is hard: </p> </li> <li>Data\u2011related challenges: <ul> <li>Healthcare data are sensitive, heterogeneous, and often siloed, complicating large\u2011scale collection and sharing.  </li> <li>Ethical, legal, and social concerns (privacy, consent, bias) strongly constrain data use.  </li> </ul> </li> <li>Algorithmic challenges: <ul> <li>HFMs must balance scale and adaptability with reliability, interpretability, and safety.  </li> <li>Domain shift, spurious correlations, and label noise can degrade performance when models are deployed.  </li> </ul> </li> <li>Computing and infrastructure: <ul> <li>Training large FMs on 3D images, WSIs, or multi\u2011omics data is computationally and environmentally expensive.  </li> <li>Many healthcare institutions lack the infrastructure to host or fine\u2011tune very large models.  </li> </ul> </li> <li>Clinical constraints: <ul> <li>HFMs must integrate into existing workflows and meet regulatory and validation requirements before impacting patient care.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>The survey organizes healthcare data for HFMs into several key categories:</p> <ul> <li>Text data: </li> <li>Biomedical literature, clinical guidelines, and medical textbooks.  </li> <li>Clinical notes (progress notes, discharge summaries, radiology and pathology reports).  </li> <li> <p>Used to train medical LLMs and language\u2011centric HFMs.  </p> </li> <li> <p>Medical imaging data: </p> </li> <li>Radiology (CT, MRI, X\u2011ray, ultrasound), pathology (whole\u2011slide images), ophthalmology, endoscopy, etc.  </li> <li> <p>Large imaging datasets with segmentation masks, labels, or reports support VFMs and MMVLFMs.  </p> </li> <li> <p>Bioinformatics and omics data: </p> </li> <li>DNA, RNA, protein sequences, 3D protein structures, and other molecular data.  </li> <li> <p>Used for BFMs such as protein or genomic FMs (e.g., AlphaFold2\u2011like models).  </p> </li> <li> <p>Multimodal clinical data: </p> </li> <li>EHR tables (labs, vitals, medications), physiological signals (ECG, EEG), and combined image\u2013text\u2013omics datasets.  </li> <li> <p>Essential for MFMs that model whole\u2011patient states.  </p> </li> <li> <p>Preprocessing / representation themes: </p> </li> <li>Tokenization and vocabulary design for clinical/biomedical text and sequences.  </li> <li>Patch and voxel\u2011based representations for images and volumes.  </li> <li>Graphs, sequences, or 3D coordinate representations for proteins and molecular structures.  </li> <li>Harmonization and standardization across institutions for multi\u2011center datasets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type (subfields reviewed): </li> <li>Language FMs / LLMs: medical variants of BERT, GPT, and other transformers (e.g., BioBERT, clinical LLMs).  </li> <li>Vision FMs (VFMs): models like SAM\u2011style or ViT\u2011based encoders for medical imaging.  </li> <li>Bioinformatics FMs (BFMs): AlphaFold2\u2011like models, protein language models, DNA/RNA sequence models.  </li> <li> <p>Multimodal FMs (MFMs): models integrating text, images, omics, and clinical variables.  </p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>This is a survey, summarizing many HFMs rather than introducing a single new model.</p> </li> <li> <p>Key conceptual components:</p> </li> </ul> Aspect Details Pretraining Large\u2011scale self\u2011supervised or weakly supervised objectives on broad healthcare data Adaptation Fine\u2011tuning, prompting, and parameter\u2011efficient methods (e.g., adapters, LoRA) Applications Diagnosis, prognosis, treatment planning, report generation, drug discovery, and more Evaluation Domain\u2011specific benchmarks, robustness tests, and clinical validation studies <ul> <li>Training setup themes: </li> <li>Self\u2011supervised learning (masked language modeling, masked image modeling, contrastive learning).  </li> <li>Multi\u2011task and continual learning strategies to adapt HFMs to new tasks while retaining prior knowledge.  </li> <li>Use of transfer learning and model reuse across related tasks and modalities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>While much of the survey covers unimodal FMs, multimodal HFMs are a central focus for future directions.</p> <ul> <li>Modalities integrated: </li> <li>Images + text (e.g., radiology report generation).  </li> <li>Text + omics (e.g., linking genomic variants with phenotypes).  </li> <li> <p>Images + omics + clinical variables in comprehensive patient\u2011level models.  </p> </li> <li> <p>Integration strategies discussed: </p> </li> <li>Early fusion of embeddings from different modalities.  </li> <li>Cross\u2011attention and joint latent spaces for intermediate fusion.  </li> <li>Late fusion via ensemble or decision\u2011level integration for heterogeneous modalities.  </li> <li> <p>CLIP\u2011style and other contrastive learning approaches for image\u2013text alignment.  </p> </li> <li> <p>New capabilities enabled: </p> </li> <li>Richer patient representation and more accurate prediction by combining multiple data sources.  </li> <li>Cross\u2011modal retrieval (e.g., finding images that match textual descriptions or molecular profiles).  </li> <li>Enhanced interpretability when multimodal signals agree or disagree in clinically meaningful ways.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>As a survey, the paper does not present new experiments, but summarizes outcomes from many HFMs.</p> <ul> <li>Tasks reviewed: </li> <li>Improved diagnosis and prognosis from imaging and text.  </li> <li>Accurate protein structure prediction and molecular property forecasting.  </li> <li>Automated or assisted report generation and documentation.  </li> <li> <p>Drug discovery and treatment recommendation tasks.  </p> </li> <li> <p>High\u2011level trends: </p> </li> <li>HFMs consistently outperform smaller, task\u2011specific models when sufficient pretraining data are available.  </li> <li>Zero\u2011shot and few\u2011shot capabilities enable deployment in settings with limited labeled data.  </li> <li>However, performance can degrade sharply when task distributions differ from pretraining data, highlighting the importance of robust evaluation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths (of HFMs and the survey):</p> <ul> <li>Presents a comprehensive taxonomy of HFMs across language, vision, bioinformatics, and multimodal domains.  </li> <li>Explicitly links technical developments (e.g., transformers, self\u2011supervision) to concrete healthcare applications.  </li> <li>Identifies both opportunities (precision medicine, automation, discovery) and systemic challenges.  </li> <li>Serves as a high\u2011level entry point for researchers from different backgrounds.</li> </ul> <p>Limitations and challenges (field\u2011level):</p> <ul> <li>Data availability is constrained by privacy, ethics, and heterogeneity, limiting the diversity of pretraining corpora.  </li> <li>Algorithmic issues such as hallucination, bias, and lack of interpretability are particularly serious in clinical contexts.  </li> <li>Computing requirements raise questions about sustainability, equity of access, and reproducibility.  </li> <li>Regulatory frameworks are still catching up with the capabilities and risks of HFMs.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can we design HFMs that are simultaneously scalable, reliable, and interpretable in clinical settings?  </li> <li>What governance structures and data\u2011sharing mechanisms are needed to support responsible, multi\u2011institutional pretraining?  </li> <li>How can HFMs be adapted and validated for under\u2011served populations and low\u2011resource health systems?  </li> <li>What new architectures and objectives are needed to incorporate causal reasoning and mechanistic knowledge into HFMs?  </li> <li>How should we evaluate the real\u2011world clinical impact of HFMs beyond traditional accuracy metrics?</li> </ol>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>This survey sits at the intersection of AI and healthcare policy, offering a bird\u2019s\u2011eye view of HFMs rather than focusing on a single modality or task.  </li> <li>It complements more focused reviews on medical MLLMs or MMFMs by connecting them to broader trends in healthcare AI.  </li> <li>Relation to well-known ideas: </li> <li>Builds directly on the concept of foundation models articulated by Bommasani et al., extending it to healthcare.  </li> <li>Discusses iconic HFMs such as BERT, CLIP, SAM, AlphaFold2, and domain\u2011specific medical LLMs and VFMs.  </li> <li>Why this paper is a useful reference: </li> <li>Helps new researchers understand the big picture of how different kinds of HFMs fit together and where healthcare is headed.  </li> <li>Encourages critical thinking about the technical, ethical, and infrastructural requirements for deploying HFMs safely.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Healthcare needs AI models that can generalize across diverse tasks and modalities, but current systems are often narrow and brittle.  </p> </li> <li> <p>Method / model (conceptual): </p> </li> <li>Healthcare foundation models (HFMs) apply the foundation\u2011model paradigm\u2014large\u2011scale pretraining and broad transfer\u2014to clinical text, images, omics, and multimodal data.  </li> <li> <p>The survey categorizes HFMs into language, vision, bioinformatics, and multimodal branches, highlighting representative models and datasets.  </p> </li> <li> <p>Results / insights: </p> </li> <li>HFMs show strong performance and data efficiency, enabling zero\u2011shot and few\u2011shot adaptation across many healthcare tasks.  </li> <li> <p>Yet they face significant challenges in data governance, algorithmic robustness, fairness, interpretability, and deployment.  </p> </li> <li> <p>Why it matters: </p> </li> <li>HFMs are poised to become central infrastructure for future healthcare AI, but realizing this potential will require coordinated advances in data, algorithms, computing, and regulation.  </li> <li>This survey offers a strategic overview for anyone aiming to contribute to that evolution.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/","title":"Generator: A Long-Context Generative Genomic Foundation Model","text":"<p>Authors: Wei Wu, Qiuyi Li, Mingyang Li, Kun Fu, Fuli Feng, Jieping Ye, Hui Xiong, Zheng Wang Year: 2025 Venue: arXiv (preprint)</p>"},{"location":"generated/kb_curated/papers-md/generator_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. The paper develops a large generative language model over eukaryotic DNA sequences and evaluates it on a broad set of genomics prediction and design tasks.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. The main contribution is a new 1.2B-parameter generative genomic foundation model with long context, specialized tokenization, and a carefully curated pretraining corpus.</p> </li> <li> <p>Key Modalities: </p> </li> <li>DNA sequence (eukaryotic genomes, gene regions vs whole-genome segments).  </li> <li>Derived protein sequences and enhancer activity measurements are used mainly as downstream evaluation targets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces Generator, a large generative foundation model trained directly on eukaryotic DNA sequences to understand and design functional genomic sequences. The authors argue that existing genomic language models either lack generative ability, have short context, or are trained on relatively narrow datasets, which limits their usefulness for realistic genomic tasks. Generator is a 1.2B-parameter transformer decoder with a context length of about 98k base pairs, trained on 386 billion nucleotides from annotated gene regions of diverse eukaryotic species using next-token prediction and a 6\u2011mer tokenizer. The model achieves state-of-the-art performance on standard genomic benchmarks (Genomic Benchmarks, Nucleotide Transformer tasks) and on new \u201cGener\u201d tasks that probe long-range sequence understanding. Beyond benchmarks, Generator can generate DNA coding sequences whose translated proteins have realistic structure and statistics, and can design enhancer sequences with controllable activity levels. Overall, the work demonstrates that a large, long-context generative DNA model can serve as a versatile tool for genomic analysis and sequence design, pointing toward future applications in synthetic biology and precision genomics.</p>"},{"location":"generated/kb_curated/papers-md/generator_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Build a general-purpose generative model of eukaryotic DNA that can both understand genomic sequences (for prediction tasks) and generate new, functional sequences (for design tasks).  </li> <li> <p>Specifically, the model should handle long genomic contexts (tens of kilobases), capture the semantics of gene regions and regulatory elements, and support downstream tasks like classification, next\u2011k\u2011mer prediction, protein-coding sequence generation, and enhancer design.</p> </li> <li> <p>Why existing approaches are limited: </p> </li> <li>Many prior genomic models are masked language models (MLMs) trained with BERT-style objectives; they are strong for understanding but weaker or awkward for generation.  </li> <li>Context lengths are often only hundreds to a few thousand base pairs, which is too short for many realistic gene and regulatory contexts that span tens of thousands of base pairs.  </li> <li>Some generative models like HyenaDNA, megaDNA, and Evo are either limited to specific organism groups (e.g., bacteriophages, prokaryotes/viruses, or human-only) or use relatively small models or datasets, leaving a gap for large-scale eukaryotic generative models.  </li> <li> <p>Naively training on entire genomes can flood the model with non-gene, low-information regions, potentially hurting downstream performance even if pretraining loss decreases.</p> </li> <li> <p>Why this is hard (modeling and data challenges): </p> </li> <li>DNA sequences are extremely long and lack clear word boundaries, making tokenization and context management nontrivial.  </li> <li>Computational cost scales badly with sequence length in standard transformers, so long-context training is expensive.  </li> <li>Functional regions (genes, promoters, enhancers) form only a small fraction of the genome; most bases are relatively redundant or low-entropy, so choosing what to train on matters.  </li> <li>Evaluating generative quality is difficult because there is not always a ground truth for \u201ccorrect\u201d generated DNA; one must rely on indirect metrics (e.g., downstream performance, protein structure statistics, enhancer activity predictions).</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used (pretraining): </li> <li>DNA sequences are drawn from all eukaryotic organisms in the RefSeq database.  </li> <li> <p>Two main strategies are compared:  </p> <ul> <li>Gene Sequence Training (Scheme 1): Uses annotated gene regions (including protein-coding genes, various RNAs, and regulatory elements like promoters and enhancers). This yields about 386 billion nucleotides and is the configuration used for Generator.  </li> <li>Whole Sequence Training (Scheme 2): Mixes gene and non-gene regions from all eukaryotes, totaling about 2 trillion nucleotides, producing the Generator-All variant.  </li> </ul> </li> <li> <p>Datasets used (downstream): </p> </li> <li>Nucleotide Transformer (NT) tasks: A suite of genomics classification tasks (original and revised versions) covering promoters, enhancers, splice sites, chromatin marks, etc., across many species.  </li> <li>Genomic Benchmarks: Primarily human-centric tasks such as enhancer vs non-enhancer, promoter vs non-promoter, regulatory element classification, and species discrimination.  </li> <li>Gener tasks (proposed by this paper): <ul> <li>Gene classification: Predict gene type from sequences of length 100\u20135000 bp.  </li> <li>Taxonomic classification: Predict taxonomic group from sequences of length 10,000\u2013100,000 bp containing both gene and non-gene regions.  </li> </ul> </li> <li>Central dogma tasks: Protein-coding DNA sequences for specific protein families (Histone and Cytochrome P450) from UniProt and RefSeq.  </li> <li> <p>Enhancer design: The DeepSTARR dataset of enhancers with measured activity values (developmental vs housekeeping), including train/validation/test splits from the original DeepSTARR work.</p> </li> <li> <p>Modalities and representations: </p> </li> <li>Core input: DNA sequence, represented via a 6\u2011mer tokenizer (each token is a contiguous 6\u2011base string).  </li> <li>Protein-level evaluation: DNA sequences are translated into amino acid sequences using the genetic code; protein language models and structure prediction tools are applied downstream.  </li> <li> <p>Enhancer activity: numerical activity values (log2-transformed) from DeepSTARR, plus textual prompts <code>&lt;high&gt;</code> / <code>&lt;low&gt;</code> used as conditioning tokens in sequence design experiments.</p> </li> <li> <p>Preprocessing / representation details: </p> </li> <li>For gene sequence training, the authors select annotated gene regions and treat them as semantically rich de facto \u201csentences\u201d for the model.  </li> <li>For whole sequence training, the model sees both gene and non-gene sequences directly.  </li> <li>The 6\u2011mer tokenizer is applied with a random starting offset from 0 to 5 for each sample so that the model is not tied to a fixed phase of the genomic coordinate system.  </li> <li>Long sequences are chunked into segments respecting the maximum token context.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Autoregressive transformer decoder, broadly following the LLaMA-style architecture for causal language modeling, adapted to DNA.</p> </li> <li> <p>New vs existing FM: </p> </li> <li>This is a new foundation model, called Generator, designed specifically for eukaryotic DNA.  </li> <li> <p>It is trained from scratch with DNA-specific choices for tokenization, data selection, and context length.</p> </li> <li> <p>Key architectural configuration (approximate):</p> </li> </ul> Component Value / Choice Architecture Transformer decoder (LLaMA\u2011like) Layers 26 Hidden size 2048 MLP / intermediate size 5632 Attention heads 32 (with 8 KV heads) Vocabulary size 4128 (6\u2011mer tokens) Max token context 16,384 tokens Max base-pair context \u2248 98,304 bp (because each token is 6 bp) Positional encoding RoPE (rotary position embeddings) Activation SiLU <ul> <li>Training objective and setup: </li> <li>Pretraining objective is next-token prediction (NTP) on 6\u2011mer tokens, analogous to language modeling in NLP.  </li> <li>Batch size: about 2 million tokens per batch.  </li> <li>Optimizer: AdamW with standard \u03b2 values and weight decay, using cosine learning rate schedule with warmup.  </li> <li>Training spans 6 epochs over 386B tokens (gene-only scheme), totaling \u2248185k steps.  </li> <li> <p>Implementation uses FlashAttention and Zero Redundancy Optimizer (ZeRO) to make long-context training efficient on GPUs (32 A100s).</p> </li> <li> <p>Key components and innovations: </p> </li> <li>Data selection strategy: Emphasis on gene regions only (Generator) versus gene + non-gene (Generator-All), showing that restricting to functional sequences can increase downstream performance even if pretraining loss is higher.  </li> <li>Tokenization study: Systematic comparison of single-nucleotide, k\u2011mer, and BPE tokenizers for causal DNA LMs; finds that 6\u2011mer tokenization gives the best generative performance in NTP settings.  </li> <li>Long-context capability: By combining 6\u2011mer tokens with a large context window (16k tokens \u2248 98k bp), the model can see gene-scale and sub-chromosomal contexts in a single forward pass.  </li> <li> <p>Generative downstream pipelines: </p> <ul> <li>Fine-tuning for central dogma tasks, i.e., generating protein-coding DNA whose translated proteins look realistic to protein language models and structure predictors.  </li> <li>Prompt-conditioned enhancer design, where <code>&lt;high&gt;</code> / <code>&lt;low&gt;</code> prompts steer the model toward sequences with desired activity.</li> </ul> </li> <li> <p>Fine-tuning / downstream usage: </p> </li> <li>For classification benchmarks, the model is fine-tuned using a linear head on top of embeddings (e.g., of an end-of-sequence token), with hyperparameter search over learning rates and batch sizes.  </li> <li>For central dogma and enhancer design, a supervised fine-tuning (SFT) stage adapts the model to particular protein families or enhancer datasets, after which autoregressive generation is used with temperature and nucleus sampling controls.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Is this paper multimodal? </li> <li>The core model operates on single-modality DNA sequence data.  </li> <li> <p>Protein sequences and enhancer activity measurements appear as derived evaluation signals, not as fully co-modeled input modalities in a single end-to-end multimodal architecture.</p> </li> <li> <p>Relation to multimodal / integration ideas: </p> </li> <li>Although Generator itself is not a multimodal model, its embeddings and generative outputs could be used in late fusion pipelines with other modalities (e.g., chromatin marks, expression, imaging) as described in the integration baseline plan, where per-modality representations are concatenated and fed into shallow models (logistic regression, GBDTs).  </li> <li>The emphasis on semantically rich gene regions is conceptually similar to the plan\u2019s principle of preserving modality-specific signal\u2014here, \u201cfunctional DNA segments\u201d act as the high-value \u201cmodality\u201d within the genome, and adding non-gene regions acts like injecting noise.  </li> <li>The careful evaluation practices (cross-validation, multiple benchmarks, explicit metrics) echo the baseline plan\u2019s focus on robustness and disciplined evaluation, even though the work is not explicitly an integration study.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tokenizer and objective experiments (Next K-mer Prediction): </li> <li>The authors train multiple models with identical architecture but different tokenizers (single nucleotide, various k\u2011mers, BPE with different vocabulary sizes) and evaluate on next k\u2011mer prediction tasks.  </li> <li>They show that k\u2011mer tokenizers outperform BPE, and within k\u2011mers, the 6\u2011mer tokenizer gives the best accuracy across different input lengths.  </li> <li> <p>They also compare a large Mamba-based state space model to transformer baselines and find that, despite its longer context and efficiency, it does not outperform the transformer with 6\u2011mer tokens as the input length grows.</p> </li> <li> <p>Gene-only vs whole-genome training (Generator vs Generator-All): </p> </li> <li>While the whole-sequence model (Generator-All) achieves lower pretraining loss, it underperforms Generator on almost all downstream tasks.  </li> <li>The authors argue that non-gene regions are often redundant or non-functional, so including them may dilute the high-information gene signal, effectively \u201ccontaminating\u201d the data.  </li> <li> <p>This supports the idea that curated, semantically rich pretraining data can be more valuable than raw volume.</p> </li> <li> <p>Benchmark evaluations (Nucleotide Transformer tasks and Genomic Benchmarks): </p> </li> <li>On both the revised and original Nucleotide Transformer tasks, Generator generally achieves state-of-the-art or top-tier performance, often surpassing Enformer, DNABERT\u20112, HyenaDNA, Nucleotide Transformer variants, Caduceus, and GROVER.  </li> <li>On Genomic Benchmarks (human-focused classification tasks), Generator again performs at or near the top; smaller specialized models like Caduceus sometimes come close but do not consistently dominate.  </li> <li> <p>The Gener tasks (gene and taxonomic classification) further highlight Generator\u2019s strengths, especially for long-range sequence understanding, where it reaches very high weighted F1 scores, including near-perfect performance on taxonomic classification.</p> </li> <li> <p>Central dogma experiments (protein-coding sequence generation): </p> </li> <li>After fine-tuning on DNA sequences encoding specific protein families (Histones and Cytochrome P450), Generator is used to generate new coding sequences.  </li> <li> <p>Translating these sequences to proteins and analyzing them reveals:  </p> <ul> <li>Length distributions of generated proteins match those of natural families.  </li> <li>Protein language model perplexities (from Progen2) for generated sequences closely match those of natural proteins and differ from random shuffled controls.  </li> <li>AlphaFold3 predictions and Foldseek structure search show many generated proteins with high TM-scores (&gt;0.8) and high confidence, even when sequence identity is low (&lt;0.3), indicating that the model is not simply memorizing training sequences.</li> </ul> </li> <li> <p>Enhancer design experiments: </p> </li> <li>A predictor fine-tuned from Generator on DeepSTARR enhancer data achieves higher Pearson correlations between predicted and measured enhancer activity than DeepSTARR itself and NT\u2011multi, setting a new state of the art.  </li> <li>A further SFT stage adds <code>&lt;high&gt;</code> and <code>&lt;low&gt;</code> prompts to condition the model on desired activity levels.  </li> <li> <p>Generated enhancers labeled <code>&lt;high&gt;</code> and <code>&lt;low&gt;</code> show clear separation in predicted activity distributions relative to each other and to natural enhancers, suggesting that Generator can perform prompt-guided enhancer design.</p> </li> <li> <p>Overall empirical message: </p> </li> <li>Generator is a strong generalist across many genomic classification benchmarks and competent as a generative model for both coding and regulatory sequences, with particular strengths stemming from its long context, curated pretraining data, and 6\u2011mer tokenization.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths: </li> <li>Long-context, large-scale generative model tailored to eukaryotic DNA, filling a gap left by prior prokaryotic or short-context genomic models.  </li> <li>Careful tokenization and data selection studies, providing empirical guidance (e.g., 6\u2011mer tokens, gene-only data) that can inform future genomic FMs.  </li> <li>Strong, consistent performance across a wide range of benchmarks, including newly proposed long-context tasks.  </li> <li>Demonstrated ability to generate functional-like coding sequences and to perform prompt-conditioned enhancer design, moving beyond pure prediction into actionable sequence design.  </li> <li> <p>Extensive experimental detail (hyperparameter searches, cross-validation, ablations) that improves reproducibility and reliability.</p> </li> <li> <p>Limitations: </p> </li> <li>Pretraining focuses exclusively on eukaryotic genomes, leaving prokaryotic and viral DNA to other models like Evo; no unified genome-scale model across all domains of life is presented.  </li> <li>The model is computationally heavy (1.2B parameters, long sequences, many GPU hours), which may limit adoption in resource-constrained labs.  </li> <li>Most validations of generative quality are in silico (protein LMs, AlphaFold, enhancer predictors); there is limited or no wet-lab validation of generated sequences.  </li> <li>Despite long context, the model is still trained on 1D sequence alone, without explicit modeling of 3D genome structure, epigenetic states, or cellular context.  </li> <li> <p>As with other large models, interpretability of what the model has learned about regulatory grammar and long-range interactions remains challenging.</p> </li> <li> <p>Open Questions and Future Directions: </p> </li> <li>How would a joint eukaryotic + prokaryotic + viral Generator behave, and what design choices would be needed to balance these domains?  </li> <li>Can Generator\u2019s representations be combined with other modalities (chromatin accessibility, expression, epigenetics, single-cell data) in a systematic late-fusion or contrastive setup to improve downstream tasks like disease prediction?  </li> <li>What interpretability tools can be developed to probe the model\u2019s understanding of motifs, regulatory grammar, and long-range enhancer\u2013promoter interactions?  </li> <li>How robust are Generator\u2019s predictions and designs across different species and genomic contexts, especially for non-model organisms with sparse annotations?  </li> <li>Can Generator be adapted for clinical applications, such as prioritizing noncoding variants in GWAS regions or designing therapeutic regulatory sequences, and what safety/ethics issues would arise?</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape of foundation models in genomics: </li> <li>Generator sits alongside models like Nucleotide Transformer, HyenaDNA, Caduceus, and Evo as part of the emerging ecosystem of genomic foundation models.  </li> <li>Compared to large masked LMs (e.g., DNABERT\u20112, NT), Generator emphasizes autoregressive generation, long context, and curated gene-only training data, making it particularly suited for both understanding and designing DNA sequences.  </li> <li> <p>It complements Evo, which targets prokaryotic and viral genomes, by focusing on the more complex eukaryotic genomic setting.</p> </li> <li> <p>Conceptual analogies for intuition: </p> </li> <li>You can think of Generator as a GPT-style model for eukaryotic DNA, where tokens are 6\u2011mer substrings rather than words, and the context window spans whole genes or multi-gene regions instead of paragraphs.  </li> <li> <p>The central dogma experiments are somewhat analogous to asking a text model to generate syntactically valid and semantically coherent stories that pass external quality checks, except here the \u201ccheckers\u201d are protein LMs and structure predictors.</p> </li> <li> <p>Relevance to integration and broader research programs: </p> </li> <li>The model\u2019s strong sequence-level representations could serve as a genomic backbone in larger multimodal systems that integrate DNA with other omics or imaging modalities via late fusion, as recommended in the integration baseline plan.  </li> <li>Its success supports the broader thesis that domain-specific, long-context FMs can provide robust building blocks for downstream applications, from basic gene regulation studies to clinical genomics and synthetic biology.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: Existing genomic language models often lack generative capability, have short context windows, or are limited in organismal scope, constraining their usefulness for realistic eukaryotic genomics tasks.  </li> <li> <p>Problem: Training directly on whole genomes may emphasize low-information non-gene regions, potentially hurting performance even when pretraining loss looks better.</p> </li> <li> <p>Method / model: Generator is a 1.2B-parameter transformer decoder with \u224898k bp context length, trained with next-token prediction on 386B nucleotides of eukaryotic gene regions.  </p> </li> <li>Method / model: A systematic study of tokenizers finds that 6\u2011mer tokens work best for causal DNA language modeling, beating both single-nucleotide and BPE tokenization.  </li> <li> <p>Method / model: A comparison between gene-only and whole-genome pretraining shows that focusing on semantically rich gene regions yields better downstream performance than including vast non-gene regions.</p> </li> <li> <p>Results: Generator achieves state-of-the-art or near-SOTA performance on Nucleotide Transformer tasks, Genomic Benchmarks, and newly proposed Gener tasks, particularly excelling in long-sequence understanding.  </p> </li> <li>Results: In central dogma experiments, Generator can generate protein-coding DNA whose translated proteins have realistic lengths, protein-LM perplexities, and 3D structures, indicating true generative competence rather than memorization.  </li> <li> <p>Results: For enhancer design, a Generator-based predictor surpasses previous models on DeepSTARR data, and prompt-conditioned generation produces enhancer sequences with clearly different predicted activity profiles.</p> </li> <li> <p>Why it matters: Generator demonstrates that a large, long-context generative FM for eukaryotic DNA can serve as a powerful tool for both genomic analysis and sequence design, opening doors to more sophisticated applications in synthetic biology, variant interpretation, and future multimodal integration with other biological data types.</p> </li> </ul>"},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/","title":"Genome-wide Association Studies in Ancestrally Diverse Populations (2019)","text":"","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#1-problem-tasks","title":"1. Problem &amp; Tasks","text":"<ul> <li>Review methodological pitfalls and recommendations for conducting GWAS beyond European cohorts.</li> <li>Critical for how we handle ancestry PCs and covariates when integrating genetics with MRI.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#2-datasets","title":"2. Datasets","text":"<ul> <li>Summarizes lessons from PAGE, HCHS/SOL, UKB, biobanks in East Asia and Africa.</li> <li>Emphasizes differences in sample sizes and LD structure across populations.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#3-model-method-details","title":"3. Model / Method Details","text":"","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#31-key-recommendations","title":"3.1 Key Recommendations","text":"<ul> <li>Use linear mixed models or ancestry-informed stratification to control population structure.</li> <li>Include local ancestry estimates when available; otherwise rely on global PCs.</li> <li>Validate PRS / embeddings separately per ancestry group; avoid pooling unless harmonized.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#32-confound-handling-evaluation-discipline","title":"3.2 Confound Handling &amp; Evaluation Discipline","text":"<ul> <li>Check for residual stratification using QQ plots and genomic control.</li> <li>Report ancestry composition and site distribution with each GWAS/PRS result.</li> <li>Use jackknife/bootstraps for uncertainty when sample sizes are small per subgroup.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#4-results-tables","title":"4. Results &amp; Tables","text":"<ul> <li>Quantifies performance drop (R\u00b2 decrease up to 5\u00d7) when PRS trained in Europeans applied to African ancestry cohorts.</li> <li>Provides tables linking LD differences to effect estimation bias.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#5-limitations-cautions","title":"5. Limitations &amp; Cautions","text":"<ul> <li>2019 state of the field; newer multi-ancestry methods exist but core cautions remain.</li> <li>Does not cover foundation models directly; we must adapt recommendations to embeddings.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#6-hooks-into-neuro-omics-kb","title":"6. Hooks into Neuro-Omics KB","text":"<p>Relevant KB assets</p> <ul> <li><code>kb/paper_cards/gwas_diverse_populations.yaml</code></li> <li><code>kb/datasets/ukb_manifest_stub.yaml</code> (records ancestry composition, PCs).</li> </ul> <p>Configs / recipes informed</p> <ul> <li>Covariate list (age/sex/site/PCs) in <code>configs/experiments/01_cca_gene_smri.yaml</code> and <code>02_prediction_baselines.yaml</code>.</li> <li>Future documentation for PRS/embedding fairness analyses.</li> </ul> <p>Concrete guidance for our project</p> <ul> <li>Always log ancestry distribution and include PCs in dataset cards; cite this paper when explaining why.</li> <li>If/when we add non-European cohorts, run stratified evaluations instead of assuming global models transfer.</li> <li>Keep hooks for local ancestry or site-specific PCs should we extend beyond UKB Europeans.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/m3fm_2025/","title":"M3FM (2025)","text":""},{"location":"generated/kb_curated/papers-md/m3fm_2025/#m3fm-a-multimodal-multidomain-multilingual-medical-foundation-model-for-zeroshot-clinical-diagnosis","title":"M3FM: A Multimodal, Multidomain, Multilingual Medical Foundation Model for Zero\u2011Shot Clinical Diagnosis","text":"<p>Authors: Fenglin Liu, Zheng Li, Qingyu Yin, Jinfa Huang, Jiebo Luo, Anshul Thakur, Kim Branson, Patrick Schwab, Bing Yin, Xian Wu, Yefeng Zheng, David A. Clifton Year: 2025 Venue: npj Digital Medicine DOI: 10.1038/s41746-024-01339-7</p>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical VLM / MLLM / MMFM  </li> <li> <p>This paper proposes a medical multimodal vision\u2013language foundation model that jointly handles radiology images and multilingual text for diagnosis and report generation.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration  </p> </li> <li> <p>Key Modalities: </p> </li> <li>2D chest X\u2011rays (CXR)  </li> <li>3D CT images  </li> <li>English radiology reports  </li> <li>Chinese radiology reports (machine-translated for training; human reports for evaluation on some datasets)</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>M3FM (Multimodal Multidomain Multilingual Foundation Model) is a medical foundation model designed to perform zero-shot radiology report generation and disease diagnosis across imaging domains (CXR and CT) and languages (English and Chinese). The core idea is to first learn a shared vision\u2013language embedding space (MultiMedCLIP) using large English-centric image\u2013report and English\u2013Chinese text pairs, and then train a multilingual medical language model (MultiMedLM) on top of this space. By aligning visual features from different imaging modalities and textual features from multiple languages to English, M3FM can generate reports and perform diagnosis in languages and domains where little or no labeled data exist. The model is pretrained on hundreds of thousands of CXR and CT images with English reports, plus translated Chinese corpora, and is evaluated on nine downstream datasets that cover report generation and disease classification for infectious (COVID\u201119, TB) and noninfectious diseases. Across zero\u2011shot, few\u2011shot, and fully supervised settings, M3FM often matches or outperforms strong supervised baselines that have full access to labeled data, especially for cross-language and cross-domain generalization. For a new grad student, this paper is a canonical example of how to build a medical CLIP\u2011style + medical LLM stack and use it for multilingual, low\u2011label clinical scenarios.</p>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Build a single foundation model that can:  <ul> <li>Generate radiology reports from CXR and CT images in both English and Chinese.  </li> <li>Diagnose diseases from images and/or generated reports, including rare and emerging conditions.  </li> </ul> </li> <li> <p>Crucially, the model should work in zero-shot or few-shot regimes where labeled data in the target language or domain are scarce or absent.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Data scarcity and imbalance: <ul> <li>High-quality labeled data for rare diseases and new pathogens (e.g., early COVID\u201119 waves) are limited exactly when they are most needed.  </li> <li>Non\u2011English radiology reports, especially in languages like Chinese, are much less abundant and standardized than English reports.  </li> </ul> </li> <li>Multidomain heterogeneity: <ul> <li>CXR and CT have very different appearance, resolution, and information content, yet clinicians want unified reporting and diagnosis workflows.  </li> </ul> </li> <li>Multilingual alignment: <ul> <li>Reports in different languages describe similar findings but with different vocabularies, structures, and clinical conventions.  </li> </ul> </li> <li>Label efficiency: <ul> <li>Training supervised models separately for each disease, modality, and language combination is infeasible; a foundation model should generalize with minimal task\u2011specific labeling.  </li> </ul> </li> <li>Safety and fairness: <ul> <li>Failure to support low\u2011resource languages and rare diseases risks widening disparities in access to AI\u2011assisted care.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data (English-centric corpora): </li> <li>MIMIC\u2011CXR: <ul> <li>\u2248377k chest X\u2011ray images with \u2248228k English radiology reports.  </li> </ul> </li> <li>COVID\u201119\u2011CT\u2011CXR: <ul> <li>\u22481k CT/CXR images with English reports focused on COVID\u201119.  </li> </ul> </li> <li> <p>Chinese\u2013English parallel corpora: </p> <ul> <li>Constructed by machine-translating portions of the English reports into Chinese to form Chinese\u2013English text pairs for multilingual training.</li> </ul> </li> <li> <p>Downstream datasets (disease reporting and diagnosis): </p> </li> <li>Report generation: <ul> <li>IU\u2011Xray (CXR\u2013English reports).  </li> <li>COVID\u201119\u2011CT (CT\u2013Chinese reports).  </li> <li>COV\u2011CTR (CT images with English &amp; Chinese reports).  </li> <li>Additional qualitative examples for CXR\u2011to\u2011Chinese where human\u2011annotated datasets are unavailable.  </li> </ul> </li> <li> <p>Disease classification: </p> <ul> <li>Shenzhen Tuberculosis (CXR, TB vs normal).  </li> <li>COVID\u2011CXR (CXR, COVID\u201119 vs non\u2011COVID).  </li> <li>NIH ChestX\u2011ray14 (14\u2011label multi\u2011label classification).  </li> <li>CheXpert (multi\u2011label classification).  </li> <li>RSNA Pneumonia (pneumonia detection).  </li> <li>SIIM\u2011ACR Pneumothorax (pneumothorax detection).</li> </ul> </li> <li> <p>Modalities and languages: </p> </li> <li>Imaging: CXR, CT.  </li> <li> <p>Text: radiology reports in English and Chinese.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Images are encoded by a vision backbone into dense features.  </li> <li>Text reports (English and Chinese) are tokenized and embedded for contrastive learning and language modeling.  </li> <li>English reports from different datasets are treated as separate corpora to avoid leakage between training and evaluation institutions.</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>A two\u2011stage medical foundation model composed of:  </p> <ul> <li>MultiMedCLIP: CLIP\u2011style vision\u2013language encoder that aligns images and text.  </li> <li>MultiMedLM: multilingual medical language model (LLM) trained for report generation and text understanding.</li> </ul> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>M3FM is a new medical foundation model stack that builds on standard transformer backbones but introduces a specific pretraining strategy for multidomain, multilingual radiology.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Vision encoder CNN/ViT-style backbone encoding CXR and CT into visual embeddings Text encoder / decoder Transformer-based encoders/decoders for English and Chinese reports Alignment module CLIP-like contrastive loss aligning images and English text, and English\u2013Chinese text pairs Language model MultiMedLM trained on multilingual medical corpora to reconstruct and generate reports Inference strategy Zero-shot and few-shot report generation and diagnosis using aligned embeddings <ul> <li>Training setup (high level):</li> <li>Stage 1 \u2013 MultiMedCLIP (vision\u2013language alignment): <ul> <li>Pretrain on English-centric corpora: CXR\u2013English, CT\u2013English, and Chinese\u2013English pairs.  </li> <li>Contrastive objective encourages matched image\u2013text pairs (or bilingual pairs) to be close in a shared latent space and mismatched ones to be far apart.  </li> <li>Enables alignment of new non\u2011English reports by mapping them into the same space as English text and images.</li> </ul> </li> <li>Stage 2 \u2013 MultiMedLM (multilingual medical LLM): <ul> <li>Train an autoregressive language model over the aligned text embeddings, reconstructing inputs across languages.  </li> <li>Leverages large text corpora (including machine\u2011translated Chinese) to learn biomedical vocabulary and style.  </li> </ul> </li> <li>Inference: <ul> <li>For zero\u2011shot report generation, visual embeddings are fed through the aligned text space into MultiMedLM to decode reports in English or Chinese without downstream supervised training.  </li> <li>For diagnosis, image features and generated reports can be combined to drive disease classifiers.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Modalities integrated: </li> <li> <p>Radiology images (CXR, CT) and textual reports in English and Chinese.</p> </li> <li> <p>How integration works: </p> </li> <li>CLIP\u2011style two\u2011tower alignment (MultiMedCLIP): <ul> <li>Separate image and text encoders are trained with a contrastive loss so that images and their corresponding reports are nearby in the joint embedding space.  </li> <li>English text is the anchor; Chinese reports are aligned via English\u2013Chinese text pairs, effectively \u201cpivoting\u201d through English.  </li> </ul> </li> <li>Text\u2011only language modeling (MultiMedLM): <ul> <li>Trained on multilingual text (including translated reports) to generate fluent medical text conditioned on embeddings from MultiMedCLIP.  </li> </ul> </li> <li> <p>Zero\u2011shot transfer: </p> <ul> <li>Once everything is aligned, CXR or CT images from unseen datasets can be fed through the image encoder and decoded into English or Chinese reports, even when no labeled image\u2013text pairs exist for that specific institution or language.</li> </ul> </li> <li> <p>Why this integration is useful / new capabilities: </p> </li> <li>Enables zero\u2011shot multilingual report generation from images, including generating Chinese reports without any human\u2011labeled CXR\u2011Chinese or CT\u2011Chinese training pairs.  </li> <li>Supports zero\u2011shot and few\u2011shot disease diagnosis by combining image features and generated reports across diseases and datasets.  </li> <li>Provides a flexible template for extending to additional modalities (e.g., other imaging types or languages) by adding new aligned corpora.</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Multilingual, multidomain report generation: <ul> <li>CXR\u2011to\u2011English, CT\u2011to\u2011English, CXR\u2011to\u2011Chinese (qualitative), CT\u2011to\u2011Chinese.  </li> </ul> </li> <li>Disease diagnosis (classification): <ul> <li>Binary tasks (COVID vs non\u2011COVID, TB vs normal) and multi\u2011label disease prediction (ChestX\u2011ray14, CheXpert) using image + generated reports.  </li> </ul> </li> <li> <p>Settings: </p> <ul> <li>Zero\u2011shot report generation and diagnosis (no downstream labels).  </li> <li>Few\u2011shot and fully supervised diagnosis using limited or full labels on top of M3FM embeddings.</li> </ul> </li> <li> <p>Baselines: </p> </li> <li>Supervised report generation models such as R2Gen and other encoder\u2013decoder methods trained on specific datasets.  </li> <li>Supervised disease classifiers trained directly on images (e.g., CNNs) with full labels.  </li> <li> <p>Few\u2011shot and fully supervised methods that do not use a unified foundation model.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>M3FM achieves strong zero\u2011shot report generation performance, often matching or surpassing supervised baselines on CXR\u2011to\u2011English and CT\u2011to\u2011Chinese tasks despite using no downstream labels.  </li> <li>In zero\u2011shot and few\u2011shot diagnosis, combining M3FM representations and generated reports yields performance close to fully supervised baselines that use large labeled training sets.  </li> <li>M3FM is particularly strong in cross-language generalization, outperforming baselines on Chinese report generation and diagnosis tasks built from machine\u2011translated training corpora.  </li> <li>Across nine benchmark datasets spanning infectious and noninfectious diseases, M3FM provides consistently competitive or superior results, especially in low\u2011label regimes.</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Provides a single foundation model that unifies multimodal (image + text), multidomain (CXR + CT), and multilingual (English + Chinese) clinical diagnosis and report generation.  </li> <li>Demonstrates genuine zero\u2011shot capabilities, generating reports and diagnoses without labeled downstream data in the target domain or language.  </li> <li>Uses existing English-centric corpora and machine translation to bootstrap multilingual capabilities, which is practical for many health systems.  </li> <li>Offers a concrete, reproducible blueprint for combining CLIP\u2011style alignment with a domain\u2011specific medical LLM.</li> </ul> <p>Limitations:</p> <ul> <li>Relies heavily on machine\u2011translated Chinese text, which may introduce translation artifacts and biases into the model\u2019s understanding of Chinese medical language.  </li> <li>Focuses on CXR and CT only; other imaging modalities and richer EHR data are not modeled.  </li> <li>Zero\u2011shot and few\u2011shot performance, while strong, may still fall short of what clinicians require for fully autonomous deployment.  </li> <li>Training such a model still requires substantial compute and careful data curation; scaling to more modalities and languages further raises costs.  </li> <li>Evaluation emphasizes standard generation and classification metrics, which may not fully capture clinical safety and decision impact.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How well does the M3FM paradigm extend to additional languages (e.g., Spanish, Arabic) and imaging modalities (MRI, ultrasound, pathology) when only weak supervision is available?  </li> <li>Can higher\u2011quality human\u2011translated corpora or bilingual clinical notes substantially improve multilingual performance and safety relative to machine translation alone?  </li> <li>How should we design clinical trials and human\u2011in\u2011the\u2011loop workflows to safely deploy zero\u2011shot report generators in real hospitals?  </li> <li>What is the best way to combine M3FM with structured EHR data and other signals (labs, medications) for end\u2011to\u2011end diagnostic support?  </li> <li>Can similar multimodal, multilingual foundations be built in a more compute\u2011efficient way, for example via parameter\u2011efficient fine\u2011tuning or distillation?</li> </ol>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>M3FM is to radiology report generation and diagnosis what CLIP\u2011style models and GPT\u2011style LLMs are to generic vision\u2013language tasks: a unified backbone that supports multiple tasks, domains, and languages from a single pretraining run.  </li> <li>It sits alongside other medical multimodal FMs (e.g., radiology VLMs, medical CLIP variants) but is distinctive in explicitly targeting multilingual, multidomain zero\u2011shot diagnosis.</li> <li>Relation to well-known ideas: </li> <li>Architecturally, MultiMedCLIP is a CLIP\u2011like two\u2011tower model for medical images and text, while MultiMedLM is a medical LLM akin to Me\u2011LLaMA but optimized for radiology report style and multilinguality.  </li> <li>The system follows the modern pattern of using a strong vision encoder + language encoder/decoder, aligned via contrastive learning and then instruction\u2011 or task\u2011tuned for downstream use.</li> <li>Why this paper is a useful reference: </li> <li>It provides a clear recipe for building multilingual, multidomain medical VLMs and demonstrating their advantages in low\u2011label regimes.  </li> <li>For a grad student, it serves as a practical starting point for designing new medical MLLMs, extending them to other modalities, or exploring safer, more equitable deployment strategies in global health.</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Clinical radiology needs AI systems that can generate reports and support diagnosis across imaging domains and languages, especially when labeled data for rare diseases or non\u2011English populations are scarce.  </p> </li> <li> <p>Method / model: </p> </li> <li>M3FM combines MultiMedCLIP, a CLIP\u2011style vision\u2013language encoder trained on English-centric image\u2013text corpora and English\u2013Chinese text pairs, with MultiMedLM, a multilingual medical language model trained on large text corpora.  </li> <li> <p>The model aligns CXR and CT images with English and Chinese reports in a shared latent space and then uses an LLM to generate reports and support downstream classification.  </p> </li> <li> <p>Results: </p> </li> <li>Achieves strong zero\u2011shot and few\u2011shot performance on nine datasets covering report generation and disease diagnosis, often matching or exceeding supervised baselines that rely on labeled data.  </li> <li> <p>Particularly strong for cross-language tasks such as CT\u2011to\u2011Chinese report generation and COVID\u201119 diagnosis with minimal labeled data.  </p> </li> <li> <p>Why it matters: </p> </li> <li>Demonstrates that a single medical multimodal foundation model can reduce dependence on large labeled datasets and extend AI benefits to low\u2011resource languages and rare diseases.  </li> <li>Provides a concrete blueprint for future medical VLMs and MLLMs targeting multilingual, label\u2011efficient clinical decision support.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/","title":"Me-LLaMA (2024)","text":""},{"location":"generated/kb_curated/papers-md/me_llama_2024/#me-llama-medical-foundation-large-language-models-for-comprehensive-text-analysis-and-clinical-reasoning","title":"Me-LLaMA: Medical Foundation Large Language Models for Comprehensive Text Analysis and Clinical Reasoning","text":"<p>Authors: Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Xinyu Zhou, Lingfei Qian, Huan He, Dennis Shung, Lucila Ohno\u2011Machado, Yonghui Wu, Hua Xu, Jiang Bian Year: 2024 Venue: Preprint (medical AI / biomedical informatics)</p>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical LLM  </li> <li> <p>This work develops large language models specialized for biomedical literature and clinical notes, targeting broad medical text understanding and generation.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Text only: biomedical research articles, clinical guidelines, radiology and clinical notes, discharge summaries, and question\u2013answer style datasets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>Me\u2011LLaMA is a family of medical foundation large language models (LLMs) built by continually pretraining and instruction\u2011tuning LLaMA\u20112 on one of the largest medical text corpora assembled to date. The authors construct a 129\u2011billion\u2011token pretraining dataset from biomedical literature and clinical notes, and a 214k\u2011example instruction\u2011tuning corpus spanning diverse medical NLP tasks. They release 13B and 70B base models plus chat\u2011optimized versions and evaluate them on six core text analysis task families\u2014question answering, relation extraction, named entity recognition, text classification, summarization, and natural language inference\u2014across 12 benchmarks, as well as on complex clinical case diagnosis. Me\u2011LLaMA substantially outperforms previous open\u2011source medical LLMs and, with targeted instruction tuning, surpasses commercial models such as ChatGPT and even GPT\u20114 on several benchmarks, while matching them on challenging clinical case reasoning. For a new grad student, this paper illustrates how to scale a domain\u2011specific medical LLM from raw corpora through pretraining, instruction tuning, and evaluation, and how specialized data can close the gap to frontier proprietary models.</p>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Build open\u2011source medical foundation LLMs that:  </p> <ul> <li>Understand and generate medical text across biomedical research and clinical documentation.  </li> <li>Perform well on a wide range of NLP tasks (QA, NER, RE, classification, summarization, NLI).  </li> <li>Support complex clinical case reasoning comparable to commercial LLMs.  </li> </ul> </li> <li> <p>Why this is hard: </p> </li> <li>Domain knowledge gap: <ul> <li>General LLMs trained primarily on web and general\u2011domain corpora often lack reliable medical knowledge and may hallucinate clinically incorrect content.  </li> </ul> </li> <li>Data access and diversity: <ul> <li>Clinical notes and EHR text are sensitive; assembling large, representative corpora across health systems is difficult.  </li> <li>Existing medical LLMs often rely only on biomedical literature or only on clinical notes, limiting coverage.  </li> </ul> </li> <li>Compute costs: <ul> <li>Continual pretraining at the 13B\u201370B scale with &gt;100k GPU hours is expensive, making it hard to explore multiple design choices.  </li> </ul> </li> <li>Evaluation breadth: <ul> <li>Many prior models are evaluated mainly on QA, giving an incomplete picture of generalization to other medical NLP tasks.  </li> </ul> </li> <li>Clinical reliability: <ul> <li>Matching or exceeding commercial LLMs on clinical case diagnosis requires nuanced reasoning and safe behavior, not just surface\u2011level metrics.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data (129B tokens): </li> <li>Biomedical literature: <ul> <li>Millions of PubMed abstracts and full\u2011text articles from biomedical journals.  </li> </ul> </li> <li>Clinical notes: <ul> <li>\u22482.9M de\u2011identified clinical notes from electronic health records, capturing real\u2011world medical language, abbreviations, and workflows.  </li> </ul> </li> <li> <p>General text: </p> <ul> <li>Tens of billions of tokens from high\u2011quality general\u2011domain sources to preserve broad language competence.</li> </ul> </li> <li> <p>Instruction\u2011tuning data (214k examples): </p> </li> <li> <p>Curated and synthesized instructions covering:  </p> <ul> <li>Question answering (QA).  </li> <li>Named entity recognition (NER).  </li> <li>Relation extraction (RE).  </li> <li>Text classification.  </li> <li>Summarization.  </li> <li>Natural language inference (NLI).  </li> <li>Clinical diagnosis and case\u2011based reasoning prompts.</li> </ul> </li> <li> <p>Evaluation benchmarks: </p> </li> <li>12 datasets across six task families, spanning biomedical and clinical domains (e.g., medical QA benchmarks, clinical NER and RE datasets, classification tasks, and summarization corpora).  </li> <li> <p>Additional clinical case diagnosis benchmark where models read long case descriptions and propose diagnoses.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Standard subword tokenization adapted to biomedical terminology.  </li> <li>Careful de\u2011identification and filtering for clinical text.  </li> <li>Mixture weighting between general, biomedical, and clinical sources to balance domain specialization and general language ability.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Decoder\u2011only transformer LLMs (LLaMA\u20112\u2013style) with 13B and 70B parameters, plus chat\u2011optimized instruction\u2011tuned variants.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>Builds on existing LLaMA\u20112 backbones, but Me\u2011LLaMA defines new medical foundation models via large\u2011scale continual pretraining and instruction tuning on medical corpora.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Backbone LLaMA\u20112 13B and 70B decoder\u2011only transformers Continual pretraining 129B tokens from biomedical literature + clinical notes + general text Instruction tuning 214k multi\u2011task medical instructions, covering 6+ task types Model family Base models (Me\u2011LLaMA\u201113B/70B) and chat models (Me\u2011LLaMA\u201113B/70B\u2011chat) Evaluation 12 benchmarks + clinical case diagnosis vs open\u2011source and commercial LLMs <ul> <li>Training setup (high level):</li> <li>Continual pretraining: <ul> <li>Start from open\u2011source LLaMA\u20112 checkpoints.  </li> <li>Continue next\u2011token prediction on the mixed general + biomedical + clinical corpus, with careful scheduling to ensure domain specialization without catastrophic forgetting.  </li> <li>70B variant requires &gt;100,000 A100 GPU hours.  </li> </ul> </li> <li>Instruction tuning: <ul> <li>Supervised fine\u2011tuning on 214k instruction\u2013response pairs spanning multiple task types.  </li> <li>Chat models additionally tuned for conversational robustness and safety.  </li> </ul> </li> <li>Optimization: <ul> <li>Standard transformer training with AdamW, learning\u2011rate warmup + decay, and careful gradient scaling for large\u2011batch training.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>Me\u2011LLaMA is text\u2011only, so it does not directly integrate images or other modalities. However, it is designed to be a textual backbone that could sit atop or alongside medical vision or multimodal encoders.</p> <ul> <li>In the broader MMFM ecosystem, Me\u2011LLaMA can:  </li> <li>Serve as the language component in multimodal LLMs that accept imaging inputs (e.g., by attaching image encoders via projection or query\u2011based connectors, as in CLIP\u2011to\u2011LLM pipelines).  </li> <li>Act as a medical \u201creasoning engine\u201d for systems that convert images, signals, or EHR tables into textual descriptions or structured prompts.  </li> <li>The paper primarily focuses on text\u2011only performance but positions Me\u2011LLaMA as a foundation LLM that other multimodal medical models (e.g., M3FM, radiology MLLMs) can build upon.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Question answering (factoid and multi\u2011hop medical QA).  </li> <li>Named entity recognition and relation extraction for biomedical and clinical entities.  </li> <li>Text classification (e.g., document or sentence\u2011level labeling).  </li> <li>Summarization of biomedical and clinical documents.  </li> <li>Natural language inference (NLI) for medical entailment and contradiction.  </li> <li> <p>Complex clinical case diagnosis tasks where the model reads rich case descriptions and proposes diagnoses.</p> </li> <li> <p>Baselines: </p> </li> <li>General\u2011domain LLaMA\u20112 models without medical specialization.  </li> <li>Prior open\u2011source medical LLMs: MedAlpaca, ChatDoctor, AlpaCare, Clinical LLaMA, Meditron, PMC\u2011LLaMA.  </li> <li> <p>Commercial models: ChatGPT, GPT\u20114, and other proprietary LLMs on subsets of tasks.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>Versus general LLaMA\u20112: Me\u2011LLaMA strongly outperforms its backbone on essentially all medical benchmarks, confirming the value of large\u2011scale domain\u2011specific pretraining.  </li> <li>Versus open\u2011source medical LLMs: Me\u2011LLaMA achieves the best or near\u2011best scores on most QA, NER, RE, classification, summarization, and NLI datasets, with especially strong gains on tasks involving clinical notes.  </li> <li>Versus commercial LLMs: With task\u2011specific instruction tuning, Me\u2011LLaMA often surpasses ChatGPT on 7/8 datasets and GPT\u20114 on 5/8 datasets, while achieving comparable performance on complex clinical case diagnosis.  </li> <li>The 70B model performs better than the 13B variant, but the 13B models offer a compelling trade\u2011off between performance and compute.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>One of the largest and most comprehensive open\u2011source medical LLM families, with both literature and clinical notes in the pretraining mix.  </li> <li>Demonstrates that continual pretraining + instruction tuning can push open\u2011source models into the performance regime of commercial systems on many medical tasks.  </li> <li>Evaluated across a broad task spectrum, giving a realistic sense of the model\u2019s capabilities beyond QA.  </li> <li>Released models, data summaries, and evaluation scripts (under appropriate DUAs) provide a valuable community resource.</li> </ul> <p>Limitations:</p> <ul> <li>Extremely high compute cost (&gt;100k A100 hours for 70B), making replication and further scaling difficult for many groups.  </li> <li>Training data, though large, are drawn from a limited set of institutions and sources, raising concerns about bias and representativeness.  </li> <li>Evaluation, while broad, is still primarily offline, and does not fully capture real\u2011world deployment issues such as hallucination under pressure, long\u2011term safety, and clinician trust.  </li> <li>The work is text\u2011only; multimodal grounding (imaging, waveforms, EHR tables) is left to future MLLM architectures.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can we make domain\u2011specific LLM pretraining more compute\u2011efficient (e.g., via better initialization, parameter\u2011efficient tuning, or distillation)?  </li> <li>What is the best way to integrate Me\u2011LLaMA with medical vision foundation models (e.g., TITAN, M3FM\u2011style vision encoders) into full MLLMs?  </li> <li>How do we rigorously evaluate and mitigate hallucination, bias, and unsafe recommendations in complex clinical decision\u2011support settings?  </li> <li>Can we design continual learning strategies so Me\u2011LLaMA can be safely updated with new medical knowledge without catastrophic forgetting?  </li> <li>How should data governance and DUAs evolve so that multiple institutions can collaboratively train safer, more representative medical LLMs?</li> </ol>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>Me\u2011LLaMA is a flagship example of a medical foundation LLM, analogous to Me\u2011PaLM or Med\u2011PaLM style models but built on LLaMA\u20112 and fully open\u2011source.  </li> <li>It anchors the language side of the emerging ecosystem of medical multimodal foundation models (MMFMs and MLLMs).  </li> <li>Relation to well-known ideas: </li> <li>Follows the now\u2011standard recipe of continual pretraining on domain corpora plus instruction tuning for downstream task and chat performance.  </li> <li>Serves as a natural language counterpart to medical vision FMs (e.g., TITAN) and multimodal FMs (e.g., M3FM), which could plug in via CLIP\u2011style or LLaVA\u2011style connectors.  </li> <li>Why this paper is a useful reference: </li> <li>Provides a detailed case study in building, scaling, and evaluating a domain\u2011specialized LLM family.  </li> <li>For a grad student, it is an excellent blueprint for data curation, training strategy, and evaluation design in domain\u2011specific LLM research.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>General\u2011domain LLMs are not sufficiently reliable or specialized for medical applications, and existing open\u2011source medical LLMs are limited in scale, data diversity, and task coverage.  </p> </li> <li> <p>Method / model: </p> </li> <li>Me\u2011LLaMA continually pretrains LLaMA\u20112 on 129B tokens of biomedical literature and clinical notes, then instruction\u2011tunes on 214k multi\u2011task medical instructions, yielding 13B and 70B base and chat models.  </li> <li> <p>The model family is designed as a medical foundation LLM for broad text analysis and clinical reasoning.  </p> </li> <li> <p>Results: </p> </li> <li>Strongly outperforms prior open\u2011source medical LLMs and general LLaMA\u20112 on 12 benchmarks spanning QA, NER, RE, classification, summarization, and NLI.  </li> <li> <p>With instruction tuning, Me\u2011LLaMA matches or exceeds ChatGPT and GPT\u20114 on many benchmarks and achieves comparable performance on complex clinical case diagnosis.  </p> </li> <li> <p>Why it matters: </p> </li> <li>Shows that carefully designed domain\u2011specific data and training can make open\u2011source medical LLMs competitive with proprietary systems, improving transparency, reproducibility, and access.  </li> <li>Provides a powerful language backbone that future multimodal medical foundation models can build upon.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/","title":"MM-LLM imaging (2025)","text":""},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#multimodal-large-language-models-in-medical-imaging-current-state-and-future-directions","title":"Multimodal Large Language Models in Medical Imaging: Current State and Future Directions","text":"<p>Authors: Yoojin Nam, Dong Yeong Kim, Sunggu Kyung, Jinyoung Seo, Jeong Min Song, Jimin Kwon, Jihyun Kim, Wooyoung Jo, Hyungbin Park, Jimin Sung, Sangah Park, Heeyeon Kwon, Taehee Kwon, Kanghyun Kim, Namkug Kim Year: 2025 Venue: Korean Journal of Radiology</p>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical VLM / MLLM / MMFM + General FM survey / theory  </li> <li> <p>This is a review article that surveys multimodal large language models (MLLMs) for medical imaging, especially radiology, and analyzes architectures, datasets, capabilities, and challenges.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Multimodal FM or cross-modal integration (survey of existing systems)  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Imaging: 2D chest X\u2011ray, CT, MRI, ultrasound, endoscopy, digital pathology, and other clinical photos.  </li> <li>Text: radiology reports, clinical notes, question\u2013answer pairs, and structured EHR data.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>This review paper provides a comprehensive overview of multimodal large language models (MLLMs) in medical imaging, focusing on how they integrate image and text data to support tasks such as radiology report generation, visual question answering (VQA), and interactive diagnostic assistance. The authors first introduce LLMs and vision transformers (ViTs) as the core building blocks, then explain how multimodal connectors and training strategies turn them into MLLMs. They categorize architectures by how images and text are encoded, how connectors project visual features into the LLM\u2019s token space, and how multimodal fusion is achieved (contrastive pretraining, instruction\u2011tuned fusion, generative pipelines, etc.). The paper surveys available datasets, clinical applications, and early systems, highlighting both impressive capabilities and serious limitations such as hallucination, poor region grounding, and the scarcity of large-scale medical multimodal datasets. It closes with a roadmap for future research, emphasizing region\u2011grounded reasoning, robust pretraining on medical data, and safe clinical integration. For a new grad student, this review is an accessible map of the design space and open problems in medical MLLMs.</p>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Understand how to build and deploy multimodal LLMs that can:  </p> <ul> <li>Interpret medical images together with clinical text.  </li> <li>Generate accurate, clinically useful reports and answers.  </li> <li>Act as trustworthy assistants in radiology workflows.  </li> </ul> </li> <li> <p>Why this is hard: </p> </li> <li>Data challenges: <ul> <li>Large, high\u2011quality multimodal datasets (images + reports + EHR) are scarce and often siloed by institution.  </li> <li>Annotations such as region\u2011level labels and detailed textual descriptions are expensive to obtain.  </li> <li>Privacy regulations constrain data sharing and centralized training.  </li> </ul> </li> <li>Modeling challenges: <ul> <li>Radiology images (2D/3D, multi\u2011phase) and clinical notes are heterogeneous and high\u2011dimensional.  </li> <li>Aligning visual features with language at the right granularity (organ, lesion, pixel) is non\u2011trivial.  </li> <li>LLMs trained on web text may hallucinate findings or misuse clinical jargon when connected to images.  </li> </ul> </li> <li>Clinical deployment challenges: <ul> <li>Need for region\u2011grounded explanations and robust behavior across scanners, sites, and populations.  </li> <li>Integration into PACS/RIS and clinician workflows without increasing cognitive load or risk.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets and modalities covered (high level): </li> <li>Imaging: <ul> <li>Chest X\u2011ray (e.g., MIMIC\u2011CXR, CheXpert, ChestX\u2011ray14).  </li> <li>CT and MRI for various organs.  </li> <li>Ultrasound, endoscopy, ophthalmology images, and digital pathology slides.  </li> </ul> </li> <li> <p>Text and structured data: </p> <ul> <li>Radiology and pathology reports.  </li> <li>Clinical notes and EHR fields (demographics, lab values, vital signs).  </li> <li>QA pairs and instruction\u2011style prompts for training medical MLLMs.  </li> </ul> </li> <li> <p>Pretraining / representation patterns: </p> </li> <li>Vision encoders (often ViTs or CNNs) map images to dense feature maps or patch tokens.  </li> <li>Text encoders/decoders (LLMs) operate on tokenized reports and prompts.  </li> <li>Multimodal connectors (projection layers, query transformers, fusion modules, or expert\u2011driven converters) transform image features into token sequences consumable by the LLM.  </li> <li> <p>For contrastive pretraining, image and report embeddings are projected into a shared space for CLIP\u2011like alignment.</p> </li> <li> <p>Limitations of current datasets: </p> </li> <li>Many datasets are single\u2011center, with limited diversity in disease spectrum, scanners, and languages.  </li> <li>Public multimodal datasets often focus on chest imaging; other organs and modalities are underrepresented.  </li> <li>Region\u2011level and temporal annotations (for localization, progression tracking) are relatively rare.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type (surveyed archetypes): </li> <li>Contrastive VLMs: CLIP\u2011like models that learn a shared embedding space for images and reports.  </li> <li>Fusion\u2011based MLLMs: LLaVA\u2011style architectures where image tokens are injected into an LLM via cross\u2011attention or fusion blocks.  </li> <li> <p>Generative models: Systems that generate images or segmentations conditioned on text, or generate text conditioned on images (e.g., report generation).  </p> </li> <li> <p>New FM vs existing: </p> </li> <li> <p>The paper does not introduce a single new model; instead, it synthesizes architectural patterns and design choices across many existing MLLMs.</p> </li> <li> <p>Key components and innovations (framework level):</p> </li> </ul> Aspect Details Encoders Pretrained vision encoders (ViTs, CNNs) and LLMs as backbones Connectors Projection\u2011based, query\u2011based (Q\u2011former), fusion\u2011based, and expert\u2011driven language transformers Training strategies Contrastive pretraining, instruction tuning, chain\u2011of\u2011thought prompting, RLHF for clinical alignment Capabilities Report generation, VQA, retrieval, triage, decision support, image\u2011grounded dialog <ul> <li>Training setup (typical): </li> <li>Pretrain image\u2013text alignment on paired radiology datasets.  </li> <li>Adapt a general or medical LLM to accept visual tokens through connectors.  </li> <li>Instruction\u2011tune on multimodal tasks (RRG, VQA, captioning, dialog) using curated or synthetic data.  </li> <li>Optionally apply RLHF or preference optimization to improve clinical safety and usefulness.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This review is fundamentally about multimodal integration in medical imaging MLLMs.</p> <ul> <li>Modalities integrated: </li> <li> <p>Radiology and other medical images + text (reports, clinical notes, QA) + sometimes structured EHR signals.  </p> </li> <li> <p>Integration mechanisms: </p> </li> <li>CLIP\u2011style two\u2011tower alignment: <ul> <li>Separate image and text encoders trained with contrastive loss for retrieval and zero\u2011shot classification.  </li> </ul> </li> <li> <p>Connector\u2011based fusion: </p> <ul> <li>Projection\u2011based: linear or MLP projections from image features into the LLM token space.  </li> <li>Query\u2011based: learnable query tokens attend to visual features and feed condensed information to the LLM.  </li> <li>Fusion\u2011based: cross\u2011attention layers inside or around the LLM that jointly process image and text tokens.  </li> <li>Expert\u2011driven language transformation: upstream models convert imaging findings into textual descriptions consumed by an LLM.  </li> </ul> </li> <li> <p>New capabilities enabled: </p> </li> <li>Image\u2011grounded natural\u2011language interaction (VQA, \u201cchat with your scan\u201d).  </li> <li>Zero\u2011shot or few\u2011shot disease classification via text prompts.  </li> <li>Automated or draft radiology report generation.  </li> <li>Multimodal retrieval (image \u2194 text, patient\u2011level search).</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks and benchmarks discussed: </li> <li>Radiology report generation (RRG) from chest X\u2011rays and CT.  </li> <li>Visual question answering about imaging findings and clinical context.  </li> <li>Image\u2011text retrieval and cross\u2011modal search.  </li> <li>Disease detection and classification from multimodal inputs.  </li> <li> <p>Early explorations of planning, triage, and longitudinal reasoning.  </p> </li> <li> <p>Baselines and comparison themes: </p> </li> <li>Traditional unimodal CNN/ViT models vs multimodal systems.  </li> <li>General LLMs with simple image adapters vs domain\u2011specific medical MLLMs.  </li> <li> <p>Trade\u2011offs between model size, task performance, and compute requirements.  </p> </li> <li> <p>Key findings (high\u2011level trends): </p> </li> <li>MLLMs show promising capabilities for RRG, VQA, and multimodal reasoning, often outperforming unimodal baselines on complex tasks.  </li> <li>However, performance can be unstable across datasets and institutions, and models frequently hallucinate or misinterpret subtle findings.  </li> <li>Region grounding and localization remain weak; many models reason about images at a coarse, global level.  </li> <li>There is a growing shift toward foundation\u2011model approaches, leveraging large general or medical LLMs and pre\u2011trained vision encoders.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths of the review and current field:</p> <ul> <li>Provides a clear taxonomy of MLLM architectures, connectors, and training strategies for medical imaging.  </li> <li>Highlights the importance of multimodal reasoning that mirrors how radiologists combine images and clinical context.  </li> <li>Synthesizes evidence from recent prototypes and studies, giving readers an overview of what is currently feasible.  </li> <li>Emphasizes practical considerations for clinical adoption (data needs, infrastructure, workflow integration).</li> </ul> <p>Limitations and challenges (field\u2011level):</p> <ul> <li>Scarcity of large, diverse, high\u2011quality multimodal datasets with region\u2011level labels and outcome annotations.  </li> <li>High risk of hallucinated findings and uncalibrated confidence, especially when MLLMs operate outside their training distribution.  </li> <li>Limited interpretability and weak region grounding, making it hard to trust model outputs for critical decisions.  </li> <li>Heavy computational and infrastructure demands, which may be unsuitable for resource\u2011constrained hospitals.  </li> <li>Regulatory, privacy, and liability questions around deploying MLLMs in clinical care.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can we design MLLMs with robust region grounding, so that textual outputs are tightly coupled to specific image regions?  </li> <li>What training strategies and evaluation protocols are needed to reduce hallucination and ensure clinically safe behavior?  </li> <li>How can we leverage synthetic data, weak supervision, and federated learning to overcome data scarcity and privacy constraints?  </li> <li>What are effective ways to integrate EHR data, temporal imaging series, and multi\u2011organ information into a unified multimodal reasoning system?  </li> <li>How should guidelines, benchmarks, and regulations evolve to evaluate and govern MLLMs in real radiology workflows?</li> </ol>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>This paper is one of the first detailed reviews focused specifically on MLLMs for medical imaging, complementing broader MMFM and HFM surveys.  </li> <li>It connects general LLM and VLM advances (e.g., CLIP, LLaVA\u2011style architectures) to radiology\u2011specific tasks and constraints.  </li> <li>Relation to well-known ideas: </li> <li>Frames medical MLLMs as extensions of CLIP\u2011like alignment and LLM\u2011centric fusion architectures, adapted to clinical data.  </li> <li>Discusses how instruction tuning, chain\u2011of\u2011thought prompting, and RLHF\u2014successful in general AI\u2014might be adapted to medical imaging.  </li> <li>Why this review is a useful reference: </li> <li>For a grad student, it offers a curated tour of design patterns, datasets, and open challenges, making it easier to choose a research direction.  </li> <li>It also highlights the gap between prototype demos and clinically robust systems, encouraging critical evaluation and responsible innovation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Radiology practice is inherently multimodal, but most traditional AI systems are unimodal and cannot fully leverage combined image + text + EHR information.  </p> </li> <li> <p>Method / model (conceptual): </p> </li> <li>MLLMs couple powerful LLMs with vision encoders via multimodal connectors (projection, query, fusion, or expert\u2011driven), enabling joint reasoning over images and text.  </li> <li> <p>This review categorizes these architectures and training strategies, providing a design space for medical imaging MLLMs.  </p> </li> <li> <p>Results / insights: </p> </li> <li>Early MLLMs show strong potential for RRG, VQA, and multimodal decision support, often surpassing unimodal baselines.  </li> <li> <p>However, they are hampered by data scarcity, hallucination, poor region grounding, and deployment challenges.  </p> </li> <li> <p>Why it matters: </p> </li> <li>Understanding MLLMs is crucial for building the next generation of clinically useful, trustworthy multimodal foundation models in radiology.  </li> <li>This review gives practitioners and researchers a roadmap for tackling open problems and responsibly advancing the field.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/","title":"Multimodal FMs (2025)","text":""},{"location":"generated/kb_curated/papers-md/mmfm_2025/#medical-multimodal-foundation-models-in-clinical-diagnosis-and-treatment-applications-challenges-and-future-directions","title":"Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions","text":"<p>Authors: Kai Sun, Siyan Xue, Fuchun Sun, Haoran Sun, Yu Luo, Ling Wang, Siyuan Wang, Na Guo, Lei Liu, Tian Zhao, Xinzhou Wang, Lei Yang, Shuo Jin, Jun Yan, Jiahong Dong Year: 2025 Venue: Artificial Intelligence in Medicine</p>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical VLM / MLLM / MMFM + General FM survey / theory  </li> <li> <p>This work is a survey that systematizes datasets, architectures, and clinical applications of medical multimodal foundation models (MMFMs).</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Multimodal FM or cross-modal integration (survey and taxonomy)  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Medical images (CT, MRI, ultrasound, radiography, surgical video), text (reports, clinical notes), structured clinical data, and in some cases robotic and physiological signals.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>This survey reviews the rapidly growing field of medical multimodal foundation models (MMFMs), which aim to leverage diverse medical data\u2014images, text, signals, and more\u2014to support clinical diagnosis and treatment. The authors first trace the evolution of foundation models from transformers and vision transformers to multimodal models, then categorize MMFMs into medical multimodal vision foundation models (MMVFMs) and medical multimodal vision\u2013language foundation models (MMVLFMs). They systematically describe available datasets, proxy tasks (segmentation, generation, contrastive learning, hybrid tasks), and model architectures, and then connect these to downstream applications such as radiology report generation, disease diagnosis, treatment planning, and surgical robotics. The survey emphasizes both the opportunities (better generalization, data efficiency, cross\u2011task transfer) and the significant challenges (data quality, compute cost, fairness, interpretability, deployment, and regulation). For a new grad student, this paper serves as a comprehensive starting point for understanding the MMFM landscape and identifying promising research directions.</p>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Provide a unified view of medical multimodal foundation models that can:  </p> <ul> <li>Integrate heterogeneous medical data (multi\u2011organ, multi\u2011modality).  </li> <li>Support a wide spectrum of clinical tasks from diagnosis to treatment.  </li> <li>Move toward generalized medical AI rather than narrow, task\u2011specific models.  </li> </ul> </li> <li> <p>Why this is hard: </p> </li> <li>Data issues: <ul> <li>Medical data are high\u2011dimensional, heterogeneous (images, waveforms, text, lab results), and often noisy or incomplete.  </li> <li>Large multimodal datasets with consistent labeling and harmonization are rare; privacy and legal restrictions complicate sharing.  </li> </ul> </li> <li>Modeling challenges: <ul> <li>Designing architectures that scale to multiple organs, modalities, and tasks while remaining efficient.  </li> <li>Balancing generality with specialization; avoiding catastrophic forgetting while adapting to new tasks.  </li> </ul> </li> <li>Clinical and societal constraints: <ul> <li>Need for interpretability, fairness, robustness, and safe deployment in real clinical environments.  </li> <li>Regulatory frameworks and validation protocols are still evolving for foundation\u2011model\u2011based systems.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>The survey devotes substantial space to dataset landscapes for MMFMs.</p> <ul> <li>Plain text datasets: </li> <li>Large corpora of biomedical literature, clinical guidelines, and clinical notes.  </li> <li> <p>Used primarily to train or adapt medical LLM backbones that pair with vision or other modalities.</p> </li> <li> <p>Medical image datasets: </p> </li> <li>Diverse imaging modalities: CT, MRI, X\u2011ray, ultrasound, endoscopy, digital pathology, ophthalmology, etc.  </li> <li> <p>Multi\u2011organ and multi\u2011center datasets that enable cross\u2011domain learning.  </p> </li> <li> <p>Image\u2013text pair datasets: </p> </li> <li>Radiology and pathology image\u2013report pairs enabling CLIP\u2011style or CoCa\u2011style vision\u2013language pretraining.  </li> <li> <p>Datasets annotated with segmentation masks, bounding boxes, or keypoints for segmentation\u2011oriented proxy tasks.  </p> </li> <li> <p>Other modalities: </p> </li> <li> <p>Surgical videos, robotics telemetry, physiological signals, and multi\u2011omics data for advanced MMFM applications.  </p> </li> <li> <p>Preprocessing / representation themes: </p> </li> <li>Standardization of image resolutions and voxel spacing; patch extraction and multi\u2011scale crops for large images.  </li> <li>Tokenization and normalization of medical text; mapping structured EHR fields to embeddings.  </li> <li>Careful balancing of modalities to avoid dominance of one data type in multimodal pretraining.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type (taxonomy): </li> <li> <p>The paper distinguishes:  </p> <ul> <li>MMVFMs (Medical Multimodal Vision Foundation Models): multimodal vision encoders focusing on multiple medical image modalities.  </li> <li>MMVLFMs (Medical Multimodal Vision\u2013Language Foundation Models): models that combine image and text (and sometimes more modalities) in a shared framework.  </li> </ul> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>This is a survey; it does not introduce a specific model but analyzes and categorizes many.</p> </li> <li> <p>Proxy task categories (core contribution):</p> </li> </ul> Category Role in MMFMs Segmentation proxy Use segmentation tasks to learn detailed anatomical representations Generative proxy Use generative tasks (reconstruction, synthesis) for representation learning Contrastive proxy Use CLIP\u2011like or contrastive objectives for cross\u2011modal alignment Hybrid proxy Combine segmentation, generative, and contrastive objectives to cover multiple skills <ul> <li>Architectural themes: </li> <li>Transformer\u2011based encoders and decoders for both vision and language.  </li> <li>Two\u2011tower architectures for CLIP\u2011style alignment vs unified encoders for fully fused multimodal representations.  </li> <li> <p>Use of adapters, prompts, and low\u2011rank fine\u2011tuning (LoRA) for efficient adaptation of large backbones.  </p> </li> <li> <p>Training setup (generic patterns): </p> </li> <li>Large\u2011scale pretraining on multi\u2011organ, multi\u2011modality datasets using one or more proxy tasks.  </li> <li>Fine\u2011tuning or prompting on downstream tasks such as segmentation, detection, classification, report generation, and surgical control.  </li> <li>Multi\u2011task and multi\u2011stage training regimes to gradually build generalized capabilities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>Multimodal integration is the central theme of MMFMs.</p> <ul> <li>Modalities integrated: </li> <li> <p>Combinations of images, text, clinical variables, and sometimes signals like robotics trajectories or physiology.  </p> </li> <li> <p>Integration strategies: </p> </li> <li>Early fusion: combine modalities at the input or low\u2011level feature stage (e.g., concatenated embeddings).  </li> <li>Intermediate fusion: fuse modality\u2011specific encoders via cross\u2011attention or shared latent spaces.  </li> <li>Late fusion: combine modality\u2011specific model outputs via ensembles or simple aggregators.  </li> <li> <p>Vision\u2013language alignment: CLIP\u2011style or CoCa\u2011style objectives to map image and text into a shared space.  </p> </li> <li> <p>What this enables: </p> </li> <li>More holistic modeling of patient state by combining imaging, text, and structured data.  </li> <li>Cross\u2011task and cross\u2011organ transfer: representations learned for one modality or organ can help others.  </li> <li>Unified models that can support multiple downstream tasks from a shared backbone.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>As a survey, the paper does not present new experiments; instead, it summarizes trends in published MMFM work.</p> <ul> <li>Tasks discussed: </li> <li>Segmentation (organ, lesion) and detection tasks across various imaging modalities.  </li> <li>Disease classification, risk prediction, and prognosis.  </li> <li>Radiology report generation and medical image captioning.  </li> <li> <p>Surgical planning, navigation, and robotics control.  </p> </li> <li> <p>Key observations: </p> </li> <li>MMFMs often outperform single\u2011task, single\u2011modality baselines, especially when downstream labels are scarce.  </li> <li>Contrastive and hybrid proxy tasks tend to support better zero\u2011shot and few\u2011shot generalization.  </li> <li>Increasing data scale and modality diversity generally improves robustness, but also raises compute and data\u2011governance issues.  </li> <li>There is still a gap between offline benchmark performance and the reliability needed for clinical deployment.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths (of MMFMs and the survey):</p> <ul> <li>Provides a unified taxonomy that organizes MMFMs by proxy tasks, modalities, and architectures.  </li> <li>Connects method choices (segmentation vs contrastive vs hybrid) to clinical application domains.  </li> <li>Highlights how MMFMs can support precision medicine, from early diagnosis to personalized treatment.  </li> <li>Identifies key datasets and benchmarks, giving readers a practical starting point for experimentation.</li> </ul> <p>Limitations and challenges:</p> <ul> <li>Many MMFMs rely on limited or biased datasets, often from a small number of institutions or populations.  </li> <li>Compute and data requirements are high, raising concerns about environmental impact and accessibility.  </li> <li>Interpretability and explainability remain limited, especially for complex multimodal reasoning.  </li> <li>Fairness and generalization to under\u2011represented groups are not yet well studied.  </li> <li>Real\u2011world deployment faces hurdles in regulation, integration, and clinician acceptance.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can we develop data\u2011efficient MMFMs that retain strong performance without requiring enormous datasets and compute?  </li> <li>What are effective strategies for fair and robust multimodal pretraining, especially across institutions and populations?  </li> <li>How can we integrate causal and mechanistic knowledge into MMFMs to move beyond pattern recognition?  </li> <li>What evaluation frameworks are needed to measure trustworthiness, interpretability, and clinical impact of MMFMs?  </li> <li>How should MMFMs interface with clinical workflows and decision\u2011support systems to provide value without over\u2011automation?</li> </ol>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>This survey sits alongside other HFM and MLLM reviews but focuses specifically on medical multimodal foundation models that bridge multiple imaging modalities, text, and clinical tasks.  </li> <li>It articulates how MMFMs can underpin next\u2011generation clinical AI systems that span diagnosis, treatment, and surgical assistance.  </li> <li>Relation to well-known ideas: </li> <li>Builds on CLIP\u2011like vision\u2013language alignment, transformer\u2011based FMs (BERT, ViT, GPT), and recent medical FMs like M3FM, Me\u2011LLaMA, and TITAN.  </li> <li>Frames MMFMs as a path toward medical artificial general intelligence, while emphasizing the importance of safety and governance.  </li> <li>Why this paper is a useful reference: </li> <li>For students and practitioners, it provides a broad yet structured overview of the MMFM space, helping them situate specific models and choose research directions.  </li> <li>It also highlights critical non\u2011technical dimensions (fairness, regulation, deployment) that will shape the real\u2011world impact of MMFMs.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Medical AI needs foundation models that can handle multiple modalities and organs and support tasks ranging from diagnosis to treatment planning.  </p> </li> <li> <p>Method / model (conceptual): </p> </li> <li>MMFMs are built on transformer\u2011based backbones and trained using segmentation, generative, contrastive, or hybrid proxy tasks on large multimodal datasets.  </li> <li> <p>The survey categorizes models into MMVFMs and MMVLFMs and analyzes their architectures, datasets, and applications.  </p> </li> <li> <p>Results / insights: </p> </li> <li>MMFMs generally improve performance and data efficiency over narrow models, and support zero\u2011shot or few\u2011shot adaptation to new tasks.  </li> <li> <p>However, they face major challenges around data quality, compute cost, fairness, interpretability, and deployment.  </p> </li> <li> <p>Why it matters: </p> </li> <li>MMFMs are likely to be the backbone technology for future clinical AI systems; understanding their design and limitations is crucial.  </li> <li>This survey offers a roadmap for advancing MMFMs responsibly toward real\u2011world impact.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/","title":"Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models","text":"<p>Authors: Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin Year: 2025 Venue: Transactions on Machine Learning Research (TMLR)</p>"},{"location":"generated/kb_curated/papers-md/mot_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Vision / VLM / Multimodal FM  </li> <li> <p>The paper proposes a new architecture for unified multi-modal generation and understanding (text, images, and speech) in large foundation models.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Text (language tokens).  </li> <li>Images (discrete image tokens for generation and understanding).  </li> <li>Speech (discrete speech tokens in some settings).</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>Mixture-of-Transformers (MoT) is a sparse multi-modal transformer architecture designed to make large, unified foundation models for text, images, and speech much more computationally efficient. Instead of running a single dense transformer over all modalities, MoT keeps global self\u2011attention over the full mixed sequence but decouples all non\u2011embedding parameters by modality: feed\u2011forward networks, attention projections, and layer norms are specialized for each modality while sharing the same FLOP budget as the dense baseline. The authors evaluate MoT in several settings, including Chameleon\u2011style autoregressive text\u2013image and text\u2013image\u2013speech generation and Transfusion\u2011style models that combine autoregressive text generation with diffusion\u2011based image generation. Across scales from tens of millions to billions of parameters, MoT consistently matches or exceeds dense baselines while using 40\u201360% of the pretraining FLOPs, and delivers substantial wall\u2011clock speedups. The paper also studies modality separation, leave\u2011one\u2011modality\u2011out experiments, and hybrid models that mix MoT with MoE\u2011style experts, showing that modality-aware sparsity is a stable and effective alternative to learned routing. For a new grad student, MoT is a valuable example of how to introduce structured sparsity into multimodal transformers without sacrificing architectural simplicity.</p>"},{"location":"generated/kb_curated/papers-md/mot_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Build unified multi-modal foundation models that can jointly process and generate text, images, and speech, but do so with manageable compute budgets.  </li> <li> <p>Reduce the training and inference costs of early\u2011fusion multimodal transformers (like Chameleon\u2011style models) without giving up performance.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Dense transformers scale poorly when extended from text\u2011only LLMs to multi-modal settings; adding image and speech tokens massively increases sequence lengths and token diversity.  </li> <li>Different modalities have conflicting optimization dynamics and live in very different regions of representation space, so a single set of shared parameters may be suboptimal.  </li> <li>Mixture-of-Experts (MoE) architectures introduce routing instability, load\u2011balancing overhead, and complex bi\u2011level optimization; they are powerful but hard to train robustly at scale.  </li> <li>Any new architecture must preserve implementation simplicity and FLOP accounting so that practitioners can reason about cost vs. quality.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>MoT is evaluated in multiple multi-modal scenarios built on existing benchmarks and systems:</p> <ul> <li>Chameleon setting (text\u2013image, text\u2013image\u2013speech): </li> <li>Autoregressive generation over interleaved text and image tokens; extended to include discrete speech tokens in a three\u2011modality setup.  </li> <li> <p>Trained on large\u2011scale text\u2013image and text\u2013image\u2013speech datasets similar to those used for prior Chameleon models.</p> </li> <li> <p>Transfusion setting (text + diffusion images): </p> </li> <li>Text is modeled autoregressively as in standard LLMs.  </li> <li> <p>Images are modeled with diffusion\u2011based objectives, using latent image representations.  </p> </li> <li> <p>Modalities: </p> </li> <li>Text: natural language prompts, captions, and conversational context.  </li> <li>Images: discrete or latent tokens used for generation and captioning benchmarks.  </li> <li> <p>Speech: discrete speech tokens for audio generation and understanding (in the Chameleon+Speech experiments).</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>All modalities are converted into a single mixed token sequence, with tokens tagged by modality so that MoT can apply modality-specific parameters while keeping shared self\u2011attention over the whole sequence.  </li> <li>For diffusion images, latent representations and timesteps are embedded following standard diffusion\u2011transformer practices.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Sparse transformer architecture (Mixture-of-Transformers) with modality-specific parameters but dense\u2011style global self\u2011attention.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>MoT is a new architectural pattern that can be plugged into existing multimodal settings (e.g., Chameleon, Transfusion) while preserving their training objectives.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Sparsity mechanism Modality-aware sparsity over all non\u2011embedding parameters (FFNs, attention matrices, layer norms). Shared attention Full self\u2011attention over the mixed multi-modal sequence; no routing\u2011based sparsity in attention. Parameter decoupling For each modality, separate parameter sets for projections and FFNs, selected by token modality. Compatibility Drop\u2011in replacement for dense transformers in Chameleon and Transfusion architectures. Hybrid models Combining MoT with MoE\u20114x to explore complementary benefits of expert routing and modality sparsity. <ul> <li>Training setup (high level):</li> <li>Pretrain 13 models of various sizes (37M\u20137B, plus hybrid architectures) across multiple Chameleon and Transfusion settings.  </li> <li>Objectives:  <ul> <li>Autoregressive next\u2011token prediction for text and image tokens (Chameleon).  </li> <li>Autoregressive text + diffusion\u2011based image objectives (Transfusion).  </li> </ul> </li> <li>Compute measured in FLOPs and wall\u2011clock time; experiments run on multi\u2011GPU clusters (e.g., AWS p4de instances with A100s).</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>MoT directly targets multi-modal integration in unified transformers:</p> <ul> <li>Modalities integrated: </li> <li> <p>Text, images, and (in some experiments) speech, all represented as tokens in one interleaved sequence.</p> </li> <li> <p>How they are integrated: </p> </li> <li>The model applies global self\u2011attention across all tokens regardless of modality, enabling cross\u2011modal interactions at every layer.  </li> <li>For each token, a simple rule based on its modality selects which FFN, attention projections, and layer norms to use; this is rule\u2011based routing by modality, not learned MoE routing.  </li> <li> <p>This yields a sparse model where only a subset of parameters are active for each token, but the computational graph (FLOPs) matches a dense transformer.</p> </li> <li> <p>Why this integration is useful / new capabilities: </p> </li> <li>Maintains the strengths of early\u2011fusion multimodal transformers (rich cross\u2011modal attention) while reducing compute and avoiding MoE training instability.  </li> <li>Makes it feasible to train unified multi-modal foundation models at larger scales and on more complex objectives (e.g., mixed autoregressive + diffusion) with limited resources.  </li> <li>Provides a clean baseline architecture for future multi-modal FMs that want structured sparsity without heavy routing machinery.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Settings: </li> <li>Chameleon (text\u2013image, text\u2013image\u2013speech) for unified autoregressive generation.  </li> <li>Transfusion (text + diffusion images) for mixed\u2011objective training.  </li> <li> <p>Additional analyses of modality separation, leave\u2011one\u2011modality\u2011out behavior, and systems aspects (throughput, scaling).</p> </li> <li> <p>Baselines: </p> </li> <li>Dense Chameleon and Transfusion transformers at comparable parameter scales.  </li> <li> <p>Mixture-of-Experts (MoE\u20114x) variants that increase parameter count via expert routing.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>In Chameleon 7B, MoT matches dense performance on text and image metrics while using only 55.8% of pretraining FLOPs.  </li> <li>When extended to text\u2013image\u2013speech, MoT reaches speech performance comparable to the dense baseline with ~37% of the FLOPs for that modality.  </li> <li>In the Transfusion setting, a 760M MoT outperforms a 1.4B dense baseline on image generation and captioning metrics while using half the training and inference FLOPs; a 7B MoT matches the 7B dense model with roughly one third of the FLOPs for the image modality.  </li> <li>System profiling shows that MoT achieves the same image quality in 47.2% of the wall\u2011clock time and similar text quality in 75.6% of the time on A100 clusters.  </li> <li>Compared to MoE\u20114x, MoT often provides better or more stable performance at similar or lower FLOP budgets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Simple, modality-aware sparsity mechanism that integrates cleanly into standard transformers.  </li> <li>Demonstrated compute savings (FLOPs and wall\u2011clock) without sacrificing performance across several challenging multi-modal setups.  </li> <li>Extensive empirical evaluation across model sizes and tasks, including systems\u2011level profiling.  </li> <li>Provides a practical alternative to MoE that avoids routing instability and complex load balancing.</li> </ul> <p>Limitations:</p> <ul> <li>Still focused on a relatively small set of modalities (text, images, speech); does not cover more exotic or structured modalities (e.g., audio waveforms, tabular EHR, 3D point clouds).  </li> <li>Requires modality labels for tokens; does not explore more fine\u2011grained routing within a modality (e.g., by region or task).  </li> <li>Results are tied to specific training infrastructures and datasets; real\u2011world deployment costs may differ.  </li> <li>The paper does not deeply explore how MoT interacts with instruction tuning, long\u2011context training, or reinforcement learning, which are important in practice.</li> </ul> <p>Open questions and future directions:</p> <ol> <li>How well does MoT extend to more modalities and tasks, such as video, 3D data, or structured signals like EHR?  </li> <li>Can we combine modality-aware sparsity with learned experts in a principled way, getting the best of MoT and MoE while keeping training stable?  </li> <li>How does MoT behave under heavy instruction tuning or RLHF, where gradients can be noisy and task distributions shift?  </li> <li>Could similar modality-specific parameter decoupling be applied to encoder\u2013decoder or diffusion\u2011only architectures in a clean way?  </li> <li>What are the implications of MoT\u2011style specialization for interpretability, e.g., understanding what each modality-specific block has learned?</li> </ol>"},{"location":"generated/kb_curated/papers-md/mot_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>MoT sits in the line of work on unified multi-modal transformers (e.g., Chameleon, Transfusion, integrated transformer\u2011diffusion models) and offers a new way to scale them efficiently.  </li> <li>It complements MoE\u2011style sparse methods by providing a simpler, modality\u2011aware alternative that still preserves early\u2011fusion attention.  </li> <li>Relation to well-known ideas: </li> <li>Borrowing the idea of sparsity per token from MoE, but replacing learned routing with deterministic routing by modality.  </li> <li>Conceptually similar to having per\u2011modality adapters everywhere in the transformer, but integrated as full parameter sets rather than small adapter layers.  </li> <li>Why it is a useful reference: </li> <li>For researchers designing multi-modal FMs, MoT shows how to trade off computation and flexibility without sacrificing architectural clarity.  </li> <li>For systems and efficiency work, it offers a realistic case study in measuring FLOPs and wall\u2011clock savings in large multimodal training runs.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Early\u2011fusion multimodal transformers for unified text\u2013image\u2013speech generation are extremely compute\u2011hungry; MoE\u2011style sparsity is powerful but unstable and complex.</p> </li> <li> <p>Method / model: </p> </li> <li>Mixture-of-Transformers (MoT) introduces modality-aware sparsity by decoupling non\u2011embedding transformer parameters per modality while keeping global self\u2011attention and the same FLOP budget as dense models.  </li> <li> <p>It acts as a drop\u2011in replacement for dense transformers in Chameleon and Transfusion\u2011style architectures.</p> </li> <li> <p>Results: </p> </li> <li>Matches or exceeds dense baselines across Chameleon and Transfusion setups while using 40\u201360% of the pretraining FLOPs and significantly less wall\u2011clock time.  </li> <li> <p>Scales well across model sizes and remains competitive with MoE\u2011based baselines at similar or lower compute.</p> </li> <li> <p>Why it matters: </p> </li> <li>Demonstrates that structured, modality-aware sparsity is a practical way to scale multimodal foundation models, preserving rich cross\u2011modal interactions while controlling compute.  </li> <li>Provides a clean architectural template for future unified VLMs and multimodal FMs focused on efficiency.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/","title":"Oncology multimodal (Waqas 2024)","text":""},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#multimodal-data-integration-for-oncology-in-the-era-of-deep-neural-networks-a-review","title":"Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review","text":"<p>Authors: Asim Waqas, Aakash Tripathi, Ravi P. Ramachandran, Paul A. Stewart, Ghulam Rasool Year: 2024 Venue: Frontiers in Artificial Intelligence</p>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Multimodal / Integration (specifically applied to oncology/cancer research). This is a review paper surveying multimodal deep learning methods\u2014especially Graph Neural Networks (GNNs) and Transformers\u2014for integrating diverse cancer-related data modalities (imaging, omics, clinical records).</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Review of multimodal FM and integration methods. The paper does not propose a new foundation model but systematically reviews how modern deep learning architectures (GNNs, Transformers) are being applied to multimodal oncology data fusion, including references to foundation models like CLIP, GPT-4, and FLAVA.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Radiological imaging (CT, MRI, PET scans)</li> <li>Digitized histopathology (whole-slide images, H&amp;E stains)</li> <li>Multi-omics (genomics, transcriptomics, proteomics, metabolomics)</li> <li>Electronic health records (EHR) and clinical data</li> <li>Hybrid/derived modalities (radiomics, pathomics features)</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This review paper surveys the landscape of multimodal data integration in oncology, focusing on how deep neural networks\u2014particularly Graph Neural Networks (GNNs) and Transformers\u2014are being used to fuse diverse cancer data types for improved diagnosis, prognosis, and treatment prediction. Cancer research generates heterogeneous data across multiple scales and modalities: from imaging (radiology, pathology) to molecular profiles (genomics, transcriptomics, proteomics) to clinical records. Traditional unimodal analyses fail to capture the complex, interconnected nature of cancer biology. The authors present a comprehensive taxonomy of multimodal learning approaches, covering fusion strategies (early, intermediate, late), neural architectures (CNNs, RNNs, GNNs, Transformers), and domain-specific applications in oncology. They highlight how GNNs naturally model relationships among heterogeneous entities (patients, genes, images) and how Transformers, through self-attention, can integrate sequences of multimodal tokens. Key studies are reviewed across tasks like tumor classification, survival prediction, treatment response, and biomarker discovery. The paper also discusses major challenges\u2014data heterogeneity, missing modalities, alignment across scales, interpretability, and the need for large labeled datasets\u2014and points to promising directions, including foundation models and self-supervised pretraining. For researchers and clinicians, this review provides both a conceptual framework for understanding multimodal fusion and a practical roadmap for applying state-of-the-art deep learning to personalized cancer care.</p>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<p>Scientific / practical problem:</p> <ul> <li>Cancer is inherently multimodal: its biology spans genomic mutations, protein expression, tissue morphology, organ-level imaging, and clinical phenotypes.</li> <li>Prediction and personalization goals:</li> <li>Accurate early diagnosis and cancer subtype classification</li> <li>Prognosis (survival, recurrence risk)</li> <li>Treatment response prediction and therapy selection</li> <li>Discovery of prognostic and predictive biomarkers</li> <li>Traditional approaches analyze modalities in isolation (e.g., only imaging or only genomics), missing synergistic information that could improve accuracy and clinical utility.</li> </ul> <p>Why this is hard:</p> <ul> <li>Data heterogeneity:</li> <li>Different modalities have different dimensionalities, scales, and noise characteristics (e.g., high-resolution images vs sparse genomic variants vs tabular clinical data).</li> <li>Modalities are collected with different protocols, scanners, and sequencing platforms, leading to batch effects and site-specific biases.</li> <li>Integration complexity:</li> <li>No universal representation: images are spatial grids, omics are vectors or graphs, clinical data are tabular.</li> <li>Determining when and how to fuse (early vs late) requires domain knowledge and empirical tuning.</li> <li>Missing data:</li> <li>Not all patients have all modalities (e.g., some lack genomic profiling, others lack certain imaging studies).</li> <li>Models must handle partial observations gracefully.</li> <li>Label scarcity and class imbalance:</li> <li>High-quality multi-modal datasets with expert annotations are rare and expensive.</li> <li>Many cancer subtypes are rare, leading to imbalanced training sets.</li> <li>Interpretability and clinical trust:</li> <li>\"Black-box\" deep learning predictions are hard to explain, yet clinicians need to understand why a model predicts a certain outcome.</li> <li>Regulatory and ethical considerations demand transparency.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>Oncology data modalities covered in the review:</p> <ul> <li>Radiological imaging:</li> <li>CT, MRI, PET scans</li> <li> <p>Used for tumor detection, staging, and monitoring response</p> </li> <li> <p>Digitized histopathology:</p> </li> <li>Whole-slide images (WSI) of H&amp;E-stained tissue</li> <li>Immunohistochemistry (IHC) and other stains</li> <li> <p>Pathomics: quantitative features extracted from slides</p> </li> <li> <p>Multi-omics:</p> </li> <li>Genomics: DNA mutations, copy number variations, single nucleotide polymorphisms</li> <li>Transcriptomics: RNA-seq, gene expression profiles</li> <li>Proteomics: Protein abundance and post-translational modifications</li> <li>Metabolomics: Small molecule profiles</li> <li> <p>Epigenomics: DNA methylation, histone modifications</p> </li> <li> <p>Electronic Health Records (EHR) and clinical data:</p> </li> <li> <p>Demographics, clinical notes, laboratory results, treatment history, survival outcomes</p> </li> <li> <p>Radiomics:</p> </li> <li>Hand-crafted or learned features from imaging (texture, shape, intensity)</li> </ul> <p>Major datasets mentioned:</p> <ul> <li>The Cancer Genome Atlas (TCGA): Pan-cancer multi-omics and clinical data</li> <li>Genomic Data Commons (GDC): Centralized repository for TCGA and other NCI programs</li> <li>UK Biobank, All of Us: Large-scale cohorts with imaging and genomics</li> <li>TCIA (The Cancer Imaging Archive): Radiological imaging datasets</li> <li>Various cancer-specific cohorts (e.g., NSCLC, breast cancer, glioblastoma)</li> </ul> <p>Preprocessing / representation:</p> <ul> <li>Images: patches or whole-image embeddings from CNNs</li> <li>Omics: normalized vectors, sometimes projected into lower dimensions</li> <li>Clinical: tabular features, often encoded or embedded</li> <li>Graphs: patients, genes, images as nodes; relationships (co-expression, similarity) as edges</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>Model Types:</p> <p>The review covers a range of architectures for multimodal fusion:</p> Architecture Role in Multimodal Oncology CNNs Feature extraction from images (radiology, pathology) RNNs/LSTMs Sequential clinical data, temporal progression Autoencoders/VAEs Dimensionality reduction, unsupervised feature learning GANs Data augmentation, synthetic image generation Graph Neural Networks (GNNs) Model relationships among patients, genes, and multi-modal entities; handle heterogeneous graphs Transformers Self-attention over multimodal token sequences; pre-trained vision-language models adapted to oncology <p>Focus on GNNs:</p> <ul> <li>Graph representation:</li> <li>Nodes: patients, genes, images, or feature vectors from different modalities</li> <li>Edges: similarity (clinical, genomic), co-occurrence, known biological interactions</li> <li>GNN architectures reviewed:</li> <li>Graph Convolutional Networks (GCN)</li> <li>Graph Attention Networks (GAT)</li> <li>GraphSAGE</li> <li>Message Passing Neural Networks (MPNN)</li> <li>Applications:</li> <li>Patient similarity networks for survival prediction</li> <li>Gene regulatory networks combined with patient omics</li> <li>Pathology graphs (cells/patches as nodes) integrated with omics</li> </ul> <p>Focus on Transformers:</p> <ul> <li>Vanilla Transformers: Self-attention to integrate sequences of multimodal embeddings.</li> <li>Vision Transformers (ViT): Patches of histopathology or radiology images as tokens.</li> <li>Multimodal Transformers: </li> <li>Cross-modal attention between image and text/omics modalities</li> <li>CLIP-like contrastive learning adapted to radiology-pathology or image-genomics pairs</li> <li>Foundation models mentioned: CLIP, GPT-4, FLAVA, and domain-specific models like MedCLIP.</li> </ul> <p>Training setup (general patterns):</p> <ul> <li>Pretraining: Often on large unimodal datasets (e.g., ImageNet for images, public omics for gene embeddings), sometimes with self-supervised objectives.</li> <li>Fine-tuning / transfer learning: Adapt pretrained encoders to oncology tasks with smaller labeled datasets.</li> <li>Fusion stages:</li> <li>Early fusion: Concatenate raw features before modeling.</li> <li>Intermediate (joint) fusion: Learn shared representations mid-network.</li> <li>Late fusion: Train separate modality-specific models, combine predictions at the end.</li> <li>Scale: Varies widely; some studies use hundreds of patients, others leverage TCGA's thousands of samples. Model sizes range from small task-specific networks to large Transformer-based foundation models.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This is fundamentally a multimodal integration review, so this section is central.</p> <p>Which modalities are integrated:</p> <ul> <li>Common pairs/triplets:</li> <li>Radiology + pathology: CT/MRI + histopathology WSI</li> <li>Imaging + omics: Radiology or pathology + genomics/transcriptomics</li> <li>Omics + clinical: Gene expression + EHR/treatment history</li> <li>Triple integration: Imaging + omics + clinical (less common, more challenging)</li> </ul> <p>How they are integrated:</p> <ul> <li>Early fusion:</li> <li>Concatenate features from all modalities into a single vector and feed into a downstream classifier.</li> <li>Pros: Simple, allows the model to learn joint patterns from the start.</li> <li> <p>Cons: Can be dominated by the highest-dimensional modality; requires careful normalization; struggles with missing data.</p> </li> <li> <p>Late fusion:</p> </li> <li>Train separate models for each modality, then combine predictions (e.g., averaging, voting, stacking).</li> <li>Pros: Preserves modality-specific signals; robust to missing modalities; easier to interpret.</li> <li> <p>Cons: May miss complex cross-modal interactions.</p> </li> <li> <p>Intermediate (joint) fusion:</p> </li> <li>Modality-specific encoders produce embeddings that are fused at a middle layer (e.g., via concatenation, attention, or graph pooling) before final prediction.</li> <li>Pros: Balances flexibility and integration.</li> <li> <p>Cons: Requires architectural design choices; harder to optimize.</p> </li> <li> <p>GNN-based fusion:</p> </li> <li>Construct a heterogeneous graph with nodes from different modalities (patient omics, image features, clinical variables).</li> <li>GNN message passing aggregates cross-modal information.</li> <li> <p>Example: A patient node connected to its gene expression profile node and its pathology image embedding node; GNN learns to propagate and combine information.</p> </li> <li> <p>Transformer-based fusion:</p> </li> <li>Represent each modality as a sequence of tokens (e.g., image patches, genomic regions, clinical features).</li> <li>Self-attention and cross-attention layers integrate across modalities.</li> <li>Example: Multimodal Transformer taking pathology image patches and omics embeddings as separate token sets, with attention heads learning cross-modal dependencies.</li> </ul> <p>Why this integration is useful:</p> <ul> <li>Complementary information: Imaging reveals spatial tumor characteristics, omics show molecular drivers, clinical data provide context (age, stage, treatment).</li> <li>Improved prediction: Studies show multimodal models often outperform unimodal baselines on survival, classification, and treatment response tasks.</li> <li>Biological insight: Cross-modal associations (e.g., imaging phenotypes correlated with gene expression) can reveal biomarkers and mechanisms.</li> <li>Personalization: Comprehensive profiles enable tailored treatment recommendations.</li> </ul> <p>Relation to the integration baseline plan:</p> <ul> <li>Late fusion first under heterogeneous semantics:</li> <li>The review aligns with this principle: many successful oncology studies use late fusion or ensemble methods, preserving modality-specific encoders.</li> <li> <p>GNNs and Transformers can implement late fusion naturally (separate encoding + graph/attention-based aggregation).</p> </li> <li> <p>Robustness and evaluation discipline:</p> </li> <li>The review emphasizes the need for rigorous cross-validation, proper train/test splits (especially important given small sample sizes), and metrics like AUROC, AUPRC, C-index for survival.</li> <li> <p>Challenges of missing data and distribution shift are highlighted, aligning with the baseline plan's focus on residualization, covariate adjustment, and bootstrap confidence intervals.</p> </li> <li> <p>CCA and permutation testing:</p> </li> <li> <p>Not explicitly covered in the review, but the principle of exploring modality correlations before heavy fusion is implicit in studies that perform feature selection or canonical correlation analysis on omics and imaging features.</p> </li> <li> <p>Modality sequencing:</p> </li> <li>The review suggests starting with well-characterized modalities (e.g., standard imaging + omics) and progressively adding more complex ones (e.g., pathology WSI, radiomics), consistent with the plan's incremental approach.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>Tasks / benchmarks reviewed:</p> <p>The paper surveys a wide range of studies across multiple oncology tasks:</p> <ul> <li>Tumor classification and subtyping:</li> <li>GNN and Transformer models classify cancer types (e.g., glioma grades, breast cancer molecular subtypes) using combined imaging and omics.</li> <li> <p>Studies report accuracy improvements of 3-10% over unimodal baselines.</p> </li> <li> <p>Survival prediction and prognosis:</p> </li> <li>Multimodal Cox regression models, GNN-based survival networks, and attention-based models integrate clinical, omics, and imaging data.</li> <li> <p>C-index improvements of ~0.05-0.15 compared to clinical-only or omics-only models.</p> </li> <li> <p>Treatment response prediction:</p> </li> <li>Predicting response to chemotherapy, immunotherapy, or targeted therapies.</li> <li> <p>Multimodal approaches combining radiology (baseline tumor imaging) with genomics (mutation profiles) show better discrimination (AUC gains of 0.05-0.10).</p> </li> <li> <p>Biomarker discovery:</p> </li> <li>GNNs identify gene modules and image features associated with outcomes.</li> <li>Transformers' attention weights highlight cross-modal associations (e.g., specific image regions correlating with gene expression patterns).</li> </ul> <p>Baselines:</p> <ul> <li>Unimodal models (imaging-only, omics-only, clinical-only)</li> <li>Traditional ML methods (logistic regression, random forests on concatenated features)</li> <li>Early fusion baselines (simple concatenation + MLP)</li> </ul> <p>Key findings (trends and insights):</p> <ul> <li>Multimodal consistently beats unimodal: Across most studies, integrating multiple data types improves predictive performance, often significantly.</li> <li>Fusion strategy matters: Late fusion and intermediate fusion tend to outperform early fusion, especially when modalities have very different characteristics.</li> <li>GNNs excel at relational data: When patient or gene relationships are explicitly modeled, GNNs capture network effects that simpler models miss.</li> <li>Transformers scale well: Transformer-based models benefit from larger datasets and can leverage pretrained vision-language models (transfer learning from CLIP-like architectures).</li> <li>Interpretability via attention: Attention weights in Transformers and GNN message passing provide some interpretability, highlighting which modality or feature drives predictions.</li> <li>Challenges remain: Performance gains are modest in some cases; missing data and small sample sizes limit generalization; computational cost is high for large-scale models.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Comprehensive survey: Covers a wide range of architectures (CNNs, RNNs, GANs, GNNs, Transformers) and applications in oncology.</li> <li>Taxonomy and framework: Provides a clear taxonomy of fusion strategies and multimodal learning paradigms, useful for researchers entering the field.</li> <li>Focus on emerging methods: Highlights GNNs and Transformers, which are underexplored in oncology compared to computer vision and NLP.</li> <li>Identifies data resources: Lists major multimodal oncology datasets (TCGA, GDC, TCIA, UK Biobank), facilitating reproducible research.</li> <li>Balances technical depth and accessibility: Suitable for both ML researchers new to oncology and oncology researchers new to advanced deep learning.</li> </ul> <p>Limitations:</p> <ul> <li>Limited discussion of causal inference: The review focuses on predictive modeling; less attention to causal relationships or confounding (e.g., how to distinguish direct biological effects from correlations).</li> <li>Sparse coverage of robustness and fairness: Issues like model bias across demographics, generalization to external cohorts, and adversarial robustness are mentioned but not deeply explored.</li> <li>Lack of standardized benchmarks: The field lacks common evaluation protocols and public leaderboards, making it hard to compare methods across studies.</li> <li>Interpretability still nascent: While attention weights and GNN message passing offer some transparency, true mechanistic interpretability (linking predictions to biological pathways) remains an open challenge.</li> <li>Computational and data barriers: Many proposed methods require large computational resources and extensive labeled data, limiting accessibility for smaller research groups and clinical settings.</li> </ul> <p>Open Questions and Future Directions:</p> <ul> <li>Foundation models for oncology multimodal data:</li> <li> <p>Can we pretrain large multimodal Transformers on diverse cancer datasets (like CLIP for vision-language) to create a general-purpose oncology FM that transfers to many downstream tasks?</p> </li> <li> <p>Handling missing modalities robustly:</p> </li> <li> <p>Develop architectures that gracefully handle partial observations (e.g., modality dropout during training, imputation via cross-modal generation).</p> </li> <li> <p>Causal multimodal modeling:</p> </li> <li> <p>Move beyond association to causal discovery: which modality changes drive outcomes? How to design experiments or observational studies to infer causality?</p> </li> <li> <p>Fairness and generalization:</p> </li> <li> <p>Ensure multimodal models perform equitably across different patient demographics, cancer subtypes, and institutions (multi-site validation, fairness-aware training).</p> </li> <li> <p>Integration with clinical workflows:</p> </li> <li> <p>Design models that output actionable, interpretable predictions usable by oncologists in real-time decision-making.</p> </li> <li> <p>Self-supervised and few-shot learning:</p> </li> <li> <p>Leverage unlabeled multimodal data (vast amounts available) via self-supervised pretraining; adapt models to rare cancers with few labeled examples using few-shot learning.</p> </li> <li> <p>Explainability and biological insight:</p> </li> <li>Develop methods to extract mechanistic understanding from multimodal models (e.g., which genes and image features co-vary and why?).</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<p>Position in the FM and multimodal learning landscape:</p> <ul> <li>This review sits at the intersection of multimodal machine learning and biomedical AI, specifically focused on cancer.</li> <li>It relates to broader trends in foundation models:</li> <li>CLIP, GPT-4, FLAVA demonstrate that large-scale pretraining on multimodal data (vision + language) yields versatile representations.</li> <li> <p>Oncology is following suit: researchers are exploring whether similar pretraining strategies (e.g., on large radiology-pathology-omics datasets) can yield \"cancer foundation models.\"</p> </li> <li> <p>Analogy to well-known ideas:</p> </li> <li>GNNs for multimodal oncology are like \"knowledge graphs for cancer,\" where nodes and edges capture heterogeneous entities and relationships.</li> <li>Transformers for multimodal oncology are like \"BERT/GPT but for diverse cancer data tokens,\" using self-attention to integrate across modalities.</li> </ul> <p>Relation to the integration baseline plan:</p> <ul> <li>The review's taxonomy (early vs late fusion, GNN vs Transformer architectures) directly informs the baseline plan's integration strategy recommendations.</li> <li>Late fusion (modality-specific encoders + final aggregation) is a recurring theme in successful studies, consistent with the plan's preference for preserving modality-specific signals.</li> <li>The emphasis on evaluation rigor (cross-validation, proper metrics, missing data handling) aligns with the plan's robustness and evaluation discipline.</li> <li>The review highlights GNNs and Transformers as promising architectures for escalation beyond simple concatenation-based fusion, matching the plan's suggestion to explore two-tower contrastive or hub-token architectures if late fusion proves valuable.</li> </ul> <p>Why this paper is a useful reference:</p> <ul> <li>Educational value: For a new grad student, this review provides a structured entry point into multimodal oncology, with clear definitions, examples, and a roadmap of key papers.</li> <li>Design patterns: The taxonomy of fusion strategies and architectures serves as a design template for building new multimodal systems in cancer or other biomedical domains.</li> <li>Data resources: The compilation of datasets accelerates research by pointing to readily available multimodal cohorts.</li> <li>Future directions: The open questions guide thesis topics and grant proposals, highlighting high-impact areas for methodological development.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<p>Problem:</p> <ul> <li>Cancer data is inherently multimodal (imaging, omics, clinical records), but traditional analyses treat modalities in isolation.</li> <li>Integrating diverse data types promises improved diagnosis, prognosis, and treatment prediction, but poses significant technical and domain challenges.</li> </ul> <p>Method / Model:</p> <ul> <li>The review surveys deep neural network architectures for multimodal fusion in oncology:</li> <li>CNNs and RNNs for imaging and sequential data</li> <li>Graph Neural Networks (GNNs) for modeling relationships among patients, genes, and features across modalities</li> <li>Transformers for self-attention-based integration of multimodal token sequences</li> <li>Fusion strategies covered:</li> <li>Early fusion: Concatenate features before modeling</li> <li>Late fusion: Separate modality-specific models, combine predictions</li> <li>Intermediate fusion: Joint representations learned mid-network, often via attention or graph pooling</li> <li>The review emphasizes foundation model concepts (pretraining, transfer learning) and references CLIP, GPT-4, FLAVA as inspiration for oncology-specific multimodal FMs.</li> </ul> <p>Results / Insights:</p> <ul> <li>Multimodal models generally outperform unimodal baselines on classification, survival, and treatment response tasks, with typical improvements of 3-15% in accuracy/AUC and 0.05-0.15 in C-index.</li> <li>Late and intermediate fusion strategies often yield the best performance, preserving modality-specific information while capturing cross-modal interactions.</li> <li>GNNs are particularly effective when relational structure (patient networks, gene-gene interactions) is explicitly modeled.</li> <li>Transformers scale well and benefit from pretrained vision-language models, showing promise for large-scale oncology FMs.</li> <li>Interpretability is improved via attention weights and GNN message passing, but mechanistic understanding remains limited.</li> </ul> <p>Why it matters:</p> <ul> <li>This review provides a comprehensive, structured overview of multimodal deep learning in oncology, filling a gap in the literature by focusing on GNNs and Transformers.</li> <li>It offers a taxonomy and design framework that researchers can use to build and evaluate multimodal systems.</li> <li>By highlighting data resources, challenges, and future directions, it accelerates progress toward personalized cancer care powered by integrated, interpretable AI.</li> <li>For the integration baseline plan, this review validates key principles (late fusion under heterogeneity, robustness discipline, modality sequencing) and points to GNNs/Transformers as architectural choices for escalation beyond simple baselines.</li> </ul>"},{"location":"generated/kb_curated/papers-md/prs_guide/","title":"A Guide to Performing Polygenic Risk Score Analyses (2019)","text":"","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/prs_guide/#1-problem-tasks","title":"1. Problem &amp; Tasks","text":"<ul> <li>Practical tutorial for constructing, validating, and reporting polygenic risk scores (PRS).</li> <li>Relevant for defining covariate controls and baseline genetics features when combining with MRI.</li> </ul>","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/prs_guide/#2-datasets","title":"2. Datasets","text":"<ul> <li>Examples drawn from UK Biobank and PGC cohorts; outlines required inputs: GWAS summary stats, target genotypes.</li> <li>Emphasizes ancestry-matched target sets and QC thresholds (MAF, INFO, Hardy\u2013Weinberg).</li> </ul>","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/prs_guide/#3-model-method-details","title":"3. Model / Method Details","text":"","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/prs_guide/#31-pipeline","title":"3.1 Pipeline","text":"<ul> <li>QC target genotypes \u2192 LD clumping/thresholding or more advanced methods (LDPred, PRS-CS).</li> <li>Calculate PRS per subject, standardize, residualize vs covariates.</li> <li>Evaluate predictive performance with logistic regression or linear regression depending on phenotype.</li> </ul>","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/prs_guide/#32-confound-handling-evaluation-discipline","title":"3.2 Confound Handling &amp; Evaluation Discipline","text":"<ul> <li>Always include ancestry PCs (\u226510), sex, age, batch in regression models.</li> <li>Use nested CV when tuning PRS hyperparameters to avoid overfitting.</li> <li>Report incremental R\u00b2 / pseudo-R\u00b2 and calibration metrics.</li> </ul>","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/prs_guide/#4-results-tables","title":"4. Results &amp; Tables","text":"<ul> <li>Provides illustrative tables showing how PRS performance varies with clumping parameters and ancestry mismatches (drops of 30\u201350% accuracy when mismatched).</li> </ul>","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/prs_guide/#5-limitations-cautions","title":"5. Limitations &amp; Cautions","text":"<ul> <li>Focused on SNP array + European ancestry; does not directly cover WES/WGS nuance.</li> <li>Methods (clumping + thresholding) may be superseded by PRS-CS, but workflow guidance still valid.</li> </ul>","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/prs_guide/#6-hooks-into-neuro-omics-kb","title":"6. Hooks into Neuro-Omics KB","text":"<p>Relevant KB assets</p> <ul> <li><code>kb/paper_cards/prs_guide.yaml</code></li> <li><code>kb/datasets/ukb_manifest_stub.yaml</code> (covariate list, PC requirements).</li> </ul> <p>Configs / recipes informed</p> <ul> <li>Future baseline where PRS is an additional modality alongside gene embeddings and MRI.</li> <li>Provides justification for including PCs, age, sex, site covariates in <code>configs/experiments/*</code>.</li> </ul> <p>Concrete guidance for our project</p> <ul> <li>Whenever PRS features are added, include at least 10 ancestry PCs + batch covariates in modeling (per guide).</li> <li>Use nested CV for PRS hyperparameters; avoid leaking test folds.</li> <li>Document QC thresholds and summary-stat sources in dataset cards, referencing this guide.</li> </ul>","tags":["paper-notes","genetics","prs"]},{"location":"generated/kb_curated/papers-md/template/","title":"Paper Title (Year) \u2014 Short Tagline","text":"","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#1-problem-tasks","title":"1. Problem &amp; Tasks","text":"<ul> <li>What is being predicted/estimated?</li> <li>Benchmarks/tasks, evaluation settings (classification, regression, survival, etc.).</li> <li>How this connects to genetics \u00d7 MRI integration (if applicable).</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#2-datasets","title":"2. Datasets","text":"<ul> <li>Cohorts, sample sizes, populations, modalities.</li> <li>Preprocessing specifics (QC, parcellations, sequencing protocols, etc.).</li> <li>Splits: train/val/test definitions, CV type, site-aware grouping, seeds.</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#3-model-method-details","title":"3. Model / Method Details","text":"","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#31-architecture-method","title":"3.1 Architecture / Method","text":"<ul> <li>Backbone, context length, tokenization, RC-equivariance, hub tokens, etc.</li> <li>Input/output formats, objectives, losses, notable tricks.</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#32-confound-handling-evaluation-discipline","title":"3.2 Confound Handling &amp; Evaluation Discipline","text":"<ul> <li>Residualization, stratification, harmonization, missingness handling.</li> <li>Metrics (AUROC/AUPRC/Brier/calibration), statistical tests (DeLong, bootstrap, permutations, FDR).</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#4-results-tables","title":"4. Results &amp; Tables","text":"<ul> <li>Key numbers vs baselines (include effect sizes and uncertainty if reported).</li> <li>Ablation results or thresholds that matter for reuse.</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#5-limitations-cautions","title":"5. Limitations &amp; Cautions","text":"<ul> <li>Author-listed limitations plus any reuse caveats you notice.</li> <li>Cohort bias, preprocessing quirks, compute constraints, licensing limits.</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#6-hooks-into-neuro-omics-kb","title":"6. Hooks into Neuro-Omics KB","text":"<p>Relevant KB assets</p> <ul> <li><code>kb/paper_cards/&lt;slug&gt;_YYYY.yaml</code></li> <li><code>kb/model_cards/...</code> (if applicable)</li> <li><code>kb/datasets/...</code> (if applicable)</li> <li><code>docs/generated/kb_curated/integration_cards/&lt;related&gt;.md</code></li> </ul> <p>Configs / recipes informed</p> <ul> <li><code>configs/experiments/...</code></li> <li><code>docs/integration/analysis_recipes/...</code></li> <li><code>docs/integration/integration_strategy.md</code></li> </ul> <p>Concrete guidance for our project</p> <ul> <li>Fusion pattern / modality sequencing choices this paper justifies.</li> <li>Specific covariates, statistical tests, or dataset fields we adopted.</li> <li>Any LOGO/PRS/GWAS/CCA protocol parameters we ported over.</li> </ul> <p>Copy this file to <code>docs/generated/kb_curated/papers-md/&lt;slug&gt;.md</code> and fill in the sections. Keep Layer 2 MDs canonical, so future tooling (RAG, dashboards) can scan them systematically.</p>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/titan_2025/","title":"TITAN (2025)","text":""},{"location":"generated/kb_curated/papers-md/titan_2025/#titan-a-multimodal-whole-slide-foundation-model-for-computational-pathology","title":"TITAN: A Multimodal Whole-Slide Foundation Model for Computational Pathology","text":"<p>Authors: Tong Ding, Sophia J. Wagner, Andrew H. Song, Richard J. Chen, Ming Y. Lu, Andrew Zhang, Anurag J. Vaidya, Guillaume Jaume, Muhammad Shaban, Ahrong Kim, Drew F. K. Williamson, Harry Robertson, Bowen Chen, Cristina Almagro-P\u00e9rez, Paul Doucet, Sharifa Sahai, Chengkuan Chen, Christina S. Chen, Daisuke Komura, Akihiro Kawabe, Mieko Ochi, Shinya Sato, Tomoyuki Yokose, Yohei Miyagi, Shumpei Ishikawa, Georg Gerber, Tingying Peng, Long Phi Le, Faisal Mahmood Year: 2025 Venue: Nature Medicine</p>"},{"location":"generated/kb_curated/papers-md/titan_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical Vision FM + Medical VLM / MLLM / MMFM  </li> <li> <p>TITAN is a whole\u2011slide histopathology foundation model that combines vision\u2011only pretraining with vision\u2013language alignment for pathology reports and synthetic captions.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Whole\u2011slide histopathology images (WSIs) across \u224820 organ types.  </li> <li>Pathology reports (free\u2011text, slide\u2011level).  </li> <li>Synthetic fine\u2011grained region\u2011of\u2011interest (ROI) captions generated by a multimodal pathology copilot (PathChat).</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>TITAN (Transformer\u2011based pathology Image and Text Alignment Network) is a slide\u2011level foundation model for pathology designed to transform gigapixel whole\u2011slide images into general\u2011purpose feature representations that support diagnosis, prognosis, retrieval, and report generation. Instead of working at the level of raw pixels, TITAN builds on pre\u2011extracted patch embeddings from powerful histology encoders, then scales self\u2011supervised learning (SSL) to entire slides using a vision transformer with long\u2011context positional encodings. The model is pretrained in three stages: vision\u2011only self\u2011supervision on hundreds of thousands of WSIs; vision\u2013language alignment using synthetic ROI\u2011level captions; and slide\u2011level alignment with pathology reports. This yields TITANV (vision\u2011only) and full TITAN (vision\u2013language), which are evaluated across slide classification, biomarker prediction, survival analysis, rare cancer retrieval, cross\u2011modal slide\u2013report retrieval, and zero\u2011shot report generation. TITAN consistently outperforms prior ROI\u2011based and slide\u2011level foundation models across linear probing, few\u2011shot, and zero\u2011shot settings, especially in low\u2011data clinical scenarios. For a new grad student, TITAN provides a clear blueprint for scaling from patch encoders to slide\u2011level multimodal FMs in pathology.</p>"},{"location":"generated/kb_curated/papers-md/titan_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Learn slide\u2011level representations of histopathology WSIs that:  </p> <ul> <li>Capture rich tissue morphology at multiple spatial scales.  </li> <li>Support a wide range of downstream tasks (subtyping, biomarker prediction, prognosis, retrieval, report generation).  </li> <li>Work well even in low\u2011data and rare disease regimes.  </li> </ul> </li> <li> <p>Why this is hard: </p> </li> <li>Gigapixel scale and long context: <ul> <li>WSIs can contain &gt;10\u2074 patch embeddings; na\u00efvely applying transformers is computationally prohibitive.  </li> </ul> </li> <li>Limited labeled cohorts: <ul> <li>Clinical datasets for specific cancers or biomarkers are small and heterogeneous, especially for rare conditions.  </li> </ul> </li> <li>Patch vs slide gap: <ul> <li>Many existing FMs operate on small ROIs; aggregating patch features into clinically meaningful slide\u2011level signals is non\u2011trivial.  </li> </ul> </li> <li>Multimodal supervision: <ul> <li>Pathology reports and textual descriptions encode rich semantics but are noisy and unstructured; exploiting them at scale is challenging.  </li> </ul> </li> <li>Generalization and retrieval: <ul> <li>Models must generalize across organs, stains, scanner types, and institutions, and support tasks like rare cancer retrieval where labeled examples are extremely sparse.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data (Mass\u2011340K): </li> <li>\u2248335,645 WSIs across 20 organ types.  </li> <li>182,862 human pathology reports at slide level.  </li> <li> <p>Diverse stains, tissue types, and scanners to maximize coverage of histopathology morphologies.</p> </li> <li> <p>Synthetic caption data: </p> </li> <li>423,122 synthetic ROI\u2011level captions generated from 8k\u00d78k pixel regions using PathChat, a multimodal pathology copilot.  </li> <li> <p>Each caption describes fine\u2011grained morphology within an ROI (e.g., cell types, tissue organization).</p> </li> <li> <p>Downstream benchmarks (representative): </p> </li> <li>Cancer subtyping and grading across multiple tumor types.  </li> <li>Molecular biomarker prediction (e.g., mutation status, molecular subtypes).  </li> <li>Survival prediction and outcome prognosis.  </li> <li>Rare cancer retrieval: retrieve similar slides for diagnostically challenging WSIs.  </li> <li>Cross\u2011modal retrieval (slide \u2194 report).  </li> <li> <p>Zero\u2011shot and few\u2011shot slide classification guided by textual prompts.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>WSIs are divided into 512\u00d7512 patches at 20\u00d7 magnification, and each patch is encoded into a 768\u2011dimensional feature using a strong patch encoder (CONCH v1.5).  </li> <li>Patch features are arranged into a 2D grid reflecting spatial layout, then cropped into global and local views for SSL.  </li> <li>Pathology reports and synthetic captions are tokenized and embedded for vision\u2013language alignment.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Slide\u2011level Vision Transformer (ViT) foundation model with multimodal vision\u2013language pretraining.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>TITAN is a new whole\u2011slide foundation model, though it builds on existing patch encoders (e.g., CONCH) and SSL techniques (iBOT, CoCa\u2011style alignment).</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Backbone ViT\u2011style transformer operating on patch\u2011feature tokens Input tokens 2D grid of patch embeddings (from CONCH) plus [CLS] / slide tokens Vision\u2011only pretraining iBOT\u2011style masked prediction on WSI feature grids (TITANV) ROI\u2011level alignment Contrastive alignment with synthetic ROI captions (PathChat) Slide\u2011level alignment Contrastive / CoCa\u2011style alignment with pathology reports Positional encoding Long\u2011range encodings (e.g., ALiBi\u2011style) adapted to large 2D grids <ul> <li>Training setup (three stages):</li> <li>Stage 1 \u2013 Vision\u2011only SSL (TITANV): <ul> <li>Perform iBOT pretraining on region crops (16\u00d716 token grids) and their multi\u2011scale views (global 14\u00d714 and local 6\u00d76 crops).  </li> <li>Learns slide\u2011level representations that aggregate patch\u2011level morphologies.  </li> </ul> </li> <li>Stage 2 \u2013 ROI\u2011level vision\u2013language pretraining: <ul> <li>Align 423k ROI crops (8k\u00d78k regions) with synthetic captions from PathChat.  </li> <li>Encourages TITAN to associate specific morphological patterns with textual descriptions.  </li> </ul> </li> <li>Stage 3 \u2013 Slide\u2011level vision\u2013language pretraining: <ul> <li>Align 183k WSIs with their corresponding pathology reports, enabling slide\u2011level semantic understanding and cross\u2011modal retrieval.  </li> </ul> </li> <li>After pretraining, TITAN can be used for linear probing, few\u2011shot fine\u2011tuning, zero\u2011shot classification via text prompts, and report generation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Modalities integrated: </li> <li> <p>Histopathology WSIs and free\u2011text pathology reports, plus synthetic ROI\u2011level captions.</p> </li> <li> <p>How integration works: </p> </li> <li>Vision\u2011only backbone: <ul> <li>TITANV is trained with SSL on slide\u2011level patch features to learn rich visual embeddings.  </li> </ul> </li> <li>ROI\u2011level vision\u2013language alignment: <ul> <li>A contrastive or CoCa\u2011style loss aligns ROI embeddings with synthetic PathChat captions, injecting fine\u2011grained morphological semantics.  </li> </ul> </li> <li> <p>Slide\u2011level vision\u2013language alignment: </p> <ul> <li>Slide embeddings are aligned with pathology report text, enabling cross\u2011modal retrieval and zero\u2011shot, text\u2011prompted classification.  </li> </ul> </li> <li> <p>New capabilities enabled: </p> </li> <li>Zero\u2011shot and few\u2011shot slide classification using textual prompts describing subtypes or biomarkers.  </li> <li>Rare cancer retrieval: find clinically similar slides based on TITAN embeddings, even with minimal labeled examples.  </li> <li>Cross\u2011modal search: slide\u2011to\u2011report and report\u2011to\u2011slide retrieval for case exploration and education.  </li> <li>Report generation: generate slide\u2011level pathology reports conditioned on WSI embeddings and language decoders.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Slide\u2011level cancer subtyping and grading across multiple public and internal cohorts.  </li> <li>Molecular prediction (e.g., mutation and expression surrogates) from WSIs.  </li> <li>Survival prediction and risk stratification.  </li> <li>Rare cancer and challenging\u2011case retrieval.  </li> <li> <p>Cross\u2011modal slide\u2013report retrieval and zero\u2011shot classification using textual prompts.</p> </li> <li> <p>Baselines: </p> </li> <li>ROI\u2011based patch encoders combined with slide aggregators (MIL and attention\u2011pooling models).  </li> <li>Prior slide\u2011level foundation models trained with vision\u2011only SSL or smaller multimodal datasets.  </li> <li> <p>Task\u2011specific supervised models trained on individual cohorts.  </p> </li> <li> <p>Key findings (trends): </p> </li> <li>TITANV (vision\u2011only) already outperforms prior slide\u2011level models and ROI\u2011aggregator baselines on many slide classification and biomarker tasks.  </li> <li>Full TITAN (after multimodal pretraining) further improves performance, particularly in low\u2011data and few\u2011shot settings.  </li> <li>TITAN shows strong rare cancer retrieval performance, retrieving pathologically similar slides that can assist in challenging diagnoses.  </li> <li>Vision\u2013language pretraining with synthetic ROI captions and reports enables zero\u2011shot text\u2011guided classification and cross\u2011modal retrieval that prior slide FMs cannot match.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>First large\u2011scale multimodal whole\u2011slide foundation model that unifies vision\u2011only SSL and vision\u2013language alignment across ROI and slide levels.  </li> <li>Demonstrates that building on strong patch encoders and scaling to slide level yields state\u2011of\u2011the\u2011art performance across many pathology tasks.  </li> <li>Synthetic ROI captions from PathChat provide a practical way to incorporate fine\u2011grained morphological supervision at scale.  </li> <li>Extensive evaluations across tasks, organs, and settings (linear probing, few\u2011shot, zero\u2011shot) show TITAN\u2019s breadth and robustness.</li> </ul> <p>Limitations:</p> <ul> <li>Relies on a large, internal Mass\u2011340K dataset and synthetic captions that are not fully public, limiting reproducibility.  </li> <li>Synthetic captions, while powerful, may encode biases and failure modes of the PathChat generator.  </li> <li>Focuses primarily on histopathology WSIs; other modalities (radiology, multi\u2011omics, clinical text beyond reports) are not integrated.  </li> <li>Training at this scale requires substantial compute and storage, making it difficult for smaller groups to replicate or extend.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can TITAN\u2011style slide\u2011level FMs be extended to multimodal clinical contexts, integrating WSIs with genomics, radiology, and EHR data?  </li> <li>What are robust methods for validating and correcting synthetic captions, ensuring that vision\u2013language supervision does not propagate hallucinations into the slide encoder?  </li> <li>Can more efficient architectures (e.g., sparse attention, hierarchical transformers) reduce the cost of handling giga\u2011pixel WSIs without losing performance?  </li> <li>How should we design evaluation protocols and human\u2011in\u2011the\u2011loop workflows for rare cancer retrieval, where errors may have significant diagnostic consequences?  </li> <li>Could TITAN representations support interactive, region\u2011grounded explanations that show which slide regions drive predictions or retrieved cases?</li> </ol>"},{"location":"generated/kb_curated/papers-md/titan_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>TITAN is to pathology WSIs what CLIP\u2011like and CoCa\u2011style models are to natural images and text: a general\u2011purpose slide representation that supports many downstream tasks and multimodal interactions.  </li> <li>It extends the trend of patch\u2011level pathology FMs to the slide level, helping bridge the gap between detailed morphology and patient\u2011level clinical endpoints.  </li> <li>Relation to well-known ideas: </li> <li>Combines iBOT\u2011style masked image modeling, patch\u2011encoder distillation, and vision\u2013language contrastive pretraining in a three\u2011stage pipeline.  </li> <li>Conceptually similar to GigaPath and other large\u2011scale pathology FMs but emphasizes multimodal, slide\u2011level pretraining and rare cancer retrieval.  </li> <li>Why this paper is a useful reference: </li> <li>Provides a detailed design for scaling from patch encoders to slide\u2011level transformers and for incorporating synthetic and real textual supervision.  </li> <li>For a grad student, TITAN is an archetype for building high\u2011capacity medical vision FMs and integrating them into multimodal medical AI systems.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Existing pathology FMs mostly work at the patch level and lack robust, multimodal slide\u2011level representations, limiting performance in patient\u2011level prediction and rare disease scenarios.  </p> </li> <li> <p>Method / model: </p> </li> <li>TITAN is a ViT\u2011based slide foundation model trained in three stages: large\u2011scale vision\u2011only SSL on WSI patch features, ROI\u2011level alignment with synthetic PathChat captions, and slide\u2011level alignment with pathology reports.  </li> <li> <p>Operates entirely in the patch\u2011embedding space, with long\u2011range positional encodings and multi\u2011scale cropping to handle giga\u2011pixel WSIs.  </p> </li> <li> <p>Results: </p> </li> <li>Outperforms ROI\u2011based and prior slide\u2011level FMs on cancer subtyping, biomarker prediction, prognosis, and retrieval, especially in low\u2011data and few\u2011shot regimes.  </li> <li> <p>Enables zero\u2011shot text\u2011guided classification, cross\u2011modal slide\u2013report retrieval, and pathology report generation.  </p> </li> <li> <p>Why it matters: </p> </li> <li>Establishes a strong template for multimodal slide\u2011level foundation models, bringing pathology closer to the capabilities seen in natural\u2011image FMs and VLMs.  </li> <li>Opens pathways for rare disease support, better education and retrieval tools, and future integration with other medical modalities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/","title":"Yoon BioKDD (2025)","text":""},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#classifying-major-depressive-disorder-with-exon-sequence-embeddings-from-dna-foundation-models","title":"Classifying Major Depressive Disorder with Exon Sequence Embeddings from DNA Foundation Models","text":"<p>Authors: Heesun Yoon, Eunji Lee, Heehwan Wang, Jiook Cha, Xin Dai, Shinjae Yoo, Yoonjung Yoonie Joo Year: 2025 Venue: BIOKDD '25 (24th International Workshop on Data Mining in Bioinformatics)</p>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM + Application. This paper applies pretrained DNA foundation models (Caduceus, DNABERT-2) to psychiatric genomics, specifically using exon sequence embeddings to classify Major Depressive Disorder (MDD).</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Application of existing FMs. The study leverages two pretrained genomic foundation models\u2014Caduceus (Mamba-based) and DNABERT-2 (Transformer-based)\u2014to generate DNA sequence embeddings, then trains classical ML models on top of these embeddings for MDD classification.</p> </li> <li> <p>Key Modalities: </p> </li> <li>DNA sequences: Whole exome sequencing (WES) data from 38 MDD-associated genes</li> <li>Demographic/clinical features: Age, sex, household income, genetic principal components</li> <li>Derived representations: High-dimensional exon sequence embeddings from foundation models</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>This study introduces a novel framework for classifying Major Depressive Disorder (MDD) using raw DNA sequence embeddings extracted from the exonic regions of 38 MDD-associated genes. Rather than relying on traditional polygenic risk scores (PRS) that aggregate single nucleotide polymorphism (SNP) effects and typically explain only 2-3% of MDD variance, the authors leverage pretrained genomic foundation models\u2014Caduceus (based on the Mamba architecture) and DNABERT-2 (Transformer-based)\u2014to generate context-rich, high-dimensional representations of exon sequences from whole exome sequencing (WES) data. Using UK Biobank data with 10,307 MDD cases and 10,307 healthy controls, they systematically evaluate 15 machine learning pipelines combining three embedding aggregation strategies (PCA, max pooling, mean pooling) with five classifiers (logistic regression, random forest, CatBoost, MLP, 1D-CNN). The best-performing pipeline\u2014CatBoost with max pooling\u2014achieves a mean AUC of 0.851 and AUPRC of 0.812, representing a ~49-60% improvement over traditional PRS approaches. A leave-one-gene-out (LOGO) analysis reveals that the SOD2 gene contributes most significantly to classification performance despite lacking established GWAS hits in exonic regions, highlighting the model's ability to capture biological signals beyond SNP-level associations. Replication with DNABERT-2 embeddings yields identical top performance (AUC 0.851), confirming the framework's generalizability across different foundation model architectures. This work demonstrates that pretrained sequence models can effectively encode complex genomic context for psychiatric risk prediction, opening new avenues for applying foundation models to mental health genomics.</p>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<p>Scientific / practical problem:</p> <ul> <li>Predicting MDD risk from genetic data:</li> <li>Major Depressive Disorder affects ~300 million people worldwide with substantial economic burden (~$6 trillion projected by 2030)</li> <li> <p>Individual risk prediction is challenging due to extreme polygenicity and high phenotypic heterogeneity</p> </li> <li> <p>Limitations of existing genetic approaches:</p> </li> <li>Polygenic Risk Scores (PRS): Aggregate SNP-level GWAS effects but explain only ~2-3% of MDD phenotypic variance (AUC ~0.53-0.57)</li> <li>Variant-level focus: Cannot capture broader sequence-level context, regulatory motifs, structural variants, or long-range dependencies</li> <li> <p>Exonic regions understudied: GWAS emphasizes non-coding regions, but functionally important protein-coding sequences may harbor disease-relevant information missed by SNP-based methods</p> </li> <li> <p>Opportunity with DNA foundation models:</p> </li> <li>Models like DNABERT, DNABERT-2, Caduceus, and Nucleotide Transformer pretrained on large genomic corpora can generate embeddings that capture sophisticated context beyond single variants</li> <li>These models have shown success in regulatory element classification and gene expression prediction but remain unexplored in psychiatric genomics</li> </ul> <p>Why this is hard:</p> <ul> <li>Polygenicity and small effects:</li> <li> <p>MDD involves thousands of loci with tiny individual contributions; signal-to-noise ratio is low</p> </li> <li> <p>Data modality mismatch:</p> </li> <li> <p>Whole exome sequencing (WES) captures exons cost-effectively but misses intronic/intergenic regulatory elements where many GWAS hits lie</p> </li> <li> <p>Embedding aggregation:</p> </li> <li> <p>38 genes produce 38 separate high-dimensional embeddings; need effective strategies to combine them without losing gene-specific signals or introducing noise</p> </li> <li> <p>Demographic confounding:</p> </li> <li> <p>MDD cases differ from controls in age, sex, and socioeconomic status; must control these covariates</p> </li> <li> <p>Model selection and overfitting:</p> </li> <li> <p>Many hyperparameters and modeling choices; need rigorous nested cross-validation to avoid overoptimistic results</p> </li> <li> <p>Interpretability:</p> </li> <li>Which genes or exons drive predictions? Black-box embeddings require LOGO-style analyses to identify key contributors</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>Dataset:</p> <ul> <li>UK Biobank:</li> <li>10,307 MDD cases and 10,307 healthy controls (downsampled for balance)</li> <li>Whole Exome Sequencing (WES) data (Exome OQFE variant call files, interim 200k release)</li> <li> <p>Demographic and clinical features: age at recruitment, sex, household income, top 10 genetic principal components</p> </li> <li> <p>MDD case definition:</p> </li> <li>Participants meeting at least one of three depression criteria (Howard et al. 2018):<ul> <li>Broad depression</li> <li>Probable MDD</li> <li>ICD-coded MDD</li> </ul> </li> <li> <p>Cases: Single/recurrent probable major depression episodes</p> </li> <li> <p>Healthy control definition:</p> </li> <li>No bipolar disorder or major depression</li> <li>No history of psychiatric care on admission</li> <li>Never sought professional help for mental distress</li> </ul> <p>Modalities:</p> <ul> <li>Genomic sequences:</li> <li>FASTA sequences of exonic regions from 38 autosomal MDD-associated genes (genes reported \u22652 times in prior literature)</li> <li>Genome Reference Consortium Human Build 38 (GRCh38)</li> <li> <p>Individual-level sequences with reference alleles replaced by alternate alleles where applicable</p> </li> <li> <p>Demographics:</p> </li> <li>Age (mean: cases 55.4\u00b18.0 years, controls 57.1\u00b17.8 years; significant difference p&lt;0.001)</li> <li>Sex (cases 64.1% female, controls 46.9% female; p&lt;0.001)</li> <li>Household income (controls more likely in higher income brackets; p&lt;0.001)</li> <li>Genetic PCs (top 10 PCs to control population structure)</li> </ul> <p>Preprocessing / representation:</p> <ul> <li>Sequence embedding generation:</li> <li>Primary: Caduceus-PS (256 hidden dimensions, 16 MambaDNA layers)<ul> <li>Bi-directional Mamba with reverse-complement (RC) equivariance</li> <li>Hidden states averaged across forward and RC representations to obtain RC-invariant embeddings (512\u2192256 dimensions)</li> </ul> </li> <li>Replication: DNABERT-2 (Transformer-based genomic foundation model)</li> <li> <p>Each of 38 genes produces one 256-dimensional embedding per sample</p> </li> <li> <p>Embedding aggregation (three strategies tested):</p> </li> <li>PCA: Feature scaling + PCA to 256 principal components</li> <li>Max pooling: Element-wise maximum across all 38 gene embeddings</li> <li> <p>Mean pooling: Element-wise mean across all 38 gene embeddings</p> </li> <li> <p>Final input:</p> </li> <li>Aggregated gene embeddings (256 dimensions) concatenated with 14 demographic features (age, sex, income, 10 genetic PCs)</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>Foundation Models Used:</p> Model Architecture Key Features Purpose Caduceus-PS Mamba (structured state space model) Bi-directional, RC-equivariant, linear complexity, long context (~131k) Primary embedding generator DNABERT-2 Transformer (BERT-like) Bidirectional attention, multi-species genome pretraining Replication/validation of framework generalizability <p>Caduceus Details:</p> <ul> <li>Architecture: BiMamba + MambaDNA layers with parameter sharing for RC equivariance</li> <li>Advantages:</li> <li>Linear computational complexity (vs quadratic for Transformers) \u2192 handles long sequences efficiently</li> <li>Bi-directionality captures upstream and downstream context</li> <li>RC equivariance ensures equivalent representations for forward and reverse-complement strands</li> <li>Configuration: 256 hidden dimensions, 16 layers, ~131k sequence length capacity</li> <li>Output: Per-sequence hidden states split in half and averaged to yield RC-invariant 256-d embeddings</li> </ul> <p>Downstream Classification Models (15 pipelines):</p> <p>Five classifiers \u00d7 three aggregation strategies:</p> <ol> <li>Logistic Regression (LR): Linear baseline</li> <li>Random Forest (RF): Ensemble of decision trees</li> <li>CatBoost (CB): Gradient boosting with categorical feature support</li> <li>Multilayer Perceptron (MLP): Feedforward neural network</li> <li>1D Convolutional Neural Network (1D-CNN): Convolutional layers for pattern detection</li> </ol> <p>Training Setup:</p> <ul> <li>Nested cross-validation:</li> <li>Outer loop: 10-fold stratified CV for unbiased model assessment</li> <li> <p>Inner loop: 3-fold stratified CV for hyperparameter tuning</p> </li> <li> <p>Hyperparameter optimization:</p> </li> <li>Framework: Optuna with Bayesian optimization</li> <li>Pruning strategies: HyperbandPruner for neural nets, MedianPruner for others</li> <li> <p>Trials: 150 for MLP, 100 for 1D-CNN, 300 for LR/RF/CB</p> </li> <li> <p>Early stopping:</p> </li> <li>CatBoost: 10 consecutive rounds without AUC improvement</li> <li>MLP/1D-CNN: 200 epochs with binary cross-entropy loss monitoring</li> <li> <p>10% holdout set for validation</p> </li> <li> <p>Evaluation metrics:</p> </li> <li>Primary: AUC (threshold-independent, consistent)</li> <li>Secondary: AUPRC (for tied AUC values), precision, recall, F1-score</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This work is primarily unimodal (genomics) at the embedding level but incorporates late fusion of genetic embeddings with demographic/clinical features.</p> <p>Modalities integrated:</p> <ul> <li>Genomic: DNA sequence embeddings from foundation models (captures exonic sequence context)</li> <li>Demographic/clinical: Age, sex, household income, genetic PCs</li> </ul> <p>How they are integrated:</p> <ul> <li>Late fusion via feature concatenation:</li> <li>Gene embeddings aggregated into a 256-d vector</li> <li>Concatenated with 14 demographic features</li> <li>Combined feature vector fed to downstream classifiers</li> <li>This is analogous to late fusion in the integration baseline plan: modality-specific representations (gene sequences \u2192 embeddings, demographics as raw features) are combined at the feature level before final prediction</li> </ul> <p>Why this integration is useful:</p> <ul> <li>Demographics as confounders: Age, sex, and income differ significantly between MDD cases and controls; including them improves model accuracy and reduces spurious associations</li> <li>Genetic PCs control population structure: Prevent inflation due to ancestry-related confounding</li> <li>Complementary information: Genetic embeddings capture biological predisposition; demographics provide environmental/social context</li> </ul> <p>Relation to the integration baseline plan:</p> <ul> <li>Late fusion approach:</li> <li>Aligns with the plan's preference for preserving modality-specific signals before integration</li> <li> <p>Genetic embeddings are computed independently (via pretrained FMs), then concatenated with demographics</p> </li> <li> <p>Robustness and evaluation:</p> </li> <li>Nested CV with proper train/test splits mirrors the plan's emphasis on evaluation discipline</li> <li> <p>Leave-one-gene-out analysis with Wilcoxon signed-rank test + FDR correction aligns with plan's recommendation for attribution via LOGO and statistical rigor</p> </li> <li> <p>Genetics embedding hygiene:</p> </li> <li>Use of RC-equivariant Caduceus and RC-averaging reflects the plan's citation of Caduceus for proper genetics handling</li> <li> <p>Deterministic embedding generation ensures reproducibility</p> </li> <li> <p>Future escalation:</p> </li> <li>Current work uses simple concatenation; could extend to two-tower contrastive learning (genetic encoder vs demographic encoder) or attention-based fusion if performance plateaus</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>Tasks / benchmarks:</p> <ul> <li>Binary classification: MDD cases (n=10,307) vs healthy controls (n=10,307) in UK Biobank</li> <li>Primary evaluation: 15 modeling pipelines (5 classifiers \u00d7 3 aggregation strategies)</li> <li>Replication: Same 15 pipelines with DNABERT-2 embeddings</li> <li>Interpretability: Leave-one-gene-out (LOGO) analysis to rank gene contributions</li> </ul> <p>Baselines:</p> <ul> <li>Traditional PRS: Historical MDD PRS models explain ~2-3% variance, AUC ~0.53-0.57</li> <li>Within-study baselines: All 15 pipeline combinations serve as internal comparisons</li> </ul> <p>Key findings:</p> <p>1. Primary Caduceus Results (Table 2):</p> <ul> <li>Best performance: CatBoost + max pooling</li> <li>Mean AUC: 0.851 \u00b1 0.009</li> <li> <p>Mean AUPRC: 0.812 \u00b1 0.013</p> </li> <li> <p>Tied second-best: Logistic Regression + max pooling</p> </li> <li>Mean AUC: 0.851 \u00b1 0.010</li> <li> <p>Mean AUPRC: 0.809 \u00b1 0.015 (slightly lower)</p> </li> <li> <p>Third: 1D-CNN + max pooling</p> </li> <li> <p>Mean AUC: 0.849 \u00b1 0.010</p> </li> <li> <p>Aggregation strategy impact:</p> </li> <li>Max pooling: Consistently best across all classifiers</li> <li>PCA: Intermediate performance</li> <li> <p>Mean pooling: Worst performance across all classifiers (e.g., 1D-CNN dropped to AUC 0.638 \u00b1 0.010)</p> </li> <li> <p>Performance gain over PRS:</p> </li> <li>~49-60% improvement in AUC relative to traditional PRS (0.851 vs 0.53-0.57)</li> </ul> <p>2. DNABERT-2 Replication:</p> <ul> <li>Best performance: CatBoost + max pooling</li> <li>Mean AUC: 0.851 (identical to Caduceus)</li> <li> <p>Confirms framework generalizability across different FM architectures (Mamba vs Transformer)</p> </li> <li> <p>Aggregation differences:</p> </li> <li>Mean pooling performed comparably to max pooling with DNABERT-2 (unlike Caduceus)</li> <li>Suggests optimal aggregation may be FM-specific</li> </ul> <p>3. LOGO Analysis (Figure 3):</p> <ul> <li>SOD2 gene:</li> <li>Highest contribution: Median \u0394AUC = 0.190 (IQR: 0.185-0.196)</li> <li>Only statistically significant gene after FDR correction (Wilcoxon W=0, p_FDR=0.038)</li> <li> <p>Removing SOD2 drastically reduces classification performance despite no established GWAS hits in its exonic regions</p> </li> <li> <p>SOD2 biological relevance:</p> </li> <li>Encodes superoxide dismutase 2 (mitochondrial enzyme)</li> <li>Involved in oxidative stress regulation</li> <li> <p>Oxidative stress implicated in MDD pathophysiology</p> </li> <li> <p>Demographics ablation:</p> </li> <li>Removing demographic features: \u0394AUC = 0.040 (0.037-0.045), significant (p_FDR=0.038)</li> <li>Confirms demographics contribute meaningfully but less than top genes</li> </ul> <p>4. Cohort Demographics:</p> <ul> <li>Age: Cases younger than controls (55.4 vs 57.1 years, p&lt;0.001)</li> <li>Sex: Cases more female (64.1% vs 46.9%, p&lt;0.001)</li> <li>Income: Controls more likely in higher income brackets (p&lt;0.001)</li> <li>All differences significant \u2192 justifies including as covariates</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Substantial performance improvement: 49-60% AUC gain over traditional PRS demonstrates value of sequence-level context</li> <li>Novel application domain: First study to apply DNA foundation model embeddings to psychiatric disorder classification</li> <li>Rigorous methodology:</li> <li>Nested cross-validation prevents overfitting</li> <li>Multiple FM architectures tested (Caduceus + DNABERT-2) \u2192 validates generalizability</li> <li>Statistical testing (Wilcoxon + FDR correction) for LOGO analysis</li> <li>Biological interpretability: LOGO analysis identifies SOD2 as key gene, linking to known oxidative stress mechanisms in MDD</li> <li>Foundation model comparison: Shows Mamba and Transformer architectures yield similar top performance, suggesting robustness</li> <li>Embedding aggregation insights: Max pooling consistently superior for Caduceus; strategy choice matters</li> </ul> <p>Limitations:</p> <ul> <li>Exon-only analysis:</li> <li>Focus on protein-coding regions despite most GWAS hits lying in non-coding regulatory elements</li> <li> <p>Intronic and intergenic regions (much larger proportion of genome) not included</p> </li> <li> <p>Gene-level interpretation only:</p> </li> <li>LOGO analysis identifies contributing genes but not specific exons or base-pair regions</li> <li> <p>Finer-grained attribution (e.g., attention maps, saliency) not explored</p> </li> <li> <p>Single-ancestry cohort:</p> </li> <li>UK Biobank participants primarily of European ancestry</li> <li> <p>Transferability to other populations uncertain</p> </li> <li> <p>No external validation:</p> </li> <li>Framework not tested on independent cohorts outside UK Biobank</li> <li> <p>Generalization across datasets unknown</p> </li> <li> <p>Demographic differences:</p> </li> <li> <p>Cases and controls differ significantly in age, sex, income \u2192 potential residual confounding even after covariate adjustment</p> </li> <li> <p>Limited gene set:</p> </li> <li>Only 38 pre-selected MDD-associated genes; genome-wide exome analysis could yield further insights</li> </ul> <p>Open Questions and Future Directions:</p> <ul> <li>Expand genomic coverage:</li> <li>Include intronic and intergenic regions to capture regulatory elements where many GWAS hits reside</li> <li> <p>Test framework on whole-genome sequencing (WGS) data</p> </li> <li> <p>Multimodal integration:</p> </li> <li>Combine DNA embeddings with transcriptomics (RNA-seq), epigenomics (methylation), proteomics</li> <li> <p>Integrate with brain imaging (fMRI, sMRI) for brain-genomics multimodal models</p> </li> <li> <p>Fine-grained interpretability:</p> </li> <li>Exon-level or base-pair-level attribution via attention weights, integrated gradients, or saliency maps</li> <li> <p>Link specific sequence motifs to MDD risk</p> </li> <li> <p>External validation:</p> </li> <li>Test on independent cohorts (e.g., other biobanks, clinical samples)</li> <li> <p>Evaluate cross-ancestry performance and fairness</p> </li> <li> <p>Generative and design applications:</p> </li> <li>Can FMs be used to design therapeutic sequences or identify novel targets?</li> <li> <p>Generate synthetic exon variants and predict MDD risk changes</p> </li> <li> <p>Integration with PRS:</p> </li> <li>Combine exon embeddings with genome-wide PRS to capture both local and global genetic risk</li> <li>Test whether the two signals are complementary or redundant</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<p>Position in the genomics FM landscape:</p> <ul> <li>This work sits at the intersection of genomic foundation models and psychiatric genomics</li> <li>Genomics FMs (DNABERT, Caduceus, Nucleotide Transformer, Evo) have primarily been applied to:</li> <li>Regulatory element classification (promoters, enhancers)</li> <li>Variant effect prediction (pathogenicity)</li> <li>Gene expression forecasting</li> <li>Psychiatric genomics has relied on:</li> <li>GWAS to identify risk loci</li> <li>PRS for individual-level risk aggregation</li> <li>Traditional ML (SVM, random forests) with handcrafted genomic features</li> </ul> <p>Relation to well-known ideas:</p> <ul> <li>Analogy: \"Like using BERT embeddings for text classification, but for DNA sequences predicting psychiatric risk\"</li> <li>Transfer learning paradigm: Pretrain large models on diverse genomic data, then fine-tune (or use embeddings) for specific downstream tasks</li> <li>Contrastive to PRS: PRS are linear sums of SNP effects (shallow, interpretable); FM embeddings are high-dimensional, context-rich representations (deep, powerful but less transparent)</li> </ul> <p>Connection to the integration baseline plan:</p> <ul> <li>Genetics embedding hygiene:</li> <li>Paper explicitly cites Caduceus for RC-equivariance, aligning with the plan's recommendation for proper DNA handling</li> <li> <p>RC-averaging and deterministic tokenization ensure stable embeddings</p> </li> <li> <p>Late fusion baseline:</p> </li> <li> <p>Concatenating gene embeddings with demographics mirrors the plan's preference for late fusion under heterogeneous semantics</p> </li> <li> <p>Evaluation rigor:</p> </li> <li> <p>Nested CV, LOGO with Wilcoxon + FDR, proper train/test splits align with plan's robustness discipline</p> </li> <li> <p>Future escalation:</p> </li> <li>If exon embeddings prove valuable, next steps could include:<ul> <li>Two-tower contrastive learning (genetic encoder vs demographic/clinical encoder)</li> <li>Attention-based fusion across genes (hub tokens \u00e0 la TAPE)</li> <li>Integration with brain FMs (BrainLM, Brain-JEPA) for brain-genomics multimodal models</li> </ul> </li> </ul> <p>Why this paper is a useful reference:</p> <ul> <li>First application: Demonstrates feasibility and value of applying DNA FMs to psychiatric disorders</li> <li>Methodological template: Provides reusable pipeline for embedding extraction \u2192 aggregation \u2192 ML classification</li> <li>Performance benchmark: Establishes AUC 0.851 as a target for future MDD genetic prediction studies</li> <li>Gene discovery: LOGO analysis identifies SOD2 despite absence of exonic GWAS hits, suggesting FM embeddings capture signals missed by SNP-level approaches</li> <li>Foundation for multimodal work: Genetic embeddings from this study could serve as one modality in larger brain-genomics integration projects</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<p>Problem:</p> <ul> <li>Traditional polygenic risk scores (PRS) for MDD explain only 2-3% of variance (AUC ~0.53-0.57), limiting predictive utility</li> <li>PRS rely on SNP-level GWAS associations and miss broader sequence context, regulatory motifs, and long-range dependencies</li> <li>DNA foundation models (Caduceus, DNABERT-2) trained on large genomic corpora remain unexplored in psychiatric genomics</li> </ul> <p>Method / Model:</p> <ul> <li>Framework: Extract exon sequence embeddings from 38 MDD-associated genes using pretrained DNA foundation models</li> <li>Primary FM: Caduceus-PS (Mamba-based, bi-directional, RC-equivariant, 256-d embeddings per gene)</li> <li>Aggregation: Test PCA, max pooling, and mean pooling to combine 38 gene embeddings</li> <li>Downstream: Train 15 ML pipelines (5 classifiers \u00d7 3 aggregation strategies) with nested cross-validation</li> <li>Data: UK Biobank WES data, 10,307 MDD cases + 10,307 controls, adjusted for demographics and genetic PCs</li> <li>Replication: Validate framework generalizability with DNABERT-2 embeddings</li> </ul> <p>Results:</p> <ul> <li>Best performance: CatBoost + max pooling achieves AUC 0.851 \u00b1 0.009, AUPRC 0.812 \u00b1 0.013</li> <li>Improvement over PRS: ~49-60% AUC gain (0.851 vs 0.53-0.57)</li> <li>Replication: DNABERT-2 achieves identical top AUC (0.851), confirming robustness across FM architectures</li> <li>Key gene: LOGO analysis identifies SOD2 as most significant contributor (\u0394AUC 0.190), linking to oxidative stress pathways</li> <li>Aggregation matters: Max pooling consistently best for Caduceus; mean pooling severely degrades performance</li> </ul> <p>Why it matters:</p> <ul> <li>First psychiatric genomics application: Demonstrates DNA foundation models can effectively predict complex psychiatric disorders</li> <li>Paradigm shift: Moves beyond SNP-level associations to sequence-level context, capturing signals missed by GWAS</li> <li>Substantial performance gain: Large improvement over traditional PRS suggests promise for clinical risk prediction</li> <li>Methodological contribution: Provides reusable framework for applying genomic FMs to any polygenic disorder</li> <li>Foundation for integration: Exon embeddings could be combined with brain imaging, transcriptomics, or epigenomics in future multimodal models</li> </ul>"},{"location":"generated/templates/analysis_recipe_cca/","title":"Analysis Recipe: CCA + Permutation","text":""},{"location":"generated/templates/analysis_recipe_cca/#goal","title":"Goal","text":"<p>Quantify cross-modal associations (e.g., gene embeddings vs sMRI features) while controlling confounds and validating significance via permutation testing.</p>"},{"location":"generated/templates/analysis_recipe_cca/#inputs","title":"Inputs","text":"<ul> <li>Residualized &amp; z-scored feature matrices <code>X</code> (modalities A) and <code>Y</code> (modalities B) per fold.</li> <li>Covariate design matrices (age, sex, site/scanner, motion FD, SES, genetic PCs).</li> <li>Train/test splits (identical across modalities).</li> </ul>"},{"location":"generated/templates/analysis_recipe_cca/#preprocessing-checklist","title":"Preprocessing Checklist","text":"<ol> <li>Fit scaler + residualization models on train split only; apply to both train/test within the fold.</li> <li>Optional PCA/MLP projector to 512-D per modality; store fit parameters.</li> <li>Log confound regression coefficients for reproducibility.</li> </ol>"},{"location":"generated/templates/analysis_recipe_cca/#procedure","title":"Procedure","text":"<ol> <li>Fit CCA on train data: <code>cca = CCA(n_components=k, scale=False)</code> with shrinkage/regularization if needed.</li> <li>Transform: Obtain canonical variates for both train and test sets.</li> <li>Record metrics: Canonical correlations (\u03c1\u2081\u2026\u03c1_k), variance explained, loadings.</li> <li>Permutation test: Shuffle subject order in modality B, refit CCA <code>B</code> times (\u22651,000); build null distribution for each \u03c1.</li> <li>p-values: <code>p = (count(\u03c1_null \u2265 \u03c1_obs) + 1) / (B + 1)</code>.</li> <li>Confidence intervals (optional): Bootstrap subjects within folds.</li> <li>Partial correlations to outcomes: Regress canonical scores and clinical targets on covariates; correlate residuals or use covariate-adjusted regression.</li> </ol>"},{"location":"generated/templates/analysis_recipe_cca/#logging","title":"Logging","text":"<ul> <li>Save canonical correlations, permutation distributions, p-values, and loadings to <code>artifacts/generated/cca/&lt;experiment_id&gt;/</code>.</li> <li>Store config (modalities, projectors, covariates, seeds) alongside results.</li> </ul>"},{"location":"generated/templates/analysis_recipe_cca/#reporting-template","title":"Reporting Template","text":"<ul> <li>Table of top 3 \u03c1 with permutation p-values and 95% CI.</li> <li>Heatmap or bar chart of feature loadings (with sparse thresholding if needed).</li> <li>Partial correlation table linking canonical scores to clinical outcomes (effect size, p, FDR q).</li> </ul>"},{"location":"generated/templates/analysis_recipe_cca/#references","title":"References","text":"<ul> <li>EI &amp; oncology multimodal review for integration motivation.</li> <li>Classical CCA references and permutation testing guidelines.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/","title":"Analysis Recipe: Prediction Baselines (LR + GBDT)","text":""},{"location":"generated/templates/analysis_recipe_prediction/#goal","title":"Goal","text":"<p>Estimate classification performance for each modality (genes, sMRI, fMRI) and their late-fusion concatenation using strong, interpretable baselines with rigorous evaluation.</p>"},{"location":"generated/templates/analysis_recipe_prediction/#inputs","title":"Inputs","text":"<ul> <li>Fold-specific train/test splits shared across modalities.</li> <li>Residualized, z-scored, 512-D embeddings per modality (plus covariates for logging).</li> <li>Binary or multi-class clinical targets (e.g., MDD, eoMDD vs loMDD).</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#models","title":"Models","text":""},{"location":"generated/templates/analysis_recipe_prediction/#logistic-regression","title":"Logistic Regression","text":"<ul> <li>Solver: <code>saga</code> or <code>lbfgs</code>.</li> <li>Penalty: L2; tune <code>C</code> in <code>{0.01, 0.1, 1, 10}</code>.</li> <li><code>class_weight='balanced'</code>.</li> <li>Max iter \u2265 5,000; tolerance <code>1e-4</code>.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#lightgbm","title":"LightGBM","text":"<ul> <li><code>num_leaves=31</code>, <code>max_depth=-1</code>, <code>learning_rate=0.05</code>.</li> <li><code>n_estimators=1000</code> with early stopping (patience 50) on validation splits.</li> <li><code>feature_fraction=0.9</code>, <code>bagging_fraction=0.8</code>, <code>bagging_freq=5</code>.</li> <li><code>scale_pos_weight = N_neg / N_pos</code> if not using class weights.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#catboost-optional","title":"CatBoost (optional)","text":"<ul> <li><code>depth=6-8</code>, <code>learning_rate=0.05</code>, <code>iterations=2000</code>, <code>l2_leaf_reg=3</code>.</li> <li><code>loss_function='Logloss'</code>, <code>eval_metric='AUC'</code>, <code>auto_class_weights='Balanced'</code>.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#procedure","title":"Procedure","text":"<ol> <li>Fit models on train split per modality; tune hyperparameters via inner CV or validation fold.</li> <li>Evaluate on held-out fold; store probabilities and logits.</li> <li>Repeat for concatenated late-fusion features (stacked 512-D per modality).</li> <li>Record metrics: AUROC, AUPRC, accuracy, calibration (Brier/ECE optional).</li> <li>Compare modalities: run DeLong test (or bootstrap) on AUROC differences; report mean \u00b1 SD across folds.</li> <li>For LOGO analyses, compute \u0394AUC per fold and apply Wilcoxon + FDR.</li> </ol>"},{"location":"generated/templates/analysis_recipe_prediction/#logging","title":"Logging","text":"<ul> <li>Persist per-fold predictions in <code>artifacts/generated/predictions/&lt;experiment_id&gt;/</code>.</li> <li>Store trained hyperparameters, random seeds, and config YAML snapshot.</li> <li>Capture ROC/PR curves and metric tables; include class prevalence.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#reporting-template","title":"Reporting Template","text":"<ul> <li>Table with AUROC/AUPRC (mean \u00b1 SD) for Gene, Brain, Fusion.</li> <li>DeLong/Bootstrap \u0394AUROC with 95% CI and p-value.</li> <li>Calibration plot or summary if clinically relevant.</li> <li>Notes on covariates, residualization, and projector settings.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#references","title":"References","text":"<ul> <li>Ensemble Integration paper (late fusion motivation).</li> <li>Oncology multimodal review (evaluation discipline).</li> </ul>"},{"location":"generated/templates/dataset_card/","title":"Dataset card","text":"<p>md. I\u2019ll also keep carrying the specific repository + license notice on every walkthrough page so each doc makes its licensing context explicit.</p>"},{"location":"generated/templates/dataset_card/#dataset-card-template","title":"Dataset Card Template","text":""},{"location":"generated/templates/dataset_card/#metadata","title":"Metadata","text":"<ul> <li>Dataset name: <code>&lt;UKB/ABCD/etc&gt;</code></li> <li>Source / accession: <code>&lt;URL or data agreement&gt;</code></li> <li>License / DUA: <code>&lt;e.g., UKB Research Analysis Tool terms&gt;</code></li> <li>Primary contact: <code>&lt;PI / analyst&gt;</code></li> <li>Related YAML: <code>kb/datasets/&lt;id&gt;.yaml</code></li> </ul>"},{"location":"generated/templates/dataset_card/#access-storage","title":"Access &amp; Storage","text":"<ul> <li>Paths to raw data (e.g., <code>/secure/ukb/raw</code>), processed derivatives, and sync locations.</li> <li>Encryption / PHI considerations; who has access.</li> </ul>"},{"location":"generated/templates/dataset_card/#hugging-face-shared-mirrors","title":"Hugging Face / Shared Mirrors","text":"<ul> <li>Dataset repo + subset names, record counts, checksum or snapshot tags.</li> <li>Notes on sync cadence between mirrors and secured copies.</li> </ul>"},{"location":"generated/templates/dataset_card/#cohort-summary","title":"Cohort Summary","text":"Split N subjects Notes Train <code>&lt;N&gt;</code> <code>&lt;filters&gt;</code> Val <code>&lt;N&gt;</code> Test <code>&lt;N&gt;</code> <p>Add stratification notes (sex balance, age range, sites).</p>"},{"location":"generated/templates/dataset_card/#modality-specific-schema","title":"Modality-Specific Schema","text":"<ul> <li>Column lists per modality (genes, ROIs, clinical tables) with links back to <code>docs/data/schemas.md</code>.</li> <li>Required covariates + units.</li> </ul>"},{"location":"generated/templates/dataset_card/#overlap-pairing-logic","title":"Overlap &amp; Pairing Logic","text":"<ul> <li>How subjects overlap with other modalities (e.g., gene \u00d7 sMRI intersection).</li> <li>Inclusion/exclusion rules, QC flags (motion, missing genes, etc.).</li> </ul>"},{"location":"generated/templates/dataset_card/#genomics-base-pair-stats","title":"Genomics / Base-Pair Stats","text":"<ul> <li>Total base pairs, mean/median sequence length, tokenizer context alignment details.</li> </ul>"},{"location":"generated/templates/dataset_card/#preprocessing-pipelines","title":"Preprocessing Pipelines","text":"<ul> <li>Imaging: software versions, atlases, smoothing, censoring thresholds.</li> <li>Genetics: variant calling, phasing, annotation, gene list definition.</li> <li>Clinical: diagnosis coding, questionnaire scoring.</li> </ul>"},{"location":"generated/templates/dataset_card/#available-features-covariates","title":"Available Features &amp; Covariates","text":"<ul> <li>Feature tables (e.g., FreeSurfer ROIs, FC z-maps, gene embeddings).</li> <li>Covariates recommended for residualization (age, sex, site, motion, PCs, SES).</li> <li>Missingness patterns and imputation strategy if required.</li> </ul>"},{"location":"generated/templates/dataset_card/#qc-harmonization","title":"QC &amp; Harmonization","text":"<ul> <li>Thresholds (FD &lt; 0.3 mm, min TR length).</li> <li>Site/scanner harmonization (e.g., ComBat) or reasons for not applying.</li> <li>Logs / reports location.</li> </ul>"},{"location":"generated/templates/dataset_card/#notes-risks","title":"Notes &amp; Risks","text":"<ul> <li>Any data use restrictions (no redistribution, publication review).</li> <li>Confounds or biases to monitor (ancestry imbalance, site skew).</li> <li>Planned updates or reprocessing tasks.</li> </ul>"},{"location":"generated/templates/dataset_card/#references","title":"References","text":"<ul> <li>Link to cohort publications / documentation.</li> <li>Internal tickets or notebooks for preprocessing runs.</li> </ul>"},{"location":"generated/templates/dataset_card/#linked-walkthroughs-external-repos","title":"Linked Walkthroughs &amp; External Repos","text":"<ul> <li>Walkthrough pages that describe preprocessing.</li> <li><code>external_repos/*</code> scripts or configs that produced the released tensors (commit/tag + entrypoint).</li> </ul>"},{"location":"generated/templates/integration_strategy/","title":"Integration Strategy Template","text":""},{"location":"generated/templates/integration_strategy/#metadata","title":"Metadata","text":"<ul> <li>Doc owner: <code>&lt;name&gt;</code></li> <li>Last updated: <code>&lt;YYYY-MM-DD&gt;</code></li> <li>Related decisions: <code>docs/decisions/&lt;file&gt;.md</code></li> <li>Model cards referenced: <code>&lt;comma-separated&gt;</code></li> </ul>"},{"location":"generated/templates/integration_strategy/#fusion-taxonomy-guidance","title":"Fusion Taxonomy &amp; Guidance","text":"Level When to use Risks / Notes Early fusion Homogeneous modalities with aligned semantics Modality collapse, confound bleed-through Intermediate fusion Shared latent required (e.g., cross-attn, hub tokens) Heavy engineering, risk of overfitting Late fusion Heterogeneous semantics (DNA vs brain) Requires good per-modality baselines <p>Summarize takeaways from the oncology multimodal review + EI paper; note that default stance is late integration until baselines show reliable gains.</p>"},{"location":"generated/templates/integration_strategy/#baseline-pipeline","title":"Baseline Pipeline","text":"<ol> <li>Preprocess per modality: z-score within train folds, residualize age/sex/site/scanner/motion/SES/genetic PCs.</li> <li>Project to 512-D: PCA (preferred) or tiny MLP (<code>Linear 1024\u2192512 \u2192 GELU \u2192 Dropout 0.1 \u2192 Linear 512\u2192512 \u2192 LayerNorm</code>).</li> <li>Association analysis: CCA + 1,000 permutations; report top canonical correlations, permutation p-values, loadings.</li> <li>Prediction: Logistic Regression (<code>class_weight='balanced'</code>) and LightGBM/CatBoost per modality + concatenated fusion; same CV folds; report AUROC/AUPRC \u00b1 SD.</li> <li>Statistical tests: DeLong or bootstrap for AUROC differences; Wilcoxon + FDR for LOGO \u0394AUC.</li> </ol>"},{"location":"generated/templates/integration_strategy/#confound-controls","title":"Confound Controls","text":"<ul> <li>Demographics: age, sex.</li> <li>Imaging: site/scanner, motion FD, TR group, SES if available.</li> <li>Genetics: top PCs (\u226510) or ancestry group.</li> <li>Technical: sequencing batch, acquisition protocol indicators.</li> <li>Residualize within fold; log design matrices in <code>artifacts/generated/confounds/</code>.</li> </ul>"},{"location":"generated/templates/integration_strategy/#evaluation-plan","title":"Evaluation Plan","text":"<ul> <li>CV design: Stratified K-fold (k=5 or 10) with group/site-aware splits if leakage risk.</li> <li>Metrics: AUROC, AUPRC, calibration (Brier/ECE optional), canonical correlations.</li> <li>Significance: DeLong for AUROC, bootstrap for AUPRC, permutation for CCA, Benjamini\u2013Hochberg for multiple tests.</li> <li>Logging: Store fold predictions, ROC/PR curves, permutation distributions under <code>artifacts/generated/metrics/&lt;experiment_id&gt;/</code>.</li> </ul>"},{"location":"generated/templates/integration_strategy/#extension-roadmap","title":"Extension Roadmap","text":"<ol> <li>Two-tower contrastive alignment: Freeze encoders, train 512-D projectors with InfoNCE; assess retrieval R@k and downstream AUROC.</li> <li>Ensemble Integration (stacking / ensemble selection): Train heterogeneous base learners per modality, stack with logistic meta-learner; record EI interpretation ranks.</li> <li>Joint latent models: Brain Harmony hub tokens / TAPE or cross-attention fusion, only after late-fusion + EI baselines saturate.</li> <li>Deployment hygiene: Missing-modality handling, calibration transfer, privacy considerations.</li> </ol>"},{"location":"generated/templates/integration_strategy/#references","title":"References","text":"<ul> <li>Ensemble Integration (Li et al., Bioinformatics Advances 2022)</li> <li>Multimodal oncology review (Waqas et al., 2024)</li> <li>BrainLM / Brain-JEPA / Brain Harmony primary papers</li> <li>Internal decisions (<code>docs/decisions/2025-11-integration-direction.md</code>)</li> </ul>"},{"location":"generated/templates/modality_features_fmri/","title":"Modality Features: Functional MRI (rs-fMRI)","text":""},{"location":"generated/templates/modality_features_fmri/#source-inputs","title":"Source Inputs","text":"<ul> <li>Preprocessed rs-fMRI timeseries (e.g., fMRIPrep outputs, custom pipelines).</li> <li>Atlas definitions (Schaefer-400, Gordon, volumetric grid).</li> <li>Motion / QC metrics (FD, DVARS, censoring masks).</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#option-a-functional-connectivity-fast-baseline","title":"Option A: Functional Connectivity (Fast Baseline)","text":"<ol> <li>Parcellate time series per subject.</li> <li>Compute Pearson correlations per ROI pair; apply Fisher z-transform.</li> <li>Flatten upper triangle; optionally reduce via PCA (100\u2013256 dims) before projecting to 512-D.</li> <li>Residualize covariates (age, sex, site/scanner, motion FD, TR, SES) within folds.</li> <li>Store embeddings + QC flags in <code>artifacts/generated/embeddings/fmri_fc/</code>.</li> </ol>"},{"location":"generated/templates/modality_features_fmri/#option-b-foundation-model-embeddings","title":"Option B: Foundation Model Embeddings","text":"<ul> <li>Supported encoders: BrainLM, Brain-JEPA, Brain Harmony, SwiFT, BrainMT.</li> <li>Preprocessing needs: TR normalization (Harmony TAPE), mask collators, gradient positional encodings.</li> <li>Pooling: CLS token, mean over spatial tokens, hub tokens; document choice.</li> <li>512-D projector (PCA/MLP) fit per fold; log checkpoint versions.</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#motion-site-handling","title":"Motion &amp; Site Handling","text":"<ul> <li>Include FD (mean, max) and number of censored volumes as covariates.</li> <li>Consider site-aware splits or mixed-effects modeling if imbalance severe.</li> <li>For heterogeneous TRs: align via resampling or Harmony\u2019s TAPE (PI-resize) before embedding.</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#covariate-residualization","title":"Covariate Residualization","text":"<ul> <li>Age, sex, site/scanner, motion FD, TR group, SES, acquisition batch.</li> <li>Document design matrices and residualization scripts.</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#integration-notes","title":"Integration Notes","text":"<ul> <li>Align subject IDs with genomics/sMRI intersections; record dropouts.</li> <li>Provide config references (<code>configs/projectors/fmri_pca512.yaml</code>, <code>kb/datasets/&lt;cohort&gt;.yaml</code>).</li> <li>Track version of preprocessing (e.g., fMRIPrep 23.2) and smoothing parameters.</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#references","title":"References","text":"<ul> <li>BrainLM, Brain-JEPA, Brain Harmony, SwiFT, BrainMT papers.</li> <li>Internal notebooks for FC pipeline and FM embedding extraction.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/","title":"Modality Features: Genomics","text":""},{"location":"generated/templates/modality_features_genomics/#source-inputs","title":"Source Inputs","text":"<ul> <li>Candidate gene/exon list: <code>&lt;link to table or YAML&gt;</code></li> <li>Sequence extraction pipeline: <code>&lt;script / notebook&gt;</code></li> <li>Reference genome build: <code>&lt;GRCh38/...&gt;</code></li> </ul>"},{"location":"generated/templates/modality_features_genomics/#tokenization-rc-hygiene","title":"Tokenization &amp; RC Hygiene","text":"<ul> <li>Specify tokenizer per model (character, k-mer length, BPE vocab).</li> <li>For RC-equivariant models (Caduceus, Evo/Evo2): compute forward + reverse-complement embeddings and average.</li> <li>For k-mer/BPE models (DNABERT-2, GENERator): RC raw sequence first, then re-tokenize to keep alignment; log tokenizer version and casing.</li> <li>Enforce consistent padding/truncation rules; document how Ns/ambiguous bases handled.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/#embedding-procedure","title":"Embedding Procedure","text":"<ol> <li>Load pretrained checkpoints (paths under <code>external_repos/&lt;repo&gt;/checkpoints</code>).</li> <li>Apply pooling (mean or CLS) per exon/gene; annotate what constitutes a subject-level aggregation (e.g., mean across exons).</li> <li>Normalize embeddings with fold-specific scaler.</li> <li>Project to 512-D via PCA or small MLP; store projector weights.</li> </ol>"},{"location":"generated/templates/modality_features_genomics/#covariates-residualization","title":"Covariates &amp; Residualization","text":"<ul> <li>Always residualize against age, sex, ancestry PCs (\u226510), sequencing batch, and other study-specific covariates.</li> <li>Document covariate sources and residualization scripts; output to <code>artifacts/generated/confounds/</code>.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/#logo-attribution","title":"LOGO / Attribution","text":"<ul> <li>Outline Leave-One-Gene-Out loop (nested CV, \u0394AUC, Wilcoxon signed-rank, Benjamini\u2013Hochberg FDR).</li> <li>Capture tables of \u0394AUC (mean \u00b1 SD) with p/q-values.</li> <li>Store plots showing top contributing genes.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Provide projector config reference (e.g., <code>projectors/genomics_pca512.yaml</code>).</li> <li>Note how to combine with brain embeddings for late fusion (concatenate 512-D vectors).</li> <li>Mention any pathways/enrichment analyses tied to CCA or LOGO outputs.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/#references","title":"References","text":"<ul> <li>Caduceus, DNABERT-2, Evo2, GENERator papers.</li> <li>Internal notebooks for sequence extraction, tokenizer QA, RC sanity checks.</li> </ul>"},{"location":"generated/templates/modality_features_smri/","title":"Modality Features: Structural MRI (sMRI)","text":""},{"location":"generated/templates/modality_features_smri/#source-inputs","title":"Source Inputs","text":"<ul> <li>FreeSurfer / CIVET outputs path.</li> <li>Atlas / parcellation used (e.g., Desikan, Schaefer-400 volumetric maps).</li> <li>QC reports (Euler number thresholds, manual edits).</li> </ul>"},{"location":"generated/templates/modality_features_smri/#feature-extraction","title":"Feature Extraction","text":"<ul> <li>ROI metrics: cortical thickness, volume, surface area, subcortical volumes.</li> <li>Derived composites (asymmetry indices, global measures) if needed.</li> <li>File formats (<code>.tsv</code>, <code>.csv</code>, HDF5) and loader scripts.</li> </ul>"},{"location":"generated/templates/modality_features_smri/#preprocessing-harmonization","title":"Preprocessing &amp; Harmonization","text":"<ul> <li>Z-score within train folds; residualize covariates (age, sex, ICV, site/scanner, SES).</li> <li>Optionally apply ComBat or mixed-effects models; document rationale.</li> <li>Handle missing ROIs (impute, drop subject, or add mask).</li> </ul>"},{"location":"generated/templates/modality_features_smri/#embedding-projection","title":"Embedding / Projection","text":"<ul> <li>Stack ROI vectors per subject; optionally reduce via PCA to 512-D (fit per train fold).</li> <li>Alternative: use pretrained encoders (Brain Harmony hub tokens) when available; document pooling strategy.</li> <li>Persist embeddings + scalers in <code>artifacts/generated/embeddings/smri/</code>.</li> </ul>"},{"location":"generated/templates/modality_features_smri/#covariates","title":"Covariates","text":"<ul> <li>Minimum set: age, sex, intracranial volume, site/scanner, scanner software version.</li> <li>Additional: handedness, SES, acquisition batch.</li> </ul>"},{"location":"generated/templates/modality_features_smri/#integration-notes","title":"Integration Notes","text":"<ul> <li>Use same subject IDs as genomics/fMRI intersections; log any exclusions.</li> <li>Provide YAML hooks for experiments: <code>kb/datasets/&lt;cohort&gt;.yaml</code>, <code>configs/projectors/smri_pca512.yaml</code>.</li> <li>Mention reliability metrics (test-retest ICC if available).</li> </ul>"},{"location":"generated/templates/modality_features_smri/#references","title":"References","text":"<ul> <li>Documentation for FreeSurfer pipeline version.</li> <li>Papers motivating ROI selection / harmonization steps.</li> </ul>"},{"location":"generated/templates/model_card_template/","title":"Model Card Template","text":"<p>Reminder: Include a license note such as \u201cThis walkthrough references <code>&lt;repo&gt;</code> under <code>&lt;license&gt;</code>,\u201d plus links to the repo, latest tag/commit, the associated <code>kb/model_cards/*.yaml</code>, and the generated PDF export (stored under <code>artifacts/pdf/code_walkthroughs/</code> or released assets).</p>"},{"location":"generated/templates/model_card_template/#metadata","title":"Metadata","text":"<ul> <li>Model name: <code>&lt;Friendly alias&gt;</code></li> <li>External repo: <code>&lt;URL&gt;</code></li> <li>Latest tag / commit: <code>&lt;tag-or-sha&gt;</code></li> <li>License: <code>&lt;e.g., Apache-2.0&gt;</code></li> <li>Model card YAML: <code>kb/model_cards/&lt;id&gt;.yaml</code></li> <li>Download PDF: <code>&lt;artifact link&gt;</code></li> </ul>"},{"location":"generated/templates/model_card_template/#purpose-scope","title":"Purpose &amp; Scope","text":"<ul> <li>What the model is designed for (e.g., DNA sequence embeddings, rs-fMRI latents, multimodal fusion).</li> <li>Intended tasks / datasets; out-of-scope uses.</li> </ul>"},{"location":"generated/templates/model_card_template/#architecture-inductive-biases","title":"Architecture &amp; Inductive Biases","text":"<ul> <li>Brief bullets on backbone, depth/width, notable blocks (e.g., JEPA, MAE, Swin, RC-equivariant BiMamba).</li> <li>Any modality-specific design (hub tokens, TAPE, gradient positional encodings).</li> </ul>"},{"location":"generated/templates/model_card_template/#tokenization-input-constraints","title":"Tokenization &amp; Input Constraints","text":"<ul> <li>Tokenizer type (character, k-mer, BPE, voxel patches, ROI tensors).</li> <li>Context length / TR windows / voxel grids.</li> <li>Required preprocessing (sorting genes, TR normalization, motion censoring).</li> </ul>"},{"location":"generated/templates/model_card_template/#pooling-subject-level-embeddings","title":"Pooling &amp; Subject-Level Embeddings","text":"<ul> <li>How to pool token embeddings (mean, CLS, hub-token average).</li> <li>RC handling (average forward/RC) or TR alignment notes.</li> <li>Aggregation to subject/session level (e.g., exon \u2192 gene \u2192 subject).</li> </ul>"},{"location":"generated/templates/model_card_template/#training-data-checkpoints","title":"Training Data &amp; Checkpoints","text":"<ul> <li>Source datasets, sample counts, preprocessing assumptions.</li> <li>Checkpoint paths / download links; expected placement under <code>external_repos/&lt;repo&gt;/checkpoints</code>.</li> </ul>"},{"location":"generated/templates/model_card_template/#recommended-embedding-procedure","title":"Recommended Embedding Procedure","text":"<ol> <li>Preprocess inputs (tokenization, z-score, residualization).</li> <li>Run encoder with key flags (e.g., gradient checkpointing, mask configs).</li> <li>Pool + project to 512-D (PCA or tiny MLP) with fold-aware fitting.</li> <li>Persist embeddings + covariates for downstream analyses.</li> </ol>"},{"location":"generated/templates/model_card_template/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>512-D projector config (<code>Linear \u2192 GELU \u2192 Dropout \u2192 Linear \u2192 LayerNorm</code>).</li> <li>Late-fusion guidance (LR/GBDT baselines, CCA requirements).</li> <li>LOGO / attribution tips (for gene models).</li> </ul>"},{"location":"generated/templates/model_card_template/#strengths-limitations","title":"Strengths &amp; Limitations","text":"<ul> <li>Where the model excels (e.g., long-context DNA, heterogeneous TRs).</li> <li>Known failure modes (site sensitivity, large VRAM needs, tokenizer quirks).</li> </ul>"},{"location":"generated/templates/model_card_template/#references-links","title":"References &amp; Links","text":"<ul> <li>Paper citation(s) with DOI/arXiv.</li> <li>Repo link, docs, issue tracker.</li> <li>Related KB cards (dataset, integration strategy, analysis recipes).</li> </ul>"},{"location":"guide/kb_overview/","title":"Navigating the Neuro-Omics KB","text":"<p>This page orients new readers to the structure of the knowledge base, how the YAML cards feed into the rendered docs, and where to find integration-critical metadata for gene\u2013brain\u2013behaviour foundation models. The KB began with adult UK Biobank\u2013centric gene\u2013brain FM alignment (genetics FM + MRI Brain FM outputs) and now supports developmental and neurodevelopmental longitudinal multimodal FMs spanning MRI/fMRI Brain FMs, EEG/EPhys FMs, genetics / multi-omics FMs, and behavioural / developmental phenotypes.</p>"},{"location":"guide/kb_overview/#architecture-at-a-glance","title":"Architecture at a glance","text":"<pre><code>flowchart LR\n    A[External repos &amp; datasets&lt;br/&gt;UKB \u00b7 HCP \u00b7 Gene FMs \u00b7 Dev cohorts] --&gt; B[Metadata curation&lt;br/&gt;kb/model_cards \u00b7 kb/datasets]\n    B --&gt; C[Integration strategy &amp; modality specs&lt;br/&gt;docs/integration/*]\n    C --&gt; D[Experiment configs&lt;br/&gt;configs/experiments/*.yaml]\n    D --&gt; E[Analysis outputs &amp; decisions&lt;br/&gt;docs/decisions \u00b7 docs/generated/*]\n    B --&gt; F[Embedding &amp; harmonization registries&lt;br/&gt;kb/integration_cards/*.yaml]\n    F --&gt; G[Semantic / hub alignments&lt;br/&gt;LLM \u00b7 VLM]\n    G --&gt; D\n</code></pre> <ul> <li>Source repos / datasets live under <code>external_repos/</code> (git-ignored) and upstream storage (UKB, HCP, Cha Hospital developmental cohorts, and future ARPA-H Brain-Omics Model (BOM) data sources).</li> <li>Metadata cards (<code>kb/**</code>) are the single source of truth for model/dataset specs, including adult and developmental gene\u2013brain\u2013behaviour datasets.</li> <li>Docs pages (this site) summarize how to use those cards for integration.</li> <li>Strategy registries (embedding/harmonization/preprocessing) connect raw exports to subject-level vectors.</li> <li>Semantic alignment / hub registries describe how modalities (gene, brain, EEG, behaviour) are aligned to LLM and VLM spaces in ARPA-H\u2013style Brain-Omics Model (BOM) projects.</li> <li>Experiment configs log the exact recipe IDs and folds before any analysis job starts.</li> </ul>"},{"location":"guide/kb_overview/#core-navigation-map","title":"Core navigation map","text":"<p>???+ info \"Critical sections\"     - Decisions \u2192 Integration plan (Nov 2025): why we start with late fusion + CCA before escalating to larger Brain-Omics Model (BOM) alignment.     - Integration \u2192 Strategy: shared preprocessing doctrine and escalation triggers.     - Integration \u2192 Analysis recipes: step-by-step guides for CCA, prediction baselines, partial correlations, and LOGO attribution.     - Integration \u2192 Modality features: per-modality feature prep with links to the embedding strategy IDs (Genomics, sMRI, fMRI, and planned EEG / developmental behavioural specs).     - Models: each FM\u2019s walkthrough plus the YAML card referenced in <code>kb/model_cards/*.yaml</code> (genetic FMs, Brain FMs, and future EEG / Multi-Omics FMs).     - Data: governance/QC logs and schema maps, including UKB and future developmental / neurodevelopmental cohorts.</p>"},{"location":"guide/kb_overview/#metadata-you-must-log-per-run","title":"Metadata you must log per run","text":"<p>Use the CLI helpers to print the canonical recipes and copy their IDs into your run metadata:</p> <pre><code># Embedding (e.g., FreeSurfer PCA-512)\npython scripts/manage_kb.py ops strategy smri_free_surfer_pca512_v1\n\n# Harmonization (e.g., MURD for T1/T2)\npython scripts/manage_kb.py ops harmonization murd_t1_t2\n\n# Preprocessing stack (e.g., HCP-like rs-fMRI)\npython scripts/manage_kb.py ops strategy rsfmri_swift_segments_v1\n</code></pre> <p>Record:</p> <ol> <li><code>embedding_strategies.&lt;id&gt;</code> for every modality in the experiment.</li> <li><code>harmonization_methods.&lt;id&gt;</code> (even if it is <code>none_baseline</code>).</li> <li><code>rsfmri_preprocessing_pipelines.&lt;id&gt;</code> whenever an rs-fMRI FM is involved.</li> <li>CV scheme (<code>StratifiedGroupKFold</code>, seed, groups) and manifest used.</li> </ol>"},{"location":"guide/kb_overview/#how-content-stays-consistent","title":"How content stays consistent","text":"<ul> <li>Template-first editing. Every new card starts from the templates in <code>kb/templates/</code>.</li> <li>Docs \u2194 YAML parity. If a doc cites a field (e.g., embedding recipe level), the corresponding YAML must include it.</li> <li>Strict builds. <code>mkdocs build --strict</code> guards against broken navigation before publishing.</li> </ul>"},{"location":"guide/kb_overview/#suggested-reading-order","title":"Suggested reading order","text":"<ol> <li>KB overview (this page).</li> <li>Integration strategy for the big-picture method.</li> <li>Modality feature specs for the modality you plan to touch.</li> <li>Relevant model walkthrough under Code Walkthroughs.</li> <li>Experiment config template that matches your goal (CCA, prediction, LOGO).</li> </ol> <p>Use this map whenever you add new cards or plan analyses\u2014it keeps the documentation, YAML metadata, and experiment configs aligned.</p>"},{"location":"integration/","title":"Integration Hub","text":"<p>Everything in this section supports the late-fusion-first plan documented in the decision log. Use it as the connective tissue between per-modality preprocessing, harmonization, and experiment execution.</p> <ul> <li>Strategy. The high-level playbook: covariates to regress, projection dims, when to escalate beyond late fusion.</li> <li>Analysis recipes. Copy-ready runbooks for CCA + permutation, prediction baselines, and partial correlations.</li> <li>Modality specs. Concrete instructions for genomics, sMRI, and rs-fMRI features, each pointing to the canonical <code>embedding_strategies.&lt;id&gt;</code>.</li> <li>Design patterns &amp; benchmarks. Reusable integration motifs plus prior benchmark targets to compare against.</li> </ul> <p>Before running anything, grab the relevant IDs via <code>python scripts/manage_kb.py ops strategy \u2026</code> and <code>python scripts/manage_kb.py ops harmonization \u2026</code>, then log them with your CV fold definitions. This keeps downstream reports auditable even when raw datasets (e.g., UKB) cannot be shared.</p>"},{"location":"integration/benchmarks/","title":"Integration benchmarks","text":""},{"location":"integration/benchmarks/#datasets","title":"Datasets","text":"<ul> <li>UK Biobank</li> <li>Targets:</li> <li>Primary: MDD diagnosis (binary; Howard et al. GWAS), PHQ-9 depression severity (continuous)</li> <li>Secondary cognitive: Fluid intelligence, reaction time, cognitive composite</li> <li>Stratifications: Early-onset MDD (age &lt; 21), late-onset MDD (age \u2265 21)</li> </ul>"},{"location":"integration/benchmarks/#splits","title":"Splits","text":"<ul> <li>GroupKFold by site</li> <li>Deterministic seeds with versioned <code>splits.json</code></li> </ul>"},{"location":"integration/benchmarks/#metrics-reporting","title":"Metrics &amp; reporting","text":"<ul> <li>AUC, PR-AUC, Brier score</li> <li>Calibration curves + subgroup reporting with confidence intervals</li> </ul>"},{"location":"integration/design_patterns/","title":"Integration Design Patterns","text":"<p>Default strategy: Late fusion via separate gene/brain heads (see Integration Strategy). The patterns below document escalation paths for when baseline fusion demonstrates significant gains, with risks and implementation guidance noted.</p>"},{"location":"integration/design_patterns/#overview","title":"Overview","text":"<p>This document catalogs integration architectures from simplest (late fusion) to most complex (unified multimodal transformers). Each pattern includes: - Description: How the pattern works - Use cases: When to apply it - Risks: What can go wrong - Examples: Reference models demonstrating the pattern - Escalation criteria: When to move to this pattern from simpler baselines</p>"},{"location":"integration/design_patterns/#pattern-1-late-fusion-baseline","title":"Pattern 1: Late Fusion (Baseline)","text":""},{"location":"integration/design_patterns/#description","title":"Description","text":"<p>Train separate encoders for each modality, extract fixed-size embeddings, concatenate, and pass to a simple classifier (LR, GBDT). No parameter sharing between modalities during training.</p>"},{"location":"integration/design_patterns/#architecture","title":"Architecture","text":"<pre><code>Genetics FM \u2192 gene_embed [512-D]  \u2500\u2500\u2510\n                                     \u251c\u2500\u2192 concat [1024-D] \u2192 LR/GBDT \u2192 prediction\nBrain FM    \u2192 brain_embed [512-D] \u2500\u2500\u2518\n</code></pre>"},{"location":"integration/design_patterns/#use-cases","title":"Use Cases","text":"<ul> <li>Initial baselines: Establish single-modality vs. fusion performance</li> <li>Heterogeneous modalities: Genetics (sequence) vs. brain (spatiotemporal) have different semantics</li> <li>Small sample sizes: Fewer parameters to tune than joint training</li> </ul>"},{"location":"integration/design_patterns/#risks","title":"Risks","text":"<ul> <li>Suboptimal if modalities have strong cross-modal dependencies</li> <li>No learned alignment between modality spaces</li> </ul>"},{"location":"integration/design_patterns/#implementation-current","title":"Implementation (Current)","text":"<ul> <li>Genetics: Caduceus/DNABERT-2 \u2192 gene-level embeddings \u2192 PCA-512</li> <li>Brain: BrainLM/Brain-JEPA \u2192 subject embeddings \u2192 PCA-512</li> <li>Fusion: <code>np.concatenate([gene_embed, brain_embed], axis=-1)</code></li> <li>Classifier: LogisticRegression (balanced, C=0.01) or LightGBM</li> </ul>"},{"location":"integration/design_patterns/#escalation-criteria","title":"Escalation Criteria","text":"<p>\u2705 Move to Pattern 2 when: Fusion significantly outperforms max(Gene, Brain) with p &lt; 0.05 (DeLong test)</p>"},{"location":"integration/design_patterns/#pattern-2-two-tower-contrastive-alignment","title":"Pattern 2: Two-Tower Contrastive Alignment","text":""},{"location":"integration/design_patterns/#description_1","title":"Description","text":"<p>Freeze pretrained modality encoders, add small learnable projectors, align via contrastive loss (InfoNCE). Creates shared embedding space without modifying foundation models.</p>"},{"location":"integration/design_patterns/#architecture_1","title":"Architecture","text":"<pre><code>Genetics FM (frozen) \u2192 gene_embed \u2192 projector_gene [256-D]  \u2500\u2500\u2510\n                                                               \u251c\u2500\u2192 InfoNCE loss\nBrain FM (frozen)    \u2192 brain_embed \u2192 projector_brain [256-D] \u2500\u2500\u2518\n                                           \u2193\n                               aligned_space [256-D] \u2192 classifier\n</code></pre>"},{"location":"integration/design_patterns/#use-cases_1","title":"Use Cases","text":"<ul> <li>Cross-modal retrieval: Find genes associated with brain patterns</li> <li>Zero-shot transfer: Align modalities on one cohort, transfer to another</li> <li>Foundation model preservation: Keep pretrained weights frozen</li> </ul>"},{"location":"integration/design_patterns/#risks_1","title":"Risks","text":"<ul> <li>Requires paired gene-brain samples (not all subjects have both modalities)</li> <li>Contrastive loss sensitive to negative sampling strategy</li> <li>May not capture complex non-linear interactions</li> </ul>"},{"location":"integration/design_patterns/#examples","title":"Examples","text":"<ul> <li>CLIP: Image-text contrastive alignment (OpenAI)</li> <li>M3FM: Medical image-text two-tower fusion (walkthrough)</li> </ul>"},{"location":"integration/design_patterns/#implementation-strategy","title":"Implementation Strategy","text":"<ol> <li>Freeze Caduceus and BrainLM checkpoints</li> <li>Add 512\u2192256 projectors (2-layer MLP with ReLU)</li> <li>Sample positive pairs: (gene_i, brain_i) from same subject</li> <li>Sample negatives: (gene_i, brain_j\u2260i) within batch</li> <li>Optimize InfoNCE loss on train split</li> <li>Extract aligned embeddings \u2192 downstream classifier</li> </ol>"},{"location":"integration/design_patterns/#escalation-criteria_1","title":"Escalation Criteria","text":"<p>\u2705 Move to Pattern 3 when: Cross-modal retrieval tasks emerge or need end-to-end joint training</p>"},{"location":"integration/design_patterns/#pattern-3-early-fusion-with-shared-encoder","title":"Pattern 3: Early Fusion with Shared Encoder","text":""},{"location":"integration/design_patterns/#description_2","title":"Description","text":"<p>Concatenate or interleave modality embeddings early, process through shared transformer layers. Enables cross-modal attention but requires careful preprocessing alignment.</p>"},{"location":"integration/design_patterns/#architecture_2","title":"Architecture","text":"<pre><code>gene_tokens [N_genes, D]  \u2500\u2500\u2510\n                            \u251c\u2500\u2192 concat \u2192 Shared Transformer \u2192 pooled_embed \u2192 classifier\nbrain_tokens [N_parcels, D] \u2500\u2518\n</code></pre>"},{"location":"integration/design_patterns/#use-cases_2","title":"Use Cases","text":"<ul> <li>Complex interactions: When modalities have intricate dependencies (e.g., gene regulatory networks affecting brain circuits)</li> <li>Multi-task learning: Share encoder across prediction tasks (MDD, cognitive scores)</li> <li>End-to-end optimization: Allow gradients to flow through all layers</li> </ul>"},{"location":"integration/design_patterns/#risks_2","title":"Risks","text":"<ul> <li>Modality imbalance: Dominant modality can suppress weaker one</li> <li>Overfitting: More parameters, higher risk with small N</li> <li>Preprocessing coupling: Requires consistent tokenization/normalization across modalities</li> </ul>"},{"location":"integration/design_patterns/#examples_1","title":"Examples","text":"<ul> <li>BAGEL: Unified decoder over text+image+video tokens (paper card)</li> <li>Brain Harmony: Joint sMRI+fMRI processing with hub tokens (model card)</li> </ul>"},{"location":"integration/design_patterns/#implementation-strategy_1","title":"Implementation Strategy","text":"<ol> <li>Tokenize both modalities to fixed dimensions</li> <li>Add modality-specific positional encodings</li> <li>Concatenate token sequences: <code>[gene_tok_1, ..., gene_tok_N, brain_tok_1, ..., brain_tok_M]</code></li> <li>Process through transformer layers with cross-modal attention</li> <li>Pool final layer \u2192 classification head</li> </ol>"},{"location":"integration/design_patterns/#escalation-criteria_2","title":"Escalation Criteria","text":"<p>\u2705 Move to Pattern 4 when: Need modality-specific parameter sets (avoid modality collapse)</p>"},{"location":"integration/design_patterns/#pattern-4-mixture-of-transformers-mot-sparse-fusion","title":"Pattern 4: Mixture-of-Transformers (MoT) Sparse Fusion","text":""},{"location":"integration/design_patterns/#description_3","title":"Description","text":"<p>Shared self-attention over all modality tokens, but separate FFNs and layer norms per modality. Balances cross-modal attention with modality-specific processing.</p>"},{"location":"integration/design_patterns/#architecture_3","title":"Architecture","text":"<pre><code>gene_tokens + brain_tokens + behavior_tokens\n    \u2193\nShared Self-Attention (all tokens interact)\n    \u2193\nModality-specific FFN branches:\n  \u251c\u2500 genetics_ffn\n  \u251c\u2500 brain_ffn\n  \u2514\u2500 behavior_ffn\n    \u2193\nPooled embedding \u2192 task heads\n</code></pre>"},{"location":"integration/design_patterns/#use-cases_3","title":"Use Cases","text":"<ul> <li>Compute efficiency: ~55% FLOPs vs. full dense multimodal transformer</li> <li>Modality preservation: Each modality retains specialized processing</li> <li>Scalable fusion: Handle 3+ modalities without parameter explosion</li> </ul>"},{"location":"integration/design_patterns/#risks_3","title":"Risks","text":"<ul> <li>More complex architecture than dense baseline</li> <li>Requires careful initialization of per-modality parameters</li> <li>May underperform dense if modalities highly correlated</li> </ul>"},{"location":"integration/design_patterns/#examples_2","title":"Examples","text":"<ul> <li>MoT paper: Sparse multimodal transformer (arXiv:2411.04996, card)</li> </ul>"},{"location":"integration/design_patterns/#implementation-strategy_2","title":"Implementation Strategy","text":"<ol> <li>Initialize shared attention layers (all modalities)</li> <li>Create separate FFN/norm modules per modality (genetics_ffn, brain_ffn, behavior_ffn)</li> <li>Forward pass: attention(all_tokens) \u2192 route_to_modality_ffn(token) \u2192 merge</li> <li>Train end-to-end with task-specific heads</li> </ol>"},{"location":"integration/design_patterns/#escalation-criteria_3","title":"Escalation Criteria","text":"<p>\u2705 Move to Pattern 5 when: Need generative capabilities (e.g., clinical report generation from gene-brain data)</p>"},{"location":"integration/design_patterns/#pattern-5-unified-brain-omics-model-bom","title":"Pattern 5: Unified Brain-Omics Model (BOM)","text":""},{"location":"integration/design_patterns/#description_4","title":"Description","text":"<p>Single decoder-only transformer with Mixture-of-Experts (MoE) processing all modalities as token sequences: genetics (nucleotide tokens), brain (parcel/voxel tokens), behavior (structured tokens), language (text tokens). Inspired by BAGEL/GPT-4o-style unified multimodal architectures.</p>"},{"location":"integration/design_patterns/#architecture_4","title":"Architecture","text":"<pre><code>Tokenize all modalities:\n  - Genetics: nucleotide sequences \u2192 tokens\n  - Brain MRI: 3D patches \u2192 tokens (ViT-style)\n  - fMRI: parcel time series \u2192 tokens\n  - EEG: channel \u00d7 time \u2192 tokens\n  - Behavior: structured data \u2192 embedding tokens\n  - Language: text \u2192 BPE tokens\n\n  \u2193\nUnified Decoder-Only Transformer (e.g., LLaMA-style)\n  - Mixture-of-Experts (understanding vs. generation)\n  - Cross-modal self-attention\n  - Next-token prediction objective\n\n  \u2193\nDownstream tasks:\n  - Gene-brain association discovery\n  - Clinical report generation\n  - Counterfactual reasoning (\"what if gene X was mutated?\")\n  - Cognitive decline prediction\n</code></pre>"},{"location":"integration/design_patterns/#use-cases_4","title":"Use Cases","text":"<ul> <li>ARPA-H Brain-Omics Model (BOM): Unified foundation model for neuro-omics</li> <li>LLM as semantic bridge: Language model embeddings as \"lingua franca\" for cross-modal reasoning</li> <li>Generative tasks: Report generation, sequence design, counterfactual prediction</li> <li>Unified pretraining: Single model handles all neuro-omics modalities</li> </ul>"},{"location":"integration/design_patterns/#risks_4","title":"Risks","text":"<ul> <li>Massive compute: Requires trillions of tokens, large-scale infrastructure</li> <li>Data curation: Need high-quality interleaved multimodal corpus</li> <li>Complexity: Hardest to debug, longest training time</li> <li>Evaluation: Requires diverse benchmarks across modalities</li> </ul>"},{"location":"integration/design_patterns/#examples_3","title":"Examples","text":"<ul> <li>BAGEL: Unified text+image+video+web model (walkthrough)</li> <li>GPT-4o: Unified multimodal reasoning (proprietary)</li> <li>Chameleon: Text-image unified autoregressive model</li> </ul>"},{"location":"integration/design_patterns/#implementation-strategy-long-term-vision","title":"Implementation Strategy (Long-term Vision)","text":"<p>Phase 1: Corpus Curation - Collect interleaved multimodal neuro-omics data:   - Genetic variants + brain scans + cognitive assessments + clinical notes   - Longitudinal trajectories (developmental, disease progression)   - Multimodal annotations (gene function descriptions, brain region labels, symptom text)</p> <p>Phase 2: Tokenization - Genetics: Nucleotide sequences (A/C/G/T) or k-mer tokens - Brain MRI: 3D patch tokens (ViT-style, 16\u00b3 patches) - fMRI: Parcel time series \u2192 temporal tokens - EEG: Channel-time matrices \u2192 spectral-spatial tokens - Behavior: Structured scores \u2192 learned embeddings - Language: Standard BPE/SentencePiece tokens</p> <p>Phase 3: Architecture - Decoder-only transformer (LLaMA/Qwen base) - Mixture-of-Experts: Understanding vs. generation experts - Modality-specific input embedders, shared transformer backbone - Task-specific heads (classification, generation, retrieval)</p> <p>Phase 4: Training - Next-token prediction across all modalities - Interleaved sequence objective (language \u2192 genetics \u2192 brain \u2192 language) - Multitask loss: prediction + generation + contrastive</p> <p>Phase 5: Evaluation - Gene-brain association discovery (AUC, correlation) - Clinical report generation (BLEU, METEOR, clinician ratings) - Cognitive prediction (AUROC on MDD, fluid intelligence) - Cross-modal retrieval (gene\u2192brain, brain\u2192phenotype) - Counterfactual reasoning (GPT-4-judge evaluation)</p>"},{"location":"integration/design_patterns/#escalation-criteria_4","title":"Escalation Criteria","text":"<p>\u2705 Implement BOM when: Phases 1-4 patterns exhausted, significant funding secured, compute infrastructure available</p>"},{"location":"integration/design_patterns/#arpa-h-integration-roadmap","title":"ARPA-H Integration Roadmap","text":""},{"location":"integration/design_patterns/#timeline","title":"Timeline","text":"Phase Pattern Status Target Completion Phase 1 Late fusion baselines In progress Nov 2025 Phase 2 Two-tower contrastive Pending Q1 2026 Phase 3 Early fusion / MoT Pending Q2 2026 Phase 4 Unified BOM (pilot) Planned Q3-Q4 2026 Phase 5 Scaled BOM deployment Vision 2027+"},{"location":"integration/design_patterns/#current-focus-nov-2025","title":"Current Focus (Nov 2025)","text":"<p>Active: - \u2705 Late fusion: Gene (Caduceus) + Brain (BrainLM) \u2192 LR/GBDT - \u2705 CCA + permutation: Assess cross-modal correlation structure - \u2705 LOGO attribution: Gene-level importance (Yoon et al. protocol)</p> <p>Next steps: 1. Complete late fusion baselines on UKB gene-brain data 2. Pilot two-tower contrastive alignment (frozen encoders) 3. Design interleaved corpus for Phase 3+ experiments</p>"},{"location":"integration/design_patterns/#reference-materials","title":"Reference Materials","text":"<p>Multimodal architecture examples: - Multimodal Architectures Overview \u2014 Detailed patterns from BAGEL, MoT, M3FM, Me-LLaMA, TITAN</p> <p>Integration strategies: - Integration Strategy \u2014 Preprocessing, harmonization, escalation criteria - CCA + Permutation Recipe \u2014 Statistical testing before fusion - Prediction Baselines \u2014 Late fusion implementation</p> <p>Model documentation: - Brain Models Overview - Genetics Models Overview</p> <p>Decision logs: - Integration Baseline Plan (Nov 2025) \u2014 Why late fusion first</p>"},{"location":"integration/integration_strategy/","title":"Integration Strategy","text":"<p>Overall philosophy</p> <ul> <li>Late fusion / integration first, then scale if we see gains.</li> </ul> <p>Why this applies to genes \u00d7 brain</p> <ul> <li>Heterogeneous semantics: nucleic-acid sequence vs morphology/dynamics \u2192 maximize modality specificity before fusion.</li> <li>Confounds differ: ancestry/batch vs site/motion/TR \u2192 deconfound independently.</li> </ul> <p>Baselines</p> <ul> <li>Preprocess per modality</li> <li>Z-score features.</li> <li>Residualize against: age, sex, site/scanner, motion (FD), SES (if available), genetic PCs (PC1\u2013PC10).</li> <li>Dimensionality</li> <li>Project to 512 dims per modality (PCA or tiny MLP).</li> <li>CCA + permutation</li> <li>CCA on train folds; 1,000 shuffles; report \u03c11\u2013\u03c13 with p-values.</li> <li>Prediction</li> <li>LR (balanced) and LightGBM/CatBoost on Gene, Brain, Fusion; same CV folds; AUROC/AUPRC; DeLong/bootstrap for Fusion vs single-modality.</li> </ul>"},{"location":"integration/integration_strategy/#embedding-strategy-registry","title":"Embedding strategy registry","text":"<ul> <li>Recipes live under <code>kb/integration_cards/embedding_strategies.yaml</code>. Print them via <code>python scripts/manage_kb.py ops strategy &lt;id&gt;</code> before each extraction and log the ID in experiment metadata.</li> <li>sMRI (<code>smri_free_surfer_pca512_v1</code>). FreeSurfer ROI table (~176 features) \u2192 fold-wise z-score \u2192 residualize age/sex/site/ICV \u2192 PCA\u2192512. Future variants: FM encoders, diffusion MRI. Sources: <code>docs/integration/modality_features/smri.md</code>, FreeSurfer refs.</li> <li>rs-fMRI baseline (<code>rsfmri_swift_segments_v1</code>). SwiFT exports per 20-frame segment \u2192 mean pool tokens \u2192 run mean \u2192 subject mean \u2192 residualize age/sex/site/motion \u2192 PCA\u2192512. Variants exist for BrainLM (<code>rsfmri_brainlm_segments_v1</code>), Brain-JEPA (<code>rsfmri_brainjepa_roi_v1</code>), and BrainMT (<code>rsfmri_brainmt_segments_v1</code>); each references the corresponding walkthrough/code.</li> <li>Genetics (<code>genetics_gene_fm_pca512_v1</code>). RC-averaged gene FMs (Caduceus/Evo2/GENERaTOR) \u2192 exon \u2192 gene pooling \u2192 concatenate curated gene set \u2192 residualize age/sex/ancestry PCs/batch \u2192 PCA\u2192512.</li> <li>Fusion (<code>fusion_concat_gene_brain_1024_v1</code>). Concatenate the 512-D genetics vector with the chosen 512-D brain vector; z-score each block independently before concatenation.</li> </ul> <p>Experiments now reference these IDs (see <code>configs/experiments/*.yaml</code>) to keep per-subject embeddings traceable.</p>"},{"location":"integration/integration_strategy/#harmonization-site-effects","title":"Harmonization &amp; site effects","text":"<ul> <li>Cataloged in <code>kb/integration_cards/harmonization_methods.yaml</code>; query via <code>python scripts/manage_kb.py ops harmonization &lt;id&gt;</code>.</li> <li>Default (<code>none_baseline</code>). Feature-level z-score + covariate residualization; always record site/motion covariates.</li> <li>Statistical (<code>combat_smri</code>). ROI-wise ComBat before PCA for sMRI (Fortin et al., 2018). Run the <code>02_harmonization_ablation_smri</code> config to benchmark vs. the baseline.</li> <li>Deep (<code>murd_t1_t2</code>). Apply MURD (Liu &amp; Yap 2024) to T1/T2 volumes before FreeSurfer extraction; compare vs. ComBat and baseline to judge if image-space harmonization improves CCA/prediction.</li> <li>Representation unlearning (<code>site_unlearning_module</code>). Optional adversarial head that removes site labels from embedding space (Dinsdale et al., 2021); treat as experimental until harmonization ablations justify it.</li> </ul> <p>Record harmonization IDs (and preprocessing pipeline IDs such as <code>rsfmri_preprocessing_pipelines.hcp_like_minimal</code>) alongside embedding strategy IDs in every run.</p> <p>Escalation criteria</p> <ul> <li>If Fusion &gt; max(Gene, Brain) with p &lt; 0.05 (DeLong/bootstrap), consider:</li> <li>Two-tower contrastive alignment (frozen encoders; small projectors).</li> <li>EI stacking over per-modality models.</li> <li>Harmony-style hub tokens/TAPE if TR/site heterogeneity limits fMRI.</li> </ul> <p>Interpretability</p> <ul> <li>LOGO \u0394AUC with Wilcoxon + FDR for gene attribution.</li> <li>CCA loadings; partial correlations of axes with outcomes (covariate-adjusted).</li> </ul>"},{"location":"integration/integration_strategy/#tabular-fm-tabpfn-baseline","title":"Tabular FM (TabPFN) baseline","text":"<ul> <li>TabPFN (Nature 2024) is tracked under <code>kb/model_cards/tabpfn.yaml</code>. It is a predictor, not an fMRI encoder.</li> <li>Use TabPFN as a strong small-N tabular baseline on:</li> <li>Raw sMRI ROI tables (<code>smri_free_surfer_raw_176</code>).</li> <li>Genetics summary tables (<code>genetics_pgs_20traits</code>).</li> <li>Early-fusion tabular features (ROI + PGS).</li> <li>Compare TabPFN vs. LR/LightGBM in <code>configs/experiments/03_prediction_baselines_tabular.yaml</code> to quantify how much structured embeddings help beyond a tabular FM.</li> </ul> <p>Risks and mitigations</p> <ul> <li>Leakage: do scaling/residualization within train folds; apply transforms to test.</li> <li>Site imbalance: use group/site-aware CV when feasible.</li> <li>Overfitting at high dims: prefer 256\u2013512; regularize LR; early stopping for GBDT.</li> </ul>"},{"location":"integration/multimodal_architectures/","title":"Multimodal Architecture Patterns for Brain-Omics Models","text":"<p>This document catalogs architectural patterns from multimodal foundation models that inform the design of ARPA-H-style Brain-Omics Model (BOM) systems. These models demonstrate how to fuse heterogeneous modalities (vision, language, time series, structured data) at scale\u2014lessons directly applicable to gene\u2013brain\u2013behavior\u2013language integration.</p>"},{"location":"integration/multimodal_architectures/#overview","title":"Overview","text":"<p>Purpose: Extract design principles from state-of-the-art multimodal FMs to guide Neuro-Omics KB integration strategies as they escalate from late fusion \u2192 two-tower contrastive \u2192 unified multimodal architectures.</p> <p>Scope: Medical/clinical multimodal FMs, unified vision-language-speech models, and sparse multimodal transformers.</p>"},{"location":"integration/multimodal_architectures/#1-bagel-unified-multimodal-foundation-model","title":"1. BAGEL \u2014 Unified Multimodal Foundation Model","text":""},{"location":"integration/multimodal_architectures/#architecture-summary","title":"Architecture Summary","text":"<p>Model: BAGEL (Emerging Properties in Unified Multimodal Pretraining) Paper: arXiv:2505.14683 | Card: <code>kb/paper_cards/bagel_2025.yaml</code></p> <ul> <li>Backbone: Qwen2.5 decoder-only transformer (7B active, 14B total with MoT experts)</li> <li>Modalities: Text, images, video, web data</li> <li>Architecture: Mixture-of-Transformer-Experts (MoT) with separate experts for understanding vs. generation</li> <li>Visual encoding: SigLIP2-style ViT encoder for understanding</li> <li>Visual generation: FLUX VAE + rectified-flow diffusion conditioned on transformer states</li> <li>Training: Trillions of interleaved multimodal tokens with reasoning-oriented curation</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns","title":"Key Design Patterns","text":"<p>\u2705 Unified decoder-only architecture: Single transformer processes all modalities as token sequences \u2705 Mixture-of-experts (MoT): Separate experts for understanding (comprehension) vs. generation tasks \u2705 Interleaved data: Reasoning-oriented multimodal corpus with natural task diversity \u2705 Emergent capabilities: Complex reasoning, free-form manipulation, 3D understanding from unified pretraining</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Gene-brain-language unification: Treat genetics (nucleotide tokens), brain (parcel tokens), and behavior (structured tokens) as additional modalities alongside text - MoT for neuro-omics: Separate experts for discriminative (gene-brain association) vs. generative (report generation, counterfactual prediction) tasks - Interleaved corpus design: Create multimodal corpus pairing genetic variants + brain scans + cognitive assessments + clinical narratives</p> <p>Escalation path: 1. Late fusion baselines (current) 2. Two-tower contrastive (gene encoder \u2194 brain encoder) 3. MoT-style unified architecture where genetics/brain/behavior tokens share decoder with modality-specific experts</p> <p>Reference materials: - BAGEL walkthrough - BAGEL paper card</p>"},{"location":"integration/multimodal_architectures/#2-mot-mixture-of-transformers","title":"2. MoT \u2014 Mixture-of-Transformers","text":""},{"location":"integration/multimodal_architectures/#architecture-summary_1","title":"Architecture Summary","text":"<p>Model: Mixture-of-Transformers (Sparse and Scalable for Multi-Modal FMs) Paper: arXiv:2411.04996 | Card: <code>kb/paper_cards/mot_2025.yaml</code></p> <ul> <li>Backbone: Sparse multimodal transformer with modality-aware FFNs/attention</li> <li>Modalities: Text, images, speech</li> <li>Sparsity mechanism: Separate FFN/attention projections per modality; shared global self-attention</li> <li>Settings: Chameleon-style autoregressive + Transfusion-style diffusion</li> <li>Efficiency: ~55.8% FLOPs of dense baseline, similar or better performance</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns_1","title":"Key Design Patterns","text":"<p>\u2705 Modality-aware sparsity: Decouple non-embedding parameters by modality \u2705 Shared global attention: All tokens interact via self-attention (no routing) \u2705 Drop-in replacement: Compatible with existing dense transformer architectures \u2705 Stable scaling: Maintains performance across model sizes (1B \u2192 7B \u2192 30B)</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_1","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Per-modality FFNs: Separate feed-forward networks for genetics, brain MRI, fMRI, EEG, behavior tokens - Shared attention: Global self-attention over all modalities captures cross-modal dependencies - Compute efficiency: Critical for scaling to large cohorts (UK Biobank N=500k+)</p> <p>Integration with Neuro-Omics KB: - Implement modality-specific projectors (genetics_ffn, brain_ffn, behavior_ffn) - Retain shared attention over concatenated gene+brain+behavior tokens - Compare vs. learned MoE routing (simpler, more interpretable)</p> <p>Reference materials: - MoT walkthrough - MoT paper card</p>"},{"location":"integration/multimodal_architectures/#3-m3fm-multilingual-medical-model","title":"3. M3FM \u2014 Multilingual Medical Model","text":""},{"location":"integration/multimodal_architectures/#architecture-summary_2","title":"Architecture Summary","text":"<p>Model: M3FM (Multilingual Chest X-ray Report Generator) Repo: ai-in-health/M3FM | Card: <code>kb/model_cards/m3fm.yaml</code></p> <ul> <li>Backbone: Multilingual CLIP encoder + relational-memory Transformer decoder</li> <li>Modalities: Chest X-ray images, bilingual text (English/Chinese)</li> <li>Architecture: Two-tower (vision encoder + language decoder) with relational memory</li> <li>Decoder: Language selection via BOS token (1=English, 2=Chinese)</li> <li>Training: COV-CTR COVID-era CXR dataset with multilingual reports</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns_2","title":"Key Design Patterns","text":"<p>\u2705 Two-tower fusion: Vision encoder outputs \u2192 cross-attention in language decoder \u2705 Language-aware generation: Single decoder handles multiple languages via BOS conditioning \u2705 Relational memory: Augmented attention for capturing long-range report dependencies \u2705 Medical domain adaptation: CLIP text embeddings projected for medical terminology</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_2","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Brain-omics-to-language: Project brain/genetics embeddings into CLIP-like space \u2192 generate clinical narratives - Bilingual reporting: Extend to English/Korean for Cha Hospital developmental cohorts - Relational memory for clinical context: Track longitudinal patient history across visits</p> <p>Integration strategy: - Use M3FM-style two-tower for brain scan \u2192 clinical report generation - Adapt relational memory for multi-visit longitudinal modeling - Explore gene embedding \u2192 language generation (explain genetic risk in natural language)</p> <p>Reference materials: - M3FM walkthrough - M3FM model card</p>"},{"location":"integration/multimodal_architectures/#4-me-llama-medical-llm","title":"4. Me-LLaMA \u2014 Medical LLM","text":""},{"location":"integration/multimodal_architectures/#architecture-summary_3","title":"Architecture Summary","text":"<p>Model: Me-LLaMA (Medical LLaMA) Repo: BIDS-Xu-Lab/Me-LLaMA | Card: <code>kb/model_cards/me_llama.yaml</code></p> <ul> <li>Backbone: LLaMA-2/3 (13B/70B) with continual pretraining + LoRA instruction tuning</li> <li>Modality: Medical text (biomedical literature, clinical notes, guidelines)</li> <li>Pretraining ratio: 15:1:4 (biomedical : clinical : general)</li> <li>Training: 129B medical tokens + 214K instruction samples</li> <li>Evaluation: 12+ medical QA/NLP tasks with prompt templates</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns_3","title":"Key Design Patterns","text":"<p>\u2705 Continual pretraining: Adapt general LLM to medical domain with curated corpus \u2705 LoRA instruction tuning: Parameter-efficient adaptation for clinical reasoning \u2705 Prompt engineering: Modality-specific prompts for different clinical tasks \u2705 Evaluation harness: Structured benchmarking across medical NLP tasks</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_3","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Neuro-omics LLM: Continual pretrain LLaMA on neuroscience literature + genetics papers + clinical neurology notes - Instruction tuning for clinical tasks: Adapt for cognitive assessment interpretation, genetic counseling, neuroimaging report generation - Prompt templates: Create standardized prompts for gene-brain-behavior reasoning</p> <p>As semantic bridge in BOM: - Me-LLaMA-style medical LLM serves as semantic hub for Brain-Omics Model - Project genetics/brain/EEG embeddings into LLM token space for cross-modal reasoning - Enable natural language queries over multimodal neuro-omics data</p> <p>Reference materials: - Me-LLaMA walkthrough - Me-LLaMA model card</p>"},{"location":"integration/multimodal_architectures/#5-titan-whole-slide-image-fm","title":"5. TITAN \u2014 Whole-Slide Image FM","text":""},{"location":"integration/multimodal_architectures/#architecture-summary_4","title":"Architecture Summary","text":"<p>Model: TITAN (Transformer for Integrative Tissue Analysis) Repo: mahmoodlab/TITAN | Card: <code>kb/model_cards/titan.yaml</code></p> <ul> <li>Backbone: Slide-level transformer with multi-scale patch aggregation</li> <li>Modality: Whole-slide histopathology images</li> <li>Architecture: Hierarchical attention over gigapixel images (millions of patches)</li> <li>Applications: Cancer diagnosis, survival prediction, treatment response</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns_4","title":"Key Design Patterns","text":"<p>\u2705 Multi-scale patch processing: Handle gigapixel images via hierarchical aggregation \u2705 Attention-based pooling: Learn to aggregate informative regions \u2705 Slide-level embeddings: Compress millions of patches \u2192 fixed-size vectors \u2705 Task-specific heads: Shared encoder for multiple downstream tasks</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_4","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Brain MRI analogy: Whole-brain 3D volumes \u2192 hierarchical patch aggregation (similar to TITAN's slide processing) - Multi-scale fusion: Combine region-level (parcels) and voxel-level (fine-grained) brain features - Histology + genetics: If histopathology data available (e.g., brain tissue banks), TITAN-style processing + genetics fusion</p> <p>Integration with Neuro-Omics KB: - Adapt TITAN's multi-scale attention for 3D MRI volumes - Use TITAN-style patch aggregation for whole-brain sMRI + fMRI fusion - Explore cross-modal attention: pathology patches \u2194 genetic variants</p> <p>Reference materials: - TITAN walkthrough - TITAN model card</p>"},{"location":"integration/multimodal_architectures/#6-fms-medical-catalog","title":"6. FMS-Medical Catalog","text":""},{"location":"integration/multimodal_architectures/#resource-summary","title":"Resource Summary","text":"<p>Catalog: Awesome Foundation Models for Advancing Healthcare Repo: YutingHe-list/Awesome-Foundation-Models</p> <ul> <li>Scope: 200+ medical foundation models across modalities (text, vision, multimodal, protein, genomics, clinical time series)</li> <li>Organization: Bilingual (English/Chinese) with taxonomy by modality and task</li> <li>Usage: Reference catalog for discovering relevant medical FMs</li> </ul>"},{"location":"integration/multimodal_architectures/#key-resources","title":"Key Resources","text":"<p>\u2705 Medical vision FMs: CXR, CT, MRI, histopathology encoders \u2705 Medical LLMs: Clinical text understanding and generation models \u2705 Genomics/proteomics FMs: Sequence models for molecular biology \u2705 Multimodal FMs: Vision-language models for radiology, pathology reports</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_5","title":"Implications for Brain-Omics Models","text":"<p>Discovery and benchmarking: - Identify relevant medical imaging FMs for brain scan processing - Find medical LLMs for clinical narrative generation - Discover multimodal architectures to adapt for gene-brain-behavior fusion</p> <p>Reference for ARPA-H integration: - Survey multimodal medical FMs to inform BOM architecture choices - Benchmark against medical FM baselines (e.g., CXR report generation \u2192 adapt for neuroimaging)</p> <p>Reference materials: - FMS-Medical walkthrough - FMS-Medical catalog YAML</p>"},{"location":"integration/multimodal_architectures/#integration-roadmap-for-neuro-omics-kb","title":"Integration Roadmap for Neuro-Omics KB","text":""},{"location":"integration/multimodal_architectures/#phase-1-late-fusion-baselines-current","title":"Phase 1: Late Fusion Baselines (Current)","text":"<ul> <li>Models: Separate encoders (Caduceus, BrainLM, FreeSurfer ROIs)</li> <li>Fusion: Concatenate embeddings \u2192 LR/GBDT prediction</li> <li>Evaluation: CCA + permutation, AUROC/AUPRC, DeLong tests</li> </ul>"},{"location":"integration/multimodal_architectures/#phase-2-two-tower-contrastive","title":"Phase 2: Two-Tower Contrastive","text":"<ul> <li>Architecture: Frozen gene encoder \u2194 frozen brain encoder with learnable projectors</li> <li>Loss: InfoNCE or similar contrastive objective</li> <li>Inspiration: CLIP-style alignment (M3FM two-tower paradigm)</li> </ul>"},{"location":"integration/multimodal_architectures/#phase-3-mot-style-sparse-integration","title":"Phase 3: MoT-Style Sparse Integration","text":"<ul> <li>Architecture: Shared self-attention over gene+brain+behavior tokens</li> <li>Sparsity: Modality-specific FFNs (genetics_ffn, brain_ffn, behavior_ffn)</li> <li>Inspiration: MoT paper (arXiv:2411.04996)</li> </ul>"},{"location":"integration/multimodal_architectures/#phase-4-unified-brain-omics-model-bom","title":"Phase 4: Unified Brain-Omics Model (BOM)","text":"<ul> <li>Architecture: BAGEL-style decoder-only with MoT experts</li> <li>Modalities: Genetics (nucleotide tokens) + brain (parcel/voxel tokens) + behavior (structured tokens) + language (text tokens)</li> <li>Semantic bridge: Me-LLaMA-style medical LLM as central hub</li> <li>Training: Interleaved multimodal corpus (genetic variants + brain scans + cognitive assessments + clinical notes)</li> </ul>"},{"location":"integration/multimodal_architectures/#next-steps","title":"Next Steps","text":"<ol> <li>Complete Phase 1 baselines (CCA + prediction on UKB gene-brain data)</li> <li>Pilot two-tower contrastive (gene-brain alignment with frozen encoders)</li> <li>Explore MoT-style sparsity (modality-specific FFNs vs. full early fusion)</li> <li>Design ARPA-H BOM architecture (unified multimodal transformer with neuro-omics tokens)</li> <li>Curate interleaved corpus (multimodal neuro-omics data for unified pretraining)</li> </ol>"},{"location":"integration/multimodal_architectures/#reference-index","title":"Reference Index","text":"<p>Walkthrough documents: - BAGEL walkthrough - MoT walkthrough - M3FM walkthrough - Me-LLaMA walkthrough - TITAN walkthrough - FMS-Medical walkthrough</p> <p>Paper/model cards: - <code>kb/paper_cards/bagel_2025.yaml</code> - <code>kb/paper_cards/mot_2025.yaml</code> - <code>kb/model_cards/m3fm.yaml</code> - <code>kb/model_cards/me_llama.yaml</code> - <code>kb/model_cards/titan.yaml</code> - <code>kb/datasets/fms_medical_catalog.yaml</code></p> <p>Integration recipes: - Integration Strategy - Design Patterns - CCA + Permutation</p>"},{"location":"integration/analysis_recipes/cca_permutation/","title":"CCA + Permutation","text":"<p>Inputs</p> <ul> <li>X_gene, X_brain: residualized and standardized matrices (N \u00d7 d_gene, N \u00d7 d_brain) projected to ~512 dims.</li> <li>Covariates: used upstream during residualization.</li> <li>Metadata: record <code>embedding_strategies.&lt;id&gt;</code>, <code>harmonization_methods.&lt;id&gt;</code>, and (for fMRI) <code>rsfmri_preprocessing_pipelines.&lt;id&gt;</code> to ensure results are traceable.</li> </ul> <p>Context in integration plan</p> <ul> <li>This recipe is part of the diagnostic / exploration layer of the integration stack.</li> <li>Run it after per-modality sanity checks but before heavier fusion models; it tells you whether there is cross-modal structure worth chasing.</li> <li>Treat it as a companion to the late-fusion-first baselines rather than a replacement for prediction experiments.</li> </ul> <p>Protocol</p> <p>1) Fold discipline - Use K stratified folds (group/site-aware if needed). - Within each train fold:   - Fit CCA on X_gene_train, X_brain_train.   - Transform train and test to canonical scores. 2) Permutation - For b in 1..B (B = 1,000):   - Permute subject alignment in one modality within the train fold.   - Fit CCA on permuted pairs.   - Record \u03c11_null. - p = (count(\u03c11_null \u2265 \u03c11_obs) + 1) / (B + 1). 3) Reporting - \u03c11\u2013\u03c13 with permutation p-values. - Optional: bootstrap CIs on \u03c11. - Loadings/feature contributions for interpretation.</p> <p>Why pair CCA with permutations?</p> <ul> <li>CCA will always produce non-zero canonical correlations\u2014even when there is no shared structure\u2014because it can overfit high-dimensional spaces.</li> <li>The permutation loop builds a modality-shuffled null distribution so we can report p-values (or FDR-adjusted thresholds) and avoid over-interpreting noise.</li> <li>This statistical check is lightweight enough for \u201cquick tests\u201d while still respecting site/ancestry confounds.</li> </ul> <p>Pitfalls - Never fit CCA on all data. - Keep the same permutation protocol across folds for comparability.</p>"},{"location":"integration/analysis_recipes/partial_correlations/","title":"Partial Correlations","text":"<p>Goal - Associate canonical scores or PCs with outcomes controlling covariates.</p> <p>Context in integration plan</p> <ul> <li>Use this after you have stable embeddings and late-fusion baselines: it helps interpret axes (CCA components, PCs) rather than build new predictors.</li> <li>Treat it as an analysis layer sitting on top of the late-fusion-first stack, not as a standalone modeling approach.</li> <li>Prefer simple, regularized models here; if interpretation depends on heavy models, revisit whether the underlying embeddings/CCA steps are well-behaved.</li> </ul> <p>Continuous outcome (e.g., PHQ-9) - Residualize x and y on covariates within train folds \u2192 rx, ry. - Correlate rx, ry (Pearson/Spearman); aggregate across folds.</p> <p>Binary outcome (e.g., MDD) - Preferred: logistic regression y ~ x + covariates; report OR, CI, p. - Optional: approximate partial correlation via residuals y \u2212 p\u0302 from covariate-only logistic.</p> <p>Report - Per-axis coefficients/correlations with CIs; FDR across multiple tests if many axes.</p>"},{"location":"integration/analysis_recipes/prediction_baselines/","title":"Prediction Baselines","text":"<p>Inputs - Gene, Brain, and Fusion = [Gene | Brain], all 512-D after preprocessing (<code>embedding_strategies.genetics_gene_fm_pca512_v1</code>, <code>embedding_strategies.smri_free_surfer_pca512_v1</code>, <code>embedding_strategies.fusion_concat_gene_brain_1024_v1</code>). - Tabular mode: raw ROI tables (<code>smri_free_surfer_raw_176</code>) and genetics PGS (<code>genetics_pgs_20traits</code>) for TabPFN / LR / GBDT baselines.</p> <p>Context in integration plan</p> <ul> <li>This recipe is the primary late-fusion baseline: compare Gene-only, Brain-only, and simple Fusion ([Gene | Brain]) features using shallow models.</li> <li>Escalate beyond this (e.g., contrastive two-tower, EI stacking, hub tokens) only if Fusion clearly and consistently outperforms both single-modality baselines.</li> <li>Keep this runbook as the reference when deciding whether more complex integration architectures are justified.</li> </ul> <p>Models - Logistic Regression   - penalty=L2, C\u2208{0.5,1,2}, solver=saga/liblinear, class_weight=balanced, max_iter=5,000. - LightGBM   - num_leaves=31, learning_rate=0.05, n_estimators=1,000 with early stopping, scale_pos_weight \u2248 N_neg/N_pos. - CatBoost   - depth=6\u20138, learning_rate=0.05, iterations=2,000 with early stopping, loss_function=Logloss, auto class weights. - TabPFN (<code>kb/model_cards/tabpfn.yaml</code>)   - Max 10k samples and 500 features per forward pass; chunk folds if N is larger.   - Use for tabular baselines (raw ROI, PGS, ROI+PGS fusion) to benchmark whether representation learning beats a tabular FM.</p> <p>Evaluation - Same CV folds across modalities. - Metrics: AUROC, AUPRC; report mean \u00b1 SD across folds. - Significance: DeLong or bootstrap for Fusion vs each single-modality on held-out predictions.</p> <p>Outputs to save - Per-fold predictions and labels for later DeLong/bootstrap and calibration checks. - Embedding/harmonization IDs used to produce each feature set (copy from experiment config metadata).</p>"},{"location":"integration/modality_features/fmri/","title":"fMRI Features","text":""},{"location":"integration/modality_features/fmri/#reference-preprocessing-stacks","title":"Reference preprocessing stacks","text":"<ul> <li>Cataloged in <code>kb/integration_cards/rsfmri_preprocessing_pipelines.yaml</code>. Default: <code>hcp_like_minimal</code> (motion + distortion correction, nuisance regression, 0.01\u20130.1\u202fHz filter, Schaefer-400 parcellation).</li> <li>Document which pipeline ID you used per run; experiments reference it alongside the embedding strategy ID.</li> </ul>"},{"location":"integration/modality_features/fmri/#subject-level-embedding-strategies","title":"Subject-level embedding strategies","text":"<ul> <li><code>rsfmri_swift_segments_v1</code> (SwiFT):</li> <li>20-frame segments \u2192 mean-pool tokens from the last hidden state \u2192 mean over segments per run \u2192 mean over runs per subject.</li> <li>Fold-wise z-score + residualize(age, sex, site/scanner, mean FD, DVARS) \u2192 PCA\u2192512.</li> <li>Source references: <code>docs/code_walkthroughs/swift_walkthrough.md</code>.</li> <li><code>rsfmri_brainlm_segments_v1</code> (BrainLM ViT-MAE):</li> <li>32-frame windows with stride 16 \u2192 CLS pooling per window \u2192 attention pooling across windows with inverse-FD weights \u2192 mean over runs.</li> <li>Fold-wise z-score + residualize(age, sex, site, FD, DVARS) \u2192 PCA\u2192512.</li> <li><code>rsfmri_brainjepa_roi_v1</code> (Brain-JEPA):</li> <li>ROI tokens (Schaefer-400 + Tian-50) \u2192 mean pooling across unmasked tokens \u2192 option to average per functional network \u2192 subject mean.</li> <li>Residualize age/sex/site/motion (optionally GSR flag) \u2192 PCA\u2192512.</li> <li><code>rsfmri_brainmt_segments_v1</code> (BrainMT):</li> <li>32-frame conv/Mamba segments \u2192 mean patch tokens per segment \u2192 run mean \u2192 subject mean.</li> <li>Residualize age/sex/site/motion \u2192 PCA\u2192512.</li> </ul> <p>All recipes live in <code>kb/integration_cards/embedding_strategies.yaml</code>; call <code>python scripts/manage_kb.py ops strategy &lt;id&gt;</code> to print the full pipeline (including preprocessing notes and sources) before launching extraction.</p>"},{"location":"integration/modality_features/fmri/#classical-atlas-baseline-optional","title":"Classical atlas baseline (optional)","text":"<ul> <li>For quick sanity checks, you can still run atlas-based functional connectivity:</li> <li>Schaefer-400 time courses \u2192 Pearson FC matrix \u2192 Fisher z \u2192 vectorize upper triangle \u2192 z-score/residualize (include motion/site covariates) \u2192 PCA to 100\u2013256 \u2192 optionally pad to 512.</li> <li>Track this by creating its own <code>embedding_strategies</code> entry if you plan to use it beyond ad-hoc QA.</li> </ul>"},{"location":"integration/modality_features/genomics/","title":"Genomics Features","text":""},{"location":"integration/modality_features/genomics/#gene-fm-embedding-genetics_gene_fm_pca512_v1","title":"Gene FM embedding (<code>genetics_gene_fm_pca512_v1</code>)","text":"<ul> <li>Models: Caduceus, Evo\u202f2, GENERaTOR, DNABERT-2 (see <code>kb/model_cards/</code>).</li> <li>RC hygiene:</li> <li>Run the encoder on forward and reverse-complement sequences; average token embeddings before pooling.</li> <li>Tokenization: maintain deterministic k-mer/BPE framing; avoid random masking for inference exports.</li> <li>Pooling hierarchy:</li> <li>Token \u2192 exon (mean or CLS).</li> <li>Exon \u2192 gene (mean, or attention if pathway-weighted).</li> <li>Gene set \u2192 subject vector (concatenate curated genes; align order with manifest).</li> <li>Covariates: residualize age, sex, ancestry PCs 1\u201310, sequencing batch.</li> <li>Dimensionality: PCA \u2192 512 (fit on train fold).</li> <li>Retrieve the latest recipe with <code>python scripts/manage_kb.py ops strategy genetics_gene_fm_pca512_v1</code>.</li> </ul>"},{"location":"integration/modality_features/genomics/#tabular-genetics-features-genetics_pgs_20traits","title":"Tabular genetics features (<code>genetics_pgs_20traits</code>)","text":"<ul> <li>20 curated UKB PGS + ancestry PCs.</li> <li>Preprocessing: mean-impute missing PGS, z-score each feature inside the train fold.</li> <li>Intended for tabular prediction baselines (including TabPFN) and for fusion with sMRI ROI tables.</li> </ul>"},{"location":"integration/modality_features/genomics/#attribution","title":"Attribution","text":"<ul> <li>Leave-one-gene-out (LOGO) \u0394AUC with Wilcoxon across folds + FDR control remains the recommended approach once embeddings feed prediction models.</li> </ul>"},{"location":"integration/modality_features/smri/","title":"sMRI Features","text":""},{"location":"integration/modality_features/smri/#baseline-embedding-smri_free_surfer_pca512_v1","title":"Baseline embedding (<code>smri_free_surfer_pca512_v1</code>)","text":"<ul> <li>Input: FreeSurfer 7.x <code>aparc.stats</code> (thickness + volume) + <code>aseg.stats</code> (~176 ROIs).</li> <li>Fold discipline:</li> <li>Train-fold z-score per feature.</li> <li>Residualize covariates: age, sex, site/scanner, intracranial volume (\u00b1SES).</li> <li>Dimensionality: PCA \u2192 512-D subject vector (fit on train fold, apply to train/test).</li> <li>Harmonization hooks: default <code>none_baseline</code>; optional <code>combat_smri</code> or <code>murd_t1_t2</code> before FreeSurfer (see <code>kb/integration_cards/harmonization_methods.yaml</code>).</li> <li>Reference the recipe via <code>python scripts/manage_kb.py ops strategy smri_free_surfer_pca512_v1</code> and log the ID inside experiment configs.</li> </ul>"},{"location":"integration/modality_features/smri/#future-extensions-to-log-as-new-strategies","title":"Future extensions to log as new strategies","text":"<ul> <li>sMRI FM encoders. Whole-volume ViTs or hub-token encoders (Brain Harmony Stage\u202f0) that emit subject embeddings directly; register as <code>smri_fm_encoder_*</code>.</li> <li>Diffusion MRI. Tract-based spatial stats or tractography metrics \u2192 z-score \u2192 residualize \u2192 PCA\u2192512.</li> <li>Tabular predictors. When skipping PCA (raw 176-D features), reference <code>smri_free_surfer_raw_176</code> and evaluate TabPFN vs. LR/GBDT for tabular baselines.</li> </ul> <p>Always pair sMRI embeddings with consistent covariates and document which harmonization method (if any) preceded ROI extraction.</p>"},{"location":"kb/","title":"KB Cards","text":"<p>We maintain lightweight, reusable \u201ccards\u201d to capture:</p> <ul> <li>Integration principles (cross-domain guidance we adopt)</li> <li>Method families (e.g., Ensemble Integration)</li> <li>External model patterns (two-tower, JEPA/hub-tokens)</li> <li>Cross-domain evaluation</li> <li>Model and dataset cards (internal)</li> </ul> <p>Templates live in docs/kb/templates/.</p>"},{"location":"kb/embedding_policies/","title":"Embedding policies","text":""},{"location":"kb/embedding_policies/#embedding-naming-and-pca-policies","title":"Embedding naming and PCA policies","text":"<p>This page documents how we name embedding strategies in <code>kb/integration_cards/embedding_strategies.yaml</code> and how we choose PCA dimensionality before locking in a strategy ID.</p>"},{"location":"kb/embedding_policies/#naming-conventions","title":"Naming conventions","text":"<ul> <li>UKB sMRI PCA embeddings</li> <li><code>smri_ukb_pca32_v1</code>, <code>smri_ukb_pca64_v1</code>, <code>smri_ukb_pca128_v1</code>, <code>smri_ukb_pca256_v1</code></li> <li> <p>All refer to PCA-compressed FreeSurfer ROI features from <code>ukb_smri_freesurfer.yaml</code>, differing only in <code>dim</code>.</p> </li> <li> <p>CHA developmental sMRI PCA embeddings</p> </li> <li><code>smri_cha_dev_pca64_v1</code>, <code>smri_cha_dev_pca128_v1</code></li> <li> <p>Both wrap the same preprocessing pipeline (age/sex adjustment, pediatric QC) on <code>cha_dev_longitudinal.yaml</code>,     differing only in PCA <code>dim</code>.</p> </li> <li> <p>Genetics / Joo embeddings</p> </li> <li><code>genetics_gene_fm_pca512_v1</code>: generic gene-FM PCA embedding for adult UKB.</li> <li><code>genetics_joo_mdd_cog_v1</code>: Prof. Joo MDD + cognition gene embeddings (Yoon BIOKDD'25-style); dimension and FM     backbone to be filled once confirmed.</li> </ul> <p>In general:</p> <ul> <li>Start IDs with the modality (<code>smri_</code>, <code>rsfmri_</code>, <code>eeg_</code>, <code>genetics_</code>, <code>behaviour_</code>).</li> <li>Follow with the cohort or context (<code>ukb</code>, <code>cha_dev</code>, etc.).</li> <li>Then append the method and key hyperparameters (<code>pca64</code>, <code>pca128</code>, <code>pgs20traits</code>, etc.).</li> <li>End with a monotonically increasing version suffix (<code>_v1</code>, <code>_v2</code>, \u2026).</li> </ul>"},{"location":"kb/embedding_policies/#pca-dimensionality-policy","title":"PCA dimensionality policy","text":"<p>When choosing PCA dims for new strategies (especially sMRI):</p> <ol> <li>Upper bound </li> <li>Let (p) = number of input features (e.g., \u2248176 sMRI ROIs) and (n) = usable subjects.  </li> <li> <p>Do not set <code>dim &gt; p</code>, and keep <code>dim \u226a n</code> (e.g., \u2264 n/2) to avoid unstable components.</p> </li> <li> <p>Variance explained check (one-off) </p> </li> <li>On a representative training fold, fit PCA and inspect the cumulative variance curve.</li> <li> <p>Record dims where cumulative variance hits ~70, 80, 90% (e.g., 64, 128, 256).</p> </li> <li> <p>Grid search with nested CV </p> </li> <li>Define a small grid:<ul> <li>UKB sMRI: <code>[32, 64, 128, 256]</code> \u2192 registered as <code>smri_ukb_pca32_v1</code>\u2026<code>pca256_v1</code>.</li> <li>CHA sMRI: <code>[64, 128]</code> (given smaller N and higher heterogeneity).</li> </ul> </li> <li>Run nested CV for the actual downstream task (prediction or CCA+permutation) and compare dims by:<ul> <li>AUROC/AUPRC for classifiers, or</li> <li>significance and stability of canonical correlations for CCA.</li> </ul> </li> <li> <p>Choose the smallest dim within the 1 SE band of the best-performing dim, and record that as the default.</p> </li> <li> <p>Hardening the choice</p> </li> <li>Once a dim is selected, promote that strategy ID (e.g., <code>smri_ukb_pca128_v1</code>) as the default in downstream      experiment configs and document the decision (date, grid, metric) in results/metadata.</li> </ol>"},{"location":"kb/embedding_policies/#policy-summary","title":"Policy summary","text":"<ul> <li>Rule of thumb: start with <code>dim = min(128, p, n/2)</code> and adjust via nested CV.</li> <li>Never change the definition of an existing <code>*_v1</code> strategy silently; instead, create <code>_v2</code> and link to the decision.</li> <li>Always tie a strategy ID back to:</li> <li>a dataset card (<code>kb/datasets/*.yaml</code>),</li> <li>a preprocessing description (this page + modality-specific docs), and</li> <li>at least one experiment config that demonstrates its use.</li> </ul>"},{"location":"kb/templates/cross_domain_eval_card/","title":"Cross domain eval card","text":"<p>title: \"Cross-Domain Evaluation\" status: draft updated: {{DATE}} tags: [evaluation]</p>"},{"location":"kb/templates/cross_domain_eval_card/#cross-domain-evaluation","title":"Cross-Domain Evaluation","text":"<p>Metrics - AUROC vs AUPRC; calibration (Brier/ECE)</p> <p>Statistical comparisons - DeLong, bootstrap, CIs</p> <p>CV/split strategy - Stratified, group/site-aware</p> <p>Leakage checklist</p> <p>Reporting template</p>"},{"location":"kb/templates/dataset_card/","title":"Dataset card","text":"<p>title: \"{{DATASET_NAME}} \u2014 Dataset Card\" status: draft updated: {{DATE}} tags: [dataset]</p>"},{"location":"kb/templates/dataset_card/#dataset_name","title":"{{DATASET_NAME}}","text":"<p>Paths and versions</p> <p>Access &amp; licensing (DUA, contacts, HF mirrors)</p> <p>Sample sizes, subset breakdowns, and inclusion criteria</p> <p>Base-pair / modality-specific statistics (e.g., total bp, TR windows)</p> <p>Modality column map (per-modality feature names + schema refs)</p> <p>Overlap logic and subject linking</p> <p>QC thresholds and preprocessing</p> <p>Linked assets - External repos used (e.g., <code>external_repos/caduceus</code>) - Walkthroughs covering extraction/validation</p> <p>Available covariates</p> <p>Notes and caveats</p>"},{"location":"kb/templates/experiment_config_stub/","title":"Experiment config stub","text":"<p>title: \"{{EXPERIMENT_NAME}} \u2014 Experiment Config\" status: draft updated: {{DATE}} tags: [experiment]</p>"},{"location":"kb/templates/experiment_config_stub/#experiment_name","title":"{{EXPERIMENT_NAME}}","text":"<p>Objective</p> <p>Datasets / splits</p> <p>Feature prep - Modality-specific transforms - Covariates/confounds handled</p> <p>Model config - Algorithms, key hyperparameters</p> <p>Evaluation - Metrics, statistical tests, logging</p> <p>Outputs to store</p> <p>Next decisions / escalation triggers</p>"},{"location":"kb/templates/external_model_pattern_card/","title":"External model pattern card","text":"<p>title: \"{{PATTERN}} \u2014 External Model Pattern\" status: draft updated: {{DATE}} tags: [pattern, multimodal]</p>"},{"location":"kb/templates/external_model_pattern_card/#pattern","title":"{{PATTERN}}","text":"<p>Core idea</p> <p>Data assumptions</p> <p>Strengths</p> <p>Limitations</p> <p>Portability to our stack (now/later)</p> <p>Engineering cost</p> <p>Triggers to adopt</p> <p>References</p>"},{"location":"kb/templates/integration_principles_card/","title":"Integration principles card","text":"<p>title: \"{{CARD_TITLE}} \u2014 Integration Principles\" status: draft updated: {{DATE}} tags: [integration, principles]</p>"},{"location":"kb/templates/integration_principles_card/#card_title","title":"{{CARD_TITLE}}","text":"<p>Citation</p> <p>One-line takeaway</p> <p>Taxonomy and trade-offs - Early / intermediate / late fusion (pros/cons)</p> <p>Cautions and failure modes - Heterogeneous semantics, alignment, missingness, over-smoothing, leakage</p> <p>Practices to adopt (bullets)</p> <p>Not adopting (for now)</p> <p>Implications for our project</p> <p>References/links</p>"},{"location":"kb/templates/method_family_card/","title":"Method family card","text":"<p>title: \"{{METHOD_FAMILY}} \u2014 Method Family\" status: draft updated: {{DATE}} tags: [method, ensembles]</p>"},{"location":"kb/templates/method_family_card/#method_family","title":"{{METHOD_FAMILY}}","text":"<p>What problem it solves</p> <p>How it works (brief) - Base learners - Stacking/meta-learner - Ensemble selection / interpretation</p> <p>When to use</p> <p>Implementation notes - Libraries - CV for stacking - Missingness handling</p> <p>Caveats</p> <p>Adoption plan</p>"},{"location":"kb/templates/model_card_template/","title":"Model card template","text":"<p>title: \"{{MODEL_NAME}} \u2014 Model Card\" status: draft updated: {{DATE}} tags: [model, {{DOMAIN}}]</p>"},{"location":"kb/templates/model_card_template/#model_name","title":"{{MODEL_NAME}}","text":"<p>Purpose &amp; scope</p> <p>Architecture &amp; inductive biases</p> <p>Inputs, tokenization, constraints</p> <p>Pooling / subject-level embedding</p> <p>Typical dims and projector guidance</p> <p>Strengths / limitations</p> <p>Representative results (relevant to us)</p> <p>Implications for our pipeline</p> <p>References/links</p>"},{"location":"models/brain/","title":"Brain Foundation Models","text":"<p>This section documents the neuroimaging foundation models used for brain representation learning in the Neuro-Omics KB. These models extract embeddings from structural MRI (sMRI), functional MRI (fMRI), and other brain imaging modalities for downstream integration with genomic data, behavioral phenotypes, and clinical outcomes.</p>"},{"location":"models/brain/#overview","title":"Overview","text":"<p>All brain FMs documented here:</p> <ul> <li>Operate on neuroimaging data (volumetric MRI, parcel time series, or raw BOLD signals)</li> <li>Support subject-level embeddings via aggregation across spatial regions or temporal windows</li> <li>Are pretrained on large multi-site datasets (UK Biobank, HCP, ABCD, etc.)</li> <li>Enable cross-modal alignment with genomic and behavioral representations</li> </ul>"},{"location":"models/brain/#model-registry","title":"Model registry","text":"Model Modality Architecture Key feature Integration role BrainLM fMRI ViT-MAE Masked autoencoding of parcel time series Primary fMRI encoder; site-robust embeddings Brain-JEPA fMRI JEPA Joint-embedding prediction; no reconstruction loss Alternative fMRI encoder; lower-latency option Brain Harmony sMRI + fMRI ViT + TAPE Multi-modal fusion via target-aware projection ensemble Cross-modal sMRI+fMRI fusion; TAPE for multi-task BrainMT sMRI (+ fMRI planned) Hybrid Mamba-Transformer Efficient long-range dependencies for 3D volumes sMRI encoder; Mamba for computational efficiency SwiFT fMRI Swin Transformer Hierarchical windows for spatiotemporal fMRI Exploratory fMRI encoder; sequence-free modeling"},{"location":"models/brain/#usage-workflow","title":"Usage workflow","text":""},{"location":"models/brain/#for-fmri-models-brainlm-brain-jepa-swift","title":"For fMRI models (BrainLM, Brain-JEPA, SwiFT)","text":"<ol> <li>Preprocess rs-fMRI: parcellation (Schaefer/AAL), bandpass filter, motion scrubbing</li> <li>Tokenize parcel time series (or 4D volumes for SwiFT)</li> <li>Embed via pretrained encoder</li> <li>Pool to subject-level representation (mean over tokens/time)</li> <li>Project to 512-D for cross-modal alignment</li> </ol>"},{"location":"models/brain/#for-smri-models-brainmt-brain-harmony","title":"For sMRI models (BrainMT, Brain Harmony)","text":"<ol> <li>Run FreeSurfer or FSL FAST for tissue segmentation</li> <li>Extract IDPs (cortical thickness, subcortical volumes) or feed raw T1w volumes</li> <li>Embed via pretrained encoder</li> <li>Pool to subject-level representation</li> <li>Project to 512-D for fusion</li> </ol>"},{"location":"models/brain/#key-considerations","title":"Key considerations","text":""},{"location":"models/brain/#sitescanner-harmonization","title":"Site/scanner harmonization","text":"<p>Multi-site pretraining (e.g., BrainLM on UKB+HCP) improves site robustness, but residualize scanner/site effects before fusion:</p> <ul> <li>Regress site dummy variables from embeddings</li> <li>Use ComBat or similar harmonization if needed (see Integration Strategy)</li> </ul>"},{"location":"models/brain/#motion-artifacts","title":"Motion artifacts","text":"<p>fMRI embeddings are sensitive to head motion. Quality control:</p> <ul> <li>Exclude high-motion frames (FD &gt; 0.5 mm)</li> <li>Regress mean FD as confound in downstream prediction</li> <li>Report motion distributions stratified by diagnosis (e.g., ADHD vs TD)</li> </ul>"},{"location":"models/brain/#multimodal-fusion","title":"Multimodal fusion","text":"<p>Brain Harmony natively fuses sMRI and fMRI via TAPE (Target-Aware Projection Ensemble). For other models, use late fusion (concatenate embeddings) or two-tower contrastive alignment (see Design Patterns).</p>"},{"location":"models/brain/#integration-targets","title":"Integration targets","text":"<p>Brain embeddings are integrated with:</p> <ul> <li>Genetics embeddings (Caduceus, DNABERT-2) for gene\u2013brain association discovery</li> <li>Behavioral phenotypes (cognitive scores, psychiatric diagnoses) via multimodal prediction</li> <li>Clinical data (longitudinal assessments, EHR records) for developmental trajectories</li> </ul> <p>See Integration Strategy for fusion protocols and modality-specific feature specs:</p> <ul> <li>Modality Features: sMRI</li> <li>Modality Features: fMRI</li> </ul>"},{"location":"models/brain/#source-repositories","title":"Source repositories","text":"<p>All brain FM source code lives in <code>external_repos/</code>:</p> <ul> <li><code>external_repos/brainlm/</code> \u2014 vandijklab/BrainLM</li> <li><code>external_repos/brainjepa/</code> \u2014 janklees/brainjepa</li> <li><code>external_repos/brainharmony/</code> \u2014 hzlab/Brain-Harmony</li> <li><code>external_repos/brainmt/</code> \u2014 arunkumar-kannan/brainmt-fmri</li> <li><code>external_repos/swift/</code> \u2014 Transconnectome/SwiFT</li> </ul> <p>Each model page includes walkthrough links to <code>docs/code_walkthroughs/</code> and structured YAML cards in <code>kb/model_cards/</code>.</p>"},{"location":"models/brain/#next-steps","title":"Next steps","text":"<ul> <li>Validate brain embedding reproducibility across cohorts (UK Biobank, Cha Hospital developmental cohort)</li> <li>Benchmark fMRI encoder stability across different parcellation schemes (Schaefer 100/200/400, AAL)</li> <li>Explore EEG/EPhys foundation models for pediatric/clinical settings (e.g., LaBraM, TBD)</li> <li>Integrate diffusion MRI embeddings for white matter microstructure (exploratory)</li> </ul>"},{"location":"models/brain/brainharmony/","title":"Brain Harmony","text":""},{"location":"models/brain/brainharmony/#overview","title":"Overview","text":"<p>Type: Multi-modal brain foundation model Architecture: ViT + TAPE (Temporal Adaptive Patch Embedding) Modalities: sMRI + fMRI (unified) Primary use: Cross-modal brain embeddings for heterogeneous cohorts</p>"},{"location":"models/brain/brainharmony/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Brain Harmony addresses a critical challenge in multi-site neuroimaging: heterogeneous TRs and scanning protocols. By introducing TAPE (Temporal Adaptive Patch Embedding), the model resizes temporal tokens to a fixed duration \u03c4, enabling unified processing of fMRI data with variable repetition times. Hub tokens fuse sMRI and fMRI modalities into a shared representation space.</p> <p>Key innovation: TAPE + hub tokens allow robust multimodal fusion even when different sites use different TR/scanner configurations.</p>"},{"location":"models/brain/brainharmony/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: Vision Transformer with TAPE for fMRI, standard patches for sMRI</li> <li>TAPE mechanism: Resizes temporal patches to fixed \u03c4 duration regardless of TR</li> <li>Hub tokens: Cross-modal attention for sMRI \u2194 fMRI fusion</li> <li>Input: T1w structural scans + parcel time series</li> <li>Output: Unified multimodal subject embeddings</li> </ul>"},{"location":"models/brain/brainharmony/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/brainharmony/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>multimodal_brain_harmony_v1</code> - Extract both sMRI and fMRI features through shared encoder - Hub tokens aggregate cross-modal information - Project to 512-D unified representation - Residualize: age, sex, site, scanner, ICV (sMRI), mean FD (fMRI)</p> <p>Fusion targets: - Gene-brain-behavior triangulation: Single unified brain vector + genomics - Multi-site robustness: Critical for UKB + Cha Hospital + ABCD combinations - Developmental trajectories: Handle TR changes across pediatric age ranges</p>"},{"location":"models/brain/brainharmony/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Brain Harmony exemplifies modality-adaptive fusion for Brain-Omics systems: - TAPE-style mechanisms can extend to other time-varying modalities (EEG, longitudinal behavior) - Hub tokens provide blueprint for cross-modal attention in gene-brain-language models - TR heterogeneity handling essential for federated Brain-Omics Model (BOM) deployment</p>"},{"location":"models/brain/brainharmony/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess sMRI \u2192 FreeSurfer / volumetric tensor\n# 2. Preprocess fMRI \u2192 parcellate + retain TR metadata\n# 3. Load pretrained Brain Harmony checkpoint\n# 4. Forward pass with TAPE temporal adaptation\n# 5. Extract hub token embeddings (not individual modality tokens)\n# 6. Project to 512-D if needed\n# 7. Log embedding_strategy ID + TR range in metadata\n</code></pre>"},{"location":"models/brain/brainharmony/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/brainharmony/#strengths","title":"Strengths","text":"<ul> <li>TR heterogeneity handling: TAPE critical for multi-site/longitudinal studies</li> <li>Multi-modal fusion: Native sMRI+fMRI joint embeddings</li> <li>Hub token architecture: Flexible attention mechanism for modality integration</li> <li>Practical engineering: Addresses real-world scanning protocol variations</li> </ul>"},{"location":"models/brain/brainharmony/#limitations","title":"Limitations","text":"<ul> <li>Higher complexity: TAPE + hub tokens increase training/inference cost</li> <li>Engineering overhead: More complex than single-modality encoders</li> <li>Limited public checkpoints: Newer model, fewer pretrained weights available</li> <li>Overkill for homogeneous cohorts: If TR is fixed, simpler models may suffice</li> </ul>"},{"location":"models/brain/brainharmony/#when-to-use-brain-harmony","title":"When to Use Brain Harmony","text":"<p>\u2705 Use when: - Combining UKB (TR=0.72s) + HCP (TR=0.72s) + Cha Hospital (TR=TBD) + ABCD (TR=0.8s) - Need both structural and functional information in single embedding - Site/scanner heterogeneity limits other approaches - Preparing for ARPA-H-style federated multimodal systems</p> <p>\u26a0\ufe0f Consider alternatives: - Late fusion (BrainLM + FreeSurfer): Simpler baseline if TR is homogeneous - BrainMT: If temporal modeling more critical than structural integration - SwiFT: For 4D volumetric approaches without explicit parcellation</p>"},{"location":"models/brain/brainharmony/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: Brain Harmony (2025) - Code walkthrough: Brain Harmony walkthrough - YAML card: <code>kb/model_cards/brainharmony.yaml</code> - Paper card: <code>kb/paper_cards/brainharmony_2025.yaml</code></p> <p>Integration recipes: - Modality Features: sMRI - Modality Features: fMRI - Integration Strategy</p> <p>Source repository: - Local: <code>external_repos/brainharmony/</code> - GitHub: hzlab/Brain-Harmony</p>"},{"location":"models/brain/brainharmony/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>TR profiling: Document TR distributions across UKB/Cha Hospital/ABCD</li> <li>Baseline comparison: Brain Harmony vs. late fusion of BrainLM+FreeSurfer</li> <li>Hub token analysis: Visualize what cross-modal patterns hub tokens capture</li> <li>Gene-multimodal-brain CCA: Test whether unified embeddings improve genetics alignment</li> <li>ARPA-H scalability: Evaluate TAPE mechanism for EEG time-varying modalities</li> </ol>"},{"location":"models/brain/brainjepa/","title":"Brain-JEPA","text":""},{"location":"models/brain/brainjepa/#overview","title":"Overview","text":"<p>Type: Joint-Embedding Predictive Architecture for fMRI Architecture: JEPA with functional gradient positioning Modality: Functional MRI (parcel time series) Primary use: Semantic-consistent subject embeddings for zero-shot and linear probing</p>"},{"location":"models/brain/brainjepa/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Brain-JEPA extends JEPA (Joint-Embedding Predictive Architecture) to fMRI by learning latent representations that predict masked brain regions without pixel-level reconstruction. The model emphasizes semantic consistency across brain states by using functional gradient positioning and spatiotemporal masking strategies (Cross-ROI, Cross-Time).</p> <p>Key innovation: Avoids reconstruction loss collapse; achieves better linear probe performance than MAE-based approaches on reported benchmarks.</p>"},{"location":"models/brain/brainjepa/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: JEPA encoder-predictor with functional gradient positional encoding</li> <li>Input: Parcel time series (ROI \u00d7 timepoints)</li> <li>Pretraining: Predict latent representations of masked regions/timeframes</li> <li>Masking: Cross-ROI (spatial) and Cross-Time (temporal) strategies</li> <li>Output: Token latents \u2192 pooled to compact subject vectors</li> </ul>"},{"location":"models/brain/brainjepa/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/brainjepa/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>rsfmri_brainjepa_roi_v1</code> - Extract token latents from pretrained encoder (no reconstruction decoder) - Pool latent tokens \u2192 subject-level embedding - Project to 512-D for downstream tasks - Residualize: age, sex, site, mean FD</p> <p>Fusion targets: - Gene-brain alignment: Late fusion with genomic embeddings (Caduceus, Evo2) - Behavioral prediction: Cognitive scores, psychiatric diagnoses - Zero-shot transfer: Leverage semantic consistency for unseen tasks</p>"},{"location":"models/brain/brainjepa/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Brain-JEPA provides lower-latency fMRI encoding compared to full autoencoding: - No reconstruction decoder \u2192 faster inference for large-scale screening - Semantic latents align well with language/vision embeddings in multimodal hubs - Functional gradient positioning preserves anatomical relationships for cross-modal reasoning</p>"},{"location":"models/brain/brainjepa/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess fMRI \u2192 parcellate (standard atlas)\n# 2. Load pretrained Brain-JEPA encoder (not predictor/decoder)\n# 3. Forward pass \u2192 extract token latents\n# 4. Pool (mean/attention) \u2192 subject embedding\n# 5. Optional: Apply harmonization before projection\n# 6. Log embedding_strategy ID: rsfmri_brainjepa_roi_v1\n</code></pre>"},{"location":"models/brain/brainjepa/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/brainjepa/#strengths","title":"Strengths","text":"<ul> <li>Better linear probing: Reported improvements over MAE on cognitive/behavioral tasks</li> <li>Lower inference cost: No reconstruction decoder needed at embedding extraction time</li> <li>Semantic consistency: Latent predictions enforce functional coherence</li> <li>Interpretability: Functional gradient positioning maintains anatomical structure</li> </ul>"},{"location":"models/brain/brainjepa/#limitations","title":"Limitations","text":"<ul> <li>Heavier engineering: JEPA training more complex than standard MAE</li> <li>Less mature ecosystem: Fewer public checkpoints vs. BrainLM</li> <li>Requires careful masking: Cross-ROI/Time strategies need domain expertise</li> <li>Limited long-context claims: Not explicitly designed for ultra-long temporal dependencies</li> </ul>"},{"location":"models/brain/brainjepa/#when-to-use-brain-jepa","title":"When to Use Brain-JEPA","text":"<p>\u2705 Use when: - Need semantic consistency for zero-shot/few-shot tasks - Want faster inference than full autoencoding models - Prioritize linear probe performance over reconstruction fidelity</p> <p>\u26a0\ufe0f Consider alternatives: - BrainLM: More mature, extensive benchmarks, simpler architecture - BrainMT: For long-range temporal modeling with Mamba blocks - Brain Harmony: Multi-modal sMRI+fMRI fusion - SwiFT: 4D volume input without parcellation</p>"},{"location":"models/brain/brainjepa/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: Brain-JEPA (2024) - Code walkthrough: Brain-JEPA walkthrough - YAML card: <code>kb/model_cards/brainjepa.yaml</code> - Paper card: <code>kb/paper_cards/brainjepa_2024.yaml</code></p> <p>Integration recipes: - Modality Features: fMRI - Integration Strategy - Design Patterns</p> <p>Source repository: - Local: <code>external_repos/brainjepa/</code> - GitHub: janklees/brainjepa</p>"},{"location":"models/brain/brainjepa/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Benchmark vs. BrainLM: Compare linear probe performance on UKB cognitive tasks</li> <li>Latency profiling: Quantify inference speedup vs. full MAE reconstruction</li> <li>Gene-brain fusion: Test whether semantic latents improve CCA with genomic features</li> <li>Zero-shot evaluation: Assess transfer to Cha Hospital developmental cohort</li> <li>Multimodal alignment: Explore projection into shared LLM embedding space</li> </ol>"},{"location":"models/brain/brainlm/","title":"BrainLM","text":""},{"location":"models/brain/brainlm/#overview","title":"Overview","text":"<p>Type: Self-supervised foundation model for fMRI Architecture: Vision Transformer with Masked Autoencoding (ViT-MAE) Modality: Functional MRI (parcel time series) Primary use: Subject-level embeddings for downstream prediction tasks</p>"},{"location":"models/brain/brainlm/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>BrainLM applies masked autoencoding to fMRI parcel time series, learning site-invariant brain representations through large-scale multi-cohort pretraining (UK Biobank + HCP). The model reconstructs masked parcels across time, forcing the encoder to capture functional relationships and temporal dynamics without relying on task-specific supervision.</p> <p>Key innovation: Site-robust pretraining enables strong linear probe performance and generalization across diverse cohorts.</p>"},{"location":"models/brain/brainlm/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: ViT-MAE with spatial-temporal masking</li> <li>Input: Parcel time series (e.g., Schaefer-400 @ TR=0.72s)</li> <li>Pretraining: Mask random parcels/timepoints \u2192 reconstruct from latent tokens</li> <li>Output: Subject-level embeddings via mean pooling over latent tokens</li> </ul>"},{"location":"models/brain/brainlm/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/brainlm/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>rsfmri_brainlm_segments_v1</code> - Extract latent embeddings from pretrained encoder - Mean pool over time/tokens \u2192 subject vector - Project to 512-D for cross-modal alignment - Residualize: age, sex, site, mean FD, tSNR</p> <p>Fusion targets: - Gene-brain associations: Late fusion with Caduceus/DNABERT-2 embeddings - Behavioral prediction: MDD, fluid intelligence, cognitive composites - Developmental trajectories: Longitudinal cohorts (Cha Hospital, ABCD)</p>"},{"location":"models/brain/brainlm/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>BrainLM serves as a brain modality encoder in larger multimodal systems: - Embeddings can be projected into shared LLM/VLM spaces for cross-modal reasoning - Site-robust features critical for federated/multi-institution Brain-Omics Models - Natural baseline before escalating to multimodal encoders (Brain Harmony, BrainMT)</p>"},{"location":"models/brain/brainlm/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess fMRI \u2192 parcellate (Schaefer-400)\n# 2. Load pretrained BrainLM checkpoint\n# 3. Extract latent tokens (no masking during inference)\n# 4. Pool to subject vector\n# 5. Apply harmonization (ComBat/MURD) if needed\n# 6. Log embedding strategy ID in experiment config\n</code></pre>"},{"location":"models/brain/brainlm/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/brainlm/#strengths","title":"Strengths","text":"<ul> <li>Multi-site robustness: Pretraining on UKB+HCP reduces site effects</li> <li>Strong baselines: High linear probe accuracy on cognitive/behavioral tasks</li> <li>Computational efficiency: ViT inference faster than recurrent/SSM alternatives</li> <li>Well-documented: Extensive benchmarks vs. classical FC approaches</li> </ul>"},{"location":"models/brain/brainlm/#limitations","title":"Limitations","text":"<ul> <li>Requires parcellation: No raw 4D volume support (unlike SwiFT/BrainMT)</li> <li>Fixed TR assumption: Variable TR cohorts need TAPE-style adaptation</li> <li>Embedding interpretability: Latent space less directly tied to functional networks than FC matrices</li> </ul>"},{"location":"models/brain/brainlm/#when-to-use-brainlm","title":"When to Use BrainLM","text":"<p>\u2705 Use when: - Starting fMRI integration baselines (Option B in Nov 2025 plan) - Need site-robust features across UKB/HCP/developmental cohorts - Want efficient inference for large-N experiments</p> <p>\u26a0\ufe0f Consider alternatives: - Brain-JEPA: Lower latency, better semantic consistency claims - Brain Harmony: Multi-modal sMRI+fMRI fusion with TAPE for TR heterogeneity - BrainMT: Long-range temporal dependencies via Mamba blocks - SwiFT: 4D volume input without explicit parcellation</p>"},{"location":"models/brain/brainlm/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: BrainLM (2024) - Code walkthrough: BrainLM walkthrough - YAML card: <code>kb/model_cards/brainlm.yaml</code> (detailed architecture specs) - Paper card: <code>kb/paper_cards/brainlm_2024.yaml</code> (structured takeaways)</p> <p>Integration recipes: - Modality Features: fMRI - Integration Strategy - CCA + Permutation Recipe</p> <p>Source repository: - Local: <code>external_repos/brainlm/</code> - GitHub: vandijklab/BrainLM</p>"},{"location":"models/brain/brainlm/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Validate extraction: Ensure consistent embeddings across UKB/Cha Hospital cohorts</li> <li>Benchmark stability: Test across different parcellation schemes (Schaefer 100/200/400)</li> <li>Gene-brain CCA: Align BrainLM embeddings with Caduceus gene vectors</li> <li>Fusion experiments: Compare late fusion vs. two-tower contrastive alignment</li> <li>Developmental extension: Adapt to pediatric fMRI (shorter scans, higher motion)</li> </ol>"},{"location":"models/brain/brainmt/","title":"BrainMT","text":""},{"location":"models/brain/brainmt/#overview","title":"Overview","text":"<p>Type: Hybrid State Space + Transformer for fMRI Architecture: Mamba blocks + Multi-Head Self-Attention (Hybrid SSM-Transformer) Modality: Functional MRI (3D volumes or parcels) Primary use: Long-range temporal dependency modeling with computational efficiency</p>"},{"location":"models/brain/brainmt/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>BrainMT fuses bidirectional Mamba blocks (State Space Models with temporal-first scanning) with Transformer attention to model long-range fMRI dependencies more efficiently than pure transformers. The architecture targets multitask learning across fluid intelligence regression, sex classification, and harmonization tasks on UKB/HCP cohorts.</p> <p>Key innovation: Mamba's sub-quadratic complexity enables processing longer temporal sequences (\u2265200 frames) without the memory explosion of full attention.</p>"},{"location":"models/brain/brainmt/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Hybrid blocks: Bidirectional Mamba (temporal scanning) + MHSA (global attention)</li> <li>Patch embedding: 3D Conv \u2192 flatten \u2192 linear projection</li> <li>Temporal modeling: Mamba handles sequence dependencies; attention captures global structure</li> <li>Multitask heads: Shared encoder \u2192 task-specific prediction heads</li> <li>Training: Requires fused CUDA kernels (Mamba-ssm library)</li> </ul>"},{"location":"models/brain/brainmt/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/brainmt/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>rsfmri_brainmt_segments_v1</code> - Extract embeddings from shared encoder (before task heads) - Mean pool over sequence length \u2192 subject vector - Project to 512-D for downstream fusion - Residualize: age, sex, site, mean FD - Metadata requirement: Log sequence length (BrainMT performance depends on context \u2265200)</p> <p>Fusion targets: - Long-context gene-brain alignment: When temporal dynamics critical (e.g., task fMRI) - Developmental trajectories: Pediatric longitudinal fMRI with evolving patterns - Multitask prediction: Joint cognitive + diagnostic tasks</p>"},{"location":"models/brain/brainmt/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>BrainMT demonstrates efficient long-context modeling for multimodal systems: - Mamba architecture adaptable to other sequential modalities (EEG, longitudinal assessments) - Hybrid SSM-Transformer paradigm balances efficiency vs. expressiveness - Multitask framework aligns with Brain-Omics Model (BOM) joint training over diverse phenotypes</p>"},{"location":"models/brain/brainmt/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess fMRI \u2192 3D volumes or parcels (\u2265200 frames preferred)\n# 2. Load pretrained BrainMT checkpoint\n# 3. Forward through encoder (Mamba blocks + MHSA layers)\n# 4. Extract pre-head embeddings (not task-specific outputs)\n# 5. Pool to subject-level vector\n# 6. Log: sequence_length, mamba_config, embedding_strategy_id\n</code></pre>"},{"location":"models/brain/brainmt/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/brainmt/#strengths","title":"Strengths","text":"<ul> <li>Efficient long-context: Mamba scales sub-quadratically vs. full attention</li> <li>Multitask learning: Single encoder serves multiple downstream tasks</li> <li>Hybrid architecture: Balances local temporal patterns (Mamba) + global structure (attention)</li> <li>Benchmarked on UKB/HCP: Published results on fluid intelligence and sex classification</li> </ul>"},{"location":"models/brain/brainmt/#limitations","title":"Limitations","text":"<ul> <li>Heavy dependencies: Requires Mamba-ssm CUDA kernels (custom build)</li> <li>Training complexity: Hybrid architecture harder to debug than pure ViT</li> <li>Checkpoint availability: Fewer public pretrained weights vs. BrainLM</li> <li>Overkill for short sequences: &lt;200 frames may not fully leverage Mamba's strengths</li> </ul>"},{"location":"models/brain/brainmt/#when-to-use-brainmt","title":"When to Use BrainMT","text":"<p>\u2705 Use when: - Need long-context modeling (task fMRI, naturalistic viewing) - Multitask setting with shared encoder across cognitive/diagnostic tasks - Want efficiency gains over pure Transformer for \u2265200 frame sequences - Exploring SSM architectures for neuro-omics applications</p> <p>\u26a0\ufe0f Defer until: - BrainLM/Brain-JEPA baselines exhausted (per Nov 2025 integration plan) - Engineering resources available for custom kernel setup - Sufficient GPU memory for hybrid block training/inference</p> <p>\u26a0\ufe0f Consider alternatives: - BrainLM: Simpler baseline, more mature ecosystem - Brain-JEPA: Faster inference, better for semantic consistency - SwiFT: 4D volumes without explicit sequence modeling - Brain Harmony: Multi-modal sMRI+fMRI fusion</p>"},{"location":"models/brain/brainmt/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: BrainMT (2025) \u2014 Conference paper (LNCS, pp. 150\u2013160) - Code walkthrough: BrainMT walkthrough - YAML card: <code>kb/model_cards/brainmt.yaml</code> - Paper card: <code>kb/paper_cards/brainmt_2025.yaml</code></p> <p>Integration recipes: - Modality Features: fMRI - Integration Strategy - Design Patterns</p> <p>Source repository: - Local: <code>external_repos/brainmt/</code> - GitHub: arunkumar-kannan/brainmt-fmri</p>"},{"location":"models/brain/brainmt/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Baseline comparisons: BrainMT vs. BrainLM on UKB cognitive tasks (same train/test splits)</li> <li>Sequence length ablation: Test performance vs. context length (100, 200, 400 frames)</li> <li>Gene-brain alignment: Evaluate whether long-context embeddings improve genetics CCA</li> <li>Developmental extension: Adapt to pediatric fMRI (Cha Hospital, ABCD)</li> <li>SSM exploration: Investigate Mamba-style architectures for EEG/EPhys modalities</li> </ol>"},{"location":"models/brain/brainmt/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Capture masking ratio and sequence length in metadata for reproducibility</li> <li>Multitask heads are task-specific; extract shared encoder embeddings for fusion</li> <li>When exporting weights, ensure Mamba kernel version compatibility across systems</li> </ul>"},{"location":"models/brain/swift/","title":"SwiFT","text":""},{"location":"models/brain/swift/#overview","title":"Overview","text":"<p>Type: Spatiotemporal foundation model for fMRI Architecture: Swin Transformer (hierarchical windows) Modality: Functional MRI (4D volumes) Primary use: Direct 4D volume encoding without explicit parcellation</p>"},{"location":"models/brain/swift/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>SwiFT (Swin Transformer for fMRI Time series) applies hierarchical windowed attention to 4D fMRI volumes, eliminating the need for explicit parcellation while capturing spatiotemporal patterns across multiple scales. The model processes raw BOLD signals through cascaded Swin blocks, enabling direct learning from volumetric data.</p> <p>Key innovation: Sequence-free 4D modeling with hierarchical attention windows preserves fine-grained spatial structure while capturing temporal dynamics.</p>"},{"location":"models/brain/swift/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: 4D Swin Transformer with shifted windows</li> <li>Input: Raw BOLD volumes (X \u00d7 Y \u00d7 Z \u00d7 T)</li> <li>Windowing: Hierarchical 4D patches with local/global attention</li> <li>No parcellation: Learns spatial structure end-to-end</li> <li>Output: Subject-level embeddings via global pooling or CLS token</li> </ul>"},{"location":"models/brain/swift/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/swift/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>rsfmri_swift_segments_v1</code> - Process 4D volumes through Swin blocks (typically 20-frame segments) - Extract final layer representations - Pool across spatial-temporal dimensions \u2192 subject vector - Project to 512-D for cross-modal alignment - Residualize: age, sex, site, mean FD</p> <p>Fusion targets: - Gene-brain associations: When fine-grained spatial patterns matter - Atlasing-free analysis: Avoid parcellation scheme dependence - Multi-resolution modeling: Capture both local and global brain dynamics</p>"},{"location":"models/brain/swift/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>SwiFT's hierarchical 4D processing offers advantages for Brain-Omics systems: - No parcellation bias \u2192 better cross-site generalization - Multi-scale attention aligns with hierarchical biological organization - 4D paradigm extensible to other volumetric time series (perfusion imaging, DCE-MRI) - Can serve as blueprint for spatiotemporal EEG source reconstruction</p>"},{"location":"models/brain/swift/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess fMRI \u2192 motion correction, normalization (no parcellation)\n# 2. Segment into overlapping 4D windows (e.g., 20-frame chunks)\n# 3. Load pretrained SwiFT checkpoint\n# 4. Forward pass through Swin blocks\n# 5. Extract global representation (CLS token or spatial average)\n# 6. Aggregate across segments \u2192 subject embedding\n# 7. Log: window_size, stride, preprocessing_pipeline_id\n</code></pre>"},{"location":"models/brain/swift/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/swift/#strengths","title":"Strengths","text":"<ul> <li>No parcellation required: Learns spatial structure end-to-end</li> <li>Multi-scale processing: Hierarchical windows capture local and global patterns</li> <li>Strong performance: Reported competitive results vs. parcellation-based methods</li> <li>Parcellation-agnostic: No bias from atlas choice (Schaefer vs. AAL vs. Gordon)</li> </ul>"},{"location":"models/brain/swift/#limitations","title":"Limitations","text":"<ul> <li>Computational cost: 4D convolutions and windowed attention memory-intensive</li> <li>Longer training: Hierarchical architecture requires more epochs to converge</li> <li>Preprocessing critical: Motion and spatial normalization quality directly impact performance</li> <li>GPU memory: Full 4D volumes with fine temporal resolution may exceed typical GPU limits</li> </ul>"},{"location":"models/brain/swift/#when-to-use-swift","title":"When to Use SwiFT","text":"<p>\u2705 Use when: - Want to avoid parcellation scheme dependence - Need fine-grained spatial analysis (subcortical structures, small nuclei) - Have sufficient compute for 4D volume processing - Exploring multi-resolution spatiotemporal patterns</p> <p>\u26a0\ufe0f Consider alternatives: - BrainLM/Brain-JEPA: If parcellation acceptable and want faster baselines - BrainMT: For longer temporal contexts with lower memory footprint - Brain Harmony: Multi-modal sMRI+fMRI fusion with TAPE</p>"},{"location":"models/brain/swift/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: Pending full KB curation \u2014 see upstream paper and repo README - Code walkthrough: SwiFT walkthrough - YAML card: <code>kb/model_cards/swift.yaml</code> - Paper card: TBD (add to <code>kb/paper_cards/swift_2024.yaml</code> after curation)</p> <p>Integration recipes: - Modality Features: fMRI - Integration Strategy - Preprocessing Pipelines</p> <p>Source repository: - Local: <code>external_repos/swift/</code> - GitHub: Transconnectome/SwiFT</p>"},{"location":"models/brain/swift/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Parcellation comparison: SwiFT vs. BrainLM (Schaefer-400) on same UKB cognitive tasks</li> <li>Memory profiling: Document GPU requirements across different volume resolutions</li> <li>Preprocessing sensitivity: Test robustness to motion correction/spatial normalization choices</li> <li>Gene-brain fusion: Evaluate whether 4D embeddings improve genetics alignment</li> <li>Developmental adaptation: Assess performance on pediatric datasets with smaller brain volumes</li> </ol>"},{"location":"models/brain/swift/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Segment long scans into overlapping windows to fit GPU memory</li> <li>Log window size, stride, and overlap for reproducibility</li> <li>Spatial normalization quality critical \u2014 consider using MURD/ComBat preprocessing</li> <li>When comparing to parcellation-based models, ensure fair preprocessing parity</li> </ul>"},{"location":"models/genetics/","title":"Genetics Foundation Models","text":"<p>This section documents the DNA sequence foundation models used for genomic representation learning in the Neuro-Omics KB. These models are used to extract gene-level embeddings from raw genomic sequences (DNA/RNA) for downstream integration with brain imaging, behavioral phenotypes, and clinical outcomes.</p>"},{"location":"models/genetics/#overview","title":"Overview","text":"<p>All genetics FMs documented here:</p> <ul> <li>Operate on nucleotide sequences (A, C, G, T) rather than pre-computed variant calls or SNP arrays</li> <li>Support gene-level embeddings via forward/reverse-complement (RC) averaging and pooling strategies</li> <li>Enable interpretability through attribution methods like LOGO \u0394AUC</li> <li>Are pretrained on large genomic corpora (human reference genomes, multi-species datasets, or RefSeq)</li> </ul>"},{"location":"models/genetics/#model-registry","title":"Model registry","text":"Model Architecture Key feature Integration role Caduceus Mamba (BiMamba) + RC-equivariance Strand-robust, efficient long-context Primary gene encoder for UK Biobank WES DNABERT-2 BERT (multi-species pretraining) BPE tokenization, cross-species transfer Alternative gene encoder; comparison baseline Evo 2 StripedHyena (1M context) Ultra-long-range dependencies Exploratory; regulatory region capture GENERator Generative 6-mer LM Generative modeling, sequence design Reference for generative vs discriminative tradeoffs"},{"location":"models/genetics/#usage-workflow","title":"Usage workflow","text":"<ol> <li>Extract sequences from reference genome (hg38) for target genes</li> <li>Tokenize using model-specific vocabularies (k-mers, BPE, or single-nucleotide)</li> <li>Embed forward and reverse-complement sequences</li> <li>Pool to gene-level representation (mean/CLS depending on model)</li> <li>Project to 512-D for cross-modal alignment with brain embeddings</li> </ol>"},{"location":"models/genetics/#key-considerations","title":"Key considerations","text":""},{"location":"models/genetics/#rc-equivariance","title":"RC-equivariance","text":"<p>DNA has no inherent directionality; models like Caduceus enforce BiMamba RC-equivariance to avoid strand bias. For non-equivariant models, manually average forward and RC embeddings.</p>"},{"location":"models/genetics/#variant-handling","title":"Variant handling","text":"<p>Foundation models operate on reference sequences by default. To incorporate subject-specific variants:</p> <ul> <li>Patch reference with VCF alleles</li> <li>Re-embed variant sequences</li> <li>Compare \u0394AUC between reference and variant embeddings (exploratory)</li> </ul>"},{"location":"models/genetics/#attribution","title":"Attribution","text":"<p>Use LOGO (Leave-One-Gene-Out) \u0394AUC to assess which genes contribute most to downstream prediction tasks (e.g., MDD risk, cognitive scores). See Yoon et al. BioKDD 2025 for protocol details.</p>"},{"location":"models/genetics/#integration-targets","title":"Integration targets","text":"<p>Genetics embeddings are integrated with:</p> <ul> <li>sMRI IDPs (structural phenotypes) via CCA, late fusion, or contrastive alignment</li> <li>fMRI embeddings (e.g., BrainLM, Brain-JEPA) for gene\u2013brain\u2013behaviour triangulation</li> <li>Behavioral phenotypes (cognitive scores, psychiatric diagnoses) via multimodal prediction</li> </ul> <p>See Integration Strategy for fusion protocols and Modality Features: Genomics for preprocessing specs.</p>"},{"location":"models/genetics/#source-repositories","title":"Source repositories","text":"<p>All genetics FM source code lives in <code>external_repos/</code>:</p> <ul> <li><code>external_repos/caduceus/</code> \u2014 kuleshov-group/caduceus</li> <li><code>external_repos/dnabert2/</code> \u2014 Zhihan1996/DNABERT2</li> <li><code>external_repos/evo2/</code> \u2014 ArcInstitute/evo2</li> <li><code>external_repos/generator/</code> \u2014 GenerTeam/GENERator</li> </ul> <p>Each model page includes walkthrough links to <code>docs/code_walkthroughs/</code> and structured YAML cards in <code>kb/model_cards/</code>.</p>"},{"location":"models/genetics/#next-steps","title":"Next steps","text":"<ul> <li>Validate gene embedding reproducibility across cohorts (UK Biobank WES, Cha Hospital panel sequencing)</li> <li>Benchmark LOGO \u0394AUC stability under different embedding projection dimensions (256, 512, 1024)</li> <li>Explore regulatory region embeddings (enhancers, promoters) with long-context models like Evo 2</li> </ul>"},{"location":"models/genetics/caduceus/","title":"Caduceus","text":""},{"location":"models/genetics/caduceus/#overview","title":"Overview","text":"<p>Type: RC-equivariant DNA foundation model Architecture: BiMamba (bidirectional Mamba) + Hyena Modality: Nucleotide sequences (DNA/RNA) Primary use: Strand-robust gene-level embeddings for multimodal fusion</p>"},{"location":"models/genetics/caduceus/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Caduceus enforces reverse-complement (RC) equivariance through bidirectional Mamba/Hyena layers, ensuring embeddings are invariant to DNA strand orientation. This addresses a fundamental biological constraint: DNA has no inherent directionality, yet many language model architectures introduce strand bias. Caduceus learns sequence grammar while respecting this symmetry.</p> <p>Key innovation: RC-equivariance as architectural constraint (not post-hoc averaging) \u2192 more parameter-efficient and biologically principled.</p>"},{"location":"models/genetics/caduceus/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: BiMamba (Mamba blocks with bidirectional RC scanning) or RC-augmented Hyena</li> <li>RC enforcement: Built into attention/convolution layers</li> <li>Input: Raw nucleotide sequences (A, C, G, T) with k-mer or single-base tokenization</li> <li>Pretraining: Masked language modeling on large genomic corpora (human + multi-species)</li> <li>Output: Per-position embeddings \u2192 gene-level via mean pooling</li> </ul>"},{"location":"models/genetics/caduceus/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/caduceus/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>genetics_gene_fm_pca512_v1</code> (Caduceus variant) - Extract gene sequences from reference genome (hg38) - Tokenize with Caduceus vocabulary (typically 4-mer or 6-mer) - Forward pass \u2192 per-nucleotide embeddings - RC hygiene: Caduceus natively RC-equivariant, but verify with sanity check (forward == RC) - Mean pool over gene length \u2192 gene-level vector - Concatenate target gene set (e.g., 38 MDD genes from Yoon et al.) - Project to 512-D via PCA - Residualize: age, sex, ancestry PCs (1-10), batch</p> <p>Fusion targets: - Gene-brain CCA: Align with BrainLM/Brain-JEPA embeddings - LOGO attribution: Leave-one-gene-out \u0394AUC for gene importance (Yoon et al. protocol) - Variant impact: Compare reference vs. subject-specific sequences (exploratory)</p>"},{"location":"models/genetics/caduceus/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Caduceus provides strand-robust genetic representations for Brain-Omics systems: - RC-equivariance critical when sequences are sampled from forward or reverse strands - Gene embeddings can be projected into shared LLM/VLM spaces for cross-modal reasoning - Efficient Mamba architecture scales to whole-genome or regulatory region analysis - Natural encoder for \"genetic modality\" in unified multimodal Brain-Omics Model (BOM)</p>"},{"location":"models/genetics/caduceus/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Extract gene sequences from hg38 reference (GENCODE annotations)\n# 2. Tokenize with Caduceus vocabulary (e.g., 6-mer overlapping)\n# 3. Load pretrained Caduceus checkpoint\n# 4. Forward pass \u2192 per-position embeddings\n# 5. Verify RC equivariance (optional but recommended):\n#    embed(seq) \u2248 embed(reverse_complement(seq))\n# 6. Mean pool over gene \u2192 gene-level vector\n# 7. Concatenate gene set \u2192 subject genotype embedding\n# 8. Log: gene_list, reference_version, embedding_strategy_id\n</code></pre>"},{"location":"models/genetics/caduceus/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/genetics/caduceus/#strengths","title":"Strengths","text":"<ul> <li>RC-equivariant by design: No manual averaging needed</li> <li>Parameter efficient: Mamba/Hyena scale better than full attention for long sequences</li> <li>Strong benchmarks: Competitive performance on regulatory element prediction, variant effect</li> <li>Interpretable: Attention/conv patterns respect biological constraints</li> </ul>"},{"location":"models/genetics/caduceus/#limitations","title":"Limitations","text":"<ul> <li>Requires RC-aware tokenization: Some vocabularies break RC symmetry (use carefully)</li> <li>Limited to reference sequences: Variant handling requires re-embedding (computationally expensive)</li> <li>Checkpoint availability: Fewer pretrained scales vs. DNABERT-2 or ESM-style models</li> <li>K-mer choice matters: Different tokenizations yield different embedding quality</li> </ul>"},{"location":"models/genetics/caduceus/#when-to-use-caduceus","title":"When to Use Caduceus","text":"<p>\u2705 Use when: - Need strand-robust gene embeddings for UKB/Cha Hospital genetics - Prioritizing parameter efficiency for long sequences (&gt;10kb genes) - Want architectural RC-equivariance (not post-hoc correction) - Implementing LOGO attribution (Yoon et al. protocol)</p> <p>\u26a0\ufe0f Consider alternatives: - DNABERT-2: BPE tokenization, more public checkpoints, cross-species pretraining - Evo2: Ultra-long context (1M tokens) for regulatory regions - GENERator: Generative modeling if sequence design is goal</p>"},{"location":"models/genetics/caduceus/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: Caduceus (2024) \u2014 arXiv:2403.03234 - Code walkthrough: Caduceus walkthrough - YAML card: <code>kb/model_cards/caduceus.yaml</code> - Paper card: <code>kb/paper_cards/caduceus_2024.yaml</code></p> <p>Integration recipes: - Modality Features: Genomics - Integration Strategy - CCA + Permutation - LOGO Attribution (experiment config)</p> <p>Source repository: - Local: <code>external_repos/caduceus/</code> - GitHub: kuleshov-group/caduceus</p>"},{"location":"models/genetics/caduceus/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>UKB WES extraction: Embed 38 MDD genes + cognition-related genes from UK Biobank</li> <li>RC verification: Sanity check embed(seq) == embed(RC(seq)) on test genes</li> <li>Gene-brain CCA: Align Caduceus embeddings with BrainLM fMRI vectors</li> <li>LOGO protocol: Implement leave-one-gene-out \u0394AUC (Yoon et al. BioKDD'25)</li> <li>Variant exploration: Test impact of subject-specific SNPs on embeddings (pilot)</li> </ol>"},{"location":"models/genetics/caduceus/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Always log k-mer size and tokenization strategy in metadata</li> <li>Verify RC-equivariance on held-out genes before scaling to full cohort</li> <li>When comparing to DNABERT-2, use same gene set and reference version</li> <li>For attribution: LOGO requires nested CV to avoid leakage (see Yoon et al. protocol)</li> </ul>"},{"location":"models/genetics/dnabert2/","title":"DNABERT-2","text":""},{"location":"models/genetics/dnabert2/#overview","title":"Overview","text":"<p>Type: BERT-style DNA foundation model Architecture: BERT with BPE tokenization Modality: Nucleotide sequences (DNA/RNA) Primary use: Cross-species transfer and multi-task gene embeddings</p>"},{"location":"models/genetics/dnabert2/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>DNABERT-2 applies Byte-Pair Encoding (BPE) tokenization to DNA sequences, enabling flexible vocabulary that adapts to sequence statistics. Pretrained on multi-species genomic data, it excels at cross-species transfer and captures evolutionary conservation patterns. Unlike k-mer tokenizers, BPE can learn biologically meaningful subword units (e.g., regulatory motifs, repeat elements).</p> <p>Key innovation: BPE tokenization for genomics + multi-species pretraining \u2192 strong zero-shot transfer to understudied organisms.</p>"},{"location":"models/genetics/dnabert2/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: BERT encoder (bidirectional transformer)</li> <li>Tokenization: BPE vocabulary learned from multi-species corpus</li> <li>Pretraining: Masked language modeling across human + model organisms</li> <li>Context: Typically 512-1024 tokens (depends on checkpoint)</li> <li>Output: Per-token embeddings \u2192 aggregated to gene/region level</li> </ul>"},{"location":"models/genetics/dnabert2/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/dnabert2/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>genetics_gene_fm_pca512_v1</code> (DNABERT-2 variant) - Extract gene sequences from hg38 reference genome - Tokenize with BPE: Use pretrained DNABERT-2 tokenizer (maintain frame awareness) - Forward pass \u2192 per-token embeddings - RC handling: DNABERT-2 not RC-equivariant \u2192 manually average forward and RC embeddings - Pool to gene level (mean or CLS token, validate stability) - Concatenate target gene set - Project to 512-D via PCA - Residualize: age, sex, ancestry PCs, batch</p> <p>Fusion targets: - Gene-brain alignment: Late fusion with brain FM embeddings - Comparison baseline: DNABERT-2 vs. Caduceus RC-equivariance impact - Cross-species validation: Test on mouse/primate orthologs (exploratory)</p>"},{"location":"models/genetics/dnabert2/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>DNABERT-2 provides flexible tokenization for Brain-Omics systems: - BPE adapts to different genomic contexts (coding, regulatory, non-coding) - Multi-species pretraining enables cross-organism comparison (animal models \u2192 human) - Can serve as genetic encoder in unified multimodal architectures - BPE paradigm extensible to other biological sequences (proteins, chromatin states)</p>"},{"location":"models/genetics/dnabert2/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Extract gene sequences (hg38 reference, GENCODE annotations)\n# 2. Tokenize with DNABERT-2 BPE tokenizer\n# 3. Load pretrained checkpoint (e.g., zhihan1996/DNABERT-2-117M)\n# 4. Forward pass \u2192 extract token embeddings\n# 5. **RC correction:** Embed reverse-complement, average with forward\n# 6. Pool tokens \u2192 gene vector (test mean vs. CLS stability)\n# 7. Concatenate gene set \u2192 subject embedding\n# 8. Log: tokenizer_version, pooling_strategy, rc_averaged\n</code></pre>"},{"location":"models/genetics/dnabert2/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/genetics/dnabert2/#strengths","title":"Strengths","text":"<ul> <li>Adaptive tokenization: BPE learns biologically relevant subwords</li> <li>Cross-species transfer: Strong zero-shot performance on new organisms</li> <li>Public checkpoints: Well-supported on Hugging Face (zhihan1996/DNABERT-2-117M)</li> <li>Mature ecosystem: Compatible with transformers library, easy deployment</li> </ul>"},{"location":"models/genetics/dnabert2/#limitations","title":"Limitations","text":"<ul> <li>Not RC-equivariant: Requires manual forward/RC averaging (compute overhead)</li> <li>Tokenization complexity: BPE can introduce subtle biases if not carefully applied</li> <li>Frame shifts: BPE boundaries may not respect codon structure (issue for coding sequences)</li> <li>Longer inference: BERT attention quadratic in sequence length</li> </ul>"},{"location":"models/genetics/dnabert2/#when-to-use-dnabert-2","title":"When to Use DNABERT-2","text":"<p>\u2705 Use when: - Need comparison baseline vs. RC-equivariant models (Caduceus) - Want cross-species transfer capabilities - Prefer mature Hugging Face ecosystem - Exploring BPE tokenization for regulatory elements</p> <p>\u26a0\ufe0f Consider alternatives: - Caduceus: If RC-equivariance critical and want parameter efficiency - Evo2: For ultra-long regulatory contexts (&gt;10kb) - GENERator: If generative modeling is goal</p>"},{"location":"models/genetics/dnabert2/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: Pending full KB curation \u2014 see Hugging Face model card - Code walkthrough: DNABERT-2 walkthrough - YAML card: <code>kb/model_cards/dnabert2.yaml</code> - Paper card: TBD (add to <code>kb/paper_cards/dnabert2_2024.yaml</code> after curation)</p> <p>Integration recipes: - Modality Features: Genomics - Integration Strategy - CCA + Permutation</p> <p>Source repository: - Local: <code>external_repos/dnabert2/</code> - GitHub: Zhihan1996/DNABERT2</p>"},{"location":"models/genetics/dnabert2/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>RC averaging stability: Test embed(forward) vs. mean(embed(forward), embed(RC))</li> <li>Pooling comparison: Mean vs. CLS token for gene-level embeddings</li> <li>Caduceus benchmark: Same gene set, same cohort, compare CCA/prediction performance</li> <li>BPE analysis: Visualize learned tokens, check for motif enrichment</li> <li>Cross-species pilot: If animal model data available, test zero-shot transfer</li> </ol>"},{"location":"models/genetics/dnabert2/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Always RC-average forward and reverse-complement embeddings (critical!)</li> <li>Log tokenizer version and BPE vocabulary size in metadata</li> <li>When comparing to Caduceus, ensure same gene list and reference genome version</li> <li>BPE tokenization is non-deterministic if vocab changes \u2192 freeze tokenizer for reproducibility</li> </ul>"},{"location":"models/genetics/evo2/","title":"Evo 2","text":""},{"location":"models/genetics/evo2/#overview","title":"Overview","text":"<p>Type: Ultra-long-context DNA foundation model Architecture: StripedHyena 2 (Hyena + attention) Modality: Nucleotide sequences (DNA/RNA) Primary use: Regulatory region embeddings with 1M token context</p>"},{"location":"models/genetics/evo2/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Evo2 extends DNA foundation models to 1 million token contexts using StripedHyena 2 architecture (hybrid Hyena operators + attention layers). This enables modeling entire genes with full regulatory context (promoters, enhancers, 3D loop anchors) in a single forward pass, capturing long-range genomic interactions that shorter-context models miss.</p> <p>Key innovation: 1M context via sub-quadratic Hyena operators \u2192 whole-locus modeling including distal regulatory elements.</p>"},{"location":"models/genetics/evo2/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: StripedHyena 2 (alternating Hyena convolution + multi-head attention)</li> <li>Context length: 1,048,576 tokens (~1 Mb of genomic sequence)</li> <li>Tokenization: Single-base or 2-mer/4-mer (preserves fine resolution)</li> <li>Pretraining: Masked LM on human + multi-species genomes</li> <li>Output: Per-position embeddings \u2192 region/gene pooling</li> </ul>"},{"location":"models/genetics/evo2/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/evo2/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>genetics_regulatory_evo2_v1</code> (exploratory) - Extract extended gene loci (gene + 100kb upstream/downstream for regulatory context) - Tokenize with Evo2 vocabulary (typically single-base or 2-mer) - Forward pass \u2192 per-position embeddings for full locus - RC handling: Evo2 not explicitly RC-equivariant \u2192 average forward/RC embeddings - Pool over gene CDS \u2192 gene embedding - Optionally extract regulatory region embeddings (promoter, enhancers) separately - Project to 512-D via PCA - Residualize: age, sex, ancestry PCs, batch</p> <p>Fusion targets: - Gene expression prediction: Regulatory context improves gene-phenotype links - Enhancer-gene mapping: Identify distal elements affecting brain-expressed genes - 3D genome modeling: Capture loop anchors and TAD boundaries (exploratory)</p>"},{"location":"models/genetics/evo2/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Evo2 enables whole-locus genetic representations for Brain-Omics systems: - 1M context captures regulatory grammar spanning hundreds of kilobases - Critical for brain-specific enhancers distant from target genes - Can embed entire pathways or multi-gene clusters in single pass - Blueprint for ultra-long-context multimodal architectures (e.g., long-range EEG patterns)</p>"},{"location":"models/genetics/evo2/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Extract extended loci (gene \u00b1 100kb from hg38)\n# 2. Tokenize with Evo2 single-base or k-mer vocabulary\n# 3. Load pretrained Evo2 checkpoint\n# 4. Forward pass (may require chunking if &gt;1M tokens)\n# 5. Extract embeddings for:\n#    - Gene CDS (coding sequence)\n#    - Promoter (-2kb to TSS)\n#    - Predicted enhancers (if annotated)\n# 6. RC-average forward + reverse-complement\n# 7. Pool each region \u2192 separate vectors or concatenate\n# 8. Log: context_length, regulatory_elements_included\n</code></pre>"},{"location":"models/genetics/evo2/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/genetics/evo2/#strengths","title":"Strengths","text":"<ul> <li>Ultra-long context: 1M tokens captures distal regulatory elements</li> <li>Whole-locus modeling: No need to manually select regulatory windows</li> <li>Sub-quadratic scaling: Hyena operators enable long context without full attention cost</li> <li>Regulatory grammar: Can learn enhancer-promoter interactions end-to-end</li> </ul>"},{"location":"models/genetics/evo2/#limitations","title":"Limitations","text":"<ul> <li>Massive memory footprint: 1M context requires high-memory GPUs (80GB+ A100/H100)</li> <li>Slower inference: Even with Hyena, 1M tokens slower than short-context models</li> <li>Overkill for coding sequences: Most genes &lt;10kb don't need 1M context</li> <li>Checkpoint availability: Fewer public weights vs. DNABERT-2/Caduceus</li> </ul>"},{"location":"models/genetics/evo2/#when-to-use-evo2","title":"When to Use Evo2","text":"<p>\u2705 Use when: - Need regulatory context for brain-specific gene expression - Studying long-range enhancer-promoter interactions - Have sufficient compute (80GB+ GPU, large batch sizes) - Exploring 3D genome structure embeddings</p> <p>\u26a0\ufe0f Defer until: - Caduceus/DNABERT-2 baselines complete - Regulatory element analysis becomes critical - GPU resources available for long-context experiments</p> <p>\u26a0\ufe0f Consider alternatives: - Caduceus: For coding sequences without regulatory context - DNABERT-2: For standard gene embeddings with manageable compute - GENERator: If generative modeling is priority</p>"},{"location":"models/genetics/evo2/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: Evo2 (2024) - Code walkthrough: Evo2 walkthrough - YAML card: <code>kb/model_cards/evo2.yaml</code> - Paper card: <code>kb/paper_cards/evo2_2024.yaml</code></p> <p>Integration recipes: - Modality Features: Genomics - Integration Strategy</p> <p>Source repository: - Local: <code>external_repos/evo2/</code> - GitHub: ArcInstitute/evo2</p>"},{"location":"models/genetics/evo2/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Pilot study: Embed 5-10 brain-expressed genes with known distal enhancers</li> <li>Context ablation: Test 10kb vs. 100kb vs. 1M context for gene-brain CCA</li> <li>Memory profiling: Document GPU requirements and chunking strategies</li> <li>Enhancer-gene links: Compare Evo2 regulatory embeddings vs. eQTL databases</li> <li>ARPA-H vision: Explore Evo2-style long context for other modalities (EEG, longitudinal)</li> </ol>"},{"location":"models/genetics/evo2/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>GPU requirements: 80GB+ A100 or H100 for full 1M context</li> <li>Chunk long sequences if needed; aggregate chunk embeddings carefully</li> <li>Log context length used (may be &lt;1M for most genes)</li> <li>RC-averaging doubles compute; consider caching forward embeddings</li> <li>When comparing to short-context models, isolate regulatory contribution via ablation</li> </ul>"},{"location":"models/genetics/generator/","title":"GENERator","text":""},{"location":"models/genetics/generator/#overview","title":"Overview","text":"<p>Type: Generative DNA language model Architecture: 6-mer-based autoregressive transformer Modality: Nucleotide sequences (DNA) Primary use: Generative modeling and sequence design (with discriminative embedding extraction)</p>"},{"location":"models/genetics/generator/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>GENERator is a generative DNA language model trained on RefSeq and other genomic corpora using 6-mer tokenization. While primarily designed for sequence generation and design tasks (e.g., synthetic promoter optimization), its learned representations can be extracted for discriminative tasks like gene embedding and downstream fusion.</p> <p>Key innovation: 6-mer vocabulary balances computational tractability with sufficient resolution to capture regulatory motifs and codon structure.</p>"},{"location":"models/genetics/generator/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: Autoregressive transformer (GPT-style)</li> <li>Tokenization: 6-mer overlapping windows (4096-token vocabulary)</li> <li>Pretraining: Next-token prediction on human RefSeq + genomic corpora</li> <li>Generative objective: Likelihood maximization for sequence generation</li> <li>Output: Generative logits (design mode) or hidden states (embedding mode)</li> </ul>"},{"location":"models/genetics/generator/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/generator/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>genetics_gene_fm_pca512_v1</code> (GENERator variant) - Extract gene sequences from hg38 reference genome - Tokenize with 6-mer overlapping windows - Forward pass \u2192 extract hidden states (not generative logits) - RC handling: GENERator not RC-equivariant \u2192 average forward/RC embeddings - Mean pool over gene length \u2192 gene-level vector - Concatenate target gene set - Project to 512-D via PCA - Residualize: age, sex, ancestry PCs, batch</p> <p>Fusion targets: - Gene-brain alignment: Late fusion with brain FM embeddings - Generative vs. discriminative: Compare GENERator embeddings to Caduceus/DNABERT-2 - Sequence design (exploratory): Generate synthetic regulatory elements with desired properties</p>"},{"location":"models/genetics/generator/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>GENERator demonstrates generative modeling for biological sequences: - Hidden states from generative models can serve as discriminative features - Generative capability enables counterfactual analysis (\"what if this gene sequence changed?\") - 6-mer tokenization preserves codon structure for coding sequence analysis - Blueprint for generative components in multimodal Brain-Omics Model (BOM)</p>"},{"location":"models/genetics/generator/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># Discriminative mode (embeddings)\n# 1. Extract gene sequences (hg38 reference)\n# 2. Tokenize with 6-mer overlapping windows\n# 3. Load pretrained GENERator checkpoint\n# 4. Forward pass \u2192 extract hidden states (not output logits)\n# 5. RC-average: embed(seq) and embed(reverse_complement(seq))\n# 6. Mean pool over tokens \u2192 gene embedding\n# 7. Log: token_vocabulary, pooling_layer (e.g., layer -1)\n\n# Generative mode (sequence design)\n# 1. Define target properties (e.g., GC content, expression level)\n# 2. Sample from GENERator with conditioning\n# 3. Validate generated sequences via wet-lab or in-silico assays\n</code></pre>"},{"location":"models/genetics/generator/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/genetics/generator/#strengths","title":"Strengths","text":"<ul> <li>Generative capability: Can design novel sequences (regulatory elements, synthetic genes)</li> <li>6-mer vocabulary: Preserves codon structure, captures motifs</li> <li>Hidden states useful: Discriminative embeddings competitive with specialized models</li> <li>Interpretable: Generative likelihoods inform sequence quality</li> </ul>"},{"location":"models/genetics/generator/#limitations","title":"Limitations","text":"<ul> <li>Not RC-equivariant: Requires manual forward/RC averaging</li> <li>Generative objective: Optimized for likelihood, not discriminative tasks</li> <li>Checkpoint availability: Fewer public weights vs. DNABERT-2</li> <li>6-mer limitations: May miss patterns spanning &gt;6 bases (compare to BPE or longer k-mers)</li> </ul>"},{"location":"models/genetics/generator/#when-to-use-generator","title":"When to Use GENERator","text":"<p>\u2705 Use when: - Interested in generative modeling and sequence design - Want to compare generative vs. discriminative embeddings - Need 6-mer vocabulary (codon-aware analysis) - Exploring counterfactual sequence perturbations</p> <p>\u26a0\ufe0f Consider alternatives: - Caduceus: For discriminative tasks with RC-equivariance - DNABERT-2: BPE tokenization, stronger discriminative benchmarks - Evo2: For ultra-long regulatory contexts</p>"},{"location":"models/genetics/generator/#reference-materials","title":"Reference Materials","text":"<p>Primary sources: - Paper: GENERator (2024) - Code walkthrough: GENERator walkthrough - YAML card: <code>kb/model_cards/generator.yaml</code> - Paper card: <code>kb/paper_cards/generator_2024.yaml</code></p> <p>Integration recipes: - Modality Features: Genomics - Integration Strategy</p> <p>Source repository: - Local: <code>external_repos/generator/</code> - GitHub: GenerTeam/GENERator</p>"},{"location":"models/genetics/generator/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Discriminative benchmark: Compare GENERator vs. Caduceus/DNABERT-2 on same gene set</li> <li>Generative pilot: Design synthetic promoters, test expression predictions</li> <li>Counterfactual analysis: Perturb gene sequences, measure embedding \u0394</li> <li>6-mer analysis: Visualize learned k-mer representations</li> <li>ARPA-H vision: Explore generative components for Brain-Omics Model (BOM)</li> </ol>"},{"location":"models/genetics/generator/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Extract hidden states, not logits for discriminative embeddings</li> <li>Always RC-average forward and reverse-complement embeddings</li> <li>Log layer used for extraction (typically last layer before output)</li> <li>6-mer tokenization is deterministic but frame-dependent (start position matters)</li> <li>When generating sequences, validate via independent predictors (avoid model collapse)</li> </ul>"}]}