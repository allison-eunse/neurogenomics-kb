{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\uddec\ud83e\udde0 Neuro-Omics Knowledge Base","text":"<p>A comprehensive documentation hub for genetics and brain foundation models and their multimodal integration.</p> <p>\ud83d\udcd6 KB Overview | \ud83e\uddec Genetics Models | \ud83e\udde0 Brain Models | \ud83d\udd17 Integration Guide | \ud83d\udcbb GitHub</p>"},{"location":"#what-is-this","title":"\ud83c\udfaf What is this?","text":"<p>A documentation-first knowledge base for researchers working with:</p> <ul> <li>\ud83e\uddec Genetic foundation models \u2014 Caduceus, DNABERT-2, Evo2, GENERator</li> <li>\ud83e\udde0 Brain imaging models \u2014 BrainLM, Brain-JEPA, BrainMT, Brain Harmony, SwiFT</li> <li>\ud83c\udfe5 Multimodal/Clinical models \u2014 BAGEL, MoT, M3FM, Me-LLaMA, TITAN, FMS-Medical</li> <li>\ud83d\udd17 Integration strategies \u2014 Gene-brain-behavior-language analysis</li> </ul> <p>Scope: Documentation, metadata cards, and integration patterns \u2014 not model implementation code.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># 1. Clone and setup\ngit clone https://github.com/allison-eunse/neuro-omics-kb.git\ncd neuro-omics-kb\npython -m venv .venv &amp;&amp; source .venv/bin/activate\npip install -r requirements.txt\n\n# 2. View documentation locally\nmkdocs serve  # Visit http://localhost:8000\n\n# 3. Validate metadata cards\npython scripts/manage_kb.py validate models\n</code></pre> <p>New to foundation models? \u2192 Start with:</p> <ol> <li>\ud83d\udcd6 KB Overview</li> <li>\ud83e\uddec Genetics Models</li> <li>\ud83e\udde0 Brain Models</li> <li>\ud83d\udd17 Integration Strategy</li> </ol>"},{"location":"#use-cases","title":"\ud83d\udca1 Use Cases","text":""},{"location":"#genetics-research","title":"\u2192 Genetics research","text":"<ul> <li>Turn DNA sequences into strand-robust gene embeddings (Caduceus, DNABERT-2, Evo 2, GENERator)</li> <li>Compare variant effect predictors or run LOGO attribution with standardized configs</li> <li>Hand off vetted embeddings to integration pipelines without reimplementing data hygiene</li> </ul> <p>Go deeper: Explore Genetics Models</p>"},{"location":"#brain-imaging","title":"\u2192 Brain imaging","text":"<ul> <li>Preprocess fMRI/sMRI cohorts, harmonize sites, and extract embeddings (BrainLM, Brain-JEPA, Brain Harmony, BrainMT, SwiFT)</li> <li>Control residualization/motion covariates before fusion experiments</li> <li>Swap projection heads or pooling strategies without touching raw scans</li> </ul> <p>Go deeper: Explore Brain Models</p>"},{"location":"#multimodal-integration","title":"\u2192 Multimodal integration","text":"<ul> <li>Follow the late-fusion-first playbook (CCA + permutations, LR/GBDT fusion, contrastive escalation)</li> <li>Track embedding/processing provenance through integration cards and decision logs</li> <li>Plug in recipe-ready configs for CCA, prediction baselines, or partial correlations</li> </ul> <p>Go deeper: Explore Integration Strategy</p>"},{"location":"#clinical-multimodal-fms","title":"\u2192 Clinical &amp; multimodal FMs","text":"<ul> <li>Reuse BAGEL, MoT, M3FM, Me-LLaMA, TITAN, and FMS-Medical walkthroughs as reference builds</li> <li>Understand how vision\u2013language or sparse MoE systems align modalities before adapting to neuro-omics</li> <li>Borrow evaluation scaffolding for bilingual or imaging\u2013text setups</li> </ul> <p>Go deeper: Explore Multimodal Models</p>"},{"location":"#reproducible-research-guardrails","title":"\u2192 Reproducible research guardrails","text":"<ul> <li>Start from vetted configs (<code>configs/experiments/*</code>) with stratified CV and QC baked in</li> <li>Run codified validation steps (<code>scripts/manage_kb.py</code>, <code>codex_gate.py</code>) before sharing outputs</li> <li>Use analysis recipes as living SOPs for cohorts, baselines, and integration checkpoints</li> </ul> <p>Go deeper: Explore Analysis Recipes</p>"},{"location":"#whats-inside","title":"\ud83d\udce6 What's Inside","text":"\ud83d\udcda Documentation \u2014 Walkthroughs, playbooks, decision logs <pre><code>docs/\n\u251c\u2500\u2500 code_walkthroughs/          \u2190 15 guided FM tours\n\u2502   \u251c\u2500\u2500 \ud83e\uddec Genetics (4):  Caduceus, DNABERT-2, Evo 2, GENERator\n\u2502   \u251c\u2500\u2500 \ud83e\udde0 Brain (5):     BrainLM, Brain-JEPA, Brain Harmony, BrainMT, SwiFT\n\u2502   \u2514\u2500\u2500 \ud83c\udfe5 Multimodal (6): BAGEL, MoT, M3FM, Me-LLaMA, TITAN, FMS-Medical\n\u251c\u2500\u2500 integration/                \u2190 Fusion strategies, design patterns, benchmarks\n\u251c\u2500\u2500 data/                       \u2190 UKB data map, QC protocols, schemas\n\u2514\u2500\u2500 decisions/                  \u2190 Integration plans, validation rationale\n</code></pre>  Walkthroughs, schemas, and decision logs share the same terminology across genetics, brain, and multimodal FMs.   \ud83c\udff7\ufe0f Metadata Cards \u2014 Structured YAML for all assets <pre><code>kb/\n\u251c\u2500\u2500 model_cards/          \u2190 15 FM specs (13 FMs + 2 ARPA-H planning)\n\u251c\u2500\u2500 paper_cards/          \u2190 20 research papers with structured takeaways\n\u251c\u2500\u2500 datasets/             \u2190 Sample sizes, QC thresholds, access requirements\n\u2514\u2500\u2500 integration_cards/    \u2190 Embeddings, harmonization, preprocessing pipelines\n</code></pre>  [Browse all cards on GitHub \u2192](https://github.com/allison-eunse/neuro-omics-kb/tree/main/kb)   \ud83d\udd27 Tools &amp; Scripts \u2014 Validation, quality gates, sync <pre><code>scripts/\n\u251c\u2500\u2500 manage_kb.py             \u2190 Validate YAML, query embeddings/harmonization\n\u251c\u2500\u2500 codex_gate.py            \u2190 Pre-commit quality sweeps\n\u2514\u2500\u2500 fetch_external_repos.sh  \u2190 Sync upstream FM repos\n</code></pre>  Pair these with `verify_kb.sh` or `mkdocs serve` during review cycles.   \u2699\ufe0f Experiment Configs \u2014 Ready-to-run templates <pre><code>configs/experiments/\n\u251c\u2500\u2500 01_cca_gene_smri.yaml          \u2190 CCA + permutation baseline\n\u251c\u2500\u2500 02_prediction_baselines.yaml   \u2190 Gene vs Brain vs Fusion (LR/GBDT)\n\u251c\u2500\u2500 03_logo_gene_attribution.yaml  \u2190 Leave-one-gene-out \u0394AUC\n\u2514\u2500\u2500 dev_* templates                \u2190 CHA cohort dev stubs\n</code></pre>  Each config references the exact embeddings, covariates, and validation plan to keep runs reproducible."},{"location":"#foundation-model-registry","title":"\ud83c\udfaf Foundation Model Registry","text":""},{"location":"#genetics-models","title":"Genetics Models","text":"Model Best for Context Walkthrough \ud83e\uddec Caduceus RC-equivariant gene embeddings DNA sequences Code walkthrough \ud83e\uddec DNABERT-2 Cross-species transfer BPE tokenization Code walkthrough \ud83e\uddec Evo 2 Ultra-long regulatory regions 1M context Code walkthrough \ud83e\uddec GENERator Generative modeling 6-mer LM Code walkthrough"},{"location":"#brain-models","title":"Brain Models","text":"Model Modality Best for Walkthrough \ud83e\udde0 BrainLM fMRI Site-robust embeddings Code walkthrough \ud83e\udde0 Brain-JEPA fMRI Lower-latency option Code walkthrough \ud83e\udde0 Brain Harmony sMRI + fMRI Multi-modal fusion Code walkthrough \ud83e\udde0 BrainMT sMRI/fMRI Mamba efficiency Code walkthrough \ud83e\udde0 SwiFT fMRI Hierarchical spatiotemporal Code walkthrough"},{"location":"#multimodal-clinical-models","title":"Multimodal &amp; Clinical Models","text":"Model Type Key Innovation Walkthrough \ud83d\udd17 BAGEL Unified FM MoT experts (understanding + generation) Code walkthrough \ud83d\udd17 MoT Sparse transformer Modality-aware sparsity (~55% FLOPs) Code walkthrough \ud83c\udfe5 M3FM Radiology CXR/CT + bilingual reports (EN/CN) Code walkthrough \ud83c\udfe5 Me-LLaMA Medical LLM Continual pretrained LLaMA (129B tokens) Code walkthrough \ud83c\udfe5 TITAN Pathology Gigapixel whole-slide imaging Code walkthrough <p>\ud83d\udcd6 Explore Multimodal Models Overview \u2022 Multimodal Architectures Guide \u2022 Design Patterns</p> <p></p>"},{"location":"#integration-stack","title":"\ud83d\udd17 Integration Stack","text":"<p>\u2192 Core Strategy: Integration Strategy \u2192 Analysis Recipes: CCA + permutation \u00b7 Prediction baselines \u00b7 Partial correlations \u2192 Modality Features: Genomics \u00b7 sMRI \u00b7 fMRI \u2192 Design Patterns: Design patterns \u00b7 Multimodal architectures</p> <p>Integration Roadmap:</p> <pre><code>\ud83d\udd35 Late Fusion (baseline)\n       \u2193 If fusion wins significantly\n\ud83d\udfe2 Two-Tower Contrastive\n       \u2193 If gains plateau\n\ud83d\udfe1 EI Stacking / Hub Tokens\n       \u2193 Last resort\n\ud83d\udd34 Full Early Fusion (TAPE-style)\n</code></pre> <p>Decisions: Integration baseline plan (Nov 2025)</p> <p></p>"},{"location":"#research-papers","title":"\ud83d\udccb Research Papers","text":"<p>Every paper has three quick links: KB summary (MD) \u00b7 Annotated PDF \u00b7 Original publication</p>"},{"location":"#genetics-foundation-models","title":"\ud83e\uddec Genetics Foundation Models","text":"Paper MD notes PDF notes Source Focus \ud83e\uddec Caduceus MD notes PDF notes arXiv:2403.03234 RC-equivariant BiMamba DNA FM \ud83e\uddec DNABERT-2 MD notes PDF notes arXiv:2306.15006 BPE-tokenized multi-species encoder \ud83e\uddec Evo 2 MD notes PDF notes bioRxiv 2025.02.18 StripedHyena 1M-token model \ud83e\uddec GENERator MD notes PDF notes arXiv:2502.07272 Generative 6-mer DNA LM"},{"location":"#brain-foundation-models","title":"\ud83e\udde0 Brain Foundation Models","text":"Paper MD notes PDF notes Source Focus \ud83e\udde0 BrainLM MD notes PDF notes OpenReview RwI7ZEfR27 ViT-MAE for UKB fMRI \ud83e\udde0 Brain-JEPA MD notes PDF notes arXiv:2409.19407 Joint-embedding prediction \ud83e\udde0 Brain Harmony MD notes PDF notes arXiv:2509.24693 sMRI+fMRI fusion with TAPE \ud83e\udde0 BrainMT MD notes PDF notes LNCS 10.1007/\u2026-2_15 Hybrid Mamba-Transformer \ud83e\udde0 SwiFT MD notes PDF notes arXiv:2307.05916 Swin-style 4D fMRI"},{"location":"#multimodal-clinical-foundation-models","title":"\ud83c\udfe5 Multimodal &amp; Clinical Foundation Models","text":"Paper MD notes PDF notes Source Focus \ud83d\udd17 BAGEL MD notes PDF notes arXiv:2505.14683 Unified MoT decoder \ud83d\udd17 MoT MD notes PDF notes arXiv:2411.04996 Modality-aware sparse transformers \ud83c\udfe5 M3FM MD notes PDF notes npj Digital Medicine 2025 Multilingual medical vision-language \ud83c\udfe5 Me-LLaMA MD notes PDF notes arXiv:2404.05416 Medical LLM continual-pretraining \ud83c\udfe5 TITAN MD notes PDF notes Nature Medicine 2025 Gigapixel whole-slide pathology \ud83d\udcda MM FMs Survey MD notes PDF notes AI in Medicine 2025 Clinical MM FM patterns"},{"location":"#integration-methods","title":"\ud83d\udd17 Integration &amp; Methods","text":"Paper MD notes PDF notes Source Focus \ud83d\udcca Ensemble Integration MD notes PDF notes doi:10.1093/bioadv/vbac065 Late-fusion rationale \ud83e\uddea Oncology Multimodal MD notes PDF notes PubMed 39118787 Confounds &amp; protocols \ud83e\udde0 Yoon BIOKDD 2025 MD notes PDF notes bioRxiv 2025.02.18 LOGO attribution \ud83c\udf0d GWAS Diverse Populations MD notes PDF notes PubMed 36158455 Ancestry-aware QC \ud83d\udcc8 PRS Guide MD notes PDF notes PubMed 31607513 Polygenic risk reporting"},{"location":"#data-schemas","title":"\ud83d\udcca Data &amp; Schemas","text":"Resource Description Link \ud83c\udfe5 UKB Data Map Field mappings, cohort definitions View \u2705 Governance &amp; QC Quality control protocols, IRB guidelines View \ud83d\udd11 Subject Keys ID management and anonymization View \ud83d\udccb Schemas Data format specifications View \ud83d\udce6 FMS-Medical Catalog 100+ medical FM references View"},{"location":"#kb-assets","title":"\ud83d\uddc2\ufe0f KB Assets","text":"-   :material-file-document: **Model Cards**      ---      15 model cards: 13 foundation models + 2 ARPA-H planning cards      [Browse on GitHub](https://github.com/allison-eunse/neuro-omics-kb/tree/main/kb/model_cards)  -   :material-book-open-page-variant: **Paper Cards**      ---      Structured summaries of 20 key papers with integration hooks      [Browse on GitHub](https://github.com/allison-eunse/neuro-omics-kb/tree/main/kb/paper_cards)  -   :material-database: **Dataset Cards**      ---      Data source specifications for UKB, HCP, and benchmarks      [Browse on GitHub](https://github.com/allison-eunse/neuro-omics-kb/tree/main/kb/datasets)  -   :material-link-variant: **Integration Cards**      ---      Cross-modal fusion patterns and actionable guidance      [Browse on GitHub](https://github.com/allison-eunse/neuro-omics-kb/tree/main/kb/integration_cards)"},{"location":"#experiment-configs","title":"\u2699\ufe0f Experiment Configs","text":"<p>Ready-to-use analysis templates with validation schemas:</p> Template Purpose Key Features \ud83d\udcca 01_cca_gene_smri CCA + permutation baseline Cross-modal null distributions, p-values \ud83c\udfaf 02_prediction_baselines Gene vs Brain vs Fusion LR/GBDT comparison, DeLong tests \ud83e\uddec 03_logo_gene_attribution LOGO \u0394AUC protocol Leave-one-gene-out attribution <p>\u2192 Explore Experiment Configs</p>"},{"location":"#standard-pipeline","title":"\ud83d\ude80 Standard Pipeline","text":"<pre><code>graph LR\n    A[Raw Data] --&gt; B[Z-score normalization]\n    B --&gt; C[Residualize confounds]\n    C --&gt; D[512-D projection]\n    D --&gt; E{Analysis Type}\n    E --&gt;|Structure| F[CCA + permutations]\n    E --&gt;|Prediction| G[LR/GBDT fusion]\n    F --&gt; H[Statistical tests]\n    G --&gt; H\n    H --&gt; I[Results + validation]</code></pre> <p>Always Residualize</p> <p>Confounds to control: - Age, sex, site/scanner - Motion (mean FD for fMRI) - SES, genetic PCs - Batch effects</p> <p>Start with CCA + Permutation</p> <p>CCA always returns non-zero correlations, even on shuffled data. The permutation test builds a null distribution by re-fitting after within-fold shuffling, giving you p-values to avoid over-interpreting noise\u2014critical when sites share confounds.</p>"},{"location":"#typical-workflow","title":"\ud83d\udee0\ufe0f Typical Workflow","text":"<ol> <li>\ud83d\udcd6 Explore \u2014 Browse model cards and paper summaries</li> <li>\ud83d\udd0d Select \u2014 Choose appropriate FMs for your modalities</li> <li>\u2699\ufe0f Configure \u2014 Clone experiment config template</li> <li>\u25b6\ufe0f Run \u2014 Extract embeddings and run analysis</li> <li>\u2705 Validate \u2014 Use quality gates (<code>manage_kb.py</code>)</li> <li>\ud83d\udcdd Document \u2014 Log results back to KB</li> </ol> <p>Need help? Check the KB Overview or explore Code Walkthroughs</p>"},{"location":"code_walkthroughs/","title":"Code Walkthrough Hub","text":"<p>Each walkthrough now surfaces the KB scaffolding you need to turn narrative notes into living cards and experiment configs.</p>"},{"location":"code_walkthroughs/#quick-links","title":"Quick links","text":"<ul> <li>Integration baseline plan</li> <li>Integration strategy</li> <li>Modality features \u2014 Genomics</li> <li>Modality features \u2014 sMRI</li> <li>Modality features \u2014 fMRI</li> <li>KB templates (model/integration/method/dataset): <code>kb/templates/</code></li> <li>Experiment config stub: <code>kb/templates/experiment_config_stub.md</code></li> </ul>"},{"location":"code_walkthroughs/#walkthrough-roster","title":"Walkthrough roster","text":""},{"location":"code_walkthroughs/#brain-foundation-models","title":"\ud83e\udde0 Brain foundation models","text":"Walkthrough KB Model Card Modality Spec BrainLM BrainLM card fMRI spec Brain-JEPA Brain-JEPA card fMRI spec Brain Harmony Brain Harmony card fMRI spec, sMRI spec BrainMT BrainMT card fMRI spec SwiFT SwiFT card fMRI spec"},{"location":"code_walkthroughs/#genetics-foundation-models","title":"\ud83e\uddec Genetics foundation models","text":"Walkthrough KB Model Card Modality Spec Caduceus Caduceus card Genomics spec DNABERT-2 DNABERT-2 card Genomics spec Evo 2 Evo2 card Genomics spec GENERaTOR GENERator card Genomics spec"},{"location":"code_walkthroughs/#multimodal-clinical-models","title":"\ud83c\udfe5 Multimodal &amp; clinical models","text":"Walkthrough KB Model Card Modality Spec BAGEL BAGEL card \u2014 Flamingo Flamingo card \u2014 MoT MoT card \u2014 M3FM M3FM card \u2014 Me-LLaMA Me-LLaMA card \u2014 TITAN TITAN card \u2014 FMS-Medical FMS-Medical card \u2014"},{"location":"code_walkthroughs/#architectures-components","title":"\ud83c\udfd7\ufe0f Architectures &amp; components","text":"Walkthrough KB Model Card Modality Spec StripedHyena \u2014 \u2014"},{"location":"code_walkthroughs/bagel_walkthrough/","title":"BAGEL Code Walkthrough","text":"<p>KB references: BAGEL paper note</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#overview","title":"Overview","text":"<p>BAGEL couples a Qwen2-style Mixture-of-Transformer decoder, a SigLIP NaViT encoder, and a latent VAE so a single 7B active-parameter model can interleave text reasoning, visual understanding, and diffusion-style image synthesis. The public release ships with checkpoints, quantized inference paths, training scripts, and evaluation kits spanning understanding, text-to-image, and editing.^[<code>text title=\"external_repos/bagel/README.md</code>][<code>153:198:external_repos/bagel/README.md</code>] (lines 50-188)\"</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params / Scale Context Inputs Key capabilities Repo Qwen2 MoT decoder (packed attention, NaiveCache) + SigLIP-NaViT encoder + VAE; modality connectors align latent patches and ViT tokens with the LLM space.^[<code>text title=\"external_repos/bagel/modeling/bagel/bagel.py</code>] 7B active / 14B total parameters, trained on trillions of interleaved multimodal tokens; outperforms Qwen2.5-VL and rivals SD3 on benchmarks.^[<code>50:188:external_repos/bagel/README.md</code>] Unified understanding, text-to-image, image editing, and \u201cworld-modeling\u201d tasks surfaced through Gradio, CLI scripts, and evaluation benches.^[<code>50:200:external_repos/bagel/README.md</code>][<code>85:151:external_repos/bagel/app.py</code>] Packed batches contain text token ids, ViT patches, VAE latents, per-token positions, attention masks, and per-modality loss selectors built by <code>PackedDataset</code>.^[<code>45:305:external_repos/bagel/data/dataset_base.py</code>] Training entrypoint wires configurable branches (visual_gen / visual_und), FSDP wrapping, EMA, dataset mixing, and MFU logging.^[<code>98:870:external_repos/bagel/train/pretrain_unified_navit.py</code>] <code>external_repos/bagel</code>"},{"location":"code_walkthroughs/bagel_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Follow the Quick Start: Python\u202f3.10 env, <code>pip install -r requirements.txt</code>, then <code>pip install flash_attn==2.5.8</code> before downloading checkpoints via <code>huggingface_hub.snapshot_download</code>. Modes 1\u20133 in <code>app.py</code> toggle full-precision, NF4, or INT8 pipelines for 12\u201380\u202fGB GPUs.^[<code>text title=\"external_repos/bagel/README.md</code>][<code>25:151:external_repos/bagel/app.py</code>] (lines 107-151)\"</li> <li>Training relies on CUDA + NCCL with FSDP; <code>pretrain_unified_navit.py</code> auto-detects device TFLOPs for MFU calculation and exposes switches for freezing LLM/ViT/VAE weights, enabling FLEX packing, or running EMA-only resumes.^[<code>text title=\"external_repos/bagel/train/pretrain_unified_navit.py</code>] (lines 98-418)\"</li> <li>Inference hyperparameters (<code>cfg_text_scale</code>, <code>cfg_img_scale</code>, <code>cfg_interval</code>, <code>timestep_shift</code>, renorm mode, steps) are surfaced both in the README and the Gradio UI so you can script KB experiments consistently.^[<code>text title=\"external_repos/bagel/README.md</code>][<code>160:357:external_repos/bagel/app.py</code>] (lines 90-151)\"</li> </ul>"},{"location":"code_walkthroughs/bagel_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/bagel_walkthrough/#unified-forward-pass-modelingbagelbagelpy","title":"Unified Forward Pass (<code>modeling/bagel/bagel.py</code>)","text":"<p><code>Bagel</code> hosts the three branches: (1) language tokens (always on), (2) ViT patches for understanding, and (3) VAE latent patches for generation. It projects modality features into the LLM embedding space, injects learned positional/timestep embeddings, and multiplexes MoT experts via packed index tensors. Losses are computed per-branch (CE for text, Smooth L1/MSE for latents) and returned side-by-side.</p> <p>Multimodal packed sequence forward:</p> external_repos/bagel/modeling/bagel/bagel.py (lines 101-229)<pre><code>    def forward(..., packed_text_ids, packed_text_indexes, sample_lens, packed_position_ids,\n                ..., packed_vit_tokens=None, ..., padded_latent=None, ..., packed_timesteps=None,\n                mse_loss_indexes=None):\n        packed_text_embedding = self.language_model.model.embed_tokens(packed_text_ids)\n        packed_sequence = packed_text_embedding.new_zeros((sequence_length, self.hidden_size))\n        packed_sequence[packed_text_indexes] = packed_text_embedding\n        ...\n        if self.config.visual_und:\n            packed_vit_token_embed = self.vit_model(... )\n            packed_vit_token_embed = self.connector(packed_vit_token_embed)\n            packed_sequence[packed_vit_token_indexes] = packed_vit_token_embed + vit_pos_embed\n        if self.config.visual_gen:\n            ... # patchify VAE latents, inject timestep + position, place into packed sequence\n            packed_sequence[packed_vae_token_indexes] = packed_latent\n        last_hidden_state = self.language_model(..., packed_sequence=packed_sequence, ...)\n        if self.config.visual_gen:\n            packed_mse_preds = self.llm2vae(last_hidden_state[mse_loss_indexes])\n            mse = (packed_mse_preds - target[has_mse]) ** 2\n        if ce_loss_indexes is not None:\n            packed_ce_preds = self.language_model.lm_head(last_hidden_state[ce_loss_indexes])\n            ce = F.cross_entropy(packed_ce_preds, packed_label_ids, reduction=\"none\")\n        return dict(mse=mse, ce=ce)\n</code></pre> <p>The same class also defines cache-friendly helpers (<code>prepare_prompts</code>, <code>prepare_vit_images</code>, <code>prepare_vae_latent</code>, <code>generate_image</code>, <code>generate_text</code>) so both training and inference reuse identical packing rules.^[<code>text title=\"external_repos/bagel/modeling/bagel/bagel.py</code>] (lines 232-907)\"</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#packeddataset-sequence-plans-datadataset_basepy","title":"PackedDataset &amp; Sequence Plans (<code>data/dataset_base.py</code>)","text":"<p><code>PackedDataset</code> streams heterogenous samples, applies conditional dropout (<code>text_cond_dropout_prob</code>, etc.), and emits a single packed tensor blob per batch. Each <code>sequence_plan</code> step can insert text spans, ViT patches, or VAE tensors, automatically managing BOS/EOS vision tokens, per-split attention modes, and modality-specific losses.^[<code>text title=\"external_repos/bagel/data/dataset_base.py</code>] (lines 45-400)\"</p> <p>Packed tensor conversion:</p> external_repos/bagel/data/dataset_base.py (lines 187-305)<pre><code>    def to_tensor(self, sequence_status):\n        data = dict(\n            sequence_length=sum(sequence_status['sample_lens']),\n            sample_lens=sequence_status['sample_lens'],\n            packed_text_ids=torch.tensor(sequence_status['packed_text_ids']),\n            ...\n        )\n        if len(sequence_status['vae_image_tensors']) &gt; 0:\n            data['padded_images'] = padded_images\n            data['patchified_vae_latent_shapes'] = sequence_status['vae_latent_shapes']\n            data['packed_latent_position_ids'] = torch.cat(sequence_status['packed_latent_position_ids'], dim=0)\n        if len(sequence_status['packed_vit_tokens']) &gt; 0:\n            data['packed_vit_tokens'] = torch.cat(sequence_status['packed_vit_tokens'], dim=0)\n            data['packed_vit_position_ids'] = torch.cat(sequence_status['packed_vit_position_ids'], dim=0)\n            data['vit_token_seqlens'] = torch.tensor(sequence_status['vit_token_seqlens'])\n</code></pre> <p>The <code>pack_sequence</code> routine adds <code>&lt;|im_start|&gt; / &lt;|im_end|&gt;</code> sentinels, calls <code>patchify</code> for ViT patches, records <code>packed_timesteps</code> for diffusion supervision, and scales CE loss weights by token length so batches with different numbers of captions remain balanced.^[<code>text title=\"external_repos/bagel/data/dataset_base.py</code>] (lines 306-724)\"</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#training-entry-point-trainpretrain_unified_navitpy","title":"Training Entry Point (<code>train/pretrain_unified_navit.py</code>)","text":"<p>Three dataclasses (<code>ModelArguments</code>, <code>DataArguments</code>, <code>TrainingArguments</code>) expose practically every toggle: source checkpoints, positional interpolation, dropout per modality, packed-data limits, sharding strategy, EMA decay, LR schedule, and loss weights.^[<code>text title=\"external_repos/bagel/train/pretrain_unified_navit.py</code>] The <code>main()</code> routine then: (lines 98-405)\" - Parses args, initializes NCCL, seeds, and W&amp;B logging. - Loads or restores Qwen2/SigLIP/AE weights (optionally HF checkpoints) and wires them into <code>BagelConfig</code>. - Builds <code>PackedDataset</code> via YAML-specified groups, enabling FLEX packing or resume-friendly overflow buffers. - Wraps the model in FSDP + activation checkpointing, sets up EMA mirrors, optimizer, scheduler, gradient clipping, and MFU telemetry.^[<code>text title=\"external_repos/bagel/train/pretrain_unified_navit.py</code>] (lines 408-775)\" - Periodically logs CE/MSE/token throughput, tracks dataset sampling state for deterministic resumes, and checkpoints both base + EMA weights alongside optimizer/scheduler state.^[<code>text title=\"external_repos/bagel/train/pretrain_unified_navit.py</code>] (lines 658-867)\"</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#inference-stack-apppy-inferencerpy","title":"Inference Stack (<code>app.py</code> + <code>inferencer.py</code>)","text":"<p><code>app.py</code> bootstraps configs, shares layers across devices, and lets you choose full precision, NF4, or INT8 quantization before launching the Gradio UI. It wires UI sliders directly to CFG/timestep parameters so experiments match README defaults.^[<code>text title=\"external_repos/bagel/app.py</code>] (lines 25-357)\"</p> <p><code>InterleaveInferencer</code> encapsulates the streaming generation algorithm: it grows <code>NaiveCache</code> instances as you interleave prompts/images, clones contexts for classifier-free guidance, and alternates between textual \"thinking\" chains and latent diffusion steps.</p> <p>Streaming inference with context caching:</p> external_repos/bagel/inferencer.py (lines 22-284)<pre><code>class InterleaveInferencer:\n    def init_gen_context(self):\n        return {'kv_lens': [0], 'ropes': [0], 'past_key_values': NaiveCache(...)}\n\n    def update_context_text(...):\n        generation_input, kv_lens, ropes = self.model.prepare_prompts(...)\n        past_key_values = self.model.forward_cache_update_text(past_key_values, **generation_input)\n\n    def update_context_image(...):\n        if vae:\n            generation_input = self.model.prepare_vae_images(...)\n            past_key_values = self.model.forward_cache_update_vae(self.vae_model, past_key_values, **generation_input)\n        if vit:\n            generation_input = self.model.prepare_vit_images(...)\n            past_key_values = self.model.forward_cache_update_vit(past_key_values, **generation_input)\n\n    def gen_image(...):\n        generation_input = self.model.prepare_vae_latent(...)\n        generation_input_cfg_text = self.model.prepare_vae_latent_cfg(...)\n        unpacked_latent = self.model.generate_image(..., cfg_text_scale=cfg_text_scale, cfg_img_scale=cfg_img_scale, ...)\n        return self.decode_image(unpacked_latent[0], image_shape)\n</code></pre> <p>Understanding vs. generation differ only in whether you keep emitting text (<code>understanding_output=True</code>) or call <code>gen_image</code> with CFG contexts cloned before the last prompt.^[<code>text title=\"external_repos/bagel/inferencer.py</code>] (lines 207-314)\"</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#packed-qwen2-navit-layers-modelingbagelqwen2_navitpy","title":"Packed Qwen2-NaViT Layers (<code>modeling/bagel/qwen2_navit.py</code>)","text":"<p><code>PackedAttention</code> and <code>PackedAttentionMoT</code> extend Hugging Face\u2019s Qwen2 attention with flash-attention varlen kernels, optional flex-attention (for packed sparse masks), and modality-aware expert routing. <code>NaiveCache</code> stores per-layer KV tensors so inference can stream text/image blocks without re-encoding past context.^[<code>text title=\"external_repos/bagel/modeling/bagel/qwen2_navit.py</code>] (lines 207-379)\"</p>"},{"location":"code_walkthroughs/bagel_walkthrough/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Dataset alignment. <code>PackedDataset</code> already surfaces conditional dropout flags and CE-weight scalars; reuse them when aligning neuro-omics modality mixes (e.g., drop imaging tokens to train text-only adapters without rewriting loss code).</li> <li>Modality toggles. Training arguments <code>visual_gen</code>/<code>visual_und</code> plus freeze switches make it easy to run ablations (e.g., ViT-only understanding on KB datasets) while reusing the same packed loader.^[<code>text title=\"external_repos/bagel/train/pretrain_unified_navit.py</code>] (lines 212-405)\"</li> <li>CFG introspection. The inferencer\u2019s CFG contexts are plain dicts holding cloned caches (<code>cfg_text_precontext</code>, <code>cfg_img_precontext</code>), which means you can intercept them to log per-modality contributions or plug your own KB-guided conditioning signals.^[<code>text title=\"external_repos/bagel/inferencer.py</code>] (lines 120-172)\"</li> </ul>"},{"location":"code_walkthroughs/brainharmony_walkthrough/","title":"BrainHarmony Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 sMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#overview","title":"Overview","text":"<p>BrainHarmony (a.k.a. BrainHarmonix) is a three-stage pipeline that first extracts modality-specific embeddings from fMRI ROI time-series and structural T1 volumes, then performs JEPA-style token-space pretraining, and finally fine-tunes classification heads on downstream cohorts (e.g., ABIDE). Stage\u202f0 runs fused encoders with anatomical+functional positional priors, Stage\u202f1 trains a latent-token predictor with smooth L1 loss between student and EMA targets, and Stage\u202f2 attaches lightweight heads for clinical prediction.^[<code>text title=\"external_repos/brainharmony/README.md</code>][<code>32:138:external_repos/brainharmony/configs/harmonizer/stage0_embed/conf_embed_pretrain.py</code>] (lines 1-94)\"</p>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Dual FlexVisionTransformer encoders (fMRI + 3D T1) feeding a JEPA predictor with latent tokens, FlashAttention-2 blocks, and mask-conditioned regressors.^[<code>text title=\"external_repos/brainharmony/libs/flex_transformer.py</code>][<code>22:260:external_repos/brainharmony/modules/harmonizer/stage1_pretrain/models.py</code>][<code>23:112:external_repos/brainharmony/libs/ssl_models/jepa_flex.py</code>] Default \u201cbase\u201d uses 768-d embeddings, 12 encoder blocks, 6-layer predictor, mask ratio\u202f0.75, and 128 latent tokens (configurable via scripts).^[<code>48:138:external_repos/brainharmony/configs/harmonizer/stage0_embed/conf_embed_pretrain.py</code>][<code>38:49:external_repos/brainharmony/scripts/harmonizer/stage1_pretrain/run_pretrain.sh</code>] 400 cortical ROIs \u00d7\u202f490 TRs are chunked into 18 patches (48-step windows) plus optional 50 subcortical channels; structural MRI cubes are normalized to 160\u202f\u00d7\u202f192\u202f\u00d7\u202f160 voxels.^[<code>317:465:external_repos/brainharmony/datasets/datasets.py</code>][<code>499:561:external_repos/brainharmony/datasets/datasets.py</code>] Stage\u202f0 ingests <code>(fMRI, T1)</code> pairs via <code>UKB_FusionDataset</code>, Stage\u202f1/2 read <code>.npz</code> embeddings with attention masks/labels using <code>GenerateEmbedDataset(_downstream)</code>.^[<code>566:581:external_repos/brainharmony/datasets/datasets.py</code>][<code>803:857:external_repos/brainharmony/datasets/datasets.py</code>] Provided scripts wrap embedding extraction (Accelerate), JEPA pretraining, and downstream finetuning for reproducibility.^[<code>1:23:external_repos/brainharmony/scripts/harmonizer/stage0_embed/run_embed_pretrain.sh</code>][<code>1:49:external_repos/brainharmony/scripts/harmonizer/stage1_pretrain/run_pretrain.sh</code>][<code>1:59:external_repos/brainharmony/scripts/harmonizer/stage2_finetune/run_finetune.sh</code>] Repo checkout: <code>external_repos/brainharmony</code>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda + pip workflow. Create <code>brainharmonix</code> (Python\u202f3.10), install CUDA\u202f12.4 wheels for PyTorch\u202f2.6, then <code>pip install -r requirements.txt</code> and <code>pip install -e .</code>.^[<code>text title=\"external_repos/brainharmony/README.md</code>] (lines 40-56)\"</li> <li>Checkpoint placement. Download pretrained encoders (harmonix-f/s) plus harmonizer checkpoints and drop them under <code>checkpoints/{harmonix-f,harmonix-s,harmonizer}</code> before running Stage\u202f0/1/2.^[<code>text title=\"external_repos/brainharmony/README.md</code>] (lines 58-71)\"</li> <li>FlashAttention 2 expectation. <code>FlexVisionTransformer</code> selects FlashAttention\u202f2 when installed (see <code>is_flash_attn_2_available</code>) so ensure compatible GPU builds or fall back to \u201ceager\u201d attention.^[<code>text title=\"external_repos/brainharmony/libs/attn_utils/fa2_utils.py</code>][<code>138:214:external_repos/brainharmony/libs/flex_transformer.py</code>] (lines 8-52)\"</li> </ul>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/brainharmony_walkthrough/#stage-0-embedding-extraction-modulesharmonizerstage0_embed","title":"Stage\u202f0: Embedding Extraction (<code>modules/harmonizer/stage0_embed</code>)","text":"<p>Accelerate launches (<code>run_embed_pretrain.sh</code>) call <code>embedding_pretrain.py</code>, which loads configurable datasets (default <code>UKB_FusionDataset</code>) and wraps pretrained fMRI/T1 encoders specified in <code>conf_embed_pretrain.py</code>. The fmri encoder receives gradient+geometric-harmonic positional embeddings, while the MAE-style T1 encoder reuses volumetric patches. Each batch returns fmri tokens, T1 tokens, attention masks, and subject IDs; Stage\u202f0 runs both encoders, concatenates their representations, and persists <code>.npz</code> files along with the attention mask for later stages.^[<code>text title=\"external_repos/brainharmony/scripts/harmonizer/stage0_embed/run_embed_pretrain.sh</code>][<code>48:138:external_repos/brainharmony/configs/harmonizer/stage0_embed/conf_embed_pretrain.py</code>][<code>81:185:external_repos/brainharmony/modules/harmonizer/stage0_embed/embedding_pretrain.py</code>] (lines 1-23)\"</p> external_repos/brainharmony/datasets/datasets.py (lines 566-581)<pre><code>class UKB_FusionDataset(UKBDataset, UKB_T1_Dataset):\n    def __init__(self, **kwargs):\n        UKBDataset.__init__(self, **kwargs)\n        UKB_T1_Dataset.__init__(self, **kwargs)\n\n    def __getitem__(self, index):\n        ts, _ = self.load_fmri(index)\n        attn_mask = self.signal_attn_mask()\n        t1 = self.load_t1(index)\n        return ts, t1, self.patch_size, attn_mask, self.ids[index]\n</code></pre>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#stage-1-token-space-jepa-pretraining-modulesharmonizerstage1_pretrain","title":"Stage\u202f1: Token-Space JEPA Pretraining (<code>modules/harmonizer/stage1_pretrain</code>)","text":"<p>The generated <code>.npz</code> files are streamed by <code>GenerateEmbedDataset</code>, which yields concatenated embeddings and their attention masks; distributed loaders feed <code>OneTokRegViT</code>, a latent-token ViT that appends learnable latent vectors and mask tokens before passing through decoder blocks. <code>train_one_epoch</code> applies Smooth\u202fL1 loss between the predictor output and EMA targets from the frozen teacher encoder, mirroring the JEPA objective.^[<code>text title=\"external_repos/brainharmony/datasets/datasets.py</code>][<code>22:260:external_repos/brainharmony/modules/harmonizer/stage1_pretrain/models.py</code>][<code>13:84:external_repos/brainharmony/modules/harmonizer/stage1_pretrain/engine_pretrain.py</code>][<code>38:49:external_repos/brainharmony/scripts/harmonizer/stage1_pretrain/run_pretrain.sh</code>] (lines 803-825)\"</p> external_repos/brainharmony/datasets/datasets.py (lines 803-825)<pre><code>class GenerateEmbedDataset(Dataset):\n    def __init__(self, root_dir, portion=1.0, seed=42):\n        pattern = os.path.join(root_dir, \"*.npz\")\n        all_files = sorted(glob.glob(pattern))\n        self.files = all_files\n        if len(all_files) == 0:\n            raise RuntimeError(f\"No .npz files found in {root_dir}\")\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        filepath = self.files[idx]\n        arr = np.load(filepath)\n        tensor = torch.from_numpy(arr[\"data\"])\n        if tensor.dtype != torch.float32:\n            tensor = tensor.float()\n        return tensor.squeeze(), arr[\"attn_mask\"].squeeze()\n</code></pre> <p>Encoder with latent tokens:</p> external_repos/brainharmony/modules/harmonizer/stage1_pretrain/models.py (lines 180-223)<pre><code>    def forward_encoder(self, x, attn_mask):\n        target = x\n        if self.add_pre_mapping:\n            x = self.pre_map(x)\n        x = x + self.pos_embed[:, 1:, :]\n        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        latent_tokens = self.latent_tokens.expand(x.shape[0], -1, -1)\n        latent_tokens = latent_tokens + self.enc_latent_token_positional_embedding\n        x = torch.cat([x, latent_tokens], dim=1)\n        pad = torch.ones(attn_mask.shape[0], 1200 + latent_tokens.shape[1], dtype=attn_mask.dtype, device=attn_mask.device)\n        pad_0 = torch.ones(attn_mask.shape[0], 1, dtype=attn_mask.dtype, device=attn_mask.device)\n        attn_mask = torch.cat([pad_0, attn_mask, pad], dim=1)\n        for blk in self.blocks:\n            x = blk(x, attention_mask=attn_mask)\n        x = self.norm(x)\n        latent_tokens = torch.cat([x[:, :1, :], x[:, -self.num_latent_tokens :]], dim=1)\n        return latent_tokens, target\n</code></pre>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#stage-2-downstream-harmonizer-heads-modulesharmonizerstage2_finetune","title":"Stage\u202f2: Downstream Harmonizer Heads (<code>modules/harmonizer/stage2_finetune</code>)","text":"<p>For tasks like ABIDE diagnosis, <code>GenerateEmbedDataset_downstream</code> reads the saved embeddings plus labels, and <code>VisionTransformer</code> attaches either a CLS-token head or global pooler atop the latent-token expanded sequence. Training mixes standard augmentation knobs (mixup/cutmix) with layer-wise LR decay, and evaluation logs accuracy + F1.^[<code>text title=\"external_repos/brainharmony/datasets/datasets.py</code>][<code>1:350:external_repos/brainharmony/modules/harmonizer/stage2_finetune/main_finetune.py</code>][<code>16:166:external_repos/brainharmony/modules/harmonizer/stage2_finetune/engine_finetune.py</code>][<code>1:59:external_repos/brainharmony/scripts/harmonizer/stage2_finetune/run_finetune.sh</code>] (lines 828-857)\"</p> external_repos/brainharmony/modules/harmonizer/stage2_finetune/models.py (lines 117-170)<pre><code>    def forward_features(self, x, attn_mask):\n        if self.add_pre_mapping:\n            x = self.pre_map(x)\n        B = x.shape[0]\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        latent_tokens = self.latent_tokens.expand(x.shape[0], -1, -1)\n        latent_tokens = latent_tokens + self.enc_latent_token_positional_embedding\n        x = torch.cat([x, latent_tokens], dim=1)\n        pad = torch.ones(attn_mask.shape[0], 1200 + latent_tokens.shape[1], dtype=attn_mask.dtype, device=attn_mask.device)\n        pad_0 = torch.ones(attn_mask.shape[0], 1, dtype=attn_mask.dtype, device=attn_mask.device)\n        attn_mask = torch.cat([pad_0, attn_mask, pad], dim=1)\n        for blk in self.blocks:\n            x = blk(x, attention_mask=attn_mask)\n        if self.global_pool:\n            x = torch.cat([x[:, :1, :], x[:, -self.num_latent_tokens :]], dim=1)\n            x = x[:, 1:, :].mean(dim=1)\n            outcome = self.fc_norm(x)\n        else:\n            x = self.norm(x)\n            x = torch.cat([x[:, :1, :], x[:, -self.num_latent_tokens :]], dim=1)\n            outcome = x[:, 0]\n        return outcome\n</code></pre>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#flexvisiontransformer-masked-predictor-libsflex_transformerpy","title":"FlexVisionTransformer &amp; Masked Predictor (<code>libs/flex_transformer.py</code>)","text":"<p><code>FlexVisionTransformer</code> supports flexible patch sizes via <code>FlexiPatchEmbed</code>, optional gradient checkpointing, and either FlashAttention\u202f2 or eager attention blocks. Predictor heads (<code>VisionTransformerPredictor</code>) project encoder outputs into predictor space, tile positional embeddings for masked regions, append learnable mask tokens, and regress back to encoder dimensionality; they reuse <code>apply_masks</code> to select context/target indices.^[<code>text title=\"external_repos/brainharmony/libs/flex_transformer.py</code>][<code>322:463:external_repos/brainharmony/libs/flex_transformer.py</code>] (lines 482-610)\"</p> external_repos/brainharmony/libs/flex_transformer.py (lines 403-463)<pre><code>    def forward(self, x, masks_x, masks, attention_masks=None, return_attention=False):\n        assert (masks is not None) and (masks_x is not None)\n        if not isinstance(masks_x, list):\n            masks_x = [masks_x]\n        if not isinstance(masks, list):\n            masks = [masks]\n        B = len(x) // len(masks_x)\n        x = self.predictor_embed(x)\n        predictor_pos_embed = self.predictor_pos_embed()[1]\n        if self.cls_token is not None:\n            x_pos_embed = predictor_pos_embed.repeat(B, 1, 1)\n            x_pos_embed = apply_masks(x_pos_embed, masks_x, cls_token=True)\n            x += x_pos_embed\n            _, N_ctxt, D = x.shape\n            pos_embs = predictor_pos_embed.repeat(B, 1, 1)\n            pos_embs = apply_masks(pos_embs[:, 1:, :], masks)\n            pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n            pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n            pred_tokens += pos_embs\n        else:\n            x_pos_embed = predictor_pos_embed.repeat(B, 1, 1)\n            x += apply_masks(x_pos_embed, masks_x)\n            _, N_ctxt, D = x.shape\n            pos_embs = predictor_pos_embed.repeat(B, 1, 1)\n            pos_embs = apply_masks(pos_embs, masks)\n            pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n            pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n            pred_tokens += pos_embs\n        x = x.repeat(len(masks), 1, 1)\n        x = torch.cat([x, pred_tokens], dim=1)\n        for blk in self.predictor_blocks:\n            x = blk(x, attention_masks)\n        x = self.predictor_norm(x)\n        x = x[:, N_ctxt:]\n        x = self.predictor_proj(x)\n        return x\n</code></pre>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#positional-embeddings-mask-utilities","title":"Positional Embeddings &amp; Mask Utilities","text":"<p>BrainHarmony blends anatomical gradients with geometric harmonics to produce shared positional priors across encoder/predictor stacks, and the same module can emit decoder embeddings for Stage\u202f1. Mask helpers expose gather-style APIs for re-indexing context/target tokens.^[<code>text title=\"external_repos/brainharmony/libs/position_embedding.py</code>][<code>11:31:external_repos/brainharmony/libs/masks/utils.py</code>] (lines 137-209)\"</p> external_repos/brainharmony/libs/position_embedding.py (lines 167-209)<pre><code>        geo_harm_pos_embed = self.geo_harm_proj(self.geo_harm)\n        gradient_pos_embed = self.grad_proj(self.gradient)\n        pos_embed = (gradient_pos_embed + geo_harm_pos_embed) * 0.5\n        emb_w = pos_embed.squeeze().repeat_interleave(self.repeat_time, dim=0)\n        emb_w = (emb_w - emb_w.min()) / (emb_w.max() - emb_w.min()) * 2 - 1\n        emb_encoder = torch.cat([self.emb_h_encoder, emb_w], dim=1).unsqueeze(0)\n        if self.cls_token:\n            pos_embed_encoder = torch.concat(\n                [torch.zeros([1, 1, emb_encoder.shape[2]], requires_grad=False).to(self.device), emb_encoder],\n                dim=1,\n            )\n        else:\n            pos_embed_encoder = emb_encoder\n        if self.use_pos_embed_decoder:\n            emb_w_decoder = self.decoder_pos_embed_proj(emb_w.detach())\n            emb_decoder = torch.cat([self.emb_h_decoder, emb_w_decoder], dim=1).unsqueeze(0)\n            if self.cls_token:\n                pos_embed_decoder = torch.concat(\n                    [torch.zeros([1, 1, emb_decoder.shape[2]], requires_grad=False).to(self.device), emb_decoder],\n                    dim=1,\n                )\n            else:\n                pos_embed_decoder = emb_decoder\n            return pos_embed_encoder, pos_embed_decoder\n        return pos_embed_encoder, None\n</code></pre> external_repos/brainharmony/libs/masks/utils.py (lines 11-31)<pre><code>def apply_masks(x, masks, cls_token=False):\n    all_x = []\n    if cls_token:\n        cls_t = x[:, :1, :]\n        for m in masks:\n            mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n            all_x += [torch.cat((cls_t, torch.gather(x[:, 1:, :], dim=1, index=mask_keep)), dim=1)]\n    else:\n        for m in masks:\n            mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n            all_x += [torch.gather(x, dim=1, index=mask_keep)]\n    return torch.cat(all_x, dim=0)\n</code></pre>"},{"location":"code_walkthroughs/brainharmony_walkthrough/#integration-hooks-brain-genetics","title":"Integration Hooks (Brain \u2194 Genetics)","text":"<ul> <li>Token shapes. <code>FlexVisionTransformer.forward</code> outputs <code>[B, N_tokens, embed_dim]</code> (CLS + patches [+ latent tokens]); Stage\u202f2 heads either take the CLS vector or mean-pool latent tokens, so downstream genetics encoders should expect 768-d (base) or 1024-d (large) vectors per sample.^[<code>text title=\"external_repos/brainharmony/libs/flex_transformer.py</code>][<code>117:170:external_repos/brainharmony/modules/harmonizer/stage2_finetune/models.py</code>] (lines 563-610)\"</li> <li>Attention masks. Both <code>GenerateEmbedDataset</code> variants surface per-sample masks; reusing them when aligning with long genomic sequences preserves which ROI/time windows were padded vs. observed.^[<code>text title=\"external_repos/brainharmony/datasets/datasets.py</code>] (lines 803-857)\"</li> <li>Stage bridging. Stage\u202f0 writes <code>.npz</code> files with <code>data</code> and <code>attn_mask</code> arrays; you can append additional modalities (e.g., gene-expression embeddings) into the same <code>data</code> vector before Stage\u202f1 as long as the downstream models\u2019 positional encoders are updated accordingly.^[<code>text title=\"external_repos/brainharmony/modules/harmonizer/stage0_embed/embedding_pretrain.py</code>] (lines 138-184)\"</li> <li>Projecting to shared latent spaces. A lightweight projector keeps BrainHarmony tokens compatible with genetics embeddings:</li> </ul> <pre><code>import torch.nn as nn\n\nclass BrainHarmonyProjector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> <p>Map mean-pooled latent tokens through this projector, concatenate with genetics embeddings, and fine-tune a fusion head while reusing BrainHarmony\u2019s attention masks for masking-aware losses.</p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/","title":"Brain-JEPA Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#overview","title":"Overview","text":"<p>Brain-JEPA extends Image/Joint-Embedding Predictive Architecture ideas to 4D fMRI tensors: a Vision Transformer encoder ingests masked spatiotemporal blocks, a predictor Transformer reconstructs masked targets with gradient-informed positional encodings, and masking policies operate jointly across space and time.^[<code>text title=\"external_repos/brainjepa/src/models/vision_transformer.py</code>][<code>18:282:external_repos/brainjepa/src/masks/spatialtemporal_multiblock.py</code>] (lines 1-200)\"</p>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo 4D Swin/ViT encoder + predictor head with gradient positional embeddings^[<code>text title=\"external_repos/brainjepa/src/models/vision_transformer.py</code>] Configurable (base uses ViT-Base + predictor depth 12)^[<code>400:565:external_repos/brainjepa/src/models/vision_transformer.py</code>] 450 ROIs \u00d7 160 time frames (default)^[<code>19:210:external_repos/brainjepa/src/masks/spatialtemporal_multiblock.py</code>] Preprocessed fMRI tensors from <code>fMRIDataset</code> (UKB/S1200)^[<code>14:205:external_repos/brainjepa/src/datasets/ukbiobank_scale.py</code>] Spatiotemporal JEPA pretraining, downstream fine-tuning &amp; linear probing scripts^[<code>67:360:external_repos/brainjepa/src/train.py</code>][<code>15:94:external_repos/brainjepa/downstream_eval.py</code>] github.com/janklees/brainjepa"},{"location":"code_walkthroughs/brainjepa_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda + pip install. Long-context masking requires the project\u2019s Python\u202f3.8 environment: <code>conda create -n brain-jepa python=3.8</code> followed by <code>pip install -r requirement.txt</code>.^[<code>text title=\"external_repos/brainjepa/README.md</code>] (lines 80-82)\"</li> <li>Hardware guidance. Official docs note that pretraining ran on four A100\u202f(40\u202fGB) GPUs and provide the multi-GPU launch command <code>python main.py --fname configs/ukb_vitb_ep300.yaml --devices cuda:0 cuda:1 cuda:2 cuda:3</code>.^[<code>text title=\"external_repos/brainjepa/README.md</code>] (lines 84-92)\"</li> <li>Gradient checkpoint flag. The encoder exposes a <code>gradient_checkpointing</code> argument and wraps each block with <code>torch.utils.checkpoint.checkpoint(...)</code> whenever the flag is set, so you can trade compute for memory on large ROI \u00d7 time grids.^[<code>text title=\"external_repos/brainjepa/src/models/vision_transformer.py</code>] (lines 422-504)\"</li> </ul>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/brainjepa_walkthrough/#dataset-preprocessing-srcdatasetsukbiobank_scalepy","title":"Dataset &amp; Preprocessing (<code>src/datasets/ukbiobank_scale.py</code>)","text":"<p>The dataset class loads ROI-wise time series, applies robust scaling, optional downsampling, and returns tensors shaped <code>[channels, depth, height, width, time]</code> wrapped in dicts (<code>{'fmri': tensor}</code>) for the mask collator.</p> <p>fMRI dataset with ROI time series:</p> external_repos/brainjepa/src/datasets/ukbiobank_scale.py (lines 14-186)<pre><code>class fMRIDataset(Dataset):\n    def __getitem__(self, idx):\n        ts_cortical = self._load_ts(id, self.cortical_file)\n        ts_subcortical = self._load_ts(id, self.subcortical_file)\n        ts_array = np.concatenate((ts_subcortical, ts_cortical), axis=0).astype(np.float32)\n        if self.downsample:\n            ts_array = self._temporal_sampling(...)\n        ts = torch.unsqueeze(torch.from_numpy(ts_array), 0).to(torch.float32)\n        return {'fmri': ts}\n</code></pre>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#mask-collator-srcmasksspatialtemporal_multiblockpy","title":"Mask Collator (<code>src/masks/spatialtemporal_multiblock.py</code>)","text":"<p><code>MaskCollator_fmri</code> samples encoder/predictor windows over ROIs \u00d7 time, enforcing non-overlapping context/target regions and returning boolean masks for each batch.</p> <p>Spatiotemporal masking for JEPA:</p> external_repos/brainjepa/src/masks/spatialtemporal_multiblock.py (lines 18-282)<pre><code>class MaskCollator_fmri(object):\n    def __call__(self, batch):\n        mask_e, _ = self._sample_block_mask_e(e_size)\n        masks_p.append(self._sample_block_mask_p_roi(p_size_roi)[0])\n        mask, mask_C = self._sample_block_mask_p_ts(...)\n        mask_e = self.constrain_e_mask(mask_e, acceptable_regions=masks_C)\n        collated_masks_pred.append([mask_p_final])\n        collated_masks_enc.append([mask_e])\n        return collated_batch, collated_masks_enc, collated_masks_pred\n</code></pre>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#positional-embeddings-encoder-srcmodelsvision_transformerpy","title":"Positional Embeddings &amp; Encoder (<code>src/models/vision_transformer.py</code>)","text":"<p>Gradient-informed positional encoding (<code>GradTs_2dPE</code>) injects atlas gradients, while the encoder (<code>VisionTransformer</code>) patchifies <code>[B, C, D, H, W, T]</code> tensors, adds position encodings, and runs stacked Swin-like blocks.</p> <p>Gradient-informed positional encoding:</p> external_repos/brainjepa/src/models/vision_transformer.py (lines 22-100)<pre><code>class GradTs_2dPE(nn.Module):\n    def __init__(...):\n        self.emb_h = nn.Parameter(...)\n        self.emb_w = ... if add_w == 'origin' else predictor_pos_embed_proj(gradient)\n</code></pre> <p>Vision transformer encoder:</p> external_repos/brainjepa/src/models/vision_transformer.py (lines 430-514)<pre><code>x = self.patch_embed(x)\npos_embed = self.pos_embed_proj(self.gradient_pos_embed)\nx = x + pos_embed\nif masks is not None:\n    x = apply_masks(x, masks)\nfor blk in self.blocks:\n    x = blk(x)\n</code></pre>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#predictor-head-srcmodelsvision_transformerpy","title":"Predictor Head (<code>src/models/vision_transformer.py</code>)","text":"<p>The predictor maps context tokens to a lower-dimensional space, concatenates learnable mask tokens (with their own positional embeddings), and runs Transformer blocks to regress target embeddings.</p> <p>JEPA predictor architecture:</p> external_repos/brainjepa/src/models/vision_transformer.py (lines 280-396)<pre><code>class VisionTransformerPredictor(nn.Module):\n    x = self.predictor_embed(x)\n    predictor_pos_embed = self.predictor_2dpe_proj(self.gradient_pos_embed)\n    pos_embs = apply_masks(predictor_pos_embed.repeat(B, 1, 1), masks)\n    pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n    x = torch.cat([x, pred_tokens + pos_embs], dim=1)\n    for blk in self.predictor_blocks:\n        x = blk(x)\n    x = self.predictor_proj(x[:, N_ctxt:])\n</code></pre>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#training-loop-srctrainpy","title":"Training Loop (<code>src/train.py</code>)","text":"<p>The training script builds data loaders, mask collators, encoder/predictor pairs, and optimizers; the loss is Smooth L1 between predictor outputs and target encoder features.</p> external_repos/brainjepa/src/train.py (lines 215-360)<pre><code>def train_step():\n    def forward_target():\n        with torch.no_grad():\n            h = target_encoder(imgs)\n            h = F.layer_norm(h, (h.size(-1),))\n            h = apply_masks(h, masks_pred)\n            h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n    def forward_context():\n        z = encoder(imgs, masks_enc, return_attention=False)\n        z = predictor(z, masks_enc, masks_pred, return_attention=False)\n    def loss_fn(z, h):\n        return F.smooth_l1_loss(z, h)\n</code></pre>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#downstream-evaluation-downstream_tasksmodels_vitpy","title":"Downstream Evaluation (<code>downstream_tasks/models_vit.py</code>)","text":"<p>Linear-probe and fine-tuning scripts load pretrained encoders, optionally apply gradient checkpointing, and return global-pooled embeddings for classification/regression heads.</p> external_repos/brainjepa/downstream_tasks/models_vit.py (lines 15-74)<pre><code>self.encoder, _ = init_model(...)\nif self.global_pool:\n    outcome = self.fc_norm(self.encoder(x)[:, :, :].mean(dim=1))\nelse:\n    outcome = self.encoder(x)[:, 0]\nx = self.head(outcome)\n</code></pre>"},{"location":"code_walkthroughs/brainjepa_walkthrough/#integration-hooks-brain-genetics","title":"Integration Hooks (Brain \u2194 Genetics)","text":"<ul> <li>Embedding shape. Encoder outputs <code>[B, N_tokens, embed_dim]</code> (after flattening 4D patches). Downstream heads either take the CLS token or mean pool spatial tokens; JEPA predictors output only masked-token predictions shaped <code>[num_masks, embed_dim]</code>.^[<code>text title=\"external_repos/brainjepa/src/models/vision_transformer.py</code>] (lines 280-396)\"</li> <li>Pooling choices. For multimodal fusion, use the downstream <code>VisionTransformer</code> global pool (<code>mean(dim=1)</code>) or compute mean pooling across context tokens to mirror predictor inputs.</li> <li>Projection to shared latent. Map <code>[B, embed_dim]</code> vectors (384/768 for small/base) into 512-D shared space via a lightweight projector:</li> </ul> <p><pre><code>import torch.nn as nn\n\nclass BrainJEPAProjector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> - Mask-aware embeddings. When extracting representations for multimodal tasks, disable masking (feed identity masks) or average multiple masked views to reduce noise; the same mask collator can generate augmented views for contrastive objectives. - Gradient positional alignment. Because <code>GradTs_2dPE</code> injects atlas gradients, keep those embeddings when aligning with genetics\u2014do not strip them off\u2014so the spatial axes remain consistent across modalities.^[<code>text title=\"external_repos/brainjepa/src/models/vision_transformer.py</code>] (lines 22-100)\"</p> <p>Following these hooks yields <code>[B, 512]</code> Brain-JEPA embeddings compatible with projected DNA embeddings (Evo\u202f2, GENERator, Caduceus) for multimodal representation learning.</p>"},{"location":"code_walkthroughs/brainlm_walkthrough/","title":"BrainLM Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/brainlm_walkthrough/#overview","title":"Overview","text":"<p>BrainLM is a ViT-MAE\u2013style masked autoencoder: it slices each voxel\u2019s time course into short windows, randomly masks most of them, and reconstructs the missing segments with Nystromformer encoder layers and a lightweight decoder trained on UK Biobank fMRI.^[<code>text title=\"external_repos/brainlm/README.md</code>][<code>63:205:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>] (lines 1-48)\"</p>"},{"location":"code_walkthroughs/brainlm_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo ViT-MAE encoder (Nystromformer) + MAE decoder^[<code>text title=\"external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>] 111\u202fM / 650\u202fM checkpoints^[<code>39:48:external_repos/brainlm/README.md</code>] 424 parcels \u00d7 490 timepoints patched into windows^[<code>63:200:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>] Arrow datasets of <code>[B, voxels, time]</code> tensors + XYZ coordinates^[<code>43:205:external_repos/brainlm/train.py</code>] Masked reconstruction, downstream probes/fine-tuning via <code>BrainLMTrainer</code>^[<code>351:470:external_repos/brainlm/train.py</code>] github.com/vandijklab/BrainLM"},{"location":"code_walkthroughs/brainlm_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Setup script. The README directs users to run <code>sh setup.sh</code> to create the conda environment (with FlashAttention for the 111\u202fM/650\u202fM checkpoints) and verify PyTorch/HuggingFace installs via the provided sanity commands.^[<code>text title=\"external_repos/brainlm/README.md</code>][<code>50:52:external_repos/brainlm/README.md</code>] (lines 16-26)\"</li> <li>Gradient checkpointing toggle. Both the encoder and decoder wrap their Nystromformer layers with <code>if self.gradient_checkpointing and self.training: torch.utils.checkpoint.checkpoint(...)</code>, so you can enable <code>model.gradient_checkpointing_enable()</code> before large runs to keep memory in check.^[<code>text title=\"external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>][<code>453:500:external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>] (lines 245-269)\"</li> </ul>"},{"location":"code_walkthroughs/brainlm_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/brainlm_walkthrough/#data-collation-brainlmtrainpy","title":"Data &amp; Collation (<code>brainlm/train.py</code>)","text":"<p>Hydra dataclasses declare dataset paths, voxel/time dimensions, and labels; <code>collate_fn</code> stacks tensors into the format expected by the MAE model.</p> <p>Dataset configuration and collation:</p> external_repos/brainlm/train.py (lines 43-210)<pre><code>@dataclass\nclass DataTrainingArguments:\n    train_dataset_path: str\n    val_dataset_path: str\n    coords_dataset_path: str\n    num_timepoints_per_voxel: int = 490\n    timepoint_patching_size: int = 49\n...\ndef collate_fn(examples):\n    signal_vectors = torch.stack([example[\"signal_vectors\"] for example in examples], dim=0)\n    xyz_vectors = torch.stack([example[\"xyz_vectors\"] for example in examples])\n    labels = torch.stack([example[\"label\"] for example in examples])\n    return {\"signal_vectors\": signal_vectors, \"xyz_vectors\": xyz_vectors, \"input_ids\": signal_vectors, \"labels\": labels}\n</code></pre>"},{"location":"code_walkthroughs/brainlm_walkthrough/#embeddings-masking-brainlm_maemodeling_brainlmpy","title":"Embeddings &amp; Masking (<code>brainlm_mae/modeling_brainlm.py</code>)","text":"<p><code>BrainLMEmbeddings</code> reshapes time signals into patches, projects signals and spatial coordinates, injects positional encoding, and randomly masks patches before appending a CLS token.</p> <p>Signal embedding and masking logic:</p> external_repos/brainlm/brainlm_mae/modeling_brainlm.py (lines 63-160)<pre><code>reshaped_signal_vectors = torch.reshape(signal_vectors, (batch, num_voxels, -1, self.timepoint_patching_size))\nsignal_projection = self.signal_embedding_projection(reshaped_signal_vectors)\nxyz_projection = self.xyz_embedding_projection(xyz_vectors).unsqueeze(2).repeat(1, 1, num_patch_tokens, 1)\nx = self.pos_embedding(signal_projection + xyz_projection)\nembeddings, mask, ids_restore = self.random_masking(x, noise=noise)\ncls_tokens = self.cls_token.expand(embeddings.shape[0], -1, -1)\nembeddings = torch.cat((cls_tokens, embeddings), dim=1)\n</code></pre>"},{"location":"code_walkthroughs/brainlm_walkthrough/#encoder-decoder-brainlm_maemodeling_brainlmpy","title":"Encoder &amp; Decoder (<code>brainlm_mae/modeling_brainlm.py</code>)","text":"<p>The encoder stacks Nystromformer layers, while the decoder reintroduces mask tokens, adds spatial/time encodings again, and predicts the missing time windows.</p> <p>MAE encoder structure:</p> external_repos/brainlm/brainlm_mae/modeling_brainlm.py (lines 227-340)<pre><code>class BrainLMModel(ViTMAEModel):\n    self.embeddings = BrainLMEmbeddings(config)\n    self.encoder = BrainLMEncoder(config)\n    encoder_outputs = self.encoder(embedding_output, ...)\n    sequence_output = self.layernorm(encoder_outputs[0])\n</code></pre> <p>Decoder with mask token reconstruction:</p> external_repos/brainlm/brainlm_mae/modeling_brainlm.py (lines 355-515)<pre><code>mask_tokens = self.mask_token.repeat(batch_size, num_mask_tokens, 1)\nx_ = torch.reshape(x_, (batch_size, self.num_brain_voxels, num_patch_tokens, hidden_dim))\nx_ = x_ + self.decoder_xyz_projection(xyz_vectors)\nx_ = self.pos_embedding(x_)\nlogits = self.decoder_pred2(self.decoder_pred_nonlinearity(self.decoder_pred1(hidden_states)))\nlogits = torch.reshape(logits, (batch_size, self.num_brain_voxels, ..., self.timepoint_patching_size))\n</code></pre>"},{"location":"code_walkthroughs/brainlm_walkthrough/#loss-brainlm_maemodeling_brainlmpy","title":"Loss (<code>brainlm_mae/modeling_brainlm.py</code>)","text":"<p>Masked reconstruction loss is computed only on the masked tokens (MSE or MAE).</p> <p>Masked reconstruction loss computation:</p> external_repos/brainlm/brainlm_mae/modeling_brainlm.py (lines 562-584)<pre><code>mask = mask.unsqueeze(-1).repeat(1, 1, 1, pred_values.shape[-1])\nif self.config.loss_fn == \"mse\":\n    loss = (((pred_values - signal_values) ** 2) * mask).sum() / mask.sum()\nelif self.config.loss_fn == \"mae\":\n    loss = abs((pred_values - signal_values) * mask).sum() / mask.sum()\n</code></pre>"},{"location":"code_walkthroughs/brainlm_walkthrough/#training-driver-brainlmtrainpy","title":"Training Driver (<code>brainlm/train.py</code>)","text":"<p><code>BrainLMTrainer</code> glues everything together\u2014optimizer, scheduler, metrics, evaluation.</p> external_repos/brainlm/train.py (lines 351-470)<pre><code>trainer = BrainLMTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=None,\n    data_collator=collate_fn,\n    compute_metrics=metrics.compute_metrics,\n)\ntrainer.train()\n</code></pre>"},{"location":"code_walkthroughs/brainlm_walkthrough/#integration-hooks-brain-genetics","title":"Integration Hooks (Brain \u2194 Genetics)","text":"<ul> <li>Embedding shape. Encoder outputs <code>[B, (num_voxels * kept_tokens) + 1, hidden]</code>. Index\u202f0 is CLS; the rest represent unmasked voxel windows sorted deterministically.^[<code>text title=\"external_repos/brainlm/brainlm_mae/modeling_brainlm.py</code>] (lines 329-350)\"</li> <li>Pooling choices. CLS pooling mirrors the MAE training objective; mean pooling across tokens smooths noise; reshaping tokens back to <code>[B, voxels, windows, hidden]</code> lets you average over time first, then voxels.</li> <li>Projection to shared latent. Map pooled <code>[B, hidden]</code> vectors (hidden\u2248768 on the 111\u202fM model) into a 512-D shared space:</li> </ul> <pre><code>import torch.nn as nn\n\nclass BrainLMProjector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <ul> <li>Masking control. When extracting embeddings, set <code>mask_ratio=0.0</code> so every patch contributes; enable masking only for pretraining/augmentation.</li> <li>Alignment with genetics. After projection, normalize (LayerNorm or z-score) before concatenating with genetic embeddings (Evo\u202f2, GENERator, Caduceus) or using contrastive loss.</li> </ul> <p>This workflow delivers <code>[B, 512]</code> fMRI embeddings that align with projected DNA representations for multimodal analyses.</p>"},{"location":"code_walkthroughs/brainmt_walkthrough/","title":"BrainMT Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/brainmt_walkthrough/#overview","title":"Overview","text":"<p>BrainMT pairs bidirectional Mamba mixers (temporal-first scanning) with MHSA transformer blocks to model long-range fMRI dynamics, delivering state-of-the-art regression/classification on UKB and HCP phenotypes.^[<code>text title=\"external_repos/brainmt/README.md</code>][<code>294:462:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] The architecture is now described in an official conference paper (SpringerLink, Lecture Notes in Computer Science, pp. 150\u2013160; first online 19 September 2025), so reference the proceedings PDF in <code>docs/generated/kb_curated/papers-pdf/brainmt_2025.pdf</code> when citing. (lines 3-170)\"</p>"},{"location":"code_walkthroughs/brainmt_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo 3D Conv patch embed \u2192 bidirectional Mamba blocks \u2192 Transformer attention blocks^[<code>text title=\"external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] Configurable (default hidden 512, depth <code>[12,8]</code>)^[<code>293:375:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] 91\u00d7109\u00d791 voxels \u00d7 200 frames (default)^[<code>294:339:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] Preprocessed <code>.pt</code> tensors from <code>data/datasets.py</code>^[<code>15:80:external_repos/brainmt/src/brainmt/data/datasets.py</code>] DDP training with regression/classification heads, inference utilities^[<code>1:330:external_repos/brainmt/src/brainmt/train.py</code>][<code>1:390:external_repos/brainmt/src/brainmt/inference.py</code>] github.com/arunkumar-kannan/brainmt-fmri"},{"location":"code_walkthroughs/brainmt_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Exact environment commands. The README targets Python\u202f3.9.18 + PyTorch\u202f2.6/CUDA\u202f12.4, created via <code>python -m venv brainmt_env</code>, <code>source brainmt_env/bin/activate</code>, and <code>pip install -r requirements.txt</code>.^[<code>text title=\"external_repos/brainmt/README.md</code>] (lines 44-60)\"</li> <li>Gradient checkpoint flag. Every Mamba block accepts <code>use_checkpoint</code> and conditionally calls <code>checkpoint.checkpoint(...)</code>, so you can instantiate <code>BrainMT(..., use_checkpoint=True)</code> to reduce memory usage on long temporal contexts.^[<code>text title=\"external_repos/brainmt/src/brainmt/models/brain_mt.py</code>][<code>293:334:external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] (lines 95-125)\"</li> </ul>"},{"location":"code_walkthroughs/brainmt_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/brainmt_walkthrough/#dataset-loader-srcbrainmtdatadatasetspy","title":"Dataset Loader (<code>src/brainmt/data/datasets.py</code>)","text":"<p>The dataset stores fMRI volumes as fp16 tensors (<code>func_data_MNI_fp16.pt</code>), slices contiguous time segments, permutes them into <code>[frames, channel, depth, height, width]</code>, and returns <code>(tensor, target)</code> pairs.</p> <p>fMRI dataset with temporal slicing:</p> external_repos/brainmt/src/brainmt/data/datasets.py (lines 15-80)<pre><code>class fMRIDataset(Dataset):\n    def __getitem__(self, idx):\n        data = torch.load(img_file)\n        start_index = torch.randint(0, total_frames - num_frames + 1, (1,)).item()\n        data_sliced = data[:, :, :, start_index:end_index]\n        data_global = data_sliced.unsqueeze(0).permute(4, 0, 2, 1, 3)\n        target = self.target_dict[subject_dir]\n        return data_global, torch.tensor(target, dtype=torch.float16)\n</code></pre>"},{"location":"code_walkthroughs/brainmt_walkthrough/#patch-embed-conv-blocks-srcbrainmtmodelsbrain_mtpy","title":"Patch Embed &amp; Conv Blocks (<code>src/brainmt/models/brain_mt.py</code>)","text":"<p><code>PatchEmbed</code> downsamples the 4D tensor with strided 3D convolutions before two ConvBlocks + Downsample layers reduce spatial resolution while keeping temporal length.</p> <p>3D convolution patch embedding:</p> external_repos/brainmt/src/brainmt/models/brain_mt.py (lines 202-263)<pre><code>class PatchEmbed(nn.Module):\n    self.conv_down = nn.Sequential(\n        nn.Conv3d(in_chans, in_dim, 3, 2, 1, bias=False),\n        nn.ReLU(),\n        nn.Conv3d(in_dim, dim, 3, 2, 1, bias=False),\n        nn.ReLU()\n    )\n</code></pre>"},{"location":"code_walkthroughs/brainmt_walkthrough/#hybrid-mamba-transformer-backbone-srcbrainmtmodelsbrain_mtpy","title":"Hybrid Mamba + Transformer Backbone (<code>src/brainmt/models/brain_mt.py</code>)","text":"<p>Temporal-first processing reshapes tokens, feeds them through <code>create_block</code> (bidirectional Mamba) and then through transformer attention + MLP to capture residual spatial dependencies.</p> <p>Bidirectional Mamba blocks followed by transformer attention:</p> external_repos/brainmt/src/brainmt/models/brain_mt.py (lines 331-462)<pre><code>self.layers = nn.ModuleList([\n    create_block(embed_dim, ssm_cfg=ssm_cfg, ..., drop_path=inter_dpr[i], ...)\n    for i in range(depth[0])\n])\nself.blocks = nn.ModuleList([\n    Attention(embed_dim, num_heads=num_heads, ...)\n    for i in range(depth[1])\n])\n...\ndef forward_features(self, x, ...):\n    x = self.patch_embed(x)\n    x = self.conv_block0(x); x = self.downsample0(x)\n    x = self.conv_block1(x); x = self.downsample1(x)\n    x = rearrange(x, '(b t) n m -&gt; (b n) t m', b=B, t=T)\n    x = x + self.temporal_pos_embedding\n    for layer in self.layers:\n        hidden_states, residual = layer(hidden_states, residual, ...)\n    for block in self.blocks:\n        hidden_states = hidden_states + drop_path_attn(block(self.norm(hidden_states)))\n</code></pre>"},{"location":"code_walkthroughs/brainmt_walkthrough/#forward-head-srcbrainmtmodelsbrain_mtpy","title":"Forward &amp; Head (<code>src/brainmt/models/brain_mt.py</code>)","text":"<p>CLS token is prepended before Mamba blocks; <code>forward</code> returns final MLP head output for regression/classification.</p> <p>CLS token prepending and final head:</p> external_repos/brainmt/src/brainmt/models/brain_mt.py (lines 400-461)<pre><code>cls_token = self.cls_token.expand(x.shape[0], -1, -1)\nx = torch.cat((cls_token, x), dim=1)\n...\nreturn hidden_states[:, 0, :]\n</code></pre>"},{"location":"code_walkthroughs/brainmt_walkthrough/#training-loop-srcbrainmttrainpy","title":"Training Loop (<code>src/brainmt/train.py</code>)","text":"<p>Hydra config builds datasets, wraps the model in DDP, selects loss (MSE or BCEWithLogits), constructs layer-wise LR decay groups, and trains with <code>GradScaler</code> + cosine warm restarts.</p> <p>DDP training with mixed precision:</p> external_repos/brainmt/src/brainmt/train.py (lines 132-234)<pre><code>model = BrainMT(**model_config).to(device)\nmodel = nn.parallel.DistributedDataParallel(model, device_ids=[device_id], ...)\nif cfg.task.loss_fn == \"mse\":\n    criteria = nn.MSELoss()\n...\ntrain_loss, train_outputs, train_targets = train_one_epoch(model, criteria, train_loader, optimizer, scaler, device, epoch, cfg)\nval_loss, val_outputs, val_targets = evaluate(model, criteria, val_loader, device)\n</code></pre>"},{"location":"code_walkthroughs/brainmt_walkthrough/#inference-srcbrainmtinferencepy","title":"Inference (<code>src/brainmt/inference.py</code>)","text":"<p>The inference script mirrors dataset splits, loads checkpoints, and computes detailed metrics (accuracy/AUROC for classification, MSE/MAE/R\u00b2/Pearson for regression), plus optional plots.</p> <p>Checkpoint loading and metric computation:</p> external_repos/brainmt/src/brainmt/inference.py (lines 26-210)<pre><code>model = BrainMT(**model_config).to(device)\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nif cfg.task.name == 'classification':\n    metrics = calculate_classification_metrics(test_outputs, test_targets)\nelse:\n    metrics = calculate_regression_metrics(test_outputs, test_targets)\n</code></pre>"},{"location":"code_walkthroughs/brainmt_walkthrough/#integration-hooks-brain-genetics","title":"Integration Hooks (Brain \u2194 Genetics)","text":"<ul> <li>Embedding shape. <code>forward_features</code> returns <code>[B, hidden]</code> CLS vectors (hidden default 512). To access intermediate token embeddings, tap <code>hidden_states[:, 1:, :]</code> before the final average/MLP.^[<code>text title=\"external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] (lines 400-462)\"</li> <li>Pooling choices. CLS token encodes temporal-first, globally attentive context. For voxel-conditioned embeddings, reshape post-Mamba tensor back to <code>[B, voxels, hidden]</code> prior to the transformer block and average along the voxel axis.</li> <li>Projection to shared latent. Map <code>[B, 512]</code> BrainMT vectors into a 512-D multimodal space with a lightweight projector:</li> </ul> <p><pre><code>import torch.nn as nn\n\nclass BrainMTProjector(nn.Module):\n    def __init__(self, input_dim=512, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> - Normalization. Because BrainMT ends with LayerNorm (<code>self.norm_f</code>), additional LayerNorm in the projector keeps the scale comparable to genetic embeddings. - Temporal handling. The temporal-first scan (<code>rearrange(..., b n) t m -&gt; b (n t) m</code>) is crucial for long-range modeling\u2014preserve this ordering if you export intermediate features for contrastive alignment with DNA sequences.^[<code>text title=\"external_repos/brainmt/src/brainmt/models/brain_mt.py</code>] (lines 421-444)\"</p> <p>Projected BrainMT features can then be concatenated or contrastively aligned with Evo\u202f2/GENERator/Caduceus embeddings to study genetics\u2194fMRI correspondences.</p>"},{"location":"code_walkthroughs/caduceus_walkthrough/","title":"Caduceus Code Walkthrough","text":"<p>KB references: Model card \u00b7 Genomics feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/caduceus_walkthrough/#overview","title":"Overview","text":"<p>Caduceus couples Mamba sequence mixers with reverse-complement parameter sharing so every layer sees both DNA strands simultaneously, yielding 131\u202fkbp masked-language-model checkpoints that remain equivariant to strand flips across the published HuggingFace collection.^[<code>text title=\"external_repos/caduceus/README.md</code>] (lines 6-141)\"</p>"},{"location":"code_walkthroughs/caduceus_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo RC-equivariant BiMamba blocks inside <code>CaduceusMixerModel</code>^[<code>text title=\"external_repos/caduceus/caduceus/modeling_caduceus.py</code>] ~150\u202fM (e.g., 256\u202f\u00d7\u202f16 layers in HF releases)^[<code>15:22:external_repos/caduceus/README.md</code>] 131\u202fkbp pretraining windows^[<code>15:22:external_repos/caduceus/README.md</code>] Character tokenizer w/ explicit complement map and BOS/PAD logic^[<code>15:158:external_repos/caduceus/src/dataloaders/datasets/hg38_char_tokenizer.py</code>] Lightning training for pretraining/downstream tasks, RC embeddings for VEP^[<code>1:400:external_repos/caduceus/train.py</code>][<code>30:399:external_repos/caduceus/vep_embeddings.py</code>] github.com/kuleshov-group/caduceus"},{"location":"code_walkthroughs/caduceus_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda bootstrap. Long-context experiments rely on the repo\u2019s environment file; create it with <code>conda env create -f caduceus_env.yml</code> and activate via <code>conda activate caduceus_env</code> before running the Hydra configs.^[<code>text title=\"external_repos/caduceus/README.md</code>] (lines 53-63)\"</li> <li>Gradient checkpointing status. The backbone still has a TODO for checkpointing and the HF wrapper sets <code>supports_gradient_checkpointing = False</code>, so memory savings must come from RCPS channel splitting or ZeRO rather than built-in checkpoint flags.^[<code>text title=\"external_repos/caduceus/caduceus/modeling_caduceus.py</code>][<code>297:302:external_repos/caduceus/caduceus/modeling_caduceus.py</code>] (lines 228-231)\"</li> </ul>"},{"location":"code_walkthroughs/caduceus_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/caduceus_walkthrough/#tokenizer-preprocessing-hg38_char_tokenizerpy-rcpy","title":"Tokenizer &amp; Preprocessing (<code>hg38_char_tokenizer.py</code>, <code>rc.py</code>)","text":"<p>Tokenization is strictly character-based, enumerating all specials and precomputing a complement map so RCPS layers can look up complements without re-tokenizing. String-level utilities supply reverse complements for augmentation or evaluation.</p> <p>Character-based vocabulary with complement mapping:</p> external_repos/caduceus/src/dataloaders/datasets/hg38_char_tokenizer.py (lines 15-74)<pre><code>class CharacterTokenizer(PreTrainedTokenizer):\n    self._vocab_str_to_int = {\n        \"[CLS]\": 0,\n        ...\n        \"[UNK]\": 6,\n        **{ch: i + 7 for i, ch in enumerate(characters)},\n    }\n    ...\n    complement_map = {\"A\": \"T\", \"C\": \"G\", \"G\": \"C\", \"T\": \"A\"}\n    self.complement_map[self._vocab_str_to_int[k]] = complement_id\n</code></pre> <p>Reverse complement utilities:</p> external_repos/caduceus/src/dataloaders/utils/rc.py (lines 7-27)<pre><code>STRING_COMPLEMENT_MAP = {\"A\": \"T\", ...}\ndef string_reverse_complement(seq):\n    rev_comp = \"\"\n    for base in seq[::-1]:\n        rev_comp += STRING_COMPLEMENT_MAP.get(base, base)\n    return rev_comp\n</code></pre>"},{"location":"code_walkthroughs/caduceus_walkthrough/#positional-rc-handling-modeling_caduceuspy","title":"Positional &amp; RC Handling (<code>modeling_caduceus.py</code>)","text":"<p>Bidirectional Mamba wrappers run forward and reverse streams (optionally weight tied) and merge them, while RCPS-aware embeddings split channel dimensions so hidden states encode forward and RC halves that can be combined later.</p> <p>Bidirectional Mamba wrapper:</p> external_repos/caduceus/caduceus/modeling_caduceus.py (lines 87-141)<pre><code>class BiMambaWrapper(nn.Module):\n    self.mamba_fwd = Mamba(...)\n    if bidirectional:\n        self.mamba_rev = Mamba(...)\n        if bidirectional_weight_tie:\n            self.mamba_rev.in_proj.weight = self.mamba_fwd.in_proj.weight\n    def forward(...):\n        out = self.mamba_fwd(hidden_states, ...)\n        if self.bidirectional:\n            out_rev = self.mamba_rev(hidden_states.flip(dims=(1,))).flip(dims=(1,))\n            out = out + out_rev if self.bidirectional_strategy == \"add\" else out * out_rev\n</code></pre> <p>RCPS-aware embeddings and mixer:</p> external_repos/caduceus/caduceus/modeling_caduceus.py (lines 152-214)<pre><code>class CaduceusEmbeddings(nn.Module):\n    if config.rcps:\n        self.word_embeddings = RCPSEmbedding(...)\n...\nclass CaduceusMixerModel(nn.Module):\n    self.layers = nn.ModuleList([\n        create_block(..., rcps=config.rcps, ...) for i in range(config.n_layer)\n    ])\n    self.norm_f = norm_f if (config.fused_add_norm or not config.rcps) else RCPSAddNormWrapper(norm_f)\n</code></pre>"},{"location":"code_walkthroughs/caduceus_walkthrough/#backbone-embedding-wrapper-dna_embeddingpy","title":"Backbone &amp; Embedding Wrapper (<code>dna_embedding.py</code>)","text":"<p><code>DNAEmbeddingModelCaduceus</code> strips the LM head and exposes hidden states shaped <code>[B, L, d]</code> for standard mode or <code>[B, L, d, 2]</code> when RCPS/conjoined inference is enabled.</p> <p>Embedding extraction with RC handling:</p> external_repos/caduceus/src/models/sequence/dna_embedding.py (lines 156-195)<pre><code>class DNAEmbeddingModelCaduceus(DNAEmbeddingModel):\n    def forward(...):\n        if self.config.rcps:\n            hidden_states = self.caduceus(input_ids, return_dict=False)\n            num_chan = hidden_states.shape[-1]\n            return torch.stack(\n                [hidden_states[..., :num_chan // 2], torch.flip(hidden_states[..., num_chan // 2:], dims=[1, 2])],\n                dim=-1\n            ), None\n        if self.conjoin_train or (self.conjoin_test and not self.training):\n            hidden_states = self.caduceus(input_ids[..., 0], return_dict=False)\n            hidden_states_rc = self.caduceus(input_ids[..., 1], return_dict=False)\n            return torch.stack([hidden_states, hidden_states_rc], dim=-1), None\n        return self.caduceus(input_ids, return_dict=False), None\n</code></pre>"},{"location":"code_walkthroughs/caduceus_walkthrough/#training-loop-trainpy","title":"Training Loop (<code>train.py</code>)","text":"<p>The Lightning <code>SequenceLightningModule</code> builds datasets/encoders from Hydra configs, forwards batches through encoder/decoder stacks, logs losses/metrics, and supports distributed strategies plus gradient accumulation.</p> <p>PyTorch Lightning module structure:</p> external_repos/caduceus/train.py (lines 126-377)<pre><code>class SequenceLightningModule(pl.LightningModule):\n    def setup(...):\n        self.dataset = SequenceDataset.registry[self.hparams.dataset._name_](**self.hparams.dataset)\n        ...\n        self.encoder = U.PassthroughSequential(self.task.encoder, encoder)\n        self.decoder = U.PassthroughSequential(decoder, self.task.decoder)\n        self.loss = self.task.loss\n    def _shared_step(...):\n        x, y, w = self.forward(batch)\n        loss = self.loss(x, y, **w)\n        metrics = self.metrics(x, y, **w)\n        self.log_dict({f\"{prefix}/{k}\": v for k, v in metrics.items()}, ...)\n</code></pre>"},{"location":"code_walkthroughs/caduceus_walkthrough/#inference-vep-embeddings-vep_embeddingspy","title":"Inference &amp; VEP Embeddings (<code>vep_embeddings.py</code>)","text":"<p>The VEP helper loads any HF model (Caduceus by default), tokenizes ref/alt + RC windows, averages variant-centered windows, and writes <code>.pt</code> tensors per split\u2014handy for multimodal or downstream regression.</p> external_repos/caduceus/vep_embeddings.py (lines 30-392)<pre><code>class DNAEmbeddingModel(nn.Module):\n    def forward(self, input_ids):\n        return self.backbone(input_ids).last_hidden_state\n...\ntokens_window_ref = torch.gather(\n    item_ref, 1,\n    expanded_indices.unsqueeze(-1).expand(-1, -1, item_ref.size(2))\n).mean(dim=1)\nstorage_dict[\"concat_avg_ws\"] = torch.cat([tokens_window_ref, tokens_window_alt], dim=-1)\n</code></pre>"},{"location":"code_walkthroughs/caduceus_walkthrough/#sequence-constraints-configuration_caduceuspy","title":"Sequence Constraints (<code>configuration_caduceus.py</code>)","text":"<p>All RC/bidirectional behavior is driven by config: enabling RCPS, picking merge strategy (<code>add</code> vs <code>ew_multiply</code>), and passing complement maps extracted from the tokenizer.</p> external_repos/caduceus/caduceus/configuration_caduceus.py (lines 10-55)<pre><code>class CaduceusConfig(PretrainedConfig):\n    def __init__(..., bidirectional: bool = True, bidirectional_strategy: Union[str, None] = \"add\",\n                 bidirectional_weight_tie: bool = True, rcps: bool = False, complement_map: Optional[dict] = None, ...):\n        self.bidirectional = bidirectional\n        self.bidirectional_strategy = bidirectional_strategy\n        self.bidirectional_weight_tie = bidirectional_weight_tie\n        self.rcps = rcps\n        self.complement_map = complement_map\n</code></pre>"},{"location":"code_walkthroughs/caduceus_walkthrough/#integration-hooks-genetics-brain","title":"Integration Hooks (Genetics \u2194 Brain)","text":"<ul> <li>Embedding shapes. Outputs are <code>[B, L, d]</code> by default, <code>[B, L, d, 2]</code> when strands are stacked, or <code>[B, K, 2d]</code> after VEP window pooling\u2014mean over tokens to get <code>[B, d]</code> per strand before any fusion.^[<code>text title=\"external_repos/caduceus/src/models/sequence/dna_embedding.py</code>][<code>275:385:external_repos/caduceus/vep_embeddings.py</code>] (lines 156-195)\"</li> <li>Pooling choices. Mean pooling across tokens mirrors the masked-LM objective; to keep strand info, average forward and RC halves separately, then concatenate them to <code>[B, 2d]</code>.</li> <li>Projection to shared latent. Map pooled vectors into a 512-D multimodal space with a lightweight projector:</li> </ul> <pre><code>import torch.nn as nn\n\nclass CaduceusProjector(nn.Module):\n    def __init__(self, input_dim=512, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <ul> <li>Normalization &amp; RC alignment. When RCPS doubles channels, follow the model\u2019s own <code>RCPSAddNormWrapper</code> convention: flip the RC half, sum/average with the forward half, and only then project\u2014this keeps embeddings invariant to strand order.^[<code>text title=\"external_repos/caduceus/caduceus/modeling_caduceus.py</code>] (lines 210-274)\"</li> <li>Batch &amp; memory tips. The Lightning module instantiates DDP-aware samplers and can rehydrate checkpoints via <code>utils.instantiate</code>, so mirror that pattern (especially <code>_shared_step</code>) when adding multimodal heads to avoid redundant forward passes.^[<code>text title=\"external_repos/caduceus/train.py</code>] (lines 291-377)\"</li> </ul> <p>Using this flow yields <code>[B, 512]</code> Caduceus embeddings ready to align with BrainLM CLS tokens, average BrainMT CLS outputs, or SwiFT pooled features for cross-modal genetics\u2194fMRI experiments.</p>"},{"location":"code_walkthroughs/dnabert2_walkthrough/","title":"DNABERT-2 Code Walkthrough","text":"<p>KB references: Model card \u00b7 Genomics feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#overview","title":"Overview","text":"<p>DNABERT\u20112 swaps classic k-mer vocabularies for a DNA BPE tokenizer backed by ALiBi positional bias in a 117\u202fM parameter BERT encoder, and the repo focuses on supervised fine-tuning utilities for the Genome Understanding Evaluation benchmark.^[<code>text title=\"external_repos/dnabert2/README.md</code>] (lines 30-110)\"</p>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo BERT encoder with ALiBi bias + BPE tokenizer^[<code>text title=\"external_repos/dnabert2/README.md</code>] 117\u202fM (<code>zhihan1996/DNABERT-2-117M</code>)^[<code>30:94:external_repos/dnabert2/README.md</code>] 512 tokens (default <code>model_max_length</code>)^[<code>43:99:external_repos/dnabert2/finetune/train.py</code>] Optional k-mer preprocessing plus HF tokenizer/padding^[<code>79:185:external_repos/dnabert2/finetune/train.py</code>] Supervised fine-tuning, LoRA adapters, k-mer augmentation, evaluation^[<code>33:304:external_repos/dnabert2/finetune/train.py</code>] github.com/Zhihan1996/DNABERT2"},{"location":"code_walkthroughs/dnabert2_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Exact install command. The README instructs you to run <code>python3 -m pip install -r requirements.txt</code> (after cloning) to pull in the exact transformer/torch stack used for the GUE benchmark; no additional kernels are required because contexts stay at 512 tokens.^[<code>text title=\"external_repos/dnabert2/README.md</code>] (lines 54-68)\"</li> </ul>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/dnabert2_walkthrough/#tokenizer-dataset-pipeline-finetunetrainpy","title":"Tokenizer &amp; Dataset Pipeline (<code>finetune/train.py</code>)","text":"<p><code>SupervisedDataset</code> ingests CSVs, optionally swaps sequences for cached k-mer strings, and tokenizes them with the HF tokenizer, honoring <code>model_max_length</code> and storing label counts for dynamic classifier heads.</p> <p>Supervised dataset with k-mer preprocessing:</p> external_repos/dnabert2/finetune/train.py (lines 79-160)<pre><code>class SupervisedDataset(Dataset):\n    def __init__(..., kmer: int = -1):\n        with open(data_path, \"r\") as f:\n            data = list(csv.reader(f))[1:]\n        ...\n        if kmer != -1:\n            texts = load_or_generate_kmer(data_path, texts, kmer)\n        output = tokenizer(\n            texts,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        )\n        self.input_ids = output[\"input_ids\"]\n        self.labels = labels\n</code></pre>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#k-mer-utilities-finetunetrainpy","title":"K-mer Utilities (<code>finetune/train.py</code>)","text":"<p>Helper functions create or cache k-mer corpora for experiments that compare raw BPE tokens vs explicit k-mer inputs.</p> <p>K-mer generation and caching:</p> external_repos/dnabert2/finetune/train.py (lines 79-109)<pre><code>def generate_kmer_str(sequence: str, k: int) -&gt; str:\n    return \" \".join([sequence[i:i+k] for i in range(len(sequence) - k + 1)])\n...\ndef load_or_generate_kmer(...):\n    if os.path.exists(kmer_path):\n        with open(kmer_path, \"r\") as f:\n            kmer = json.load(f)\n    else:\n        kmer = [generate_kmer_str(text, k) for text in texts]\n        with open(kmer_path, \"w\") as f:\n            json.dump(kmer, f)\n</code></pre>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#backbone-lora-finetunetrainpy","title":"Backbone &amp; LoRA (<code>finetune/train.py</code>)","text":"<p>The trainer script loads <code>AutoModelForSequenceClassification</code>, optionally wraps LoRA adapters (with user-specified target modules), and defers the loss/eval loop to <code>transformers.Trainer</code>.</p> <p>Model loading with optional LoRA adapters:</p> external_repos/dnabert2/finetune/train.py (lines 235-304)<pre><code>tokenizer = transformers.AutoTokenizer.from_pretrained(..., model_max_length=training_args.model_max_length, ...)\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(\n    model_args.model_name_or_path,\n    cache_dir=training_args.cache_dir,\n    num_labels=train_dataset.num_labels,\n    trust_remote_code=True,\n)\nif model_args.use_lora:\n    lora_config = LoraConfig(...)\n    model = get_peft_model(model, lora_config)\ntrainer = transformers.Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator)\ntrainer.train()\n</code></pre>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#embedding-access-readme","title":"Embedding Access (README)","text":"<p>You can grab <code>[CLS]</code> embeddings straight from <code>AutoModel</code>, as shown in the README example, to feed into multimodal projectors.</p> external_repos/dnabert2/README.md (lines 98-110)<pre><code>inputs = tokenizer(dna, return_tensors = 'pt')[\"input_ids\"]\nhidden_states = model(inputs)[0] # [1, sequence_length, 768]\ncls_embedding = hidden_states[:, 0, :]\n</code></pre>"},{"location":"code_walkthroughs/dnabert2_walkthrough/#integration-hooks-genetics-brain","title":"Integration Hooks (Genetics \u2194 Brain)","text":"<ul> <li>Embedding shape. HF models return <code>last_hidden_state</code> shaped <code>[B, L_tokens, 768]</code>. Use the <code>[:, 0, :]</code> CLS token for classification-style features or mean-pool across tokens for regression-style features.^[<code>text title=\"external_repos/dnabert2/README.md</code>] (lines 98-110)\"</li> <li>Pooling choices. CLS pooling mirrors the pretraining objective; mean pooling smooths noise, and max pooling highlights motifs. You can concatenate these pooled views before projection if you need richer features.</li> <li>Projection to shared latent. Map <code>[B, 768]</code> vectors into a 512-D brain space:</li> </ul> <pre><code>import torch.nn as nn\n\nclass DNABERT2Projector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> <ul> <li>Normalization. LayerNorm (as above) or z-scoring per batch keeps embeddings compatible with BrainLM/BrainMT outputs that also end in norm layers.</li> <li>Sequence hygiene. Stick to uppercase A/C/G/T before tokenization; when enabling k-mer augmentation, remember the transformation shortens the sequence by <code>k-1</code>, so adjust <code>model_max_length</code> in <code>TrainingArguments</code> to avoid truncation.^[<code>text title=\"external_repos/dnabert2/finetune/train.py</code>][<code>43:52:external_repos/dnabert2/finetune/train.py</code>] (lines 79-109)\"</li> </ul> <p>With these steps you can turn DNABERT\u20112 outputs into <code>[B, 512]</code> embeddings ready for concatenation or contrastive alignment with BrainLM CLS tokens, BrainMT CLS vectors, or SwiFT pooled representations.</p>"},{"location":"code_walkthroughs/evo2_walkthrough/","title":"Evo 2 Code Walkthrough","text":"<p>KB references: Model card \u00b7 Genomics feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/evo2_walkthrough/#overview","title":"Overview","text":"<p>Evo 2 packages StripedHyena\u202f2 checkpoints behind a lightweight Python API so you can run 1\u202fMbp autoregressive DNA modeling, scoring, and generation without touching the underlying Vortex stack. The repo exposes the 1B/7B/40B parameter checkpoints described in the bioRxiv preprint and HuggingFace collection, all of which share the same tokenizer (single\u2010nucleotide CharLevelTokenizer with a 512-symbol vocab) and reverse-complement aware inference utilities.^[<code>text title=\"external_repos/evo2/README.md</code>] (lines 5-118)\"</p>"},{"location":"code_walkthroughs/evo2_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo StripedHyena\u202f2 mixer loaded via Vortex (<code>StripedHyena</code> backbone)^[<code>text title=\"external_repos/evo2/evo2/models.py</code>] 1B / 7B / 40B checkpoints (<code>MODEL_NAMES</code>)^[<code>1:33:external_repos/evo2/evo2/utils.py</code>] <code>max_seqlen: 1048576</code> (\u22481\u202fMbp)^[<code>3:60:external_repos/evo2/evo2/configs/evo2-7b-1m.yml</code>] Char-level tokenizer with 512 symbols; padding handled in <code>prepare_batch</code>^[<code>19:51:external_repos/evo2/evo2/models.py</code>][<code>10:34:external_repos/evo2/evo2/scoring.py</code>] Autoregressive scoring, reverse-complement averaging, cached generation^[<code>109:169:external_repos/evo2/evo2/models.py</code>][<code>127:170:external_repos/evo2/evo2/scoring.py</code>] github.com/ArcInstitute/evo2"},{"location":"code_walkthroughs/evo2_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Exact pip/conda setup for FP8 + long context. The README prescribes installing Transformer Engine and FlashAttention before running million-token inference: <code>conda install -c nvidia cuda-nvcc cuda-cudart-dev</code> <code>conda install -c conda-forge transformer-engine-torch=2.3.0</code> <code>pip install flash-attn==2.8.0.post2 --no-build-isolation</code>^[<code>text title=\"external_repos/evo2/README.md</code>] (lines 54-58)\"</li> <li>Hardware guidance. Official instructions call for Linux/WSL2 with CUDA\u202f12.1+, FP8-capable GPUs (compute capability \u22658.9) and note that the 40\u202fB checkpoints require multi-GPU partitioning via Vortex; Python\u202f3.12 is required for the packaged binaries.^[<code>text title=\"external_repos/evo2/README.md</code>] (lines 34-52)\"</li> </ul>"},{"location":"code_walkthroughs/evo2_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/evo2_walkthrough/#tokenizer-preprocessing-evo2evo2modelspy-evo2evo2scoringpy","title":"Tokenizer &amp; Preprocessing (<code>evo2/evo2/models.py</code>, <code>evo2/evo2/scoring.py</code>)","text":"<p>The API always instantiates a Vortex <code>CharLevelTokenizer(512)</code> and pairs it with pad/BOS conventions inside <code>prepare_batch</code>, which right-pads sequences to the longest length in the batch while optionally prepending an EOD token for language modeling reductions.</p> <p>Character-level tokenizer initialization:</p> external_repos/evo2/evo2/models.py (lines 19-34)<pre><code>class Evo2:\n    def __init__(self, model_name: str = MODEL_NAMES[1], local_path: str = None):\n        ...\n        self.tokenizer = CharLevelTokenizer(512)\n</code></pre> <p>Batch preparation with padding and BOS:</p> external_repos/evo2/evo2/scoring.py (lines 10-34)<pre><code>def prepare_batch(\n        seqs: List[str],\n        tokenizer: object,\n        prepend_bos: bool = False,\n        device: str = 'cuda:0'\n) -&gt; Tuple[torch.Tensor, List[int]]:\n    ...\n    padding = [tokenizer.pad_id] * (max_seq_length - len(seq))\n    input_ids.append(\n        torch.tensor(\n            ([tokenizer.eod_id] * int(prepend_bos)) + tokenizer.tokenize(seq) + padding,\n            dtype=torch.long,\n        ).to(device).unsqueeze(0)\n    )\n</code></pre>"},{"location":"code_walkthroughs/evo2_walkthrough/#positional-reverse-complement-handling-evo2evo2scoringpy","title":"Positional &amp; Reverse-Complement Handling (<code>evo2/evo2/scoring.py</code>)","text":"<p>Reverse-complement scoring duplicates the batch, flips it via Biopython, and averages the two likelihood traces so downstream metrics remain strand invariant.</p> <p>RC-aware scoring with averaging:</p> external_repos/evo2/evo2/scoring.py (lines 127-170)<pre><code>def score_sequences_rc(...):\n    batch_seqs_rc = [ str(Seq(seq).reverse_complement()) for seq in batch_seqs ]\n    ...\n    batch_scores = _score_sequences(...)\n    batch_scores_rc = _score_sequences(...)\n    batch_scores = (np.array(batch_scores) + np.array(batch_scores_rc)) * 0.5\n</code></pre>"},{"location":"code_walkthroughs/evo2_walkthrough/#backbone-loader-evo2evo2modelspy-evo2evo2configsevo2-7b-1myml","title":"Backbone Loader (<code>evo2/evo2/models.py</code>, <code>evo2/evo2/configs/evo2-7b-1m.yml</code>)","text":"<p><code>load_evo2_model</code> resolves the YAML config, instantiates <code>StripedHyena</code> with the Hyena/Mamba layer schedule, and merges HF shards (removing them afterward) before loading the <code>.pt</code> checkpoint.</p> <p>Model loading with config resolution:</p> external_repos/evo2/evo2/models.py (lines 171-258)<pre><code>def load_evo2_model(...):\n    config = dotdict(yaml.load(open(config_path), Loader=yaml.FullLoader))\n    model = StripedHyena(config)\n    load_checkpoint(model, weights_path)\n    return model\n</code></pre> external_repos/evo2/evo2/configs/evo2-7b-1m.yml (lines 3-65)<pre><code>hidden_size: 4096\nnum_layers: 32\nmax_seqlen: 1048576\ntokenizer_type: CharLevelTokenizer\nuse_flash_attn: True\n</code></pre>"},{"location":"code_walkthroughs/evo2_walkthrough/#objective-diagnostics-evo2evo2testtest_evo2py","title":"Objective &amp; Diagnostics (<code>evo2/evo2/test/test_evo2.py</code>)","text":"<p>The supplied test harness tokenizes CSV prompts, runs a forward pass, and reports cross-entropy plus next-token accuracy so you can validate checkpoints against the published numbers.</p> external_repos/evo2/evo2/test/test_evo2.py (lines 34-124)<pre><code>with torch.inference_mode():\n    logits, _ = model.model.forward(input_ids.unsqueeze(0))\n    loss = F.cross_entropy(pred_logits, target_ids.long())\n    accuracy = (target_ids == pred_tokens).float().mean().item()\n...\nexpected_metrics = {\n    'evo2_40b': {'loss': 0.2159424, 'acc': 91.673},\n    ...\n}\n</code></pre>"},{"location":"code_walkthroughs/evo2_walkthrough/#inference-helpers-evo2evo2modelspy","title":"Inference Helpers (<code>evo2/evo2/models.py</code>)","text":"<p><code>score_sequences</code> and <code>generate</code> wrap the Hyena forward pass with batching, tokenizer reuse, and memory-aware knobs such as cached generation and <code>force_prompt_threshold</code> so you can guard against OOMs on long prompts.</p> external_repos/evo2/evo2/models.py (lines 109-169)<pre><code>def score_sequences(...):\n    scoring_func = partial(score_sequences_rc if average_reverse_complement else score_sequences, ...)\n    with torch.no_grad():\n        scores = scoring_func(seqs)\n...\ndef generate(...):\n    output = vortex_generate(\n        prompt_seqs=prompt_seqs,\n        model=self.model,\n        tokenizer=self.tokenizer,\n        n_tokens=n_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        cached_generation=cached_generation,\n        force_prompt_threshold=force_prompt_threshold,\n    )\n</code></pre>"},{"location":"code_walkthroughs/evo2_walkthrough/#embedding-extraction-hooks-evo2evo2modelspy","title":"Embedding Extraction Hooks (<code>evo2/evo2/models.py</code>)","text":"<p>Forward hooks can be registered on any named submodule (e.g., <code>blocks.28.mlp</code>) so you can capture intermediate representations without modifying Vortex internals.</p> external_repos/evo2/evo2/models.py (lines 52-105)<pre><code>def forward(..., return_embeddings: bool = False, layer_names=None):\n    if return_embeddings:\n        layer = self.model.get_submodule(name)\n        handles.append(layer.register_forward_hook(hook_fn(name)))\n    logits = self.model.forward(input_ids)\n    if return_embeddings:\n        return logits, embeddings\n</code></pre>"},{"location":"code_walkthroughs/evo2_walkthrough/#sequence-constraints-evo2evo2configsevo2-7b-1myml-evo2evo2scoringpy","title":"Sequence Constraints (<code>evo2/evo2/configs/evo2-7b-1m.yml</code>, <code>evo2/evo2/scoring.py</code>)","text":"<p>Hyena checkpoints assume 1\u202fM tokens and the char tokenizer emits IDs per nucleotide, so batching must pad or truncate to keep tensors rectangular; the scorer enforces this padding and strips BOS tokens before computing positional log-likelihoods.</p> external_repos/evo2/evo2/scoring.py (lines 52-88)<pre><code>softmax_logprobs = torch.log_softmax(logits, dim=-1)\nsoftmax_logprobs = softmax_logprobs[:, :-1]\ninput_ids = input_ids[:, 1:]\nlogprobs = torch.gather(softmax_logprobs, 2, input_ids.unsqueeze(-1)).squeeze(-1)\n</code></pre>"},{"location":"code_walkthroughs/evo2_walkthrough/#integration-hooks-genetics-brain","title":"Integration Hooks (Genetics \u2194 Brain)","text":"<ul> <li>Embedding shape. The Hyena forward returns logits shaped <code>[B, L, vocab]</code> and any hooked layer keeps <code>[B, L, hidden]</code>, matching the token dimension used in <code>_score_sequences</code>. Mean pooling along the token axis reproduces the default <code>reduce_method='mean'</code> used for log-likelihoods.^[<code>text title=\"external_repos/evo2/evo2/scoring.py</code>] (lines 70-89)\"</li> <li>Pooling choices. Use mean pooling for global sequence summaries, max pooling to emphasize motifs, or last-token pooling when you want autoregressive state; all three are equivalent to selecting how you reduce <code>logprobs[idx][:seq_len]</code> inside the scorer.^[<code>text title=\"external_repos/evo2/evo2/scoring.py</code>] (lines 79-88)\"</li> <li>Reverse-complement normalization. For strand-agnostic features, reuse <code>score_sequences_rc</code> logic: tokenize both strands, run the same hook set, and average activations before pooling so the resulting <code>[B, H]</code> encodes orientation-invariant context.^[<code>text title=\"external_repos/evo2/evo2/scoring.py</code>] (lines 127-170)\"</li> <li>Projection to shared space. Once you have pooled <code>[B, H]</code> embeddings (H\u22484096 for the 7B model), feed them through a lightweight projector to align with fMRI representations. A drop-in block that mirrors common multimodal heads is:</li> </ul> <pre><code>import torch.nn as nn\n\nclass MultimodalProjector(nn.Module):\n    def __init__(self, input_dim=4096, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.proj(x)\n</code></pre> <ul> <li>Batch &amp; memory tips. Long prompts can trigger Hyena\u2019s FFT prefill path; set <code>force_prompt_threshold</code> or <code>cached_generation=False</code> when working on 24\u202fGB GPUs, and ensure you shard batches via the <code>batch_size</code> argument in <code>score_sequences</code> to keep <code>prepare_batch</code> from materializing multi-megabase tensors at once.^[<code>text title=\"external_repos/evo2/evo2/models.py</code>][<code>10:34:external_repos/evo2/evo2/scoring.py</code>] (lines 109-169)\"</li> </ul> <p>With these hooks, Evo\u202f2 embeddings (<code>[B, H]</code> after pooling) can be projected into the same 512-D latent space used by your fMRI encoder, normalized (LayerNorm) for stability, and concatenated or contrastively aligned with brain-derived features.</p>"},{"location":"code_walkthroughs/flamingo_walkthrough/","title":"Flamingo Code Walkthrough","text":"<p>KB references: Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/flamingo_walkthrough/#overview","title":"Overview","text":"<p>OpenFlamingo implements a few-shot visual-language model that interleaves pretrained vision encoders (CLIP ViT) with causal language models (MPT, RedPajama, LLaMA, OPT) via gated cross-attention layers. The architecture conditions language generation on visual features through a Perceiver resampler and sparse cross-attention blocks inserted every N decoder layers, enabling in-context learning for vision-language tasks without task-specific fine-tuning.^[<code>text title=\"external_repos/flamingo/README.md</code>][<code>17:56:external_repos/flamingo/open_flamingo/src/flamingo.py</code>] (lines 74-100)\"</p>"},{"location":"code_walkthroughs/flamingo_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo CLIP ViT encoder + PerceiverResampler + GatedCrossAttentionBlock + causal LM decoder^[<code>text title=\"external_repos/flamingo/open_flamingo/src/flamingo.py</code>][<code>68:133:external_repos/flamingo/open_flamingo/src/helpers.py</code>] 3B\u20139B (vision frozen, only cross-attn/perceiver trainable)^[<code>104:111:external_repos/flamingo/open_flamingo/src/factory.py</code>] Interleaved image-text sequences with <code>&lt;image&gt;</code> and <code>&lt;|endofchunk|&gt;</code> tokens^[<code>178:196:external_repos/flamingo/README.md</code>] <code>vision_x</code>: <code>[B, T_img, F, C, H, W]</code>; <code>lang_x</code>: tokenized text with media markers^[<code>60:122:external_repos/flamingo/open_flamingo/src/flamingo.py</code>] Few-shot captioning, VQA, image-text generation via <code>generate()</code>; FSDP training with gradient checkpointing^[<code>200:220:external_repos/flamingo/README.md</code>][<code>202:277:external_repos/flamingo/open_flamingo/src/flamingo.py</code>] github.com/mlfoundations/open_flamingo"},{"location":"code_walkthroughs/flamingo_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Installation. Base package via <code>pip install open-flamingo</code>; training/eval extras via <code>pip install open-flamingo[training]</code> or <code>pip install open-flamingo[eval]</code>. Conda environment available via <code>conda env create -f environment.yml</code>.^[<code>text title=\"external_repos/flamingo/README.md</code>] (lines 28-51)\"</li> <li>FSDP wrapping strategy. The <code>wrap_fsdp()</code> method manually wraps vision encoder, perceiver, gated cross-attention layers, and LM embeddings with double-wrapped FSDP to enable parameter offloading. Decoder layers are unfrozen for FSDP compatibility but excluded from the optimizer to effectively freeze them.^[<code>text title=\"external_repos/flamingo/open_flamingo/src/flamingo.py</code>] (lines 202-301)\"</li> <li>Gradient checkpointing. Both the perceiver and decoder layers support gradient checkpointing when <code>gradient_checkpointing=True</code> is passed to <code>init_flamingo()</code>, reducing memory during training at the cost of recomputation.^[<code>text title=\"external_repos/flamingo/open_flamingo/src/flamingo.py</code>] (lines 26-58)\"</li> </ul>"},{"location":"code_walkthroughs/flamingo_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/flamingo_walkthrough/#model-factory-open_flamingosrcfactorypy","title":"Model Factory (<code>open_flamingo/src/factory.py</code>)","text":"<p><code>create_model_and_transforms()</code> instantiates a CLIP vision encoder, loads a causal LM, extends it with <code>FlamingoLMMixin</code>, and wires cross-attention layers at configurable intervals. Special tokens (<code>&lt;image&gt;</code>, <code>&lt;|endofchunk|&gt;</code>) are added to the tokenizer, and all parameters are frozen except the perceiver, gated cross-attention layers, and optionally LM embeddings.</p> <p>Model initialization with frozen backbones:</p> external_repos/flamingo/open_flamingo/src/factory.py (lines 11-119)<pre><code>def create_model_and_transforms(\n    clip_vision_encoder_path: str,\n    clip_vision_encoder_pretrained: str,\n    lang_encoder_path: str,\n    tokenizer_path: str,\n    cross_attn_every_n_layers: int = 1,\n    ...\n):\n    vision_encoder, _, image_processor = open_clip.create_model_and_transforms(...)\n    vision_encoder.visual.output_tokens = True\n    text_tokenizer = AutoTokenizer.from_pretrained(...)\n    text_tokenizer.add_special_tokens({\"additional_special_tokens\": [\"&lt;|endofchunk|&gt;\", \"&lt;image&gt;\"]})\n    lang_encoder = AutoModelForCausalLM.from_pretrained(...)\n    extend_instance(lang_encoder, FlamingoLMMixin)\n    model = Flamingo(vision_encoder, lang_encoder, ...)\n    model.requires_grad_(False)\n    model.perceiver.requires_grad_(True)\n    model.lang_encoder.gated_cross_attn_layers.requires_grad_(True)\n</code></pre>"},{"location":"code_walkthroughs/flamingo_walkthrough/#vision-encoding-perceiver-resampling-open_flamingosrcflamingopy-open_flamingosrchelperspy","title":"Vision Encoding &amp; Perceiver Resampling (<code>open_flamingo/src/flamingo.py</code>, <code>open_flamingo/src/helpers.py</code>)","text":"<p>The vision encoder extracts patch features from images, which are then resampled by the Perceiver into a fixed number of latent tokens per image. The Perceiver uses cross-attention to compress variable-length visual sequences into a consistent representation.</p> <p>Vision encoding and conditioning:</p> external_repos/flamingo/open_flamingo/src/flamingo.py (lines 177-200)<pre><code>def _encode_vision_x(self, vision_x: torch.Tensor):\n    assert vision_x.ndim == 6, \"vision_x should be of shape (b, T_img, F, C, H, W)\"\n    b, T, F = vision_x.shape[:3]\n    assert F == 1, \"Only single frame supported\"\n    vision_x = rearrange(vision_x, \"b T F c h w -&gt; (b T F) c h w\")\n    with torch.no_grad():\n        vision_x = self.vision_encoder(vision_x)[1]\n    vision_x = rearrange(vision_x, \"(b T F) v d -&gt; b T F v d\", b=b, T=T, F=F)\n    vision_x = self.perceiver(vision_x)\n    for layer in self.lang_encoder._get_decoder_layers():\n        layer.condition_vis_x(vision_x)\n</code></pre> <p>Perceiver resampler architecture:</p> external_repos/flamingo/open_flamingo/src/helpers.py (lines 68-132)<pre><code>class PerceiverResampler(nn.Module):\n    def __init__(self, *, dim, depth=6, dim_head=64, heads=8, num_latents=64, ...):\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n        self.layers = nn.ModuleList([\n            nn.ModuleList([\n                PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                FeedForward(dim=dim, mult=ff_mult),\n            ]) for _ in range(depth)\n        ])\n    def forward(self, x):\n        latents = repeat(self.latents, \"n d -&gt; b T n d\", b=b, T=T)\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n        return self.norm(latents)\n</code></pre>"},{"location":"code_walkthroughs/flamingo_walkthrough/#gated-cross-attention-open_flamingosrchelperspy-open_flamingosrcflamingo_lmpy","title":"Gated Cross-Attention (<code>open_flamingo/src/helpers.py</code>, <code>open_flamingo/src/flamingo_lm.py</code>)","text":"<p>Gated cross-attention layers enable text tokens to attend to visual features at specific media locations. The gating mechanism (tanh-activated learnable scalars) controls the contribution of cross-modal information, allowing the model to learn when to rely on visual context versus pure language modeling.</p> <p>Gated cross-attention block:</p> external_repos/flamingo/open_flamingo/src/helpers.py (lines 236-279)<pre><code>class GatedCrossAttentionBlock(nn.Module):\n    def __init__(self, *, dim, dim_visual, dim_head=64, heads=8, ...):\n        self.attn = MaskedCrossAttention(dim=dim, dim_visual=dim_visual, ...)\n        self.attn_gate = nn.Parameter(torch.tensor([0.0]))\n        self.ff = FeedForward(dim, mult=ff_mult)\n        self.ff_gate = nn.Parameter(torch.tensor([0.0]))\n    def forward(self, x, media, media_locations=None, use_cached_media=False):\n        x = self.attn(x, media, media_locations=media_locations, use_cached_media=use_cached_media) * self.attn_gate.tanh() + x\n        x = self.ff(x) * self.ff_gate.tanh() + x\n        return x\n</code></pre> <p>Media location conditioning:</p> external_repos/flamingo/open_flamingo/src/helpers.py (lines 136-233)<pre><code>class MaskedCrossAttention(nn.Module):\n    def forward(self, x, media, media_locations=None, use_cached_media=False):\n        if exists(media_locations):\n            text_time = media_locations.cumsum(dim=-1)\n            text_to_media_mask = mask_op(\n                rearrange(text_time, \"b i -&gt; b 1 i 1\"),\n                repeat(media_time, \"j -&gt; 1 1 1 (j n)\", n=n),\n            )\n            sim = sim.masked_fill(~text_to_media_mask, -torch.finfo(sim.dtype).max)\n</code></pre>"},{"location":"code_walkthroughs/flamingo_walkthrough/#flamingo-layer-wrapper-open_flamingosrcflamingo_lmpy","title":"Flamingo Layer Wrapper (<code>open_flamingo/src/flamingo_lm.py</code>)","text":"<p><code>FlamingoLayer</code> wraps each decoder block with an optional gated cross-attention layer, conditionally applying visual conditioning based on media token locations. The mixin pattern allows retrofitting any HuggingFace causal LM with Flamingo capabilities.</p> <p>Flamingo layer structure:</p> external_repos/flamingo/open_flamingo/src/flamingo_lm.py (lines 6-66)<pre><code>class FlamingoLayer(nn.Module):\n    def __init__(self, gated_cross_attn_layer, decoder_layer, gradient_checkpointing=False):\n        self.gated_cross_attn_layer = gated_cross_attn_layer\n        self.decoder_layer = decoder_layer\n        self.vis_x = None\n        self.media_locations = None\n    def forward(self, lang_x, attention_mask=None, **decoder_layer_kwargs):\n        if self.gated_cross_attn_layer is not None:\n            lang_x = self.gated_cross_attn_layer(lang_x, self.vis_x, media_locations=self.media_locations, use_cached_media=self.use_cached_media)\n        lang_x = self.decoder_layer(lang_x, attention_mask=attention_mask, **decoder_layer_kwargs)\n        return lang_x\n</code></pre> <p>Flamingo LM mixin initialization:</p> external_repos/flamingo/open_flamingo/src/flamingo_lm.py (lines 83-126)<pre><code>def init_flamingo(self, media_token_id, lang_hidden_size, vis_hidden_size, cross_attn_every_n_layers, gradient_checkpointing):\n    self.old_decoder_blocks = self._get_decoder_layers()\n    self.gated_cross_attn_layers = nn.ModuleList([\n        GatedCrossAttentionBlock(dim=lang_hidden_size, dim_visual=vis_hidden_size)\n        if (layer_idx + 1) % cross_attn_every_n_layers == 0\n        else None\n        for layer_idx, _ in enumerate(self._get_decoder_layers())\n    ])\n    self.init_flamingo_layers(gradient_checkpointing)\n</code></pre>"},{"location":"code_walkthroughs/flamingo_walkthrough/#forward-pass-generation-open_flamingosrcflamingopy","title":"Forward Pass &amp; Generation (<code>open_flamingo/src/flamingo.py</code>)","text":"<p>The forward pass encodes vision inputs, conditions decoder layers on media locations, and runs the language model. Generation caches visual features and reuses them across autoregressive steps to avoid redundant encoding.</p> <p>Forward pass with media conditioning:</p> external_repos/flamingo/open_flamingo/src/flamingo.py (lines 60-122)<pre><code>def forward(self, vision_x, lang_x, attention_mask=None, labels=None, clear_conditioned_layers=True, past_key_values=None, use_cache=False):\n    if not self.lang_encoder._use_cached_vision_x:\n        self._encode_vision_x(vision_x=vision_x)\n        self._condition_media_locations(input_ids=lang_x)\n    output = self.lang_encoder(input_ids=lang_x, attention_mask=attention_mask, labels=labels, past_key_values=past_key_values, use_cache=use_cache)\n    if clear_conditioned_layers:\n        self.lang_encoder.clear_conditioned_layers()\n    return output\n</code></pre> <p>Generation with cached media:</p> external_repos/flamingo/open_flamingo/src/flamingo.py (lines 124-175)<pre><code>def generate(self, vision_x, lang_x, attention_mask=None, **kwargs):\n    num_beams = kwargs.pop(\"num_beams\", 1)\n    if num_beams &gt; 1:\n        vision_x = vision_x.repeat_interleave(num_beams, dim=0)\n    self.lang_encoder._use_cached_vision_x = True\n    self._encode_vision_x(vision_x=vision_x)\n    output = self.lang_encoder.generate(input_ids=lang_x, attention_mask=attention_mask, eos_token_id=eos_token_id, num_beams=num_beams, **kwargs)\n    self.lang_encoder.clear_conditioned_layers()\n    self.lang_encoder._use_cached_vision_x = False\n    return output\n</code></pre>"},{"location":"code_walkthroughs/flamingo_walkthrough/#training-entry-point-open_flamingotraintrainpy","title":"Training Entry Point (<code>open_flamingo/train/train.py</code>)","text":"<p>The training script supports distributed training with FSDP, mixed precision, gradient checkpointing, and multi-dataset mixing (MMC4, LAION). Loss multipliers allow balancing contributions from different data sources.</p> <p>Training configuration:</p> external_repos/flamingo/open_flamingo/train/train.py (lines 51-150)<pre><code>parser.add_argument(\"--lm_path\", default=\"facebook/opt-1.3b\", type=str)\nparser.add_argument(\"--cross_attn_every_n_layers\", type=int, default=1)\nparser.add_argument(\"--batch_size_mmc4\", type=int, default=128)\nparser.add_argument(\"--batch_size_laion\", type=int, default=128)\nparser.add_argument(\"--loss_multiplier_mmc4\", type=float, default=1.0)\nparser.add_argument(\"--loss_multiplier_laion\", type=float, default=1.0)\nparser.add_argument(\"--gradient_checkpointing\", action=\"store_true\")\nparser.add_argument(\"--precision\", choices=[\"amp_bf16\", \"amp_bfloat16\", \"bf16\", \"fp16\", \"fp32\"], default=\"fp32\")\n</code></pre>"},{"location":"code_walkthroughs/flamingo_walkthrough/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Vision encoder outputs. The CLIP encoder returns patch features <code>[B*T, num_patches, vis_dim]</code> which are reshaped to <code>[B, T, F, num_patches, vis_dim]</code> before Perceiver resampling. The resampler compresses these to <code>[B, T, num_latents, vis_dim]</code> where <code>num_latents=64</code> by default.^[<code>text title=\"external_repos/flamingo/open_flamingo/src/flamingo.py</code>] (lines 177-200)\"</li> <li>Media caching for evaluation. The <code>cache_media()</code> method pre-encodes images and conditions layers, enabling efficient log-likelihood evaluation on fixed visual contexts without re-encoding on each forward pass.^[<code>text title=\"external_repos/flamingo/open_flamingo/src/flamingo.py</code>] (lines 315-331)\"</li> <li>Cross-attention interval control. The <code>cross_attn_every_n_layers</code> parameter determines how frequently cross-attention layers are inserted. Published models use intervals of 1, 2, or 4, trading off parameter efficiency versus visual conditioning density.^[<code>text title=\"external_repos/flamingo/open_flamingo/src/factory.py</code>] (lines 64-68)\"</li> <li>Modality projection for neuro-omics. To adapt Flamingo for brain imaging, replace the CLIP encoder with a brain encoder (e.g., BrainLM, BrainMT) and adjust <code>vis_dim</code> to match the brain encoder's output dimension. The Perceiver resampler will automatically adapt to the new feature space.^[<code>text title=\"external_repos/flamingo/open_flamingo/src/flamingo.py</code>] (lines 48-56)\"</li> <li>Text generation with brain context. The generation API accepts interleaved sequences where <code>&lt;image&gt;</code> tokens mark brain scan embeddings. The model can generate text descriptions conditioned on brain features, enabling applications like scan summarization or clinical report generation.^[<code>text title=\"external_repos/flamingo/open_flamingo/src/flamingo.py</code>] (lines 124-175)\"</li> </ul>"},{"location":"code_walkthroughs/fms_medical_walkthrough/","title":"FMS-Medical Curation Walkthrough","text":"<p>KB references: Survey digest (pending) \u00b7 Integration strategy \u00b7 KB overview \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#overview","title":"Overview","text":"<p><code>external_repos/fms-medical</code> is an \u201cawesome list\u201d style knowledge base that tracks foundation-model research across healthcare modalities\u2014language (LFM), vision (VFM), bioinformatics (BFM), and multimodal (MFM)\u2014plus dataset catalogs, tutorials, and bilingual survey PDFs. The maintainers keep the README current with publication news (e.g., IEEE Reviews acceptance) and provide both English and Chinese summaries (<code>files/HFM_Chinese.pdf</code>), making it a convenient seed for KB model/dataset cards.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>] (lines 1-38)\"</p>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Markdown knowledge graph: NEWS \u2192 Survey references \u2192 modality-specific method lists + dataset tables.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>] Organized by year (2020\u20132024) and modality; each entry stores venue, short description, and code/paper links\u2014ready for transformation into KB YAML cards. Highlights IEEE Reviews 2024 survey + arXiv companions; hosts bilingual PDF in <code>files/</code> for regional teams.^[<code>5:38:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>] Pure Markdown + embedded images; contributions occur via pull requests (no runtime code). Quick lookup for LFM/VFM/BFM/MFM models, dataset tables (text, imaging, omics, multimodal), lectures, blogs, and related awesome lists.^[<code>39:400:/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>] github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare"},{"location":"code_walkthroughs/fms_medical_walkthrough/#repository-notes","title":"Repository Notes","text":"<ul> <li>Documentation-only. No Python modules or requirements\u2014syncing the README (and optional PDF) is sufficient to integrate the content into KB templates.</li> <li>Bilingual assets. <code>files/HFM_Chinese.pdf</code> mirrors the survey for Mandarin readers; cite it when translating KB summaries.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md</code>] (lines 17-34)\"</li> <li>Citation-first. Each entry lists papers/code, so KB automation scripts can parse the tables to populate <code>kb/model_cards</code>, <code>kb/datasets</code>, or <code>docs/generated</code> references.</li> </ul>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/fms_medical_walkthrough/#survey-metadata-news-banner","title":"Survey Metadata &amp; NEWS Banner","text":"<p>Top-of-file announcements capture publication milestones, acceptance venues, and contact information. These lines can drive KB changelogs or curated timelines.</p> <p>README header with news updates:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md (lines 5-34)<pre><code>[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n\n[NEWS.20241115] **Our survey [paper](https://ieeexplore.ieee.org/document/10750441) has been accepted by IEEE Reviews in Biomedical Engineering (IF: 17.2).**\n\n[NEWS.20240405] **The related survey [paper](https://arxiv.org/abs/2404.03264) has been released.**\n\n[NOTE] **If you have any questions, please don't hesitate to [contact us](mailto:yuting.he4@case.edu).** \n</code></pre>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#modality-method-registries-lfmvfmbfmmfm","title":"Modality Method Registries (LFM/VFM/BFM/MFM)","text":"<p>Each section groups methods by year, venue, and modality. Capturing these rows lets the KB auto-generate candidate model cards or integration experiments.</p> <p>Method listings by modality:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md (lines 82-210)<pre><code>## LFM methods\n**2024**\n- [AAAI] Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and realworld multi-turn dialogue. [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/29907) [[Code]](https://github.com/SupritYoung/Zhongjing)\n- [NeurIPS] MDAgents: An adaptive collaboration of LLMs for medical decision-making. [[Paper]](https://arxiv.org/abs/2404.15155) [[Code]](https://github.com/mitmedialab/MDAgents)\n...\n## VFM methods\n**2024**\n- [arXiv] USFM: A universal ultrasound foundation model generalized to tasks and organs towards label efficient image analysis. [[paper]](https://arxiv.org/html/2401.00153v2) \n</code></pre>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#dataset-catalogs","title":"Dataset Catalogs","text":"<p>Separate tables detail datasets per modality (text, imaging, multimodal). These rows map neatly onto <code>kb/datasets/*.yaml</code> and help ensure coverage across integrative experiments.</p> <p>Dataset tables by modality:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md (lines 339-399)<pre><code>## Datasets\n### LFM datasets\n|                           Dataset  Name                               | Text Types  |            Scale           |    Task    |                       Link                             |\n| :-------------------------------------------------------------------: | :-------: | :------------------------: | :--------: | :----------------------------------------------------: |\n|[PubMed](https://pubmed.ncbi.nlm.nih.gov/download/) | Literature | 18B tokens |  Language modeling |[*](https://pubmed.ncbi.nlm.nih.gov/download/)|\n|[MedC-I](https://arxiv.org/abs/2304.14454)| Literature | 79.2B tokens |  Dialogue |[*](https://huggingface.co/datasets/axiong/pmc_llama_instructions)|\n...\n|[CMeKG-8K](https://www.mdpi.com/2078-2489/11/4/186)| Dialogue | 8K instances |  Dialogue |[*](https://github.com/WENGSYX/CMKG)|\n</code></pre>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#other-resources-lecturesblogsawesome-lists","title":"Other Resources (Lectures/Blogs/Awesome lists)","text":"<p>The README also aggregates tutorials, blogs, and related awesome repositories under \u201cOther Resources,\u201d which can seed KB \u201cFurther reading\u201d sections or onboarding material.</p> /Users/allison/Projects/neuro-omics-kb/external_repos/fms-medical/README.md (lines 53-74)<pre><code>- [Other Resources](#other-resources)\n  - [Lectures and tutorials](#lectures-and-tutorials)\n  - [Blogs](#blogs)\n  - [Related awesome repositories](#related-awesome-repositories)\n</code></pre>"},{"location":"code_walkthroughs/fms_medical_walkthrough/#integration-hooks-curation-kb","title":"Integration Hooks (Curation \u2194 KB)","text":"<ul> <li>Automate card creation. Parse the Markdown tables (e.g., via pandoc or custom scripts) to prefill <code>kb/model_cards</code> with metadata (venue, year, links), ensuring coverage parity across modalities.</li> <li>Align dataset registries. Map the README dataset entries to <code>kb/datasets/*.yaml</code> and tag each with modality + task so integration plans can quickly reference scale/availability.</li> <li>Leverage bilingual PDFs. When publishing KB summaries for non-English audiences, cite <code>files/HFM_Chinese.pdf</code> to keep translations aligned with the upstream survey and avoid redundant localization work.</li> </ul>"},{"location":"code_walkthroughs/generator_walkthrough/","title":"GENERator Code Walkthrough","text":"<p>KB references: Model card \u00b7 Genomics feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/generator_walkthrough/#overview","title":"Overview","text":"<p>GENERator wraps GPT-style causal decoders (1.2\u202fB and 3\u202fB parameters for both eukaryote and prokaryote checkpoints) with a strict 6-mer tokenizer and long-context optimizations\u2014FlashAttention, Liger kernels, sliding-window decoding\u2014so you can score or generate up to one million base pairs per prompt.^[<code>text title=\"external_repos/generator/README.md</code>] (lines 5-125)\"</p>"},{"location":"code_walkthroughs/generator_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo HuggingFace <code>AutoModelForCausalLM</code> decoder w/ optional ChunkEnsemble Llama heads^[<code>text title=\"external_repos/generator/src/tasks/downstream/fine_tuning.py</code>][<code>508:688:external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>] 1.2\u202fB &amp; 3\u202fB checkpoints for euk/prok.^[<code>52:118:external_repos/generator/README.md</code>] 1\u202fMbp prompts via sliding windows + FlashAttention^[<code>84:99:external_repos/generator/README.md</code>][<code>612:667:external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>] 6-mer tokenizer; sequences must be multiples of 6, enforced in preprocessing^[<code>118:125:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py</code>][<code>115:235:external_repos/generator/src/tasks/downstream/fine_tuning.py</code>] Variant effect scoring, sequence recovery, classification/regression fine-tuning^[<code>141:406:external_repos/generator/src/tasks/downstream/variant_effect_prediction.py</code>][<code>400:687:external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>] github.com/GenerTeam/GENERator"},{"location":"code_walkthroughs/generator_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Long-context dependencies. For million-base contexts the README recommends installing the custom kernels explicitly: <code>pip install liger-kernel</code> <code>pip install flash-attn --no-build-isolation</code>^[<code>text title=\"external_repos/generator/README.md</code>] (lines 84-89)\"</li> <li>Gradient checkpointing flag. When operating on &gt;10\u202fkbp sequences, the authors enable <code>model.gradient_checkpointing_enable()</code> to trade compute for memory.^[<code>text title=\"external_repos/generator/README.md</code>] (lines 420-424)\"</li> </ul>"},{"location":"code_walkthroughs/generator_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/generator_walkthrough/#tokenizer-preprocessing-variant_effect_predictionpy-fine_tuningpy","title":"Tokenizer &amp; Preprocessing (<code>variant_effect_prediction.py</code>, <code>fine_tuning.py</code>)","text":"<p>The downstream scripts consistently load the HF tokenizer with <code>trust_remote_code=True</code>, force pad tokens to EOS if missing, and either truncate or pad every sequence to the nearest 6-mer boundary (<code>pad_to_multiple_of_six</code> flag) so the 6-mer BPE never emits <code>&lt;oov&gt;</code> tokens.</p> <p>Tokenizer initialization:</p> external_repos/generator/src/tasks/downstream/variant_effect_prediction.py (lines 151-176)<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n...\ninputs = tokenizer(batch_sequences, return_tensors=\"pt\", padding=True)\n</code></pre> <p>6-mer boundary truncation:</p> external_repos/generator/src/tasks/downstream/variant_effect_prediction.py (lines 118-125)<pre><code>truncate_length = len(sequence) % 6\nif truncate_length &gt; 0:\n    sequence = sequence[truncate_length:]\n</code></pre> <p>Padding to 6-mer multiples:</p> external_repos/generator/src/tasks/downstream/fine_tuning.py (lines 208-243)<pre><code>if pad_to_multiple_of_six:\n    remainder = len(seq) % 6\n    if remainder != 0:\n        pad_len = 6 - remainder\n        seq = seq + \"A\" * pad_len\ntokenized = tokenizer(\n    sequences,\n    truncation=True,\n    max_length=max_length,\n    add_special_tokens=True,\n    padding=False,\n)\n</code></pre>"},{"location":"code_walkthroughs/generator_walkthrough/#positional-long-context-handling-sequence_understandingpy","title":"Positional &amp; Long-Context Handling (<code>sequence_understanding.py</code>)","text":"<p><code>sequence_understanding.py</code> either scales RoPE via YaRN or injects sliding-window attention patches so you can extend Llama-based classifiers to &gt;1\u202fM tokens while staying numerically stable.</p> external_repos/generator/src/tasks/downstream/sequence_understanding.py (lines 596-666)<pre><code>elif length_extension_mode == \"sliding_window\":\n    config.sliding_window = int(original_model_max_length_for_scaling)\n    ...\n    def _sliding_llama_forward(...):\n        kwargs[\"sliding_window\"] = self.config.sliding_window\n        return _orig_forward(...)\n    LlamaAttention.forward = _sliding_llama_forward\n    attn_implementation = \"flash_attention_2\"\n</code></pre>"},{"location":"code_walkthroughs/generator_walkthrough/#backbone-instantiation-fine_tuningpy-sequence_understandingpy","title":"Backbone Instantiation (<code>fine_tuning.py</code>, <code>sequence_understanding.py</code>)","text":"<p>Fine-tuning uses <code>AutoModelForCausalLM</code> with optional pad ID fixes, while sequence-understanding swaps in <code>AutoModelForSequenceClassification</code> or the ChunkEnsemble wrapper to keep a rolling window over million-token sequences.</p> <p>Causal LM model loading:</p> external_repos/generator/src/tasks/downstream/fine_tuning.py (lines 257-285)<pre><code>model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n)\nif model.config.pad_token_id is None and hasattr(model.config, \"eos_token_id\"):\n    model.config.pad_token_id = model.config.eos_token_id\n</code></pre> external_repos/generator/src/tasks/downstream/sequence_understanding.py (lines 508-593)<pre><code>class ChunkEnsembleLlamaForSequenceClassification(LlamaPreTrainedModel):\n    def forward(...):\n        input_ids_chunks = input_ids.unfold(dimension=1, size=self.chunk_size, step=self.stride)\n        ...\n        chunk_eos_embedding = hidden_states[\n            torch.arange(batch_size, device=hidden_states.device),\n            sequence_lengths,\n        ]\n        stacked_embeddings = torch.stack(all_chunk_eos_embeddings, dim=1)\n        final_representation = padded_embeddings.view(batch_size, -1)\n        logits = self.classifier(final_representation)\n</code></pre>"},{"location":"code_walkthroughs/generator_walkthrough/#objective-training-loop-fine_tuningpy","title":"Objective &amp; Training Loop (<code>fine_tuning.py</code>)","text":"<p>The script wraps everything in <code>transformers.Trainer</code> with <code>DataCollatorForLanguageModeling</code> (<code>mlm=False</code>) so causal LM losses line up with the HF training stack and distributed options (DeepSpeed, FSDP) set via CLI.</p> external_repos/generator/src/tasks/downstream/fine_tuning.py (lines 351-390)<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n)\ntrainer.train()\n</code></pre>"},{"location":"code_walkthroughs/generator_walkthrough/#inference-helpers-variant_effect_predictionpy","title":"Inference Helpers (<code>variant_effect_prediction.py</code>)","text":"<p>Variant effect prediction shards ClinVar sequences across GPUs, caches logits, and computes per-base probabilities by summing over all tokens starting with ref/alt characters. This utility powers the headline ClinVar AUROC numbers.</p> external_repos/generator/src/tasks/downstream/variant_effect_prediction.py (lines 201-290)<pre><code>def compute_logits_parallel(...):\n    num_gpus = torch.cuda.device_count()\n    shards.append({'shard_id': i, 'sequences_data': sequences_data[start_idx:end_idx], ...})\n    with ctx.Pool(processes=num_gpus) as pool:\n        results = list(pool.imap(compute_logits_shard, args_list))\n</code></pre> external_repos/generator/src/tasks/downstream/variant_effect_prediction.py (lines 292-383)<pre><code>def parallel_compute_probabilities(...):\n    vocab = tokenizer.get_vocab()\n    char_indices = get_char_indices(vocab)\n    results = list(pool.imap(compute_prob, args_list, chunksize=chunksize))\n    p_ref, p_alt = zip(*results)\n</code></pre>"},{"location":"code_walkthroughs/generator_walkthrough/#embedding-extraction-sequence_understandingpy","title":"Embedding Extraction (<code>sequence_understanding.py</code>)","text":"<p>ChunkEnsemble accumulates the EOS vector from each sliding chunk, pads/truncates them to a fixed count, and flattens into a <code>[B, max_chunks * hidden]</code> representation before the classifier head\u2014exactly what you can reuse for downstream alignment.</p> external_repos/generator/src/tasks/downstream/sequence_understanding.py (lines 446-505)<pre><code>stacked_embeddings = torch.stack(all_chunk_eos_embeddings, dim=1)\nnum_padding_chunks = self.max_chunks - stacked_embeddings.shape[1]\n...\nfinal_representation = padded_embeddings.view(batch_size, -1)\nlogits = self.classifier(final_representation)\n</code></pre>"},{"location":"code_walkthroughs/generator_walkthrough/#sequence-constraints-variant_effect_predictionpy-fine_tuningpy","title":"Sequence Constraints (<code>variant_effect_prediction.py</code>, <code>fine_tuning.py</code>)","text":"<p>Both inference and training enforce the 6-mer constraint by trimming or padding raw strings and, for dataset preprocessing, only accepting columns named <code>sequence</code>, <code>seq</code>, <code>dna_sequence</code>, etc., so you cannot silently feed invalid tokens.</p> external_repos/generator/src/tasks/downstream/fine_tuning.py (lines 208-244)<pre><code>if \"sequence\" in examples:\n    sequences = examples[\"sequence\"]\nelif \"seq\" in examples:\n    sequences = examples[\"seq\"]\n...\nelse:\n    raise ValueError(\"No sequence column found in dataset.\")\n</code></pre>"},{"location":"code_walkthroughs/generator_walkthrough/#integration-hooks-genetics-brain","title":"Integration Hooks (Genetics \u2194 Brain)","text":"<ul> <li>Embedding shapes. GENERator decoders yield <code>[B, L_tokens, hidden]</code> tensors; ChunkEnsemble condenses them into <code>[B, max_chunks, hidden]</code> before flattening to <code>[B, max_chunks * hidden]</code>. You can stop just before the final classifier to grab the stacked embeddings for pooling.^[<code>text title=\"external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>] (lines 446-505)\"</li> <li>Pooling strategies. Use mean pooling along the chunk dimension for overall sequence summaries, max pooling for motif emphasis, or take the final chunk (equivalent to autoregressive \u201clast token\u201d). Because chunk embeddings correspond to non-overlapping windows, pooling behaves like low-resolution downsampling.</li> <li>Projection to shared latent. After pooling to <code>[B, H]</code> (H\u22481536 for the 1.2\u202fB model), apply a projector to map into the same 512-D space used by your brain encoder:</li> </ul> <pre><code>import torch.nn as nn\n\nclass GeneratorProjector(nn.Module):\n    def __init__(self, input_dim=1536, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <ul> <li>Normalization. LayerNorm (as in the projector above) keeps token-averaged embeddings comparable to fMRI CLS tokens (BrainLM/SwiFT), especially before cosine-similarity objectives.</li> <li>Sequence hygiene. Reuse the <code>pad_to_multiple_of_six</code> logic or <code>ensure_6mer_compatible</code> helper whenever you extract embeddings outside the packaged scripts; otherwise, HF will inject <code>&lt;oov&gt;</code> tokens that shift chunk boundaries and misalign pooling.^[<code>text title=\"external_repos/generator/src/tasks/downstream/variant_effect_prediction.py</code>] (lines 118-125)\"</li> <li>Memory tips. For million-token prompts, lean on ChunkEnsemble (<code>length_extension_mode=\"chunk_ensemble\"</code>) or sliding-window RoPE to avoid editing HF internals; both paths keep per-chunk lengths manageable and let FlashAttention v2 handle the heavy lifting.^[<code>text title=\"external_repos/generator/src/tasks/downstream/sequence_understanding.py</code>] (lines 566-666)\"</li> </ul> <p>Following these steps yields <code>[B, 512]</code> genetic embeddings that can be concatenated with or contrastively aligned against brain-model outputs such as BrainLM CLS vectors or BrainMT/SwiFT pooled features.</p>"},{"location":"code_walkthroughs/hyena_walkthrough/","title":"StripedHyena Code Walkthrough","text":"<p>KB references: Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/hyena_walkthrough/#overview","title":"Overview","text":"<p>StripedHyena is a hybrid sequence model that alternates between Hyena convolution blocks and grouped rotary attention blocks, achieving near-linear scaling with context length while maintaining competitive performance with Transformers. The architecture uses state-space models (SSMs) implemented via finite impulse response (FIR) and infinite impulse response (IIR) filters for efficient long-context processing, with attention layers providing targeted pattern recall capabilities.^[<code>text title=\"external_repos/hyena/README.md</code>][<code>336:403:external_repos/hyena/stripedhyena/model.py</code>] (lines 9-20)\"</p>"},{"location":"code_walkthroughs/hyena_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Tokenization / Inputs Key capabilities Repo Alternating Hyena blocks (gated convolutions) and Attention blocks (grouped rotary MHA)^[<code>text title=\"external_repos/hyena/stripedhyena/model.py</code>] 7B (StripedHyena-Hessian-7B, StripedHyena-Nous-7B)^[<code>32:41:external_repos/hyena/README.md</code>] Up to 32k tokens (configurable via <code>max_seqlen</code>)^[<code>35:36:external_repos/hyena/configs/7b-sh-32k-v1.yml</code>] HuggingFace tokenizer or character-level tokenizer; input shape <code>[B, L]</code>^[<code>340:371:external_repos/hyena/stripedhyena/model.py</code>] Efficient autoregressive generation (&gt;500k tokens with 80GB GPU), faster decoding than Transformers, recurrent inference mode^[<code>16:20:external_repos/hyena/README.md</code>][<code>14:158:external_repos/hyena/stripedhyena/generation.py</code>] github.com/togethercomputer/stripedhyena"},{"location":"code_walkthroughs/hyena_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Docker setup. The recommended installation path uses Docker: <code>docker build --tag sh:test .</code> followed by <code>docker run -it --gpus all ...</code>. The Dockerfile installs FlashAttention, rotary/normalization kernels, and other dependencies.^[<code>text title=\"external_repos/hyena/README.md</code>] (lines 58-68)\"</li> <li>FlashAttention requirement. FlashAttention v2 is required for the attention blocks; the code checks for <code>flash_attn.modules.mha.MHA</code> and falls back gracefully if unavailable. RMSNorm can optionally use flash kernels when <code>use_flash_rmsnorm=True</code>.^[<code>text title=\"external_repos/hyena/stripedhyena/layers.py</code>][<code>39:50:external_repos/hyena/stripedhyena/model.py</code>] (lines 15-26)\"</li> <li>Mixed precision handling. Poles and residues must remain in <code>float32</code> for numerical stability; all other parameters can be converted to <code>bfloat16</code> via <code>to_bfloat16_except_poles_residues()</code>.^[<code>text title=\"external_repos/hyena/stripedhyena/model.py</code>] (lines 438-445)\"</li> <li>Prefill modes. Long sequences support multiple prefill strategies: <code>fft</code> (default, fast for even-length sequences), <code>recurrence</code> (slower but lower memory), and <code>modal-fft</code> (hybrid). Very long prompts may require <code>prefill_style: recurrence</code> to avoid OOM.^[<code>text title=\"external_repos/hyena/stripedhyena/engine.py</code>][<code>79:82:external_repos/hyena/README.md</code>] (lines 16-23)\"</li> </ul>"},{"location":"code_walkthroughs/hyena_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/hyena_walkthrough/#model-architecture-stripedhyenamodelpy","title":"Model Architecture (<code>stripedhyena/model.py</code>)","text":"<p><code>StripedHyena</code> alternates between <code>ParallelGatedConvBlock</code> (Hyena) and <code>AttentionBlock</code> layers based on config-specified indices. The model supports both stateless (training) and stateful (inference) forward passes, with inference parameters managing recurrent state for efficient generation.</p> <p>Model structure with alternating blocks:</p> external_repos/hyena/stripedhyena/model.py (lines 336-403)<pre><code>class StripedHyena(nn.Module):\n    def __init__(self, config):\n        self.embedding_layer = VocabParallelEmbedding(config)\n        self.norm = RMSNorm(config) if config.get(\"final_norm\", True) else None\n        self.unembed = self.embedding_layer if config.tie_embeddings else VocabParallelEmbedding(config)\n        self.blocks = nn.ModuleList([\n            get_block(config, layer_idx, flash_fft=self.flash_fft) \n            for layer_idx in range(config.num_layers)\n        ])\n    def forward(self, x, inference_params_dict=None, padding_mask=None):\n        x = self.embedding_layer.embed(x)\n        if inference_params_dict is not None:\n            x, inference_params_dict_out = self.stateful_forward(x, inference_params_dict=inference_params_dict)\n        else:\n            x, inference_params_dict_out = self.stateless_forward(x, padding_mask=padding_mask)\n        x = self.norm(x)\n        x = self.unembed.unembed(x)\n        return x, inference_params_dict_out\n</code></pre> <p>Block selection logic:</p> external_repos/hyena/stripedhyena/model.py (lines 324-333)<pre><code>def get_block(config, layer_idx, flash_fft=None):\n    if layer_idx in config.attn_layer_idxs:\n        return AttentionBlock(config, layer_idx)\n    elif layer_idx in config.hyena_layer_idxs:\n        block = ParallelGatedConvBlock(config, layer_idx)\n        if config.get(\"use_flashfft\", \"False\"):\n            block.filter.fftconv_fn = flash_fft\n        return block\n</code></pre>"},{"location":"code_walkthroughs/hyena_walkthrough/#hyena-filter-stripedhyenamodelpy","title":"Hyena Filter (<code>stripedhyena/model.py</code>)","text":"<p>The <code>ParallelHyenaFilter</code> implements the core Hyena operation: a short FIR filter followed by a long IIR filter parameterized by learnable poles and residues. The filter supports both parallel (training/prefill) and sequential (autoregressive generation) modes.</p> <p>Hyena filter structure:</p> external_repos/hyena/stripedhyena/model.py (lines 85-215)<pre><code>class ParallelHyenaFilter(nn.Module):\n    def __init__(self, config, layer_idx):\n        self.short_filter_length = config.short_filter_length\n        self.short_filter_weight = nn.Parameter(torch.randn(3 * config.hidden_size, 1, config.short_filter_length))\n        self.poles = nn.Parameter(poles)\n        self.residues = nn.Parameter(torch.randn(self.num_systems, self.state_size, 1, 2))\n        self.engine = HyenaInferenceEngine(layer_idx=layer_idx)\n    def forward(self, u, inference_params=None, padding_mask=None, *args, **kwargs):\n        if inference_params is not None and self.layer_idx in inference_params.fir_state_dict.keys():\n            return self.sequential_forward(u, inference_params)\n        else:\n            return self.parallel_forward(u, inference_params, padding_mask)\n</code></pre> <p>Parallel forward (training/prefill):</p> external_repos/hyena/stripedhyena/model.py (lines 163-215)<pre><code>def parallel_forward(self, u, inference_params=None, padding_mask=None):\n    L = u.shape[1]\n    z_pre, fir_state = self.engine.parallel_fir(\n        self.fir_fn, u, self.short_filter_weight, self.short_filter_bias, L,\n        fir_length=self.short_filter_length, inference_params=inference_params, padding_mask=padding_mask\n    )\n    if self.h is None:\n        h, filter_dtype, poles, residues = self.compute_filter(L, u.device)\n    y = self.engine.parallel_iir(\n        z_pre, h, self.D, L, t=self.t, poles=self.poles, residues=self.residues,\n        dims=dims, inference_params=inference_params, layer_idx=self.layer_idx,\n        prefill_style=self.config.get(\"prefill_style\", \"fft\"), use_flashfft=self.use_flashfft, ...\n    )\n    return y, inference_params\n</code></pre> <p>Sequential forward (autoregressive generation):</p> external_repos/hyena/stripedhyena/model.py (lines 217-251)<pre><code>def sequential_forward(self, u, inference_params):\n    if len(u.shape) &gt; 2:\n        u = u[:, -1]\n    fir_state, iir_state = inference_params.fir_state_dict[self.layer_idx], inference_params.state_dict[self.layer_idx]\n    z_pre, fir_state = self.engine.step_fir(u, fir_state, weight=self.short_filter_weight, bias=self.short_filter_bias)\n    x2, x1, v = column_split(z_pre, self.num_attention_heads, self.hidden_size_per_attention_head)\n    y, iir_state = self.engine.step_iir(x2, x1, v, self.D, self.residues, self.poles, iir_state, iir_groups=self.hyena_filter_groups)\n    inference_params.fir_state_dict[self.layer_idx] = fir_state\n    inference_params.state_dict[self.layer_idx] = iir_state\n    return y[:, None], inference_params\n</code></pre>"},{"location":"code_walkthroughs/hyena_walkthrough/#hyena-inference-engine-stripedhyenaenginepy","title":"Hyena Inference Engine (<code>stripedhyena/engine.py</code>)","text":"<p>The <code>HyenaInferenceEngine</code> handles FIR and IIR computations for both parallel and sequential modes. It supports multiple prefill strategies (FFT, recurrence, modal-FFT) and manages state caching for efficient generation.</p> <p>Parallel FIR computation:</p> external_repos/hyena/stripedhyena/engine.py (lines 65-109)<pre><code>def parallel_fir(self, fir_fn, u, weight, bias, L, fir_length=3, inference_params=None, prefill_mode=None, padding_mask=None):\n    if fir_fn != torch.nn.functional.conv1d:\n        z_pre = fir_fn(u)[:, :L]\n        z_pre = z_pre.permute(0, 2, 1)\n    else:\n        u = u.permute(0, 2, 1)\n        z_pre = fir_fn(u, weight, bias=None, stride=1, padding=fir_length - 1, groups=u.shape[1])[..., :L]\n        z_pre = z_pre + bias[None, :, None]\n    if type(padding_mask) == torch.Tensor:\n        z_pre = z_pre * padding_mask[:, None]\n    if inference_params is not None:\n        fir_state = u[..., -fir_length + 1 :]\n    return z_pre, fir_state\n</code></pre> <p>Parallel IIR with FFT:</p> external_repos/hyena/stripedhyena/engine.py (lines 111-215)<pre><code>def parallel_iir(self, z_pre, h, D, L, poles, residues, t, dims, inference_params=None, prefill_style=\"fft\", fftconv_fn=None, ...):\n    x2, x1, v = z_pre.split([hidden_size, hidden_size, hidden_size], dim=1)\n    x1v = x1 * v\n    if use_flashfft and (L % 2) == 0:\n        y = fftconv_fn(x1v.to(dtype=torch.bfloat16).contiguous(), h.to(dtype=torch.float32))\n    else:\n        H = torch.fft.rfft(h.to(dtype=torch.float32), n=fft_size) / fft_size\n        X_s = torch.fft.fft(x1v.to(dtype=torch.float32), n=fft_size)\n        X = X_s[..., : H.shape[-1]]\n        y = torch.fft.irfft(X * H, n=fft_size, norm=\"forward\")[..., :L]\n    y = y.to(dtype=x1v.dtype)\n    y = (y + x1v * D.unsqueeze(-1)) * x2\n    return y, inference_params\n</code></pre>"},{"location":"code_walkthroughs/hyena_walkthrough/#attention-block-stripedhyenamodelpy","title":"Attention Block (<code>stripedhyena/model.py</code>)","text":"<p>Attention blocks use FlashAttention-v2 with grouped query attention (GQA) and rotary positional embeddings. They provide targeted pattern recall capabilities that complement the Hyena blocks' long-context processing.</p> <p>Attention block structure:</p> external_repos/hyena/stripedhyena/model.py (lines 26-82)<pre><code>class AttentionBlock(nn.Module):\n    def __init__(self, config, layer_idx):\n        self.pre_norm, self.post_norm = RMSNorm(config), RMSNorm(config)\n        self.inner_mha_cls = MHA(\n            embed_dim=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            num_heads_kv=config.num_attention_heads // self.proj_groups,\n            rotary_emb_dim=config.hidden_size // config.num_attention_heads,\n            causal=True, layer_idx=layer_idx, use_flash_attn=self.config.use_flash_attn,\n        ).to(dtype=dtype)\n        self.mlp = ParallelGatedMLP(config).to(dtype=mlp_dtype)\n    def forward(self, u, inference_params=None, padding_mask=None, *args, **kwargs):\n        u = self.inner_mha_cls(self.pre_norm(u), inference_params=inference_params) + u\n        u = self.mlp(self.post_norm(u)) + u\n        return u, None\n</code></pre>"},{"location":"code_walkthroughs/hyena_walkthrough/#gated-convolution-block-stripedhyenamodelpy","title":"Gated Convolution Block (<code>stripedhyena/model.py</code>)","text":"<p><code>ParallelGatedConvBlock</code> wraps the Hyena filter with input projections, output projections, and an MLP, following a standard transformer-like structure with residual connections.</p> <p>Gated convolution block:</p> external_repos/hyena/stripedhyena/model.py (lines 277-321)<pre><code>class ParallelGatedConvBlock(nn.Module):\n    def __init__(self, config, layer_idx):\n        self.pre_norm, self.post_norm = RMSNorm(config).to(dtype=dtype), RMSNorm(config).to(dtype=dtype)\n        self.filter = ParallelHyenaFilter(config, layer_idx).to(dtype=dtype)\n        self.projections = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n        self.out_filter_dense = nn.Linear(config.hidden_size, config.hidden_size).to(dtype)\n        self.mlp = ParallelGatedMLP(config).to(dtype=mlp_dtype)\n    def forward(self, u, inference_params=None, padding_mask=None, *args, **kwargs):\n        z = self.proj_norm_fn(u)\n        z, inference_params = self.filter(z, inference_params=inference_params, padding_mask=padding_mask)\n        z_in = self.out_filter_dense(z) + u\n        y = self.res_mlp_norm_fn(z_in)\n        return y, inference_params\n</code></pre>"},{"location":"code_walkthroughs/hyena_walkthrough/#generation-stripedhyenagenerationpy","title":"Generation (<code>stripedhyena/generation.py</code>)","text":"<p>The <code>Generator</code> class handles autoregressive text generation with support for cached generation (recurrent mode) and standard generation. It manages inference parameters and state updates across tokens.</p> <p>Generation with caching:</p> external_repos/hyena/stripedhyena/generation.py (lines 14-158)<pre><code>class Generator:\n    def generate(self, device, input_string=None, input_ids=None, num_tokens=32, cached_generation=False, ...):\n        if cached_generation:\n            inference_params_dict_out = self.model.initialize_inference_params()\n        for i in range(int(num_tokens)):\n            post_prefill = cached_generation and i &gt; 0\n            if post_prefill:\n                x = x[:, -1:]\n                inference_params_dict_out[\"mha\"].seqlen_offset += 1\n                inference_params_dict_out[\"hyena\"].seqlen_offset += 1\n            with torch.no_grad():\n                logits, inference_params_dict_out = self.model(x, inference_params_dict=inference_params_dict_out)\n            last_logits = logits[:, -1]\n            new_idx = sample(last_logits, top_k=self.top_k, top_p=self.top_p, temperature=self.temperature)\n            if post_prefill:\n                x = new_idx[:, None]\n            else:\n                x = torch.cat([x, new_idx[:, None]], dim=-1)\n        return generation[:, : i + 1], scores[:, : i + 1]\n</code></pre>"},{"location":"code_walkthroughs/hyena_walkthrough/#configuration-configs7b-sh-32k-v1yml","title":"Configuration (<code>configs/7b-sh-32k-v1.yml</code>)","text":"<p>The YAML config specifies layer indices for attention vs. Hyena blocks, filter parameters, and inference settings. The default 7B model alternates attention and Hyena blocks.</p> <p>Configuration structure:</p> external_repos/hyena/configs/7b-sh-32k-v1.yml (lines 1-53)<pre><code>model_name: sh-7b-32k-v1\nvocab_size: 32000\nhidden_size: 4096\nnum_filters: 4096\nattn_layer_idxs: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\nhyena_layer_idxs: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]\nnum_layers: 32\nshort_filter_length: 3\nnum_attention_heads: 32\nstate_size: 2\nrotary_emb_base: 500000\nproj_groups: 4\nhyena_filter_groups: 1\nmax_seqlen: 32768\nuse_flash_attn: True\nuse_flash_rmsnorm: True\nprefill_style: fft\n</code></pre>"},{"location":"code_walkthroughs/hyena_walkthrough/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Embedding extraction. The model returns logits <code>[B, L, vocab_size]</code> from <code>forward()</code>. To extract embeddings, access hidden states before the final norm: <code>hidden_states = model.embedding_layer.embed(input_ids)</code> then pass through blocks manually, or modify <code>forward()</code> to return <code>x</code> before <code>self.unembed.unembed(x)</code>.^[<code>text title=\"external_repos/hyena/stripedhyena/model.py</code>] (lines 358-371)\"</li> <li>Pooling strategies. Mean pooling across sequence length yields <code>[B, hidden_size]</code> representations. For long sequences, consider pooling only over the last N tokens or using attention-weighted pooling. The alternating block structure means embeddings capture both local (Hyena) and global (attention) patterns.^[<code>text title=\"external_repos/hyena/stripedhyena/model.py</code>] (lines 381-387)\"</li> <li>State management for long contexts. When processing sequences longer than training length, use <code>cached_generation=True</code> to enable recurrent mode. The inference parameters (<code>InferenceParams</code> for attention, <code>RecurrentInferenceParams</code> for Hyena) manage KV caches and FIR/IIR state across generation steps.^[<code>text title=\"external_repos/hyena/stripedhyena/model.py</code>] (lines 389-402)\"</li> <li>Projection to shared latent space. Map pooled <code>[B, hidden_size]</code> embeddings (hidden_size=4096 for 7B) to a 512-D multimodal space using a lightweight projector similar to other foundation models. The Hyena blocks' long-context capabilities make this architecture suitable for processing extended genomic or brain imaging sequences.^[<code>text title=\"external_repos/hyena/stripedhyena/model.py</code>] (lines 358-371)\"</li> <li>Filter precomputation for efficiency. For fixed-length inputs, call <code>model.precompute_filters(L, device)</code> before training/inference to cache the IIR filter <code>h</code> and avoid recomputing it on each forward pass. This is especially beneficial when batch processing many sequences of the same length.^[<code>text title=\"external_repos/hyena/stripedhyena/model.py</code>] (lines 404-420)\"</li> <li>Alternating block design. The model alternates between Hyena and attention blocks, with Hyena blocks handling the majority of computation. This design allows the model to leverage both efficient long-context processing (Hyena) and targeted pattern recall (attention), making it suitable for diverse sequence modeling tasks in neuro-omics applications.^[<code>text title=\"external_repos/hyena/stripedhyena/model.py</code>] (lines 324-333)\"</li> </ul>"},{"location":"code_walkthroughs/m3fm_walkthrough/","title":"M3FM Code Walkthrough","text":"<p>KB references: Model card (pending) \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#overview","title":"Overview","text":"<p>M3FM couples multilingual CLIP text embeddings with the original R2Gen relational-memory Transformer decoder to generate bilingual COVID-era chest X-ray reports. The entrypoint <code>M3FM.py</code> wires tokenization, dataset splits, optimizer/scheduler, and the trainer while <code>modules/text_extractor.py</code> handles medical text preprocessing and embedding, and <code>modules/encoder_decoder.py</code> implements the Transformer + RelationalMemory decoder that outputs report logits for teacher-forced training; inference routes beam/greedy decoding through English or Chinese heads via CLI flags.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/README.md</code>][<code>15:126:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/M3FM.py</code>][<code>16:53:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/text_extractor.py</code>][<code>227:355:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/encoder_decoder.py</code>][<code>130:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] (lines 1-72)\"</p>"},{"location":"code_walkthroughs/m3fm_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Multilingual CLIP text embeddings \u2192 relational-memory Transformer decoder (beam/greedy) for bilingual CXRs.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/text_extractor.py</code>][<code>227:355:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/encoder_decoder.py</code>][<code>130:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] Defaults: <code>d_model=512</code>, FFN 2048, 3 decoder layers, 8 heads, <code>rm_num_slots=3</code>, <code>beam_size=3</code>, <code>epochs=15</code>.^[<code>29:81:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/M3FM.py</code>][<code>34:76:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] 224\u00d7224 CXRs with max 100 tokens; BOS token <code>1</code> (English) or <code>2</code> (Chinese) selects the generation language.^[<code>18:75:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/dataloaders.py</code>][<code>20:115:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/datasets.py</code>][<code>162:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] <code>annotation.json</code> + image roots streamed by <code>R2DataLoader</code>, yielding <code>(reports_ids, reports_ids_use)</code> tensors for teacher forcing.^[<code>18:75:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/dataloaders.py</code>][<code>20:115:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/datasets.py</code>] Trainer wraps SGD + StepLR, gradient clipping, multilingual greedy decoding, and BLEU/SPICE-compatible evaluation utilities.^[<code>91:124:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/M3FM.py</code>][<code>203:221:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/trainer.py</code>][<code>130:210:/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] github.com/ai-in-health/M3FM"},{"location":"code_walkthroughs/m3fm_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda + pip workflow. Create <code>conda create -n M3FM python==3.9</code>, activate, install CUDA 11.8-compatible PyTorch (<code>torch&gt;=1.10.1</code>, <code>torchvision&gt;=0.11.2</code>, <code>pytorch-cuda==11.8</code>) followed by <code>pip install -r requirements.txt</code>; repo validated on <code>torch==2.2.1</code>.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/README.md</code>] (lines 4-21)\"</li> <li>Metric prerequisites. Java, <code>pycocoevalcap</code>, <code>pycocotools</code>, and Stanford CoreNLP jars are required for SPICE; README documents manual download/placement steps to avoid firewalls.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/README.md</code>] (lines 46-71)\"</li> <li>Language evaluation assets. Place <code>stanford-corenlp-4.5.2</code> under <code>data/</code> and keep <code>corenlp_root</code> in <code>configs/__init__.py</code> synchronized when switching between English and Chinese inference.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/README.md</code>] (lines 61-71)\"</li> </ul>"},{"location":"code_walkthroughs/m3fm_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/m3fm_walkthrough/#tokenizer-data-interface-modulesdataloaderspy-modulesdatasetspy","title":"Tokenizer + Data Interface (<code>modules/dataloaders.py</code>, <code>modules/datasets.py</code>)","text":"<p><code>R2DataLoader</code> centralizes resizing/normalization, dataset selection (IU X-Ray vs. MIMIC/COV), and a collate function that pads both the teacher-forced <code>reports_ids</code> (targets) and decoder inputs (<code>reports_ids_use</code>). The dataset class uses cleaned strings to build token IDs, tracks language label via the leading token, and emits both full targets and shifted inputs.</p> <p>Data loader with transforms:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/dataloaders.py (lines 8-45)<pre><code>class R2DataLoader(DataLoader):\n    def __init__(self, args, tokenizer, split, shuffle):\n        self.args = args\n        self.dataset_name = args.dataset_name\n        self.batch_size = args.batch_size\n        self.shuffle = shuffle\n        self.num_workers = args.num_workers\n        self.tokenizer = tokenizer\n        self.split = split\n\n        if split == 'train':\n            self.transform = transforms.Compose([\n                transforms.Resize(256),\n                transforms.RandomCrop(224),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406),\n                                     (0.229, 0.224, 0.225))])\n        else:\n            self.transform = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406),\n                                     (0.229, 0.224, 0.225))])\n\n        if self.dataset_name == 'iu_xray':\n            self.dataset = IuxrayMultiImageDataset(self.args, self.tokenizer, self.split, transform=self.transform)\n        else:\n            self.dataset = MimiccxrSingleImageDataset(self.args, self.tokenizer, self.split, transform=self.transform)\n</code></pre> <p>Collate function with padding:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/dataloaders.py (lines 48-74)<pre><code>    @staticmethod\n    def collate_fn(data):\n        images_id, images, reports_ids, report, seq_lengths, seq_length1,image_path_all, reports_ids_use = zip(*data)\n\n        images = torch.stack(images, 0)\n        max_seq_length = max(seq_lengths)\n\n        targets = np.zeros((len(reports_ids), max_seq_length), dtype=int)\n        targets_masks = np.zeros((len(reports_ids), max_seq_length), dtype=int)\n\n        for i, report_ids in enumerate(reports_ids):\n            targets[i, :len(report_ids)] = report_ids\n\n        max_seq_length_us = max(seq_length1)\n        targets_us = np.zeros((len(reports_ids_use),  max_seq_length_us), dtype=int)\n        targets_masks1 = np.zeros((len(reports_ids_use), max_seq_length_us ), dtype=int)\n\n        for i,reports_ids_use1 in enumerate(reports_ids_use):\n            targets_us[i, :len(reports_ids_use1)] = reports_ids_use1\n\n        return images_id, images, torch.LongTensor(targets), report,image_path_all ,torch.LongTensor(targets_us)\n</code></pre>"},{"location":"code_walkthroughs/m3fm_walkthrough/#multilingual-textextractor-modulestext_extractorpy","title":"Multilingual TextExtractor (<code>modules/text_extractor.py</code>)","text":"<p>The <code>TextExtractor</code> loads <code>M-CLIP/XLM-Roberta-Large-Vit-L-14</code>, averages contextual token embeddings with attention masking, projects them through CLIP's linear head, then applies a learnable affine + ReLU to map the 768-d output to the 512-d hidden size expected by the decoder. Reports are cleaned per language before tokenization, enabling bilingual support without retraining the encoder.</p> <p>Multilingual CLIP text encoder:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/text_extractor.py (lines 16-53)<pre><code>class TextExtractor(nn.Module):\n    def __init__(self, args):\n        super(TextExtractor, self).__init__()\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        self.model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'\n        self.model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(self.model_name, device=self.device)\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name, device=self.device)\n        self.clean_report = self.clean_report_cov\n        \n        self.transformer = self.model.transformer\n        self.LinearTransformation = self.model.LinearTransformation\n        \n        self.affine_aa = nn.Linear(768, 512).cuda()\n        \n    def forward(self, reports):\n        if isinstance(reports, tuple):\n            texts=[]\n            for example in reports:\n                example=self.clean_report(example)\n                texts.append(example)\n        else:\n            texts=self.clean_report(reports)\n\n\n        with torch.no_grad():\n            txt_tok = self.tokenizer(texts, padding=True, return_tensors='pt').to(self.device)\n            embs = self.transformer(**txt_tok)[0]\n            att = txt_tok['attention_mask']\n            embs = (embs * att.unsqueeze(2)).sum(dim=1) / att.sum(dim=1)[:, None]\n            embeddings = self.LinearTransformation(embs).cuda()\n\n        \n\n\n        embeddings = F.relu(self.affine_aa(embeddings)).cuda() #batch*768--\u300bbatch*512\n        return embeddings #batch*512\n</code></pre>"},{"location":"code_walkthroughs/m3fm_walkthrough/#relational-memory-transformer-decoder-modulesencoder_decoderpy","title":"Relational-Memory Transformer Decoder (<code>modules/encoder_decoder.py</code>)","text":"<p><code>Transformer</code> wraps a Decoder-only stack augmented with conditional layer norm controlled by a relational memory module. Before projection, the model reshapes logits to match token vocab (default 464). Memory slots capture long-range dependencies from the previous tokens, improving report fluency over vanilla Transformer decoders.</p> <p>Decoder with relational memory:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/encoder_decoder.py (lines 228-252)<pre><code>class Transformer(nn.Module):\n    def __init__(self):\n        super(Transformer, self).__init__()\n        #self.encoder = Encoder().cuda()\n        self.decoder = Decoder().cuda()\n        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()\n        self.rm=RelationalMemory()\n        self.tgt_emb = Embeddings().cuda()\n\n    def forward(self, enc_outputs, dec_inputs):\n        '''\n        enc_inputs: [batch_size, src_len]\n        dec_inputs: [batch_size, tgt_len]\n        '''\n        # tensor to store decoder outputs\n        dec_outputs = self.decode(dec_inputs,  enc_outputs)\n        dec_logits = self.projection(dec_outputs)  # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n        \n        return dec_logits.reshape(-1, dec_logits.size(-1))\n\n    def decode(self,  dec_inputs, enc_outputs):\n        memory = self.rm.init_memory(enc_outputs.size(0)).to( enc_outputs)\n        memory = self.rm(self.tgt_emb(dec_inputs), memory)\n        return self.decoder(dec_inputs, enc_outputs, memory)\n</code></pre>"},{"location":"code_walkthroughs/m3fm_walkthrough/#trainer-scheduler-modulestrainerpy","title":"Trainer &amp; Scheduler (<code>modules/trainer.py</code>)","text":"<p><code>Trainer._train_epoch</code> streams teacher-forced batches, clips gradients to <code>0.1</code>, steps SGD and the StepLR schedule every iteration, and records average loss per epoch. Mixed precision isn't enabled here, so plan GPU memory accordingly.</p> <p>Training loop with gradient clipping:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/trainer.py (lines 203-221)<pre><code>    def _train_epoch(self, epoch):\n\n        train_loss = 0\n        self.model.cuda().train()\n        for batch_idx, (images_id, images, reports_ids, report,image_path_all,reports_ids_use) in enumerate(self.train_dataloader):\n            images, reports_ids,reports_ids_use= images.to(self.device), reports_ids.to(self.device),reports_ids_use.to(self.device)\n\n            output = self.model(report, reports_ids_use)\n            \n            loss = self.criterion(output, reports_ids[:, 1:].reshape(-1))\n            train_loss += loss.item()\n            self.optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.model.parameters(), 0.1)\n            self.optimizer.step()\n        log = {'train_loss': train_loss / len(self.train_dataloader)}\n\n        self.lr_scheduler.step()\n        return log\n</code></pre>"},{"location":"code_walkthroughs/m3fm_walkthrough/#bilingual-inference-script-inferencepy","title":"Bilingual Inference Script (<code>inference.py</code>)","text":"<p><code>inference.py</code> mirrors the training CLI, loads both English and Chinese <code>R2GenModel</code> variants, performs greedy decoding conditioned on the BOS token, and prints generated reports. Changing <code>--language</code> toggles which head runs and when the search halts.</p> <p>Language-specific greedy decoding:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py (lines 162-210)<pre><code>if args.language=='English' or args.language=='All':\n    model_en.eval()\n    output = False\n    with torch.no_grad():\n        for batch_idx, (images_id, images, reports_ids, report, image_path_all, reports_ids_use) in enumerate(\n                test_dataloader):\n            images, reports_ids, reports_ids_use = images.to(device), reports_ids.to(\n                device), reports_ids_use.to(device)\n\n            for i in range(len(images_id)):\n                if reports_ids[i][0] == 1:\n                    greedy_dec_input = greedy_decoder(model_en, image_path_all[i], reports_ids[i], start_symbol=1)\n                    predict = model_en(image_path_all[i], greedy_dec_input)\n                    predict = predict.data.max(1, keepdim=True)[1]\n                    output = True\n                    predict = predict.squeeze()\n                    report = model_en.tokenizer.decode(predict.cpu().numpy())\n                    print(\"----------------------------------------------------------------------------------------\")\n                    print(\"Generated English Report:\")\n                    print(report)\n                    print(\"----------------------------------------------------------------------------------------\")\n                    break\n            if output:\n                break\n</code></pre>"},{"location":"code_walkthroughs/m3fm_walkthrough/#integration-hooks-vision-clinical-language","title":"Integration Hooks (Vision \u2194 Clinical Language)","text":"<ul> <li>Tap 512-d text embeddings. <code>TextExtractor</code> already outputs normalized 512-d vectors before relational memory; export them for multimodal alignment (e.g., with genetic embeddings) or to seed cross-modal contrastive losses.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/text_extractor.py</code>] (lines 16-53)\"</li> <li>Language-aware batching. The dataset prefixes each sequence with <code>1</code> (English) or <code>2</code> (Chinese); filtering on the first token lets you run per-language evaluators or remap tokens to KB-friendly ontologies without retraining.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/modules/datasets.py</code>] (lines 20-75)\"</li> <li>Reuse greedy decoder outputs. <code>greedy_decoder</code> yields raw token IDs before detokenization, making it straightforward to log intermediate logits or route them into KB experiment trackers for BLEU/SPICE comparisons across modalities.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/M3FM/inference.py</code>] (lines 130-210)\"</li> </ul>"},{"location":"code_walkthroughs/melamma_walkthrough/","title":"Me-LLaMA Code Walkthrough","text":"<p>KB references: Me-LLaMA paper note</p>"},{"location":"code_walkthroughs/melamma_walkthrough/#overview","title":"Overview","text":"<p>Me-LLaMA is a suite of open-source medical foundation models (13B/70B) developed through continual pre-training and instruction tuning of LLaMA2. It leverages a heterogeneous corpus of biomedical literature (PubMed), clinical notes (MIMIC-IV/MIMIC-CXR), and general domain data to balance domain specificity with general reasoning. The repository provides evaluation scripts, training recipes, and inference examples for both base and chat-aligned versions.^[<code>text title=\"external_repos/me-lamma/README.md</code>] (lines 33-36)\"</p>"},{"location":"code_walkthroughs/melamma_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params / Scale Context Inputs Key capabilities Repo LLaMA-2 based (Continual Pre-training + LoRA Tuning) 13B &amp; 70B parameters 129B tokens mixed corpus (15:1:4 biomedical:clinical:general) Text (clinical notes, papers, guidelines) Medical reasoning, instruction following, zero-shot evaluation on PubMedQA/MedQA. GitHub / PhysioNet"},{"location":"code_walkthroughs/melamma_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Installation: Requires <code>torch</code> and <code>transformers</code>. Evaluation dependencies managed via <code>poetry</code> in <code>src/medical-evaluation</code>.^[<code>text title=\"external_repos/me-lamma/README.md</code>] (lines 144-152)\"</li> <li>Compute: Developed on A100 GPUs (160x for pre-training) and H100 GPUs (8x for tuning). Local inference runs on standard GPU setups via Hugging Face pipelines.^[<code>text title=\"external_repos/me-lamma/README.md</code>] (lines 80-89)\"</li> <li>Access: Models require PhysioNet credentialed access; datasets available via Hugging Face collection.^[<code>text title=\"external_repos/me-lamma/README.md</code>] (lines 41-44)\"</li> </ul>"},{"location":"code_walkthroughs/melamma_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/melamma_walkthrough/#training-pipeline-readmemd","title":"Training Pipeline (<code>README.md</code>)","text":"<p>The training strategy emphasizes continual pre-training followed by instruction tuning: 1.  Continual Pre-training: 129B tokens mixed from PubMed (15), MIMIC (1), and RedPajama (4). Uses AdamW, cosine scheduler (0.05 warmup), and bf16 precision with DeepSpeed parallelism. 2.  Instruction Tuning: 214K samples trained for 3 epochs using LoRA parameter-efficient fine-tuning on H100s. This approach mitigates catastrophic forgetting while injecting specialized medical knowledge.^[<code>text title=\"external_repos/me-lamma/README.md</code>] (lines 64-91)\"</p>"},{"location":"code_walkthroughs/melamma_walkthrough/#inference-stack-readmemd","title":"Inference Stack (<code>README.md</code>)","text":"<p>Inference is standard Hugging Face <code>transformers</code>. The README provides snippets for both high-level <code>pipeline</code> usage and low-level <code>AutoModelForCausalLM</code> control:</p> <p>Basic Generation: <pre><code>from transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"path/to/Me-LLaMA\")\nprint(pipe(\"The medical condition is characterized by\", num_return_sequences=1))\n</code></pre></p> <p>Granular Control: <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"path/to/Me-LLaMA\")\nmodel = AutoModelForCausalLM.from_pretrained(\"path/to/Me-LLaMA\")\ninput_ids = tokenizer(\"[INPUT]\", return_tensors=\"pt\").input_ids\ngen = model.generate(input_ids, max_length=50)\nprint(tokenizer.decode(gen[0]))\n</code></pre> ^[<code>text title=\"external_repos/me-lamma/README.md</code>] (lines 93-138)\"</p>"},{"location":"code_walkthroughs/melamma_walkthrough/#evaluation-harness-srcmedical-evaluation","title":"Evaluation Harness (<code>src/medical-evaluation</code>)","text":"<p>The repository includes a robust evaluation suite using <code>poetry</code> and <code>src/eval.py</code>. It supports: - Hugging Face Models: Evaluate local or Hub models (e.g., <code>hf-causal-vllm</code>) against medical benchmarks (PUBMEDQA, MedQA, BioNLI, etc.). - Commercial APIs: Compare against GPT-4 by swapping the model argument. - Metrics: Includes BARTScore integration (<code>src/metrics/BARTScore</code>).</p> <p>Run Example: <pre><code>poetry run python src/eval.py \\\n    --model \"hf-causal-vllm\" \\\n    --model_args \"pretrained=meta-llama/Llama-2-7b-chat-hf\" \\\n    --tasks \"PUBMEDQA,MedQA,MedMCQA,...\"\n</code></pre> ^[<code>text title=\"external_repos/me-lamma/README.md</code>] (lines 139-190)\"</p>"},{"location":"code_walkthroughs/melamma_walkthrough/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Benchmark Alignment: Use the task list in <code>scripts/run_evaluation.sh</code> (\"PUBMEDQA,MedQA...\") as a standard checklist for evaluating new KB models.</li> <li>Dataset Collection: The Hugging Face collection referenced is a valuable resource for populating <code>kb/datasets/</code>.</li> <li>Baseline Comparisons: Use the provided GPT-4 evaluation scripts to establish strong baselines for neuro-omics tasks.</li> </ul>"},{"location":"code_walkthroughs/mot_walkthrough/","title":"MoT Code Walkthrough","text":"<p>KB references: MoT paper note</p>"},{"location":"code_walkthroughs/mot_walkthrough/#overview","title":"Overview","text":"<p>Mixture-of-Transformers (MoT) introduces modality-aware sparsity to every non-embedding block so that each modality owns its feed-forward, attention, and normalization routes while still sharing global self-attention. In practice this lets a 7B-text+image MoT hit dense-model quality with only 55.8\u202f% of the FLOPs, extend to speech with 37.2\u202f% of the dense compute, and run multi-branch generation faster on commodity A100s.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 6-30)\"</p>"},{"location":"code_walkthroughs/mot_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params / FLOPs Context Inputs Key capabilities Repo Attach modality-untied feed-forward + attention experts to an existing transformer, using binary <code>modality_masks</code> to route tokens yet keeping shared global attention.^[<code>text title=\"external_repos/MoT/README.md</code>][<code>16:151:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>] 7B MoT (text+image) matches dense baselines at 55.8\u202f% FLOPs; 443\u202fM MoT (text+image+speech) hits dense speech quality at 37.2\u202f% FLOPs.^[<code>15:23:external_repos/MoT/README.md</code>] Chameleon (autoregressive text + raster image), Transfusion (text autoregressive + image diffusion), and broader \u201cnative multimodal\u201d projects.^[<code>15:30:external_repos/MoT/README.md</code>] Any packed token sequence as long as each token is tagged in <code>modality_masks</code>; examples show text/image/speech masks and detoured normalization rules.^[<code>129:137:external_repos/MoT/README.md</code>][<code>87:137:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>] Step-by-step tutorial covering FFN experts, attention experts, and normalization placement so you can graft MoT onto proprietary stacks.^[<code>75:330:external_repos/MoT/README.md</code>] <code>external_repos/MoT</code>"},{"location":"code_walkthroughs/mot_walkthrough/#environment-integration-notes","title":"Environment &amp; Integration Notes","text":"<ul> <li>Designed as a playbook on top of your transformer\u2014start from any stack that exposes attention/FFN modules and thread through MoT\u2019s modality-specific replacements.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 40-71)\"</li> <li>Efficient gains hinge on accurate routing masks; the README demonstrates simple boolean lists and emphasises deterministic routing per modality.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 129-137)\"</li> <li>Norm placement matters: either keep residual norms inside the expert modules (preferred) or refactor your <code>TransformerBlock</code> to avoid double-normalizing.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 307-330)\"</li> </ul>"},{"location":"code_walkthroughs/mot_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/mot_walkthrough/#modality-untied-feed-forward-srcsimple_modalityuntiedfeedforwardpy","title":"Modality-Untied Feed-Forward (<code>src/simple_ModalityUntiedFeedForward.py</code>)","text":"<p><code>SimpleModalityUntiedFeedForward</code> replicates a SiLU-gated MLP per modality, normalizes each expert\u2019s output, then stitches results back into the original token order via <code>merge_modalities</code>. Swapping only this block already covers \u224867\u202f% of non-embedding parameters, so most FLOP savings arrive after this step.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 75-119)\"</p> external_repos/MoT/src/simple_ModalityUntiedFeedForward.py (lines 17-60)<pre><code>class SimpleModalityUntiedFeedForward(torch.nn.Module):\n    def __init__(..., n_modalities: int = 2):\n        ...\n        self.local_experts = torch.nn.ModuleList([\n            SimpleFeedForward(...)\n            for _ in range(self.n_modalities)\n        ])\n        self.local_experts_ffn_norm = torch.nn.ModuleList(\n            [SimpleRMSNorm(dim, eps=1e-5) for _ in range(self.n_modalities)]\n        )\n\n    def forward(self, x, modality_masks):\n        expert_outputs = []\n        for i in range(self.n_modalities):\n            expert_input = x[modality_masks[i]]\n            expert_output = self.local_experts[i](expert_input)\n            expert_output = self.local_experts_ffn_norm[i](expert_output)\n            expert_outputs.append(expert_output)\n        return merge_modalities(expert_outputs, modality_masks)\n</code></pre> <p>Because experts only see their modality tokens, you can scale specialization (e.g., text-heavy vs. image-heavy hidden sizes) without perturbing other branches. <code>SimpleFeedForward</code> itself is the Lingua-style gated MLP that preserves tensor-parallel friendliness.^[<code>text title=\"external_repos/MoT/src/simple_ModalityUntiedFeedForward.py</code>] (lines 64-107)\"</p>"},{"location":"code_walkthroughs/mot_walkthrough/#modality-untied-attention-srcsimple_modalityuntiedattentionpy","title":"Modality-Untied Attention (<code>src/simple_ModalityUntiedAttention.py</code>)","text":"<p>The attention module mirrors the FFN pattern: per-modality projections and RMSNorms for Q/K/V/outputs, shared global attention via <code>torch.nn.MultiheadAttention</code>, and a final per-modality projection back to the model dimension.^[<code>text title=\"external_repos/MoT/README.md</code>][<code>16:151:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>] (lines 141-301)\"</p> <p>Per-modality Q/K/V projections with shared attention:</p> external_repos/MoT/src/simple_ModalityUntiedAttention.py (lines 16-151)<pre><code>class SimpleModalityUntiedAttention(torch.nn.Module):\n    def __init__(...):\n        self.local_experts_wq = self._create_experts(dim, n_heads * head_dim)\n        self.local_experts_wk = self._create_experts(dim, n_heads * head_dim)\n        self.local_experts_wv = self._create_experts(dim, n_heads * head_dim)\n        self.local_experts_wo = self._create_experts(n_heads * head_dim, dim)\n        ...\n        self.attention_comp = torch.nn.MultiheadAttention(\n            head_dim=head_dim,\n            n_heads=n_heads,\n            dropout=dropout,\n        )\n</code></pre> <p>During <code>forward</code>, tokens are first split by mask, projected/normed per modality, concatenated back for standard attention, and finally projected/normed per modality again.^[<code>text title=\"external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>] Optional QK normalization reshapes tensors to <code>[*, num_heads, head_dim]</code> before applying <code>SimpleRMSNorm</code>, which keeps the rotary-scaled statistics stable.^[<code>112:174:external_repos/MoT/src/simple_ModalityUntiedAttention.py</code>] (lines 86-151)\"</p>"},{"location":"code_walkthroughs/mot_walkthrough/#utility-primitives-srcutilspy","title":"Utility Primitives (<code>src/utils.py</code>)","text":"<p><code>merge_modalities</code> reconstructs the packed sequence according to mask order, so expert outputs can be arbitrarily sharded while still producing a contiguous tensor for the residual path. <code>SimpleRMSNorm</code> is the Lingua-derived RMSNorm variant used consistently across experts.^[<code>text title=\"external_repos/MoT/src/utils.py</code>] (lines 14-66)\"</p> <p>Modality merging utility:</p> external_repos/MoT/src/utils.py (lines 14-34)<pre><code>def merge_modalities(expert_outputs, modality_masks):\n    merged = torch.empty_like(expert_outputs[0])\n    for i in range(len(expert_outputs) - 1, -1, -1):\n        merged[modality_masks[i]] = expert_outputs[i]\n    return merged\n</code></pre>"},{"location":"code_walkthroughs/mot_walkthrough/#implementation-checklist","title":"Implementation Checklist","text":"<ul> <li>Start from your baseline <code>TransformerBlock</code>, then replace FFN/attention classes with the modality-untied versions, ensuring the residual structure still performs <code>x + module(x)</code> as shown in the README.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 307-330)\"</li> <li>Provide <code>modality_masks</code> for every forward pass. The README\u2019s three-modality example demonstrates boolean masks; in production you can precompute these from tokenizer metadata or image/video region plans.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 129-137)\"</li> <li>Keep norm layers inside the modality-specific modules to avoid double-scaling outputs; only keep block-level norms if your baseline requires them.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 307-330)\"</li> </ul>"},{"location":"code_walkthroughs/mot_walkthrough/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Routing data from KB assets. When generating multimodal batches (e.g., fMRI tokens + gene tokens) build boolean masks once per modality and pass them into MoT blocks\u2014only the masks need awareness of modality boundaries.</li> <li>Progressive specialization. Because experts are independent <code>nn.ModuleList</code> entries, you can freeze or reinitialize select modalities while fine-tuning others (useful when only one KB modality changes).</li> <li>FLOP budgeting. The FLOP savings callouts (55.8\u202f% / 37.2\u202f% / one-third) provide targets for profiling when adapting MoT to new neuro-omics settings.^[<code>text title=\"external_repos/MoT/README.md</code>] (lines 15-30)\"</li> </ul>"},{"location":"code_walkthroughs/swift_walkthrough/","title":"SwiFT Code Walkthrough","text":"<p>KB references: Model card \u00b7 fMRI feature spec \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/swift_walkthrough/#overview","title":"Overview","text":"<p>SwiFT (Swin 4D fMRI Transformer) tokenizes 4D fMRI volumes with 3D convolutions, processes them with windowed 4D self-attention (spatial + temporal windows), and trains contrastive or supervised heads via PyTorch Lightning.^[<code>text title=\"external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>][<code>1:188:external_repos/swift/project/main.py</code>] (lines 1-400)\"</p>"},{"location":"code_walkthroughs/swift_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo Swin-inspired 4D transformer w/ window attention &amp; patch merging^[<code>text title=\"external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>] Configurable (e.g., embed_dim=96, depths from config)^[<code>402:565:external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>] 96\u00d796\u00d796 voxels \u00d7 20 frames (default)^[<code>250:300:external_repos/swift/project/module/utils/data_module.py</code>] Preprocessed volumes from <code>fMRIDataModule</code> (UKB/HCP/etc.)^[<code>13:260:external_repos/swift/project/module/utils/data_module.py</code>] Lightning training with contrastive or supervised heads, downstream evaluation scripts^[<code>21:187:external_repos/swift/project/main.py</code>][<code>32:395:external_repos/swift/project/module/pl_classifier.py</code>] github.com/Transconnectome/SwiFT"},{"location":"code_walkthroughs/swift_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Conda environment. The README tells you to run <code>conda env create -f envs/py39.yaml</code> followed by <code>conda activate py39</code> to pull in the exact PyTorch/Lightning versions used for the released checkpoints.^[<code>text title=\"external_repos/swift/README.md</code>] (lines 45-55)\"</li> <li>Gradient checkpoint knobs. Every Swin4D stage accepts <code>use_checkpoint</code> and executes <code>torch.utils.checkpoint.checkpoint(...)</code> when set, so add <code>use_checkpoint=True</code> in your model config to extend contexts without exceeding GPU memory.^[<code>text title=\"external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>][<code>507:744:external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>] (lines 224-312)\"</li> </ul>"},{"location":"code_walkthroughs/swift_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/swift_walkthrough/#data-module-projectmoduleutilsdata_modulepy","title":"Data Module (<code>project/module/utils/data_module.py</code>)","text":"<p><code>fMRIDataModule</code> loads datasets (UKB, HCP, etc.), splits subjects, and returns PyTorch <code>DataLoader</code>s. Augmentations (affine/noise) are applied in the Lightning module.</p> <p>PyTorch Lightning data module:</p> external_repos/swift/project/module/utils/data_module.py (lines 13-230)<pre><code>class fMRIDataModule(pl.LightningDataModule):\n    def get_dataset(self):\n        if self.hparams.dataset_name == \"S1200\": return S1200\n        ...\n    def setup(self, stage=None):\n        Dataset = self.get_dataset()\n        params = {\"root\": self.hparams.image_path, \"sequence_length\": self.hparams.sequence_length, ...}\n        self.train_dataset = Dataset(**params, subject_dict=train_dict, ...)\n        self.train_loader = DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, ...)\n</code></pre>"},{"location":"code_walkthroughs/swift_walkthrough/#patch-embedding-window-attention-swin4d_transformer_ver7py","title":"Patch Embedding &amp; Window Attention (<code>swin4d_transformer_ver7.py</code>)","text":"<p><code>PatchEmbed</code> downsamples volumes with strided 3D convs, <code>WindowAttention4D</code> computes attention inside local 4D windows, and <code>SwinTransformerBlock4D</code> applies shifted windows for better coverage. <code>PatchMergingV2</code> reduces spatial resolution while keeping temporal size.</p> <p>4D windowed attention with patch embedding:</p> external_repos/swift/project/module/models/swin4d_transformer_ver7.py (lines 202-399)<pre><code>class PatchEmbed(nn.Module):\n    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=(1, patch_size), stride=(1, patch_size))\n...\nclass WindowAttention4D(nn.Module):\n    def forward(self, x, mask):\n        qkv = self.qkv(x).reshape(...)\n        attn = self.softmax((q @ k.transpose(-2, -1)) * self.scale)\n        x = (attn @ v)\n</code></pre>"},{"location":"code_walkthroughs/swift_walkthrough/#swin4d-backbone-swin4d_transformer_ver7py","title":"Swin4D Backbone (<code>swin4d_transformer_ver7.py</code>)","text":"<p><code>BasicLayer</code> stacks windowed blocks, handles padding, applies attention masks, and optionally downsamples. The main <code>SwinTransformer4D</code> builds multiple stages with positional embeddings, patch merging, and normalization.</p> <p>Multi-stage Swin transformer with patch merging:</p> external_repos/swift/project/module/models/swin4d_transformer_ver7.py (lines 400-796)<pre><code>class BasicLayer(nn.Module):\n    for blk in self.blocks:\n        x = blk(x, attn_mask)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n    if self.downsample is not None:\n        x = self.downsample(x)\n...\nclass SwinTransformer4D(nn.Module):\n    self.patch_embed = PatchEmbed(...)\n    self.layers = nn.ModuleList([...])\n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n        for layer in self.layers:\n            x = self.pos_embeds[i] (x)\n            x = self.layers[i] (x.contiguous())\n        return x\n</code></pre>"},{"location":"code_walkthroughs/swift_walkthrough/#lightning-module-projectmodulepl_classifierpy","title":"Lightning Module (<code>project/module/pl_classifier.py</code>)","text":"<p><code>LitClassifier</code> wraps the encoder, applies augmentations if requested, and attaches task-specific heads (classification/regression/contrastive). <code>_calculate_loss</code> routes to BCE, MSE, or contrastive losses.</p> <p>Task-specific heads with loss routing:</p> external_repos/swift/project/module/pl_classifier.py (lines 32-205)<pre><code>self.model = load_model(self.hparams.model, self.hparams)\nif self.hparams.downstream_task == 'sex':\n    self.output_head = load_model(\"clf_mlp\", self.hparams)\nelif self.hparams.downstream_task == 'age':\n    self.output_head = load_model(\"reg_mlp\", self.hparams)\n...\ndef _calculate_loss(self, batch, mode):\n    if self.hparams.pretraining:\n        # contrastive losses (NT-Xent)\n    else:\n        subj, logits, target = self._compute_logits(batch)\n        if classification:\n            loss = F.binary_cross_entropy_with_logits(logits, target)\n        else:\n            loss = F.mse_loss(logits.squeeze(), target.squeeze())\n</code></pre>"},{"location":"code_walkthroughs/swift_walkthrough/#training-entry-point-projectmainpy","title":"Training Entry Point (<code>project/main.py</code>)","text":"<p>CLI parses dataset/model/task args, instantiates the Lightning module + data module, and launches PyTorch Lightning <code>Trainer</code> with callbacks (checkpointing, LR monitor).</p> <p>CLI entrypoint with Lightning trainer:</p> external_repos/swift/project/main.py (lines 18-187)<pre><code>parser = ArgumentParser(...)\nparser = Classifier.add_model_specific_args(parser)\nparser = Dataset.add_data_specific_args(parser)\nparser = pl.Trainer.add_argparse_args(parser)\nargs = parser.parse_args()\ndata_module = Dataset(**vars(args))\nmodel = Classifier(data_module=data_module, **vars(args))\ntrainer = pl.Trainer.from_argparse_args(args, logger=logger, callbacks=callbacks)\ntrainer.fit(model, datamodule=data_module)\n</code></pre>"},{"location":"code_walkthroughs/swift_walkthrough/#integration-hooks-brain-genetics","title":"Integration Hooks (Brain \u2194 Genetics)","text":"<ul> <li>Embedding shape. Encoder outputs <code>[B, N_tokens, embed_dim]</code>. Downstream heads either global-average tokens (<code>mean(dim=[2,3,4])</code>) or use CLS-like features (depending on head). Use <code>_compute_logits</code> to capture the tensor before the head for multimodal projection.^[<code>text title=\"external_repos/swift/project/module/pl_classifier.py</code>] (lines 108-205)\"</li> <li>Pooling choices. Mean pooling across spatial dimensions (<code>features.mean(dim=[2,3,4])</code>) produces <code>[B, embed_dim]</code>; temporal pooling can be added if you keep time as a separate axis prior to patch merging.</li> <li>Projection to shared latent. Apply a lightweight projector to map <code>[B, embed_dim]</code> into a 512-D shared space:</li> </ul> <p><pre><code>import torch.nn as nn\n\nclass SwiFTProjector(nn.Module):\n    def __init__(self, input_dim=768, output_dim=512, dropout=0.1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim),\n            nn.LayerNorm(output_dim),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre> - Augmentation awareness. When extracting embeddings for alignment, disable augmentations (<code>augment_during_training=False</code>) to avoid random affine/noise perturbations that would misalign with genetic features.^[<code>text title=\"external_repos/swift/project/module/pl_classifier.py</code>] (lines 108-205)\" - Window constraints. Ensure inference volumes match training window sizes (<code>img_size</code>, <code>window_size</code>)\u2014<code>get_window_size</code> shrinks windows when needed, but you lose attention overlap if sizes are too small.^[<code>text title=\"external_repos/swift/project/module/models/swin4d_transformer_ver7.py</code>] (lines 110-200)\"</p> <p>After projection, SwiFT embeddings (global pooled or CLS) can be concatenated or contrastively aligned with Evo\u202f2/GENERator/Caduceus projections for multimodal neurogenomics.</p>"},{"location":"code_walkthroughs/titan_walkthrough/","title":"TITAN Code Walkthrough","text":"<p>KB references: Model card (pending) \u00b7 Integration strategy \u00b7 Experiment config stub</p>"},{"location":"code_walkthroughs/titan_walkthrough/#overview","title":"Overview","text":"<p>TITAN (Transformer-based pathology Image and Text Alignment Network) aggregates CONCH\u202fv1.5 patch embeddings into slide-level representations using a Transformer encoder aligned with pathology-report text via a CoCa-style captioning loss. The public release focuses on the slide &amp; text encoders (decoder weights removed) and ships Hugging Face <code>trust_remote_code</code> modules for <code>encode_slide_from_patch_features</code>, plus fine-tuning and evaluation scaffolding for tasks like TCGA-OT linear probes and zero-shot slide retrieval.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>][<code>25:111:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>] (lines 1-110)\"</p>"},{"location":"code_walkthroughs/titan_walkthrough/#at-a-glance","title":"At-a-Glance","text":"Architecture Params Context Inputs Key capabilities Repo CONCH\u202fv1.5 patch encoder \u2192 TITAN slide Transformer with learned spatial grids + vision-language alignment (CoCa-inspired).^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>] Hugging Face <code>MahmoodLab/TITAN</code> exposes 768-d slide embeddings; fine-tuning head typically 768\u2192num_classes MLP; training loop enables FP16/bfloat16 autocast + cosine sched.^[<code>25:50:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>][<code>137:199:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>] Pretrained on 335,645 WSIs + 182K real reports + 423K synthetic captions; patch grids derived from Level-0 coordinates with <code>patch_size_lv0</code> (512 @20\u00d7, 1024 @40\u00d7).^[<code>14:99:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>] <code>.h5</code> files containing <code>features</code> (N\u00d7768) and <code>coords</code> (N\u00d72) arrays, plus <code>patch_size_level0</code> attribute; sample downloads provided via Hugging Face hub.^[<code>82:99:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>] <code>finetune.py</code> (CustomSequential + cosine LR + GradScaler), <code>eval_linear_probe.py</code> (logistic regression), <code>titan/utils.py</code> (metrics/bootstrap), and TCGA config/prompts for zero-shot classification.^[<code>25:339:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>][<code>2:75:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/eval_linear_probe.py</code>][<code>13:154:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/utils.py</code>][<code>1:120:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/datasets/config_tcga-ot.yaml</code>] github.com/mahmoodlab/TITAN"},{"location":"code_walkthroughs/titan_walkthrough/#environment-hardware-notes","title":"Environment &amp; Hardware Notes","text":"<ul> <li>Install &amp; deps. Clone the repo, <code>conda create -n titan python=3.9</code>, activate, upgrade pip, then <code>pip install -e .</code> (installs PyTorch\u202f2.0.1, timm\u202f1.0.3, h5py, sklearn, transformers\u202f4.46).^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>][<code>5:18:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/setup.py</code>] (lines 25-42)\"</li> <li>Model access. Run <code>huggingface_hub.login()</code> before calling <code>AutoModel.from_pretrained(\"MahmoodLab/TITAN\", trust_remote_code=True)</code>; this downloads slide + text encoders as well as CONCH\u202fv1.5 patch encoder helpers.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>] (lines 43-59)\"</li> <li>Feature extraction options. Either (a) use TRIDENT/CLAM pipelines for CONCH features or (b) load shared <code>.h5</code> demo files and call <code>model.encode_slide_from_patch_features(features, coords, patch_size_lv0)</code> directly; set <code>patch_size_lv0</code> to 1024 (40\u00d7) or 512 (20\u00d7) per slide metadata.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/titan/README.md</code>] (lines 63-98)\"</li> </ul>"},{"location":"code_walkthroughs/titan_walkthrough/#key-components","title":"Key Components","text":""},{"location":"code_walkthroughs/titan_walkthrough/#custom-sequential-wrapper-titanfinetunepy","title":"Custom Sequential Wrapper (<code>titan/finetune.py</code>)","text":"<p>Fine-tuning wraps the frozen TITAN backbone with a lightweight MLP head. <code>CustomSequential</code> simply forwards tensor tuples into <code>encode_slide_from_patch_features</code>, then feeds slide embeddings into the classification head. <code>create_mlp</code> helps instantiate arbitrary hidden stacks.</p> <p>Sequential wrapper with MLP head:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py (lines 25-50)<pre><code>class CustomSequential(nn.Module):\n    def __init__(self, model, mlp):\n        super(CustomSequential, self).__init__()\n        self.model = model\n        self.mlp = mlp\n\n    def forward(self, *args, **kwargs):\n        x = self.model.encode_slide_from_patch_features(*args, **kwargs)\n        x = self.mlp(x)\n        return x\n\n\ndef create_mlp(in_dim=None, hid_dims=[], act=nn.ReLU(), dropout=0.0, out_dim=None, end_with_fc=True):\n    layers = []\n    if len(hid_dims) &gt; 0:\n        for hid_dim in hid_dims:\n            layers.append(nn.Linear(in_dim, hid_dim))\n            layers.append(act)\n            layers.append(nn.Dropout(dropout))\n            in_dim = hid_dim\n    layers.append(nn.Linear(in_dim, out_dim))\n    if not end_with_fc:\n        layers.append(act)\n        layers.append(nn.Dropout(dropout))\n    mlp = nn.Sequential(*layers)\n    return mlp\n</code></pre>"},{"location":"code_walkthroughs/titan_walkthrough/#training-loop-scheduler-titanfinetunepy","title":"Training Loop &amp; Scheduler (<code>titan/finetune.py</code>)","text":"<p><code>train</code> constructs two optimizer parameter groups (bias/LayerNorm vs. rest), applies cosine LR with warmup, leverages <code>torch.cuda.amp.GradScaler</code>, and evaluates on a validation loader with early stopping that tracks the best weights.</p> <p>Mixed precision training with early stopping:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py (lines 137-199)<pre><code>    model.train()\n    fp16_scaler = torch.cuda.amp.GradScaler()\n    step = 0\n    early_stopping = EarlyStopping(patience=2, verbose=True)\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        preds_all = []\n        targets_all = []\n        total_train_loss = 0\n        for features, coords, patch_size_lv0, label in tqdm(train_loader):\n            lr_scheduler(step)\n            features = features.to(device)\n            coords = coords.to(device)\n            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n                logits = model(features, coords, patch_size_lv0.to(device), **kwargs)\n                loss = loss_fn(logits, label.to(device))\n            fp16_scaler.scale(loss).backward()\n            fp16_scaler.step(optimizer)\n            fp16_scaler.update()\n            optimizer.zero_grad()\n\n            preds_all.append(logits.argmax(1).cpu().numpy())\n            targets_all.append(label.numpy())\n            step += 1\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        ...\n            tqdm.write(f\"epoch {epoch}, bacc: {np.round(bacc, 4):.4f}, bacc_val: {np.round(bacc_val, 4):.4f}, loss: {avg_train_loss:.4f}, val_loss: {avg_val_loss:.4f}\")\n            early_stopping(avg_val_loss, model)\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n</code></pre>"},{"location":"code_walkthroughs/titan_walkthrough/#linear-probe-evaluation-titaneval_linear_probepy","title":"Linear Probe Evaluation (<code>titan/eval_linear_probe.py</code>)","text":"<p>For frozen-feature experiments, <code>train_and_evaluate_logistic_regression_with_val</code> sweeps log-spaced <code>C</code>, fits <code>LogisticRegression</code>, and reports metrics (balanced accuracy, Cohen's \u03ba, AUROC) via shared utilities.</p> <p>Logistic regression with hyperparameter sweep:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/eval_linear_probe.py (lines 2-75)<pre><code>def train_and_evaluate_logistic_regression_with_val(train_data, train_labels, val_data, val_labels, test_data, test_labels, log_spaced_values=None, max_iter=500):\n    seed_torch(torch.device('cpu'), 0)\n    \n    metric_dict = {\n        'bacc': 'balanced_accuracy',\n        'kappa': 'cohen_kappa_score',\n        'auroc': 'roc_auc_score',\n    }\n    \n    if log_spaced_values is None:\n        log_spaced_values = np.logspace(np.log10(10e-6), np.log10(10e5), num=45)\n    \n    best_score = -float('inf')\n    best_C = None\n    logistic_reg_final = None\n    for log2_coeff in tqdm(log_spaced_values, desc=\"Finding best C\"):\n        ...\n        logistic_reg = LogisticRegression(\n            C=1/log2_coeff,\n            fit_intercept=True,\n            max_iter=max_iter,\n            random_state=0,\n            solver=\"lbfgs\",\n        )\n        logistic_reg.fit(train_data, train_labels)\n        ...\n    eval_metrics = get_eval_metrics(test_labels, test_preds, test_probs, roc_kwargs=roc_kwargs)\n</code></pre>"},{"location":"code_walkthroughs/titan_walkthrough/#metrics-bootstrap-zero-shot-templates-titanutilspy","title":"Metrics, Bootstrap &amp; Zero-Shot Templates (<code>titan/utils.py</code>)","text":"<p><code>get_eval_metrics</code> reports accuracy/balanced accuracy/kappa/weighted F1 (+ AUROC/log-loss when probs are provided). The module also seeds reproducibility, merges dictionaries, and defines zero-shot text templates for class prompts.</p> <p>Evaluation metrics and zero-shot templates:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/utils.py (lines 13-89)<pre><code># zeroshot prompt templates\nTEMPLATES = [\n    \"CLASSNAME.\",\n    \"an image of CLASSNAME.\",\n    ...\n    \"CLASSNAME is identified.\",\n]\n\ndef get_eval_metrics(\n    targets_all: Union[List[int], np.ndarray],\n    preds_all: Union[List[int], np.ndarray],\n    probs_all: Optional[Union[List[float], np.ndarray]] = None,\n    unique_classes: Optional[List[int]] = None,\n    get_report: bool = True,\n    prefix: str = \"\",\n    roc_kwargs: Dict[str, Any] = {},\n) -&gt; Dict[str, Any]:\n    unique_classes = unique_classes if unique_classes is not None else np.unique(targets_all)\n    bacc = balanced_accuracy_score(targets_all, preds_all) if len(targets_all) &gt; 1 else 0\n    kappa = cohen_kappa_score(targets_all, preds_all, weights=\"quadratic\")\n    nw_kappa = cohen_kappa_score(targets_all, preds_all, weights=\"linear\")\n    acc = accuracy_score(targets_all, preds_all)\n    cls_rep = classification_report(targets_all, preds_all, output_dict=True, zero_division=0, labels=unique_classes)\n\n    eval_metrics = {\n        f\"{prefix}/acc\": acc,\n        f\"{prefix}/bacc\": bacc,\n        f\"{prefix}/kappa\": kappa,\n        f\"{prefix}/nw_kappa\": nw_kappa,\n        f\"{prefix}/weighted_f1\": cls_rep[\"weighted avg\"][\"f1-score\"],\n    }\n</code></pre>"},{"location":"code_walkthroughs/titan_walkthrough/#tcga-ot-configuration-prompts-datasetsconfig_tcga-otyaml","title":"TCGA-OT Configuration &amp; Prompts (<code>datasets/config_tcga-ot.yaml</code>)","text":"<p>The YAML describes label counts, OncoTree codes, class-specific textual prompts (supporting zero-shot CLIP-like scoring), and dataset metadata. Integrate these prompts with TITAN's text encoder or other VLMs for retrieval tasks.</p> <p>Dataset configuration with class prompts:</p> /Users/allison/Projects/neuro-omics-kb/external_repos/titan/datasets/config_tcga-ot.yaml (lines 1-60)<pre><code>aggregation: slide\ncohorts:\n- TCGA\next_cohorts: []\nfolds: 1\nlabel_dict:\n  AASTR: 0\n  ACC: 1\n  AOAST: 2\n  ASTR: 3\n  BLCA: 4\n  CCRCC: 5\n  CESC: 6\n  CHRCC: 7\n  COAD: 8\n  DDLS: 9\n  DSTAD: 10\n  ESCA: 11\n  ESCC: 12\n  GBM: 13\n...\nprompts:\n  AASTR:\n  - anaplastic astrocytoma\n  - astrocytoma, anaplastic\n  - grade III astrocytoma\n  - AASTR\n  ACC:\n  - adrenocortical carcinoma\n  - adrenal cortical carcinoma\n  - adrenal cortex carcinoma\n  - ACC\n</code></pre>"},{"location":"code_walkthroughs/titan_walkthrough/#integration-hooks-pathology-multimodal-kb","title":"Integration Hooks (Pathology \u2194 Multimodal KB)","text":"<ul> <li>Slide embeddings as shared latent vectors. <code>encode_slide_from_patch_features</code> returns 768-d tensors suitable for concatenation with genetic or clinical embeddings before populating KB integration cards; store patch grids alongside metadata for reproducibility.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/finetune.py</code>] (lines 25-99)\"</li> <li>Prompt templates for zero-shot mapping. The <code>TEMPLATES</code> list and TCGA label prompts can seed cross-modal retrieval experiments or provide natural-language anchors for other models (e.g., LLaVA-Med) to align with TITAN outputs.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/utils.py</code>][<code>1:120:/Users/allison/Projects/neuro-omics-kb/external_repos/titan/datasets/config_tcga-ot.yaml</code>] (lines 13-39)\"</li> <li>Metrics + bootstrap interoperability. Use <code>get_eval_metrics</code> / <code>bootstrap</code> outputs to populate KB evaluation summaries, ensuring consistent confidence intervals across modalities when comparing TITAN features against alternative encoders.^[<code>text title=\"/Users/allison/Projects/neuro-omics-kb/external_repos/titan/titan/utils.py</code>] (lines 13-154)\"</li> </ul>"},{"location":"configs/experiments/","title":"\u2699\ufe0f Experiment Configs","text":"<p>Ready-to-use YAML templates for reproducible gene-brain-behavior analysis. Each config specifies embedding strategies, covariates, cross-validation schemes, and validation protocols.</p>"},{"location":"configs/experiments/#available-configs","title":"Available Configs","text":""},{"location":"configs/experiments/#01_cca_gene_smriyaml","title":"01_cca_gene_smri.yaml","text":"<p>Purpose: CCA + permutation baseline for gene-brain association discovery</p> <p>Key Features: - Cross-modal null distributions via within-fold permutation - Canonical correlation coefficients (\u03c1\u2081\u2013\u03c1\u2083) with p-values - Site-/scanner-aware stratified CV - Loadings/feature contributions for interpretation</p> <p>When to use: - Exploratory structure discovery before prediction tasks - Testing whether gene-brain associations exist above chance - Lightweight statistical checks that respect confounds</p> <p>References: - Analysis recipe - Integration strategy</p>"},{"location":"configs/experiments/#02_prediction_baselinesyaml","title":"02_prediction_baselines.yaml","text":"<p>Purpose: Gene vs Brain vs Fusion prediction comparison</p> <p>Key Features: - Per-modality baselines (Gene-only, Brain-only) - Late fusion (concatenated embeddings) - LR + GBDT (LightGBM/CatBoost) for each condition - DeLong tests to compare AUC across models - Stratified K-fold with site/ancestry controls</p> <p>When to use: - Quantifying whether fusion beats single-modality models - Comparing embedding strategies (e.g., Caduceus vs DNABERT-2) - Establishing baselines before escalating to contrastive or early fusion</p> <p>References: - Prediction baselines recipe - Late fusion rationale (Li 2022)</p>"},{"location":"configs/experiments/#03_logo_gene_attributionyaml","title":"03_logo_gene_attribution.yaml","text":"<p>Purpose: Leave-One-Gene-Out (LOGO) attribution protocol</p> <p>Key Features: - \u0394AUC computation: <code>AUC(all genes) - AUC(all genes except gene_i)</code> - Per-gene importance ranking - Stratified CV to avoid overfitting to site/ancestry - Optional permutation test for significance</p> <p>When to use: - Identifying which genes drive prediction performance - Validating biological hypotheses (e.g., \"Is SOD2 critical for MDD prediction?\") - Prioritizing candidates for functional follow-up</p> <p>References: - Yoon BIOKDD 2025 \u2014 LOGO + gene embeddings - Genomics modality features</p>"},{"location":"configs/experiments/#development-stubs","title":"Development Stubs","text":"<p>cha_dev_smri_pca_dimsearch_template.yaml Template for CHA Hospital developmental cohort: PCA dimensionality search for sMRI embeddings.</p> <p>dev_01_brain_only_baseline.yaml Brain-only baseline for developmental trajectory modeling.</p> <p>dev_02_gene_brain_behaviour.yaml Gene-brain-behavior integration for longitudinal developmental data.</p> <p>ukb_smri_pca_dimsearch_template.yaml UK Biobank sMRI PCA dimension search template.</p>"},{"location":"configs/experiments/#usage","title":"Usage","text":""},{"location":"configs/experiments/#1-clone-a-template","title":"1. Clone a template","text":"<pre><code>cp configs/experiments/02_prediction_baselines.yaml my_experiment.yaml\n</code></pre>"},{"location":"configs/experiments/#2-customize-for-your-cohort","title":"2. Customize for your cohort","text":"<p>Edit the YAML to specify: - Embedding strategies (references <code>kb/integration_cards/embedding_strategies.yaml</code>) - Harmonization methods (references <code>kb/integration_cards/harmonization_methods.yaml</code>) - Covariates (age, sex, site, motion, genetic PCs, etc.) - CV scheme (K-fold, stratified groups, leave-site-out) - Metrics (AUC, accuracy, calibration, DeLong tests)</p>"},{"location":"configs/experiments/#3-validate-before-running","title":"3. Validate before running","text":"<pre><code>python scripts/manage_kb.py validate experiments --config my_experiment.yaml\n</code></pre>"},{"location":"configs/experiments/#4-track-provenance","title":"4. Track provenance","text":"<p>Each config embeds: - Embedding strategy IDs (e.g., <code>smri_free_surfer_pca512_v1</code>) - Harmonization method IDs (e.g., <code>combat_harmonization</code>) - Preprocessing pipeline IDs (e.g., <code>fmriprep_standard</code>)</p> <p>This ensures results are traceable back to exact methods and can be reproduced.</p>"},{"location":"configs/experiments/#config-structure","title":"Config Structure","text":"<p>All experiment configs follow this schema:</p> <pre><code>experiment:\n  name: \"descriptive_name\"\n  objective: \"prediction | association | attribution\"\n  \ndata:\n  cohort: \"ukb | cha_dev | hcp\"\n  modalities:\n    - genetics\n    - brain_smri\n    - brain_fmri\n  embedding_strategies:\n    genetics: \"caduceus_rc_avg_v1\"\n    brain_smri: \"smri_free_surfer_pca512_v1\"\n  \ncovariates:\n  - age\n  - sex\n  - site\n  - motion_fd  \u2190 fMRI only\n  - genetic_pcs  \u2190 first 10 PCs\n  \nvalidation:\n  cv_scheme: \"stratified_kfold\"\n  k_folds: 5\n  stratify_by: [\"site\", \"ancestry\"]\n  metrics:\n    - auc\n    - accuracy\n    - calibration\n  \nbaselines:\n  - genetics_only\n  - brain_only\n  - late_fusion\n  \nstatistical_tests:\n  - delong_test  \u2190 AUC comparison\n  - permutation_test  \u2190 for CCA\n</code></pre>"},{"location":"configs/experiments/#best-practices","title":"Best Practices","text":""},{"location":"configs/experiments/#always-residualize-confounds","title":"\u2705 Always residualize confounds","text":"<p>Before fusion, remove age, sex, site, motion (fMRI), and genetic PCs from both modalities.</p>"},{"location":"configs/experiments/#use-site-aware-cv","title":"\u2705 Use site-aware CV","text":"<p>Stratify folds by site/scanner to avoid spurious site-specific signals.</p>"},{"location":"configs/experiments/#report-calibration","title":"\u2705 Report calibration","text":"<p>AUC alone can be misleading; include calibration curves or Brier scores.</p>"},{"location":"configs/experiments/#track-embedding-versions","title":"\u2705 Track embedding versions","text":"<p>Use explicit IDs (e.g., <code>caduceus_rc_avg_v1</code>) so results are reproducible even if embedding methods evolve.</p>"},{"location":"configs/experiments/#log-negative-results","title":"\u2705 Log negative results","text":"<p>If fusion doesn't beat single-modality baselines, document it\u2014that's valuable for the field.</p>"},{"location":"configs/experiments/#integration-with-kb-assets","title":"Integration with KB Assets","text":"<p>Each config references:</p> <ul> <li>Embedding strategies \u2014 How to extract gene/brain features</li> <li>Harmonization methods \u2014 Site/batch correction protocols</li> <li>Preprocessing pipelines \u2014 fMRI processing steps</li> <li>Model cards \u2014 FM architecture details</li> <li>Dataset cards \u2014 Sample sizes, QC thresholds</li> </ul> <p>This ensures every experiment is grounded in validated methods and traceable metadata.</p>"},{"location":"configs/experiments/#next-steps","title":"Next Steps","text":"<ol> <li>Start with CCA + permutation (01_cca_gene_smri.yaml) to check if gene-brain structure exists</li> <li>Run prediction baselines (02_prediction_baselines.yaml) to quantify fusion gains</li> <li>Attribute genes (03_logo_gene_attribution.yaml) to identify key drivers</li> </ol> <p>Need help? Check the Integration Strategy or Analysis Recipes</p> <p>Browse configs on GitHub: configs/experiments/</p>"},{"location":"data/governance_qc/","title":"Governance &amp; QC","text":"<ul> <li>Eligibility criteria (UKB consent, imaging + genetics availability)</li> <li>Exclusion rules (withdrawn, major QC flags, incomplete covariates)</li> <li>Missingness tracking per table + imputation policy</li> <li>Harmonization notes for imaging, genetics, and covariates</li> <li>PII handling and secure storage (encrypted buckets, audit logging)</li> <li>Data security (role-based access, key rotation, least privilege)</li> </ul>"},{"location":"data/schemas/","title":"Data schemas","text":""},{"location":"data/schemas/#genetics_embeddingsparquet","title":"genetics_embeddings.parquet","text":"<ul> <li><code>eid</code></li> <li><code>embedding_dim</code></li> <li><code>source_model</code></li> <li><code>layer</code></li> <li><code>vector</code></li> </ul>"},{"location":"data/schemas/#brain_idpsparquet","title":"brain_idps.parquet","text":"<ul> <li><code>eid</code></li> <li><code>site</code></li> <li><code>modality</code> (sMRI or fMRI)</li> <li>Selected IDPs:</li> <li>sMRI: FreeSurfer 7.x <code>aparc.stats</code> (cortical thickness, ~68 regions) + <code>aseg.stats</code> (subcortical volumes, ~40 structures) + surface area \u2192 ~176 features</li> <li>fMRI: Parcel-wise BOLD statistics (mean, variance), connectivity matrices (optional), or direct FM embeddings (BrainLM, Brain-JEPA)</li> <li>Confounds:</li> <li><code>intracranial_volume</code> (sMRI)</li> <li><code>mean_fd</code> (mean framewise displacement, fMRI)</li> <li><code>tsnr</code> (temporal SNR, fMRI)</li> <li><code>euler_number</code> (FreeSurfer QC metric, sMRI)</li> </ul>"},{"location":"data/schemas/#participantsparquet","title":"participants.parquet","text":"<ul> <li><code>eid</code></li> <li><code>age</code></li> <li><code>sex</code></li> <li><code>income_bin</code></li> <li><code>pcs_1</code>..<code>pcs_10</code></li> <li><code>site</code></li> <li><code>mdd_label</code></li> </ul>"},{"location":"data/schemas/#splitsjson","title":"splits.json","text":"<ul> <li><code>fold_id</code></li> <li><code>train</code> / <code>val</code> / <code>test</code> EID lists</li> <li><code>seed</code></li> <li><code>created_at</code></li> </ul> <p>Validation: reference <code>scripts/validate_schemas.py</code>.</p>"},{"location":"data/subject_keys/","title":"Subject key schemas","text":""},{"location":"data/subject_keys/#overview","title":"Overview","text":"<p>This page documents how subject IDs are handled across cohorts (UKB, GARD, CHA, etc.), and clarifies that raw identifiers and mapping tables never live in this repository. Only schemas and conventions are stored here.</p>"},{"location":"data/subject_keys/#uk-biobank-ukb","title":"UK Biobank (UKB)","text":"<ul> <li>Primary ID: <code>eid</code></li> <li>Used as the join key across:<ul> <li>genetics/WES tables (<code>ukb_wes.yaml</code>)</li> <li>sMRI features (<code>ukb_smri_freesurfer.yaml</code>)</li> <li>fMRI tensors (<code>ukb_fmri_tensor.yaml</code>)</li> <li>manifest (<code>ukb_manifest_stub.yaml</code>)</li> </ul> </li> <li>Derived IDs:</li> <li>Analysis repos may define hashed IDs (e.g., <code>ukb_hash_id = SHA256(eid + salt)</code>), but the mapping from <code>eid</code> to     hash must remain in secure infrastructure, not in this KB.</li> </ul>"},{"location":"data/subject_keys/#cha-developmental-cohort","title":"CHA developmental cohort","text":"<ul> <li>Primary ID: <code>subject_id</code> (local, non-PHI)</li> <li>See <code>cha_dev_longitudinal.yaml</code> \u2192 <code>keys_and_linkage.subject_id_schema</code>.</li> <li>Raw hospital IDs (<code>cha_ehr_id</code>) are stored only in secure mapping tables.</li> <li>Time axis:</li> <li>Represented as <code>months_from_birth</code> (or equivalent) in analysis code; DOB remains PHI and is not stored here.</li> </ul>"},{"location":"data/subject_keys/#cross-cohort-links-conceptual-only","title":"Cross-cohort links (conceptual only)","text":"<ul> <li>GARD / CRIS / other cohorts:</li> <li>If future projects link UKB, GARD, CHA, etc., the actual link tables must remain in a governed environment.</li> <li>This KB can store:<ul> <li>field names (e.g., <code>gard_id</code>, <code>cha_id</code>),</li> <li>high-level descriptions of linkage methods (e.g., trusted third-party linkage),</li> <li>but never the mapping content itself.</li> </ul> </li> </ul>"},{"location":"data/subject_keys/#policy","title":"Policy","text":"<ul> <li>Subject identifiers in this repo must be:</li> <li>non-PHI (no direct MRNs, names, or obvious re-identifiers),</li> <li>schema-level only (field names and usage, not values),</li> <li>and, where hashing is used, only the hash schema is documented, not salts or mapping tables.</li> <li>All actual mapping tables live in:</li> <li>secure institutional storage (e.g., hospital servers, controlled lab disks),</li> <li>under DRB/IRB-approved data governance,</li> <li>and are referenced in this KB only by description.</li> </ul>"},{"location":"data/ukb_data_map/","title":"UKB data map","text":"<p>Field ID placeholders (verify in extract): - Age: 21022 / 21003 - Sex: 31 - Income: 738 - Assessment center / site: 21054 or 54 - Genetics PCs: 22009 (pcs_1..10) - Imaging-derived phenotypes + confounds: FMRIB tables</p> <p>Note: Instances follow UKB suffixes (e.g., <code>-0.0</code> baseline vs <code>-2.0</code> repeat imaging). Capture instance + visit metadata in schemas.</p>"},{"location":"decisions/2025-11-integration-plan/","title":"Integration Baseline Plan","text":"<p>This document defines the phased escalation strategy for gene-brain-behavior integration, informed by foundation model research, oncology multimodal reviews, and ARPA-H Brain-Omics Model (BOM) vision.</p>"},{"location":"decisions/2025-11-integration-plan/#phase-1-late-fusion-baselines-current","title":"\ud83d\udd35 Phase 1: Late Fusion Baselines (Current)","text":"<p>Principle: Prefer late integration first under heterogeneous semantics.</p> <ul> <li>Sources: Ensemble Integration (Li et al. 2022), Oncology multimodal review (2024)</li> <li>Inference: Preserve modality-specific signal; avoid premature joint spaces</li> <li>Implementation:</li> <li>Concatenate compact per-modality features (genetics embeddings + sMRI PCA + fMRI FC)</li> <li>Train LR and GBDT baselines with stratified CV</li> <li>Z-score + residualize per feature vs covariates</li> <li>AUROC/AUPRC with CIs; DeLong/bootstrap for differences</li> </ul> <p>Analysis Recipes:</p> <ul> <li>CCA + permutation: Test gene-brain associations before heavy fusion</li> <li>1,000 permutations on residualized, standardized inputs</li> <li>Record canonical correlations, significance, and stability</li> <li>Prediction baselines: Gene-only vs Brain-only vs Late fusion</li> <li>Establish unimodal baselines before multimodal claims</li> <li>Partial correlations: Logistic regression with covariates</li> <li>Control for age, sex, site, scanner before interpreting effects</li> </ul> <p>Modality Sequencing:</p> <ul> <li>Start with sMRI ROIs (FreeSurfer PCA embeddings)</li> <li>Add fMRI as FC vectors (BrainLM/SwiFT embeddings)</li> <li>Later consider brain FM latents (BrainMT, Brain Harmony)</li> </ul> <p>Genetics Embedding Hygiene:</p> <ul> <li>RC-equivariance: Average forward + reverse-complement embeddings (Caduceus)</li> <li>Deterministic tokenization: Use consistent k-mer/BPE strategies (DNABERT-2, GENERator)</li> <li>Gene attribution: LOGO (leave-one-gene-out) \u0394AUC with Wilcoxon + FDR (Yoon BioKDD'25)</li> </ul>"},{"location":"decisions/2025-11-integration-plan/#phase-2-two-tower-contrastive-near-term","title":"\ud83d\udfe2 Phase 2: Two-Tower Contrastive (Near-term)","text":"<p>Trigger: Late fusion shows consistent gene-brain signal (CCA p&lt;0.001, prediction improvement &gt;5% AUROC)</p> <p>Architecture Pattern:</p> <ul> <li>Frozen encoders: Genetics FM (Caduceus/DNABERT-2) + Brain FM (BrainLM/SwiFT)</li> <li>Small projectors: 256\u2192128\u219264 for each modality</li> <li>Contrastive loss: InfoNCE with temperature tuning</li> <li>Reference models: M3FM (vision-language for medical imaging), oncology two-tower review</li> </ul> <p>What to Monitor:</p> <ul> <li>Embedding alignment quality (cosine similarity distributions)</li> <li>Downstream task performance vs. late fusion</li> <li>Computational cost (FLOPs, GPU memory)</li> </ul> <p>When to Escalate to Phase 3:</p> <ul> <li>Two-tower alignment consistently outperforms late fusion by &gt;10% AUROC</li> <li>Need for cross-modal reasoning (e.g., \"which genes explain this brain pattern?\")</li> <li>Multi-site/TR heterogeneity requires joint harmonization</li> </ul>"},{"location":"decisions/2025-11-integration-plan/#phase-3-unified-multimodal-architectures-long-term","title":"\ud83d\udd34 Phase 3: Unified Multimodal Architectures (Long-term)","text":"<p>Vision: ARPA-H-style Brain-Omics Model (BOM) \u2014 unified transformer processing gene-brain-behavior-language tokens</p> <p>Architecture Options:</p>"},{"location":"decisions/2025-11-integration-plan/#option-a-mixture-of-transformers-mot","title":"Option A: Mixture-of-Transformers (MoT)","text":"<ul> <li>Pattern: Modality-specific FFNs + shared global attention</li> <li>Advantages: </li> <li>55% FLOPs of dense baseline</li> <li>Modality-aware sparsity (genetics_ffn, brain_ffn, behavior_ffn)</li> <li>Stable scaling to 7B+ parameters</li> <li>Reference: MoT paper (2025), BAGEL paper (2025)</li> <li>Use case: Large cohorts (UK Biobank N=500k+), compute-constrained environments</li> </ul>"},{"location":"decisions/2025-11-integration-plan/#option-b-unified-decoder-bagel-style","title":"Option B: Unified Decoder (BAGEL-style)","text":"<ul> <li>Pattern: Single decoder-only transformer with interleaved modality tokens</li> <li>Advantages:</li> <li>Emergent cross-modal reasoning</li> <li>Supports both understanding (gene-brain association) and generation (clinical report)</li> <li>Mixture-of-experts for task-specific specialization</li> <li>Reference: BAGEL paper (2025)</li> <li>Use case: Gene-brain-behavior-language unification with LLM as semantic hub</li> </ul>"},{"location":"decisions/2025-11-integration-plan/#option-c-llm-as-bridge","title":"Option C: LLM-as-Bridge","text":"<ul> <li>Pattern: Project genetics/brain embeddings into LLM token space (Me-LLaMA-style)</li> <li>Advantages:</li> <li>Natural language queries over multimodal neuro-omics data</li> <li>Leverage pretrained medical LLMs for domain knowledge</li> <li>Explain genetic risk in natural language</li> <li>Reference: Me-LLaMA (2024), M3FM (2025)</li> <li>Use case: Clinical decision support, patient-facing genetic counseling</li> </ul>"},{"location":"decisions/2025-11-integration-plan/#decision-table","title":"Decision Table","text":"Signal Strength Computational Budget Primary Goal Recommended Pattern Weak (CCA p&gt;0.01) Any Establish baseline Late Fusion Moderate (CCA p&lt;0.001, \u0394AUROC&lt;5%) Low Gene-brain association Late Fusion + CCA Strong (\u0394AUROC&gt;5%) Medium Multimodal prediction Two-Tower Contrastive Very Strong (\u0394AUROC&gt;10%) High Cross-modal reasoning MoT or Unified Decoder Strong + Language High Clinical integration LLM-as-Bridge"},{"location":"decisions/2025-11-integration-plan/#current-implementation-status","title":"Current Implementation Status","text":"<p>\u2705 Phase 1 Complete: - Analysis recipes documented (CCA, prediction, partial correlations) - Modality features specified (genomics, sMRI, fMRI) - Embedding policies defined (naming, PCA dims) - Integration cards: Ensemble integration, Oncology review</p> <p>\ud83d\udea7 Phase 2 Prep: - Integration card: Multimodal FM patterns (synthesizes BAGEL, MoT, M3FM, Me-LLaMA, TITAN) - Multimodal architectures doc (detailed BAGEL/MoT/M3FM/Me-LLaMA/TITAN patterns) - Design patterns doc (late fusion, two-tower, MoT, BOM escalation logic)</p> <p>\u23f3 Phase 3 Future: - Awaiting Phase 1 validation on UK Biobank + genetics embeddings - Will pilot two-tower if CCA+permutation shows p&lt;0.001 canonical correlations - BOM architecture selection depends on computational resources and cohort size</p>"},{"location":"decisions/2025-11-integration-plan/#key-references","title":"Key References","text":"<ul> <li>Ensemble Integration: Li et al. 2022</li> <li>Oncology Multimodal Review: Waqas et al. 2024</li> <li>Multimodal FM Survey: Gupta et al. 2025</li> <li>BAGEL: arXiv:2505.14683</li> <li>MoT: arXiv:2411.04996</li> <li>M3FM: ai-in-health/M3FM</li> <li>Me-LLaMA: BIDS-Xu-Lab/Me-LLaMA</li> </ul>"},{"location":"decisions/2025-11-integration-plan/#next-steps","title":"Next Steps","text":"<ol> <li>Validate Phase 1 on UK Biobank WES + sMRI/fMRI</li> <li>Monitor trigger conditions for Phase 2 escalation</li> <li>Pilot two-tower if late fusion shows &gt;5% AUROC improvement</li> <li>Document decisions in this log as we progress through phases</li> </ol>"},{"location":"decisions/dev_validation_plan/","title":"Developmental cohort validation plan (CHA)","text":""},{"location":"decisions/dev_validation_plan/#goal","title":"Goal","text":"<p>Define a prospective validation strategy for the CHA developmental cohort so that future models (brain-only, gene\u2013brain\u2013behaviour, BOM-aligned LLM/VLM) are evaluated on truly held-out data.</p>"},{"location":"decisions/dev_validation_plan/#high-level-plan","title":"High-level plan","text":"<ul> <li>Retrospective training window: initial waves (e.g., years 1\u2013N of data collection).</li> <li>Prospective validation window: later-enrolled participants and/or later waves (e.g., year N+1 onward).</li> <li>Constraints:</li> <li>Preserve age and diagnosis distributions between train and validation where possible.</li> <li>Avoid leakage across siblings or repeated visits when defining subject-level splits.</li> </ul>"},{"location":"decisions/dev_validation_plan/#suggested-splits-to-refine-once-data-are-available","title":"Suggested splits (to refine once data are available)","text":"<ol> <li>Temporal split</li> <li>Train:<ul> <li>All subjects enrolled up to a cutoff date (e.g., end of 2027).</li> </ul> </li> <li>Prospective test:<ul> <li>Subjects enrolled after the cutoff date (e.g., 2028+).</li> </ul> </li> <li> <p>Rationale:</p> <ul> <li>Mimics real-world deployment where models trained on early waves are applied to future patients.</li> </ul> </li> <li> <p>Wave-aware cross-validation</p> </li> <li>Within the training period, use wave-aware CV (e.g., group by subject ID) so that:<ul> <li>Multiple visits for the same child never appear in both train and validation folds.</li> </ul> </li> <li> <p>Target variables:</p> <ul> <li>diagnostic status (ASD/ADHD/TD/etc.),</li> <li>cognitive and adaptive trajectories.</li> </ul> </li> <li> <p>Documentation in configs</p> </li> <li>Prospective split metadata should be referenced from:<ul> <li><code>kb/datasets/cha_dev_longitudinal.yaml</code> (manifest fields once defined).</li> <li><code>configs/experiments/dev_*.yaml</code> (explicit <code>train_period</code> / <code>test_period</code> notes).</li> </ul> </li> </ol>"},{"location":"decisions/dev_validation_plan/#what-we-can-do-now-without-data","title":"What we can do now (without data)","text":"<ul> <li>Fix the conceptual split strategy in this document.</li> <li>Ensure all CHA experiment templates:</li> <li>use non-leaky CV (<code>group_by: subject_id</code> where applicable),</li> <li>are written assuming a future prospective test set.</li> <li>Once the cohort is finalized:</li> <li>add concrete sample sizes and calendar cutoffs to this file,</li> <li>and register a canonical manifest strategy in <code>ukb_manifest_stub.yaml</code>-style metadata for CHA.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/","title":"BAGEL: Emerging Properties in Unified Multimodal Pretraining","text":"<p>Authors: Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan Year: 2025 Venue: arXiv preprint</p>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Vision / VLM / Multimodal FM  </li> <li> <p>BAGEL is a unified multimodal foundation model that jointly supports multimodal understanding and generation over text, images, video, and web data.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Text (natural language).  </li> <li>Images (single and multi\u2011image).  </li> <li>Video clips.  </li> <li>Web and interleaved multimodal content (mixed text\u2013image\u2013video sequences).</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>BAGEL is an open\u2011source unified multimodal foundation model that learns from trillions of interleaved text, image, video, and web tokens to support both understanding and generation in a single architecture. The model builds on a decoder\u2011only transformer (initialized from Qwen2.5) and uses a Mixture\u2011of\u2011Transformer\u2011Experts (MoT) design with two experts: one specialized for multimodal understanding and the other for multimodal generation, both operating on a shared token sequence through common self\u2011attention. Visual inputs are handled by separate understanding\u2011 and generation\u2011oriented encoders (SigLIP\u2011style ViT and a FLUX\u2011derived VAE), while visual outputs are produced via diffusion\u2011style rectified flow conditioned on the transformer states. The authors curate large\u2011scale, reasoning\u2011oriented, interleaved multimodal data and introduce IntelligentBench, a new benchmark suite that better reveals emerging capabilities such as free\u2011form visual manipulation, 3D manipulation, and world navigation. BAGEL outperforms prior open\u2011source unified models on standard multimodal leaderboards and delivers image generation quality competitive with state\u2011of\u2011the\u2011art public generators. For a new grad student, BAGEL illustrates how large\u2011scale interleaved pretraining and unified architectures can close part of the gap between academic models and proprietary systems like GPT\u20114o.</p>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Develop an open\u2011source unified multimodal model that can both understand and generate across many modalities (text, images, video) and tasks (VQA, captioning, editing, navigation, etc.), approaching the capabilities of proprietary systems.  </li> <li> <p>Study the emerging properties that arise when scaling interleaved multimodal pretraining, especially complex compositional reasoning and world\u2011modeling abilities.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Data complexity: <ul> <li>High\u2011quality multimodal interleaved data is hard to source, clean, and structure; it must include conversations, instructions, generation tasks, and reasoning traces.  </li> <li>Video data is large but crucial for temporal and physical continuity; integrating it at scale is expensive.  </li> </ul> </li> <li>Architectural challenges: <ul> <li>Unified models must balance strong language reasoning with high\u2011fidelity visual generation, avoiding bottlenecks between understanding and generation.  </li> <li>Na\u00efvely combining autoregressive text and diffusion\u2011style image generation can be compute\u2011intensive and tricky to optimize.  </li> </ul> </li> <li>Evaluation gaps: <ul> <li>Standard benchmarks capture basic understanding and generation, but not more advanced capabilities like free\u2011form manipulation, multiview synthesis, or world navigation.  </li> </ul> </li> <li>Open\u2011source competitiveness: <ul> <li>Academic models historically trail proprietary systems (GPT\u20114o, Gemini 2.0) by a wide margin in unified multimodal settings.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data: </li> <li>Large\u2011scale interleaved multimodal corpus combining text, images, video, and web content.  </li> <li>Data format emphasizes sequences that mix modalities naturally (e.g., conversations with inline images or video frames, web pages with embedded media).  </li> <li> <p>Includes reasoning\u2011oriented content inspired by DeepSeek\u2011R1: explicit <code>&lt;think&gt;</code> segments, chain\u2011of\u2011thought style explanations, and multimodal reasoning traces.</p> </li> <li> <p>Modalities: </p> </li> <li>Text: questions, instructions, descriptions, and dialog.  </li> <li>Images: high\u2011resolution images for understanding and generation.  </li> <li>Video: multi\u2011frame clips providing temporal continuity.  </li> <li> <p>Web / interleaved content: structured pages and documents with embedded media.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Text is tokenized with the Qwen2.5 tokenizer.  </li> <li>Visual understanding uses a SigLIP2\u2011style ViT encoder (SigLIP2\u2011so400m/14) with NaViT support for native aspect ratios; outputs image tokens.  </li> <li>Visual generation uses a pretrained FLUX VAE to convert images to latent tokens (downsampled by 8\u00d7, 16 channels), followed by patch embedding to match the transformer\u2019s hidden size.  </li> <li>2D positional encodings and timestep embeddings are applied to vision tokens for diffusion\u2011style generation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Unified multimodal decoder\u2011only transformer with a Mixture\u2011of\u2011Transformer\u2011Experts (MoT) architecture: one expert for understanding, one for generation.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>BAGEL is a new open\u2011source unified multimodal foundation model that combines MoT, interleaved data, and diffusion\u2011style generation.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Backbone Qwen2.5 decoder\u2011only transformer with RMSNorm, SwiGLU, RoPE, GQA, QK\u2011Norm Experts Understanding expert + generation expert; both operate on the same token sequence via shared self\u2011attention Visual encoders SigLIP2\u2011based ViT (understanding) + FLUX VAE (generation) with patch embeddings Objectives Next\u2011token prediction for text tokens; rectified\u2011flow diffusion objectives for visual tokens Parameter scale 7B active parameters (14B total), with MoT structure over shared attention Data design Carefully curated, reasoning\u2011oriented, interleaved multimodal data, including video and web <ul> <li>Training setup (high level):</li> <li>Initialize from Qwen2.5 and train on trillions of interleaved multimodal tokens.  </li> <li>Use unified decoding: both understanding and generation tasks operate within the same transformer, with expert specialization rather than separate modules.  </li> <li>Visual generation uses Rectified Flow, following best practice in large diffusion transformers.  </li> <li>Training emphasizes scaling length (context) and variety (modalities, tasks) rather than narrow single\u2011task optimization.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Modalities integrated: </li> <li> <p>Text, images, and video, with web content as an additional multimodal source.</p> </li> <li> <p>How integration works: </p> </li> <li>All tokens\u2014text and visual\u2014are placed into a single long token sequence and processed with shared self\u2011attention in each transformer block.  </li> <li>Two experts (understanding and generation) share this sequence but have distinct parameters; both see the same context, enabling tight coupling between reasoning and generation.  </li> <li>Vision tokens come from either the SigLIP\u2011based ViT (for understanding) or the FLUX\u2011based VAE (for generation), but they are mapped into the same hidden space.  </li> <li> <p>The architecture is bottleneck\u2011free: there is no small connector layer compressing context between understanding and generation modules.</p> </li> <li> <p>Why this integration is useful / new capabilities: </p> </li> <li>Enables seamless transfer between understanding and generation: reasoning about input images/videos can directly inform generation within the same context.  </li> <li>Long\u2011context self\u2011attention over interleaved sequences supports complex tasks such as world navigation, future\u2011frame prediction, and multi\u2011step visual manipulation.  </li> <li>The MoT expert split allows specialization without losing the benefits of a unified transformer.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Benchmarks: </li> <li>Standard multimodal understanding benchmarks (e.g., VQA, captioning, visual reasoning tasks).  </li> <li>Image and video generation quality benchmarks (e.g., FID, CLIP\u2011score, human or preference evaluations) compared with open\u2011source generators like SD3 and FLUX.  </li> <li> <p>New IntelligentBench suite introduced by the authors, focusing on complex multimodal reasoning, free\u2011form manipulation, multiview synthesis, and world navigation.</p> </li> <li> <p>Baselines: </p> </li> <li>Open\u2011source unified multimodal models and strong VLMs.  </li> <li> <p>Public image and video generators (e.g., SD3, FLUX) for generative quality comparisons.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>BAGEL outperforms prior open\u2011source unified models on a broad range of multimodal understanding and generation benchmarks.  </li> <li>Image generation quality is competitive with leading public generators, particularly when conditioned on rich prompts and reasoning traces.  </li> <li>The model exhibits emerging abilities: free\u2011form visual manipulation guided by text and image context, multiview and 3D manipulation, and navigation\u2011style tasks involving world modeling.  </li> <li>Scaling interleaved multimodal pretraining reveals a progression: basic understanding/generation \u2192 advanced editing and manipulation \u2192 long\u2011context reasoning and compositional abilities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Provides a unified, open\u2011source alternative to proprietary multimodal systems, with strong performance on both understanding and generation.  </li> <li>Uses a bottleneck\u2011free MoT architecture that allows rich interaction between reasoning and generation within a single transformer.  </li> <li>Emphasizes interleaved multimodal data and reasoning\u2011oriented content, which appear crucial for emergent abilities.  </li> <li>Introduces IntelligentBench, which better surfaces advanced multimodal reasoning and manipulation capabilities.</li> </ul> <p>Limitations:</p> <ul> <li>Training requires huge amounts of data and compute, limiting reproducibility and accessibility for smaller labs.  </li> <li>Data curation details\u2014especially for web and video sources\u2014may affect biases and coverage; not all sources are fully public.  </li> <li>While open\u2011source, deployment may still require substantial GPU resources, particularly for long\u2011context, video\u2011heavy tasks.  </li> <li>The paper focuses more on performance and qualitative capabilities than on detailed safety, robustness, or bias analyses.</li> </ul> <p>Open questions and future directions:</p> <ol> <li>How can we make BAGEL\u2011style unified models more compute\u2011efficient, especially for real\u2011time or edge deployments?  </li> <li>What are the best ways to control and align such powerful multimodal generators, especially for safety\u2011critical or high\u2011stakes tasks?  </li> <li>Can similar MoT\u2011based unified architectures be extended to additional modalities (e.g., audio waveforms, 3D scenes, sensor data) without major redesign?  </li> <li>How should benchmarks evolve to better measure world\u2011modeling and navigation capabilities beyond current tasks?  </li> <li>What data governance and licensing practices are needed to responsibly scale interleaved multimodal corpora?</li> </ol>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>BAGEL is part of a new wave of unified multimodal foundation models that aim to close the performance gap with proprietary systems while remaining open\u2011source.  </li> <li>It extends integrated transformer\u2011diffusion architectures by adding expert specialization and large\u2011scale interleaved data.  </li> <li>Relation to well-known ideas: </li> <li>Combines ideas from decoder\u2011only LLMs, diffusion transformers, and MoT/MoE\u2011style expert architectures within a single framework.  </li> <li>The use of reasoning\u2011oriented pretraining echoes DeepSeek\u2011R1\u2011style chain\u2011of\u2011thought data, but extended to multimodal sequences.  </li> <li>Why this paper is a useful reference: </li> <li>For researchers building VLMs and multimodal FMs, BAGEL provides a blueprint for scaling unified models and for designing interleaved training data.  </li> <li>It also illustrates the importance of designing new benchmarks (IntelligentBench) that reveal capabilities not captured by traditional tasks.</li> </ul>"},{"location":"generated/kb_curated/papers-md/bagel_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Academic unified multimodal models lag far behind proprietary systems in both understanding and generation, especially for complex reasoning and world\u2011modeling tasks.</p> </li> <li> <p>Method / model: </p> </li> <li>BAGEL is a 7B\u2011active-parameter, MoT\u2011based unified multimodal model initialized from Qwen2.5, with separate encoders for visual understanding and generation and rectified\u2011flow diffusion for visual outputs.  </li> <li> <p>It is trained on large\u2011scale, reasoning\u2011oriented, interleaved multimodal data covering text, images, video, and web content.</p> </li> <li> <p>Results: </p> </li> <li>Outperforms state\u2011of\u2011the\u2011art open\u2011source unified models on standard multimodal benchmarks and delivers image quality competitive with leading public generators.  </li> <li> <p>Exhibits emerging abilities such as free\u2011form visual manipulation, multiview synthesis, and navigation\u2011style tasks, especially as pretraining scale increases.</p> </li> <li> <p>Why it matters: </p> </li> <li>Shows that open\u2011source unified multimodal FMs can approach proprietary systems when trained on rich interleaved data with carefully designed architectures.  </li> <li>Highlights the role of reasoning\u2011oriented multimodal data and bottleneck\u2011free architectures in enabling complex, world\u2011modeling capabilities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/","title":"Deep learning-based unlearning of dataset bias for MRI harmonisation and confound removal","text":"<p>Authors: Nicola K. Dinsdale, Mark Jenkinson, Ana I.L. Namburete Year: 2021 Venue: NeuroImage</p>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain MRI harmonization and confound removal for multi-site neuroimaging studies.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Not a foundation model; proposes a training framework for deep networks that removes scanner and other confound information from learned representations while preserving task performance, making it highly relevant as a pre-processing and modeling strategy in FM-era pipelines.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Structural and diffusion MRI (depending on experiment), with datasets spanning multiple scanners and acquisition protocols.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper tackles the problem of dataset bias in multi-site MRI, where scanner and protocol differences introduce non-biological variability that can both confound analyses and hurt model generalization. The authors propose a deep learning\u2013based unlearning framework that treats harmonization as a form of joint domain adaptation: networks are trained to perform a main task (e.g., age regression, tissue segmentation) while actively \u201cunlearning\u201d information about the scanner or other nuisance variables. The approach extends adversarial domain adaptation ideas by alternating between (1) training a domain classifier to predict scanner from latent features and (2) updating the feature extractor to confuse the domain classifier so that its predictions become maximally uncertain. Across experiments with multiple scanners, biased datasets, limited labels, and additional confounds, the method reduces scanner predictability while maintaining or improving main-task performance. The framework is flexible, works with different architectures and tasks, and can be extended to remove non-scanner confounds such as sex or pathology labels when desired.</p>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<p>Scientific / practical problem</p> <ul> <li>Large multi-site neuroimaging datasets are essential for studying brain disorders and population variability, but combining data from different scanners introduces non-biological variance (scanner vendor, field strength, protocol differences).  </li> <li>Traditional harmonization methods (e.g., ComBat on derived measures) partially correct these effects but often cannot operate directly in image or feature space and are less suited to modern deep learning workflows.  </li> <li>Deep models trained naively on pooled multi-site data may inadvertently learn scanner-specific shortcuts, leading to biased predictions and poor generalization to new scanners or acquisition protocols.</li> </ul> <p>Why this is hard</p> <ul> <li>Scanner as a strong confound: Scanner identity can be predicted very accurately from raw or preprocessed images, indicating strong domain shifts between sites.  </li> <li>Trade-off between harmonization and performance: Removing all scanner information might also remove signal that is correlated with both scanner and the biological variable of interest, potentially harming the main task.  </li> <li>Multi-scanner, multi-task reality: Real neuroimaging pipelines involve multiple scanners, heterogeneous labels, and sometimes missing annotations; harmonization must work in all of these regimes.  </li> <li>Confounds beyond scanner: Other variables (e.g., sex, site, cohort, disease status in control groups) can also become entangled with the representations unless explicitly addressed.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>While the paper covers several experimental setups, the overall data settings share common themes.</p> <ul> <li>Datasets and settings (high level)</li> <li>Multi-site structural MRI datasets with images acquired on different scanners, possibly with different protocols and resolutions.  </li> <li>Experiments on tasks such as age regression, tissue segmentation, and other clinically relevant predictions, each involving subjects from multiple scanners.  </li> <li> <p>Scenarios include: balanced vs biased scanner distributions, varying amounts of labeled data, and different numbers of scanners (domains).</p> </li> <li> <p>Modalities</p> </li> <li> <p>Primarily structural MRI, but the framework is general and applicable to any modality where scanner/domain labels are available.</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>Standard neuroimaging pipelines (intensity normalization, registration, possibly parcellation) produce images or feature maps fed into CNN-based architectures.  </li> <li>Input to the deep network is typically image patches or whole images, processed by a feature extractor followed by task-specific heads (e.g., regression head, segmentation decoder).</li> </ul> <p>When precise dataset details are needed (e.g., number of subjects, exact scanner models), they should be taken from the full paper and supplementary material.</p>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>The core contribution is a training framework rather than a single fixed architecture. It augments standard CNNs with a domain classifier and tailored losses to create scanner-invariant yet task-relevant representations.</p> <p>Model type</p> <ul> <li>Generic feedforward CNN (or similar) backbone for the main imaging task, augmented with:  </li> <li>A label predictor head for the primary task (e.g., age, segmentation).  </li> <li>A domain classifier head that predicts scanner identity or other confounds from the learned features.</li> </ul> <p>Key components and innovations</p> Component Description Feature extractor Shared base network that maps MRI images to latent representations used by both the main task and domain classifier. Label predictor Head trained with a standard task loss (e.g., regression or segmentation loss) to ensure good performance on the clinical or scientific objective. Domain classifier Head trained to predict scanner (or other domain labels) from the same features, making explicit how much scanner information is retained. Domain loss Categorical cross-entropy loss measuring how well the domain classifier can predict scanner, used to train the domain head given a fixed feature extractor. Confusion loss Loss that encourages the domain classifier\u2019s softmax outputs to be close to a uniform distribution, i.e., maximally uncertain about scanner, used to update the feature extractor while keeping the domain head fixed. Alternating training scheme Three-stage update in each batch: (1) optimize main task loss, (2) optimize domain classifier to best predict scanner, (3) optimize feature extractor to confuse the domain classifier using the confusion loss. <p>Training setup (conceptual)</p> <ul> <li>The total loss combines main task loss, domain loss, and confusion loss with weighting coefficients (\\alpha, \\beta).  </li> <li>Data subsets used for main-task training and for unlearning can differ (e.g., more unlabeled data for domain unlearning).  </li> <li>The framework naturally extends from two domains to multiple scanners, and can incorporate additional confound labels (e.g., sex) as extra domain dimensions to unlearn.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>The framework operates within a single imaging modality (MRI) but across multiple scanner domains.  </li> <li>It does not perform multimodal integration in the sense of combining fundamentally different biological modalities; instead, it aligns feature distributions across acquisition conditions.  </li> <li>This scanner-invariant representation can be a useful component inside broader multimodal or foundation-model pipelines that combine MRI with other data sources.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>Tasks / benchmarks</p> <ul> <li>Age regression: Predicting subject age from MRI across multiple scanners, assessing the impact of unlearning on regression accuracy and scanner invariance.  </li> <li>Segmentation: Tissue or structure segmentation tasks where labels are available on subsets of scanners.  </li> <li>Biased datasets and limited labels: Experiments that intentionally skew scanner distributions or reduce labeled data to test robustness of the unlearning framework.  </li> <li>Additional confounds: Extension to removing other confounds (e.g., site, sex) in addition to scanner.</li> </ul> <p>Baselines</p> <ul> <li>Standard CNN-based models trained without any domain adaptation or unlearning (scanner information left intact).  </li> <li>Classical harmonization methods (e.g., ComBat) applied to derived measures, where relevant, as conceptual comparators.  </li> <li>Domain adaptation approaches such as Domain Adversarial Neural Networks (DANNs) relying on gradient reversal rather than confusion-based unlearning.</li> </ul> <p>Key findings (trends)</p> <ul> <li>The unlearning framework substantially reduces scanner predictability from latent features (the domain classifier becomes near-chance when confusion loss is applied), indicating successful scanner invariance.  </li> <li>Main-task performance (e.g., age prediction accuracy, segmentation quality) is maintained or improved compared to models trained without unlearning or with simpler domain adaptation schemes.  </li> <li>The method adapts well to biased datasets and low-label regimes, avoiding models that overfit to the dominant scanner.  </li> <li>Extending the framework to additional confounds shows that it can simultaneously reduce multiple unwanted biases while preserving the main-task signal.  </li> <li>Compared to DANN-style gradient reversal, the iterative confusion-based scheme often yields more balanced and stable unlearning across scanners.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths</p> <ul> <li>Provides a clear, modular training recipe that can be plugged into many existing CNN architectures and tasks.  </li> <li>Focuses directly on feature-level scanner invariance, which is closer to where deep models operate than purely statistical harmonization of derived measures.  </li> <li>Demonstrates flexibility across multiple data scenarios (balanced/unbalanced, low labels, multiple scanners) and confound types.  </li> <li>Bridges the literatures on domain adaptation and MRI harmonization, making it easier for practitioners to adopt robust techniques.</li> </ul> <p>Limitations</p> <ul> <li>Requires explicit domain labels (e.g., scanner IDs) for the unlearning step; it does not handle unknown or latent domains.  </li> <li>May not fully remove all scanner-related information, especially when scanner and biological variables are heavily entangled.  </li> <li>If hyperparameters ((\\alpha, \\beta), learning rates) are poorly tuned, the method could over- or under-correct, either leaving residual bias or harming main-task performance.  </li> <li>Experiments, while diverse, focus on certain datasets and tasks; behavior on very large-scale heterogeneous cohorts or other modalities remains to be fully characterized.</li> </ul> <p>Open questions / future directions</p> <ol> <li>How robust is the unlearning framework when scaling to dozens of scanners and complex acquisition protocols?  </li> <li>Can similar ideas be incorporated into large foundation models for MRI, where scanner invariance needs to be preserved across pretraining and fine-tuning?  </li> <li>How should one choose or adapt the weights on domain vs confusion losses ((\\alpha, \\beta)) in a principled, data-driven way?  </li> <li>Can unlearning be extended to continuous confounds (e.g., head motion, SNR, age) rather than discrete scanner categories?  </li> <li>How do we best evaluate whether important biological variation has been inadvertently removed along with nuisance variance?</li> </ol>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Within MRI harmonization: This work reframes harmonization as a representation learning and domain adaptation problem, moving beyond purely statistical corrections to directly controlling what deep networks remember about scanner and confounds.  </li> <li>Relation to foundation models: The unlearning framework can be viewed as a building block for scanner-robust feature extractors, which is crucial when training or adapting brain FMs across many sites and cohorts.  </li> <li>Bias and fairness: By making it possible to explicitly remove known confounds from representations, the method contributes tools for reducing certain kinds of dataset bias, though careful evaluation is needed to avoid discarding meaningful variation.  </li> <li>Practical impact: The approach is easy to implement in modern deep learning frameworks and can be retrofitted into existing models, making it attractive for real-world neuroimaging pipelines.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brain-mri-bias-unlearning_2021/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: Multi-site MRI datasets contain strong scanner- and protocol-induced biases that can distort analyses and hurt generalization.  </li> <li>Idea: Treat harmonization as joint domain adaptation, training networks to perform main tasks while actively unlearning scanner and confound information from their latent representations.  </li> <li>Model / framework: Standard CNN backbone plus a domain classifier and a confusion-based adversarial training loop that alternates between learning to predict scanner and learning to confuse that prediction.  </li> <li>Results: The method reduces scanner predictability to near chance while keeping or improving performance on tasks like age regression and segmentation, even under biased or low-label conditions.  </li> <li>Impact: Offers a flexible, architecture-agnostic recipe for building scanner-invariant MRI models, with clear relevance for large-scale neuroimaging analyses and the training of future foundation models that must work robustly across scanners and cohorts.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/","title":"Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens","text":"<p>Authors: Zijian Dong, Ruilin Li, Joanna Su Xian Chong, Niousha Dehestani, Yinghui Teng, Yi Lin, Zhizhou Li, Yichi Zhang, Yapei Xie, Leon Qi Rong Ooi, B.T. Thomas Yeo, Juan Helen Zhou Year: 2025 Venue: NeurIPS 2025</p>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM + Multimodal / Integration   Brain Harmony (BrainHarmonix) is a brain foundation model that jointly models structural MRI and functional MRI, explicitly targeting multimodal integration of brain morphology and dynamics.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration   The paper introduces a new multimodal brain FM architecture and training scheme, rather than applying an existing FM.</p> </li> <li> <p>Key Modalities: </p> </li> <li>T1-weighted structural MRI (cortical morphology).  </li> <li>Resting-state fMRI time series (functional dynamics, heterogeneous TRs).  </li> <li>Derived: geometric harmonics on cortical surface meshes, functional gradients, ROI-level (Schaefer-400) time series.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>Brain Harmony (BrainHarmonix) is a multimodal brain foundation model that learns a compact sequence of 1D \u201cbrain hub\u201d tokens summarizing both structural MRI and functional MRI of the human brain. The model first trains separate encoders for T1 anatomy and fMRI dynamics, then fuses their latent representations through shared hub tokens that are optimized to reconstruct both modalities, embodying the neuroscience idea that structure constrains function. To handle real-world fMRI data collected with different temporal resolutions (TRs), the authors introduce Temporal Adaptive Patch Embedding (TAPE), which adapts patch sizes and embedding filters so that each token always corresponds to a consistent time duration, and uses multi-TR downsampling as an effective data augmentation. They also incorporate geometric harmonics derived from cortical surfaces as position embeddings for fMRI, aligning functional representations with cortical geometry. Pretrained on large UK Biobank and ABCD datasets, BrainHarmonix is evaluated on six benchmarks (ASD, ADHD, PD, MCI, cognition) plus an Asian clinical cohort and consistently outperforms strong structure-only and function-only baselines, including prior brain FMs like BrainLM, Brain-JEPA, BrainMVP, and BrainMass. For a new grad student, this work is a concrete example of how to design a multimodal, large-scale brain FM that respects neuroscience principles, deals with heterogeneous acquisition, and yields versatile representations usable for many downstream tasks.</p>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<p>Scientific / practical problem: </p> <ul> <li>Learn a unified representation of brain structure and function that supports diverse downstream tasks (disease classification, cognition prediction) across datasets and sites.  </li> <li>Existing brain FMs either model only T1 structure or only fMRI dynamics, missing complementary information and known structure\u2013function relationships.  </li> <li>fMRI FMs usually assume a single TR, limiting their ability to leverage multiple datasets and protocols and ignoring important temporal details when forced to downsample.</li> </ul> <p>Why this is hard: </p> <ul> <li>High-dimensional, heterogeneous inputs: 3D T1 volumes and long fMRI time series are large and noisy, and come from multiple scanners, sites, and TRs.  </li> <li>Multimodal alignment: Structural and functional images differ in resolution and sampling; mapping them into a shared latent space requires careful design to avoid one modality dominating.  </li> <li>Physics and biology constraints: Neuroscience suggests functional waves are constrained by cortical geometry, but most models do not encode this.  </li> <li>Generalization and robustness: The model must work across age ranges (children vs adults), disorders, and acquisition protocols, while being resistant to motion and site artifacts.  </li> <li>Interpretability and clinical relevance: Representations should connect to known networks and clinical markers, not just achieve raw predictive performance.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>Pretraining data:</p> <ul> <li>UK Biobank (UKB): </li> <li>43,112 participants (ages 44\u201383).  </li> <li>46,455 T1-weighted MRI scans.  </li> <li>40,162 resting-state fMRI time series (TR = 0.735 s).  </li> <li> <p>fMRI downsampled to additional TRs of 1.47, 2.205, 2.94 s for multi-TR augmentation.</p> </li> <li> <p>ABCD: </p> </li> <li>11,221 children (8\u201311 years, baseline + 2-year follow-up).  </li> <li>18,139 T1-weighted images.  </li> <li> <p>30,771 resting-state fMRI time series (TR = 0.8 s) downsampled to TRs 1.6 and 2.4 s.</p> </li> <li> <p>Totals: </p> </li> <li>T1 pretraining: 64,594 images.  </li> <li>fMRI pretraining (after augmentation): 252,961 time series.  </li> <li>Multimodal fusion (T1\u2013fMRI pairs): 69,360 paired sessions.</li> </ul> <p>Downstream benchmarks:</p> <ul> <li>Neurodevelopmental disorders: </li> <li>ABIDE-I &amp; ABIDE-II: Autism Spectrum Disorder (ASD) vs controls (multi-site, heterogeneous TRs).  </li> <li> <p>ADHD-200: ADHD vs controls (multi-site, heterogeneous TRs).</p> </li> <li> <p>Neurodegenerative disorders and cognition: </p> </li> <li>PPMI: 4-way classification (controls, SWEDD, prodromal, PD).  </li> <li>ADNI: CN vs MCI classification.  </li> <li> <p>HCP-A: Regression of executive function (Flanker task).</p> </li> <li> <p>Additional cohort: </p> </li> <li>MACC (Asian clinical cohort): amyloid-positive vs negative classification.</li> </ul> <p>Preprocessing / representation:</p> <ul> <li>T1: skull-stripping (FreeSurfer), reorientation (FSL), registration to MNI152 (FLIRT), cropping, intensity normalization.  </li> <li>fMRI: motion correction, slice-timing correction, nuisance regression (global, white matter, CSF, motion), censoring high-motion frames, band-pass filtering, mapping to standard space or surface, then parcellation to 400 ROIs (Schaefer-400).  </li> <li>Final representation: T1 patch tokens; ROI-wise fMRI time series; geometric harmonics computed on a population-average cortical mesh.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>Model type: </p> <ul> <li>Transformer-based multimodal foundation model with:  </li> <li>A 3D Masked Autoencoder (MAE) for T1 (BrainHarmonix-S).  </li> <li>A JEPA-style fMRI dynamics encoder (BrainHarmonix-F) with geometric harmonics and TAPE.  </li> <li>A multimodal \u201cHarmonizer\u201d transformer using learnable 1D hub tokens as a shared bottleneck.</li> </ul> <p>New FM vs existing: </p> <ul> <li>BrainHarmonix is a new FM that:  </li> <li>Encodes structure\u2013function coupling via geometric harmonics.  </li> <li>Handles arbitrary TRs with TAPE.  </li> <li>Learns unified 1D hub-token representations that can reconstruct both modalities.</li> </ul> <p>Key components and innovations:</p> <ul> <li>BrainHarmonix-S (structure encoder): </li> <li>Backbone: ViT-B MAE on T1 volumes (patch size 16, ~1200 tokens).  </li> <li> <p>Objective: reconstruct masked patches, capturing rich cortical morphology.</p> </li> <li> <p>BrainHarmonix-F (functional encoder): </p> </li> <li>Backbone: ViT-B with JEPA-style masked prediction over fMRI tokens.  </li> <li>Geometric harmonics positional encoding: eigenmodes of Laplace\u2013Beltrami operator on a population-average cortical surface, downsampled to ROIs and linearly projected to positional embeddings.  </li> <li> <p>TAPE: define a canonical temporal window \u03c4, adjust patch size (k \\approx \\tau / \\text{TR}), resize embedding filters via pseudoinverse operations, pad shorter sequences with attention masks\u2014allowing consistent token semantics across TRs and enabling multi-TR augmentation.</p> </li> <li> <p>Multimodal fusion (Harmonizer + hub tokens): </p> </li> <li>Introduce NH learnable hub tokens, shared across all T1\u2013fMRI pairs.  </li> <li>Concatenate hub tokens with modality-specific latents and feed through a transformer; self-attention lets hub tokens aggregate cross-modal information.  </li> <li>Lightweight decoders map hub tokens back to structural and functional latent spaces; training minimizes reconstruction error for both modalities, yielding a compact, shared latent space.  </li> <li>For downstream tasks, average-pool hub tokens and pass through a simple projection head.</li> </ul> <p>Training setup:</p> <ul> <li>Encoders and Harmonizer use ViT-B backbones with FlashAttention; optimization via AdamW with cosine learning-rate and weight-decay schedules.  </li> <li>Pretraining runs on 8\u00d7H100 GPUs (80 GB), with fusion training (Harmonizer) taking ~10 hours for NH=128 tokens.  </li> <li>Downstream: encoders are frozen; only Harmonizer and a linear head are tuned or even just the linear head for linear probing.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>Multimodal integration in BrainHarmonix:</p> <ul> <li>Modalities integrated: T1 morphology, fMRI dynamics (multi-TR), and geometry-derived harmonic modes.  </li> <li>Integration strategy: </li> <li>Late fusion of latent representations: T1 and fMRI encoders are trained separately and then frozen, preserving modality-specific features.  </li> <li>Structure-informed functional encoding: geometric harmonics anchor fMRI ROI embeddings to cortical geometry, embedding structure\u2013function constraints directly into functional latents.  </li> <li>Hub-token bottleneck: learnable 1D tokens sit between modalities and are optimized to reconstruct both structural and functional latents, forming a compact joint embedding space.</li> </ul> <p>Why this integration is useful:</p> <ul> <li>Encodes the neuroscience principle that function follows structure, helping cross-subject and cross-dataset alignment.  </li> <li>Exploits complementary strengths: stable anatomical variation (atrophy, cortical thickness) from T1 plus dynamic network organization from fMRI.  </li> <li>TAPE + multi-TR augmentation make it practical to fuse data from many scanners and protocols, which is essential for large-scale multimodal foundation models.</li> </ul> <p>Relation to the integration baseline plan:</p> <ul> <li>The plan emphasizes late integration and preserving modality-specific signal; BrainHarmonix follows this by training separate unimodal encoders and fusing only at the level of latents via hub tokens, with reconstruction losses that discourage modality collapse.  </li> <li>The Harmonizer\u2019s learned joint embedding is analogous to a nonlinear CCA-style latent space, but constrained by physics-informed geometry, echoing the plan\u2019s suggestion to use CCA/structured methods before heavy fusion.  </li> <li>Evaluation practice (multiple datasets, consistent splits, reporting variance and significance, ablations on fusion components) aligns with the plan\u2019s focus on robustness and disciplined comparison.  </li> <li>Attention analyses reveal modality-specific and cross-modal hub tokens, consistent with the plan\u2019s goal of preserving heterogeneous modality-specific features while enabling cross-modal interactions.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>Tasks / benchmarks:</p> <ul> <li>ASD vs controls (ABIDE-I, ABIDE-II), ADHD vs controls (ADHD-200). +- 4-way PD-related classification (PPMI), CN vs MCI (ADNI), executive function regression (HCP-A).  </li> <li>Amyloid-positive vs negative classification in an Asian clinical cohort (MACC).</li> </ul> <p>Baselines:</p> <ul> <li>Structure-only: BrainMVP variants, BrainHarmonix-S.  </li> <li>Function-only: BrainNetCNN, BrainGNN, BrainNetTF, BrainMass, BrainLM, Brain-JEPA.  </li> <li>Ablations: BrainHarmonix-F, variants without geometric pre-alignment or without multi-TR augmentation, and models without multimodal fusion.</li> </ul> <p>Key findings (trends):</p> <ul> <li>Multimodal BrainHarmonix consistently matches or outperforms all baselines across neurodevelopmental and neurodegenerative tasks, often with statistically significant gains in accuracy and F1.  </li> <li>BrainHarmonix-F (functional-only) outperforms prior fMRI FMs (BrainLM, Brain-JEPA, BrainMass) and task-specific models, showing the value of multi-TR dynamics modeling and geometry-informed position embeddings.  </li> <li>BrainHarmonix-S is competitive with or better than BrainMVP despite not using multi-parametric MRI, thanks to larger T1 pretraining.  </li> <li>Multi-TR augmentation and geometric pre-alignment both yield consistent performance improvements; ablations confirm that removing either degrades results.  </li> <li>Scaling the number of hub tokens improves performance up to ~128\u2013256 tokens, after which gains plateau; even linear probing of BrainHarmonix yields state-of-the-art or competitive results.  </li> <li>On the MACC cohort, BrainHarmonix achieves the highest accuracy and F1, demonstrating promising cross-population generalization.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>First brain FM to jointly model structure and function with a principled multimodal architecture and physics-informed inductive biases.  </li> <li>Addresses a major practical barrier\u2014heterogeneous TRs\u2014via TAPE and multi-TR augmentation, enabling large-scale functional pretraining.  </li> <li>Learns compact, versatile hub-token representations that support strong linear probing and efficient fine-tuning across many tasks.  </li> <li>Thorough empirical validation with multiple public datasets, an independent clinical cohort, ablations, scaling studies, and interpretability analyses linking tokens to known networks and ASD-related patterns.</li> </ul> <p>Limitations:</p> <ul> <li>Pretraining demography is still limited (children and middle/older adults) and mostly Western datasets; infancy, early adulthood, and broader global populations are underrepresented.  </li> <li>Training requires substantial compute (8\u00d7H100 GPUs), making replication and extension difficult for small labs.  </li> <li>Only T1 and resting-state fMRI are modeled; other modalities (diffusion MRI, task fMRI, EEG/MEG, genetics, clinical variables) are not yet integrated.  </li> <li>Multimodal fusion can be sensitive to low-quality structural data (e.g., motion artifacts in ADHD-200 T1), which can degrade performance.  </li> <li>Despite some interpretability analyses, the mapping from token-level patterns to clinically actionable insights remains preliminary.</li> </ul> <p>Open questions / future directions:</p> <ol> <li>How to extend the hub-token framework to incorporate additional modalities (diffusion, task fMRI, genomic features) while preserving modality-specific signals in line with the integration baseline plan?  </li> <li>Can joint fine-tuning of unimodal encoders and Harmonizer (possibly via parameter-efficient adapters or prompts) further improve performance without prohibitive compute?  </li> <li>How does BrainHarmonix behave in low-data or few-shot clinical settings, and can light-weight adaptation strategies close the gap?  </li> <li>Can the learned structure\u2013function tokens support more mechanistic analyses, e.g., probing causal pathways, simulating interventions, or building \u201cdigital twin\u201d models of individual brains?  </li> <li>What safeguards and evaluation protocols are needed before using such multimodal FMs in clinical decision support, especially concerning bias, robustness, and privacy?</li> </ol>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Within brain FMs: BrainHarmonix extends the line of fMRI FMs (BrainLM, Brain-JEPA) and structural FMs (BrainMVP) by explicitly fusing structure and function, showing that multimodal foundation models can better capture brain organization and disease-relevant signals than unimodal models.  </li> <li>Analogy to general FMs: Conceptually, BrainHarmonix is like a CLIP-style or multimodal FM for the brain: it learns a shared token-based representation over multiple modalities (anatomy + dynamics), but with strong physics-informed priors (geometric harmonics) and careful handling of temporal sampling.  </li> <li>Relevance to multimodal integration generally: The architecture and training strategy\u2014separate encoders, late fusion via a compact bottleneck, physics-informed embeddings, robust multi-dataset evaluation\u2014provide a concrete template for integrating other modalities (e.g., genomics, cell imaging, clinical data) in line with the integration baseline plan.  </li> <li>For a grad student: This paper is a valuable reference on how to (1) scale brain FMs, (2) encode domain knowledge (geometry, TR constraints) into model design, and (3) evaluate multimodal models across heterogeneous datasets and populations.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainharmony_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: Existing brain foundation models typically handle either structural MRI or functional MRI, and struggle with heterogeneous TRs and multimodal integration, limiting their ability to capture holistic brain organization.  </li> <li>Model: BrainHarmonix introduces a multimodal brain FM with separate T1 and fMRI encoders, geometric harmonics-based positional encoding, TAPE for arbitrary TRs, and a Harmonizer transformer with learnable 1D hub tokens that form a compact joint representation.  </li> <li>Data: The model is pretrained on large UKB and ABCD datasets (tens of thousands of T1 and fMRI scans) and evaluated on six benchmark datasets plus an independent Asian clinical cohort.  </li> <li>Results: BrainHarmonix consistently outperforms structure-only and function-only baselines (including BrainLM, Brain-JEPA, BrainMVP, BrainMass) across ASD, ADHD, PD, MCI, and cognition tasks, with ablations confirming the benefits of geometric pre-alignment, multi-TR augmentation, and multimodal fusion.  </li> <li>Integration: The fusion strategy aligns with late integration principles in the integration baseline plan\u2014preserving modality-specific encoders, using a compact bottleneck for cross-modal alignment, and enforcing reconstruction of both modalities from shared tokens.  </li> <li>Significance: BrainHarmonix provides a strong template for future multimodal brain FMs and suggests how to design scalable, physics-informed integration architectures that are robust across datasets and acquisition protocols.  </li> <li>For future work: Extending the model to more modalities, broader populations, and low-resource settings, and deepening interpretability and clinical pathways, are key directions for the next generation of multimodal brain foundation models.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/","title":"Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking","text":"<p>Authors: Zijian Dong, Ruilin Li, Yilei Wu, Thuan Tinh Nguyen, Joanna Su Xian Chong, Fang Ji, Nathanael Ren Jie Tong, Christopher Li Hsian Chen, Juan Helen Zhou Year: 2024 (approx., based on arXiv:2409.19407 and NeurIPS 2024) Venue: NeurIPS 2024 (Brain-JEPA: Brain Dynamics Foundation Model)</p>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM. The paper develops a large self-supervised foundation model for fMRI time-series (resting-state fMRI across multiple cohorts) to support many downstream neuro-related tasks (demographics, traits, and disease prediction). It focuses entirely on brain activity recordings and their functional organization rather than genomics or generic multimodal integration.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. Brain-JEPA is a new brain dynamics foundation model that adapts the Joint-Embedding Predictive Architecture (JEPA) framework to fMRI, introduces a novel functional positional encoding (Brain Gradient Positioning), and a customized pretraining mask (Spatiotemporal Masking). The paper primarily proposes this new architecture and training scheme, then applies it to a broad suite of downstream tasks.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Resting-state fMRI BOLD time series (450 ROI-level time series, including cortical and subcortical regions).  </li> <li>Associated clinical / behavioral labels for downstream tasks (age, sex, cognitive scores, disease status), but the core model itself is pretrained purely on fMRI time series.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces Brain-JEPA, a large-scale brain dynamics foundation model that learns from resting-state fMRI time series using a Joint-Embedding Predictive Architecture (JEPA) instead of reconstructing raw signals. The authors argue that fMRI BOLD signals are noisy and sparsely informative, making direct reconstruction (as in the earlier BrainLM model) suboptimal, especially for off-the-shelf evaluations like linear probing. Brain-JEPA instead predicts latent representations of masked fMRI patches using a Vision Transformer (ViT) encoder, a JEPA-style predictor, and an exponential moving average (EMA) target encoder. Two core innovations tailor JEPA to brain data: Brain Gradient Positioning, which uses functional connectivity gradients to define a functional coordinate system for regions of interest (ROIs), and Spatiotemporal Masking, which structures the self-supervised prediction task across ROIs and timesteps to provide a strong inductive bias. Pretrained on large-scale UK Biobank fMRI, Brain-JEPA achieves state-of-the-art performance across a wide range of downstream tasks (demographics, cognitive traits, and disease classification) and generalizes well across ethnic groups and external datasets. For a new grad student, this paper is important as a blueprint for how to construct and pretrain foundation models for brain time series, and as a demonstration that latent prediction architectures plus principled positional encodings can yield robust, generalizable brain representations.</p>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Build a foundation model for brain dynamics that learns general-purpose representations of resting-state fMRI time series, which can be adapted to many downstream tasks: demographic prediction (age, sex), cognitive and personality traits (e.g., Neuroticism, Flanker score), and disease diagnosis/prognosis (e.g., Alzheimer\u2019s, mild cognitive impairment, amyloid positivity).  </li> <li>Move beyond task-specific fMRI models (e.g., BrainNetCNN, BrainGNN, Brain Network Transformer, SwiFT) that must be trained separately for each dataset/task and often fail to exploit large unlabeled fMRI repositories.  </li> <li> <p>Improve on BrainLM, the first fMRI foundation model that uses a masked autoencoder (MAE) to reconstruct masked BOLD time series patches, which performs well under heavy fine-tuning but has weaker off-the-shelf representations and limited evaluation across ethnicities.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Noisy, low-SNR data: fMRI BOLD signals are an indirect proxy for neural activity, influenced by physiological noise and scanner artifacts, with relatively low signal-to-noise ratio. Directly reconstructing all masked voxels or ROI time series can encourage modeling noise rather than meaningful structure.  </li> <li>Sparse, complex spatiotemporal structure: Unlike images (with local edges and dense spatial information), fMRI signals are sparse and distributed across ROIs and time, without clear local edges; reconstructive MAE losses can struggle to learn subtle patterns in such data.  </li> <li>Lack of natural spatial ordering: Transformers rely on positional embeddings, but there is no simple 1D order for ROIs across the 3D brain. Anatomical coordinates do not necessarily correspond to functional organization; spatially adjacent ROIs can have very different activity profiles.  </li> <li>Heterogeneous time-series patches: fMRI patches differ both in space (ROI location in functional networks) and time (brain states, tasks, intrinsic fluctuations). Masking and prediction strategies must reflect this heterogeneity to avoid trivial shortcuts and encourage robust learning.  </li> <li>Generalization across cohorts and ethnicities: Foundation models for clinical use must generalize beyond the pretraining cohort (e.g., UK Biobank, mostly Caucasian) to external datasets and different ethnic groups, which is challenging given domain shift in acquisition protocols and demographics.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used: </li> <li>UK Biobank (UKB): <ul> <li>Large-scale public dataset (resting-state fMRI) with 40,162 participants aged 44\u201383.  </li> <li>Used for self-supervised pretraining and internal downstream tasks (age and sex prediction); 80% for pretraining, 20% held-out for evaluation.  </li> </ul> </li> <li>HCP-Aging (Human Connectome Project \u2013 Aging): <ul> <li>656 healthy elderly participants with resting-state fMRI.  </li> <li>Used for external evaluation of demographics (age, sex) and behavioral traits (Neuroticism, Flanker score).  </li> </ul> </li> <li>ADNI (Alzheimer\u2019s Disease Neuroimaging Initiative): <ul> <li>Resting-state fMRI for 189 participants (NC vs. MCI classification) and 100 cognitively normal participants (amyloid positive vs. negative classification).  </li> </ul> </li> <li>MACC (Memory, Ageing and Cognition Centre; Asian cohort): <ul> <li>Resting-state fMRI for 539 participants, used for NC vs. MCI classification in Asian participants to test cross-ethnic generalization.  </li> </ul> </li> <li> <p>Additional external datasets (Appendix): </p> <ul> <li>OASIS-3: AD conversion prediction in MCI participants.  </li> <li>CamCAN: Depression diagnosis.</li> </ul> </li> <li> <p>Modalities: </p> </li> <li>Primary modality: Resting-state fMRI BOLD time series.  </li> <li> <p>Labels or metadata include age, sex, cognitive and personality scores (e.g., Neuroticism, Flanker), and disease/prognosis labels (NC vs. MCI, amyloid status, AD conversion, depression).</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Parcellation: All fMRI data is parcellated into n = 450 ROIs, using:  <ul> <li>Schaefer-400 atlas for cortical regions.  </li> <li>Tian-Scale III atlas for subcortical regions.  </li> </ul> </li> <li>Scaling: Robust scaling per ROI (subtract median, divide by interquartile range) across participants.  </li> <li>Temporal resolution alignment: <ul> <li>UKB and HCP-Aging: multiband acquisition (TR \u2248 0.735s).  </li> <li>ADNI and MACC: single-band acquisition (TR \u2248 2s).  </li> <li>Multi-band data are downsampled (stride 3) to align all datasets to TR \u2248 2s.  </li> </ul> </li> <li>Input size: Default model input is 450 \u00d7 160 (ROIs \u00d7 timesteps).  </li> <li>Patchify and shuffle: For JEPA pretraining, time series are patchified into temporal patches per ROI (e.g., 10 patches per ROI), with ROI shuffling and spatiotemporal partitioning into observation and target regions.</li> </ul> <p>If any fine-grained preprocessing details are missing in the main text, they are specified in Appendix B and table summaries, but the above captures the key design choices.</p>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li>Vision Transformer (ViT)-based JEPA model for fMRI time series, using a latent predictive architecture rather than reconstruction.  </li> <li>Core components:  <ul> <li>Observation encoder: ViT (ViT-Small, ViT-Base, or ViT-Large).  </li> <li>Target encoder: ViT with EMA-updated parameters (JEPA-style).  </li> <li>Predictor network: a narrower ViT that maps the observation representation to predicted target representations.  </li> </ul> </li> <li> <p>Architecturally, this is analogous to I-JEPA for images, but adapted to fMRI time series with specific choices for positional encoding and masking.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>New foundation model. Brain-JEPA is not simply an application of an existing FM; it adapts the JEPA framework to brain dynamics and introduces Brain Gradient Positioning and Spatiotemporal Masking as domain-specific innovations.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Component Description Observation encoder (f_\\theta) ViT backbone (ViT-S, ViT-B, or ViT-L), processes an observation block (subset of ROIs \u00d7 timesteps). Target encoder (f_\\theta) (EMA) Same architecture as observation encoder, parameters updated via Exponential Moving Average of (f_\\theta). Predictor (g_\\phi) Narrower ViT that takes observation representations and positional embeddings to predict target block embeddings. Brain Gradient Positioning Functional connectivity gradient-based spatial positional embedding that defines a functional coordinate system for ROIs. Temporal Positioning Standard sine\u2013cosine positional encoding applied along the time dimension for each ROI. Spatiotemporal Masking Partitioning of the input into Cross-ROI (\u03b1), Cross-Time (\u03b2), and Double-Cross (\u03b3) regions, plus overlapped sampling to control masking ratios. <ul> <li> <p>Brain Gradient Positioning (spatial positional encoding): </p> <ul> <li>Compute a non-negative affinity matrix (A(i,j)) from functional connectivity features (c_i, c_j) across ROIs using a cosine-based similarity.  </li> <li>Use diffusion maps to compute gradients (eigenvectors) that capture macroscale functional organization; each gradient is a dimension of a latent manifold.  </li> <li>Stack eigenvectors into a gradient matrix (G \\in \\mathbb{R}^{n \\times m}) (n ROIs, m gradient components; default m = 30).  </li> <li>Map (G) through a trainable linear layer to (\\hat{G} \\in \\mathbb{R}^{n \\times d/2}), where (d) is the ViT embedding dimension.  </li> <li>Combine (\\hat{G}) with temporal positional encoding (T \\in \\mathbb{R}^{n \\times d/2}) to get final positional embeddings (P = [T, \\hat{G}] \\in \\mathbb{R}^{n \\times d}).  </li> <li>This yields a functional coordinate system where distances reflect connectivity similarity, enabling the model to respect functional rather than purely anatomical proximity.</li> </ul> </li> <li> <p>Spatiotemporal Masking and targets: </p> <ul> <li>The fMRI input (after patchifying and ROI shuffling) is divided into an observation block and three non-overlapping regions for targets:  </li> <li>Cross-ROI (\u03b1): Same time range as observation but different ROIs \u2192 forces spatial generalization across ROIs.  </li> <li>Cross-Time (\u03b2): Same ROIs but different timestep patches \u2192 forces temporal forecasting/generalization.  </li> <li>Double-Cross (\u03b3): Different ROIs and timesteps \u2192 most challenging region, requiring generalization across space and time.  </li> <li>Sample K target blocks from each region (K = 1 in experiments), giving multiple prediction sub-tasks per sample.  </li> <li>Use overlapped sampling to flexibly adjust observation-to-input ratio and encourage diverse masking patterns.  </li> <li>Remove overlaps between observation and \u03b1/\u03b2 targets to ensure non-trivial prediction.  </li> <li>Loss is mean squared error in latent space between predicted target embeddings (\\hat{s}^r_y) and target encoder outputs (s^r_y), averaged over regions and blocks.</li> </ul> </li> <li> <p>Training setup (as far as available):</p> </li> <li> <p>Pretraining objective: </p> <ul> <li>Joint-Embedding Predictive Architecture (JEPA)-style latent prediction: </li> <li>Given observation encoding (s_x) and positional embeddings (P), predictor (g_\\phi) outputs (\\hat{s}^r_y = g_\\phi(s_x | P)) for each target region r; minimize (| \\hat{s}^r_y - s^r_y |_2^2) averaged over r and targets.  </li> <li>Crucially, the model predicts representations, not raw fMRI signals, which helps ignore noise and focus on more abstract features.</li> </ul> </li> <li> <p>Model scale: </p> <ul> <li>ViT-S (~22M parameters), ViT-B (~86M), ViT-L (~307M) for the observation encoder.  </li> <li>Predictors have matching architectures but shallower depth and smaller embedding dimensions (e.g., 6 layers with dim 192/384, etc.).  </li> <li>No [CLS] token in pretraining; for downstream evaluation, they use the target encoder and average pooling over patches to obtain global fMRI embeddings.</li> </ul> </li> <li> <p>Optimization and hyperparameters (pretraining): </p> <ul> <li>Optimizer: AdamW with carefully tuned weight decay and cosine scheduling.  </li> <li>Learning rate: warmup cosine schedule, peak LR around 1e-3 with warmup and final LR 1e-6; weight decay schedule from 0.04 to 0.4.  </li> <li>EMA momentum increases from 0.996 to 1.0.  </li> <li>Batch size: effective 4 GPUs \u00d7 8 gradient accumulation steps \u00d7 16 batch size.  </li> <li>Training epochs: 300 epochs for main ViT-B model.  </li> <li>Patch size p = 16 time points; gradient vector dimension m = 30.</li> </ul> </li> <li> <p>Downstream evaluation: </p> <ul> <li>Fine-tuning and linear probing use AdamW or LARS with cosine decay, 50 training epochs, and dataset-specific settings.  </li> <li>For linear probing, they add a BatchNorm layer before the linear head, following MAE practice, to stabilize learning.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Is the paper multimodal? </li> <li>Not in the sense of integrating multiple different data modalities. Brain-JEPA operates on a single modality\u2014resting-state fMRI time series. The \u201cmultimodality\u201d here is spatiotemporal within the fMRI itself (ROIs \u00d7 time), not multiple distinct biological modalities (like EEG + MRI or fMRI + genetics).  </li> <li> <p>The paper does, however, span multiple datasets and cohorts (UKB, HCP-Aging, ADNI, MACC, OASIS-3, CamCAN), which could be viewed as multi-site integration, but there is no explicit multimodal fusion mechanism.</p> </li> <li> <p>Relation to integration baseline plan: </p> </li> <li>The Integration Baseline Plan emphasizes late fusion of modality-specific features, careful covariate adjustment, CCA-based cross-modality exploration, and robustness-focused evaluation (e.g., standardized preprocessing, consistent CV folds, significance testing).  </li> <li>Brain-JEPA\u2019s design is conceptually aligned with the plan\u2019s priority to preserve modality-specific signals (here, fMRI) by learning a strong fMRI foundation model before any cross-modal integration. The model produces high-quality, compressed fMRI embeddings that could be plugged into late fusion schemes (e.g., concatenation with genomic or clinical features followed by logistic regression or GBDT).  </li> <li>The evaluation practices\u2014multiple independent runs, reporting means and standard deviations, and significance markers ()\u2014mirror the robustness and evaluation discipline* recommended in the plan (e.g., repeated runs, clear metrics, statistical testing).  </li> <li>Although the paper does not explicitly use CCA, partial correlations, or complex multimodal fusion, it provides the fMRI tower that future work could combine with genomic, behavioral, or structural MRI towers using late fusion or more advanced contrastive frameworks, directly aligning with the \u201cmodality sequencing\u201d and \u201cescalation\u201d steps of the plan.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Internal tasks on UKB (20% held-out): <ul> <li>Age prediction (regression; MSE and Pearson correlation).  </li> <li>Sex prediction (binary classification; accuracy and F1).  </li> </ul> </li> <li>External tasks on HCP-Aging: <ul> <li>Age prediction.  </li> <li>Sex prediction.  </li> <li>Neuroticism score prediction (personality trait).  </li> <li>Flanker task performance prediction (attention / inhibitory control).  </li> </ul> </li> <li>External tasks on ADNI and MACC: <ul> <li>NC vs. MCI classification (ADNI; Caucasian cohort).  </li> <li>Amyloid-positive vs. negative classification (ADNI).  </li> <li>NC vs. MCI classification in Asian participants (MACC).  </li> </ul> </li> <li> <p>Additional tasks (Appendix C): </p> <ul> <li>AD conversion prediction (OASIS-3; MCI participants).  </li> <li>Depression diagnosis (CamCAN).</li> </ul> </li> <li> <p>Baselines: </p> </li> <li>Task-specific fMRI models: <ul> <li>BrainNetCNN (CNN-based on connectivity matrices).  </li> <li>BrainGNN (graph neural network on ROI-level graphs).  </li> <li>Brain Network Transformer (BNT; transformer-based on connectivity).  </li> <li>SwiFT (Swin Transformer-based model for raw fMRI).  </li> </ul> </li> <li>Non-deep baselines: <ul> <li>SVM/SVR trained on connectivity features.  </li> </ul> </li> <li>fMRI foundation models and self-supervised baselines: <ul> <li>BrainLM (MAE-based fMRI foundation model, ViT-B backbone).  </li> <li>BrainMass (graph-based large-scale self-supervised model for brain networks).  </li> <li>CSM (text-like representation model for brain data).  </li> </ul> </li> <li> <p>Variants and ablations of Brain-JEPA:  </p> <ul> <li>Using anatomical locations or sine\u2013cosine spatial embeddings instead of Brain Gradient Positioning.  </li> <li>Using standard multi-block sampling instead of Spatiotemporal Masking.  </li> <li>Changing number of gradient components (3 vs. 30).  </li> <li>Removing the JEPA framework (i.e., MAE/BrainLM-like framework with contributions).</li> </ul> </li> <li> <p>Key findings (high-level trends):</p> </li> <li> <p>Overall performance: </p> <ul> <li>Brain-JEPA achieves state-of-the-art performance on almost all downstream tasks compared with task-specific models (BrainNetCNN, BrainGNN, BNT, SwiFT), non-deep baselines (SVM/SVR), and prior foundation models (BrainLM, BrainMass, CSM).  </li> <li>On UKB, Brain-JEPA substantially reduces age prediction error and improves correlation and sex classification metrics compared to BrainLM and others.  </li> <li>On HCP-Aging, Brain-JEPA shows higher Pearson correlation for age, higher accuracy/F1 for sex, and strong improvements for Neuroticism and Flanker, especially in personality and cognition, where it surpasses BrainLM by a notable margin.</li> </ul> </li> <li> <p>Disease diagnosis and prognosis: </p> <ul> <li>On ADNI and MACC, Brain-JEPA achieves top or near-top performance in NC vs. MCI, amyloid positivity, and NC vs. MCI (Asian) tasks.  </li> <li>Particularly, Brain-JEPA shows superior performance in NC vs. MCI classification for Asian participants, even though it was pretrained only on Caucasian UKB data, indicating strong cross-ethnic generalization.  </li> <li>On OASIS-3 and CamCAN, Brain-JEPA again outperforms many baselines for AD conversion and depression diagnosis tasks.</li> </ul> </li> <li> <p>Scaling with model size and dataset size: </p> <ul> <li>Performance increases consistently as model size grows from ViT-S to ViT-B to ViT-L, mirroring scaling laws seen in vision and language FMs.  </li> <li>Likewise, using larger fractions of the UKB pretraining data (25% \u2192 50% \u2192 75% \u2192 100%) yields monotonic improvements in downstream metrics, indicating that Brain-JEPA benefits from more data.</li> </ul> </li> <li> <p>Fine-tuning vs. linear probing: </p> <ul> <li>Brain-JEPA achieves strong performance under fine-tuning, but also shows excellent linear probing results, outperforming BrainLM in both regimes.  </li> <li>The performance drop from fine-tuning to linear probing is smaller for Brain-JEPA than for BrainLM, suggesting that Brain-JEPA learns more robust and semantically rich representations during pretraining.</li> </ul> </li> <li> <p>Ablation: Brain Gradient Positioning &amp; Spatiotemporal Masking: </p> <ul> <li>Replacing Brain Gradient Positioning with sine\u2013cosine or anatomical location embeddings hurts performance across age, sex, and NC vs. MCI tasks.  </li> <li>This indicates that gradient-based functional coordinates better capture the brain\u2019s functional architecture and benefit representation learning.  </li> <li>Replacing Spatiotemporal Masking with standard multi-block sampling leads to slower pretraining and lower peak performance; Brain-JEPA with Spatiotemporal Masking reaches or exceeds the ablated model\u2019s best performance in fewer epochs, emphasizing its efficient inductive bias.</li> </ul> </li> <li> <p>Interpretability: </p> <ul> <li>Analysis of self-attention patterns across canonical brain networks (DMN, control network, salience/ventral attention, limbic, etc.) in NC vs. MCI tasks shows that Brain-JEPA focuses on networks known to be implicated in cognitive impairment.  </li> <li>Attention distributions are consistent across Caucasian and Asian cohorts, suggesting that the model has learned robust, neurobiologically meaningful patterns.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths:</li> <li>Tailored JEPA architecture for brain dynamics: The model combines JEPA-style latent prediction with fMRI-specific positional encoding and masking, addressing the limitations of MAE-style reconstruction for noisy, low-SNR BOLD signals.  </li> <li>Principled functional positional encoding: Brain Gradient Positioning offers a compelling way to incorporate functional connectivity gradients into transformer positional embeddings, grounding the model in known macroscale brain organization.  </li> <li>Strong and broad empirical performance: Brain-JEPA achieves state-of-the-art results across many tasks, datasets, and ethnic groups, demonstrating both versatility (demographics, traits, multiple clinical tasks) and generalization.  </li> <li>Improved off-the-shelf representations: Superior linear probing performance and smaller fine-tuning\u2013to\u2013linear-probe gaps suggest that Brain-JEPA learns more semantic and robust representations than prior MAE-based fMRI FMs.  </li> <li> <p>Interpretable attention patterns: Network-level attention aligns with known neurobiological findings regarding cognitive impairment, providing a bridge between deep models and neuroscientific insights.</p> </li> <li> <p>Limitations:</p> </li> <li>Model and compute scale: The largest model used is ViT-L (~307M parameters); larger models (e.g., ViT-H) are not explored due to compute limits, leaving open how far scaling could push performance.  </li> <li>Dataset diversity for pretraining: Pretraining is primarily on UKB (mostly Caucasian), with external datasets used only for downstream evaluation. A more diverse, multi-ethnic, multi-site pretraining corpus might further improve robustness and fairness.  </li> <li>Single-modality focus: While the paper hints at future multimodal integration (e.g., MEG, EEG, structural MRI), the current model only handles resting-state fMRI, so it does not directly address multimodal fusion challenges.  </li> <li>Complexity of gradients and masking: Gradient computation, diffusion maps, and spatiotemporal masking introduce additional complexity and hyperparameters that may be non-trivial to implement and tune for new labs.  </li> <li> <p>Interpretability depth: Although some attention analyses are provided, rich causal or mechanistic interpretation of what the model has learned (e.g., specific ROIs and pathways) remains limited and could be extended.</p> </li> <li> <p>Open Questions and Future Directions:</p> </li> <li>How does scaling up the model size (e.g., ViT-H or mixture-of-experts) and pretraining data diversity (multi-site, multi-ethnic, multi-task) affect generalization and fairness across populations?  </li> <li>Can Brain-JEPA embeddings serve as the fMRI tower in a multimodal FM that integrates genetics, structural MRI, behavior, or clinical data via late fusion, contrastive objectives, or cross-attention, aligning with the integration baseline plan?  </li> <li>How robust are Brain Gradient Positioning and Spatiotemporal Masking to changes in parcellation schemes (e.g., different atlases, voxel-level modeling) or to task-based fMRI instead of resting-state data?  </li> <li>Can we design more interpretable JEPA objectives or probing methods that link latent representations to specific cognitive processes or disease mechanisms, beyond network-level attention summaries?  </li> <li>What are the implications of using JEPA-style latent prediction (rather than generative reconstruction) for identifying causal relationships or performing counterfactual reasoning in brain data?</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the FM landscape: </li> <li>Brain-JEPA sits in the emerging class of brain foundation models, analogous to how GPT-like models function for text and ViT/MAE/Jepa-like models for images. It extends the JEPA paradigm\u2014originally developed for 2D images\u2014to spatiotemporal fMRI dynamics, emphasizing latent prediction over pixel/voxel reconstruction.  </li> <li> <p>It can be seen as a counterpart to BrainLM (MAE-based fMRI FM), providing an alternative pretraining objective that prioritizes robust, abstract representations suitable for off-the-shelf use in diverse tasks.</p> </li> <li> <p>Relation to well-known ideas: </p> </li> <li>Conceptually, Brain-JEPA is like \u201cI-JEPA but for fMRI time series\u201d, with key adaptations for brain data. Instead of predicting missing pixels, it predicts representations of masked ROI-time patches.  </li> <li>Brain Gradient Positioning draws on the functional gradient literature in neuroscience, which uses diffusion maps to uncover macroscale cortical organization. This connects the model\u2019s positional encoding to a well-established neuro-scientific framework.  </li> <li> <p>Spatiotemporal Masking is akin to structured masking strategies in self-supervised learning (e.g., masked patches in MAE), but tailored to enforce meaningful fMRI forecasting and generalization across ROIs and timesteps.</p> </li> <li> <p>Why this paper is a useful reference: </p> </li> <li>For a new grad student interested in AI for neuroscience, this paper is a strong example of how to adapt modern self-supervised architectures (JEPA, ViT) to the constraints and opportunities of brain data.  </li> <li>It provides a concrete recipe for building a foundation model for fMRI, including data preprocessing, model architecture, pretraining objective, and extensive evaluation across multiple datasets and tasks.  </li> <li>It also showcases how to connect architectural choices to neuroscience concepts (e.g., functional gradients, network-level attention), which is crucial for interdisciplinary work and for gaining acceptance in the neuroscience community.  </li> <li>Finally, it provides a robust fMRI representation that fits naturally into the integration baseline plan as the brain modality component to be later fused with other modalities using late fusion, CCA, or contrastive learning.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainjepa_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li>Brain-JEPA aims to build a general-purpose foundation model for brain dynamics that can be adapted to many downstream tasks (demographics, traits, disease diagnosis/prognosis) using resting-state fMRI data.  </li> <li> <p>Existing task-specific models and MAE-based fMRI foundation models (like BrainLM) struggle with noisy BOLD signals, limited generalizability, and weaker off-the-shelf representations.</p> </li> <li> <p>Method / Model: </p> </li> <li>Brain-JEPA adopts a Joint-Embedding Predictive Architecture (JEPA): instead of reconstructing raw fMRI signals, it predicts latent representations of masked fMRI patches in a ViT-based architecture.  </li> <li>It introduces Brain Gradient Positioning, a functional embedding of ROIs derived from diffusion maps on functional connectivity, providing a brain-specific positional encoding that reflects macroscale functional organization.  </li> <li>It proposes Spatiotemporal Masking, which partitions fMRI patches into cross-ROI, cross-time, and double-cross regions and uses overlapped sampling to encourage generalization across space and time with a strong inductive bias.  </li> <li> <p>The model is pretrained on large UK Biobank data using ViT-S/B/L backbones, with EMA target encoder and predictor networks, and evaluated via fine-tuning and linear probing on multiple external datasets.</p> </li> <li> <p>Results: </p> </li> <li>Across UKB, HCP-Aging, ADNI, MACC, OASIS-3, and CamCAN, Brain-JEPA achieves state-of-the-art performance on age, sex, trait prediction, and multiple disease diagnosis/prognosis tasks, outperforming task-specific models, SVM/SVR, and prior fMRI FMs like BrainLM and BrainMass.  </li> <li>It scales well with model size and data size, with larger ViT backbones and more pretraining data yielding better performance.  </li> <li>Brain-JEPA shows strong linear probing performance and smaller gaps between fine-tuning and linear probing than BrainLM, indicating more robust and transferable representations.  </li> <li> <p>Ablation studies confirm that Brain Gradient Positioning and Spatiotemporal Masking are crucial for performance and training efficiency.</p> </li> <li> <p>Why it matters: </p> </li> <li>Brain-JEPA demonstrates that latent prediction architectures with neuroscience-informed positional encodings can produce powerful, generalizable representations of brain activity.  </li> <li>It provides a solid foundation for future multimodal integration where fMRI embeddings are combined with genetics, structural imaging, or behavioral data using late fusion or contrastive methods, aligning well with broader integration plans.  </li> <li>For students and researchers, Brain-JEPA is an influential reference on how to design, train, and evaluate brain foundation models at scale, bridging modern self-supervised ML techniques and contemporary neuroscience.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/","title":"BrainLM: A Foundation Model For Brain Activity Recordings","text":""},{"location":"generated/kb_curated/papers-md/brainlm_2024/#brainlm-a-foundation-model-for-brain-activity-recordings","title":"BrainLM: A Foundation Model For Brain Activity Recordings","text":"<p>Authors: Josue Ortega Caro, Antonio H. de O. Fonseca, Syed A. Rizvi, Matteo Rosati, Christopher Averill, James L. Cross, Prateek Mittal, Emanuele Zappala, Rahul M. Dhodapkar, Chadi G. Abdallah, David van Dijk, et al. Year: 2024 Venue: ICLR (International Conference on Learning Representations)</p>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM. The paper develops a large foundation model specifically for functional MRI (fMRI) recordings, learning spatiotemporal representations of brain activity dynamics across the whole brain.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. BrainLM is introduced as a new foundation model architecture and pretraining scheme for fMRI, with subsequent fine-tuning and zero-shot applications.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Task-based and resting-state fMRI (BOLD time series) from large population cohorts (UK Biobank and Human Connectome Project).</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces BrainLM, a large transformer-based foundation model trained on 6,700 hours of fMRI recordings from over 77,000 scans. Instead of training separate models for each narrow decoding task, BrainLM is pretrained in a self-supervised masked-reconstruction fashion to learn general-purpose representations of whole-brain activity over time. After pretraining, the same model can be fine-tuned to predict clinical variables (age, neuroticism, PTSD, anxiety), forecast future brain states, and perform zero-shot inference such as discovering functional brain networks directly from attention patterns. The authors show that BrainLM generalizes across datasets, performing well both on held-out UK Biobank scans and on the independent Human Connectome Project cohort. They also demonstrate interpretable attention maps that align with known brain networks and clinical differences (e.g., depression severity). For a new grad student, this paper is a key example of how foundation model ideas from language and vision (e.g., masked autoencoders) can be adapted to neuroimaging, creating a versatile model that unifies many downstream tasks on fMRI data.</p>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>Build a single, general-purpose model of brain activity dynamics that can support many downstream tasks: predicting clinical variables, modeling brain networks, and forecasting future activity.</li> <li>Learn unsupervised representations from large-scale fMRI repositories rather than training separate, task-specific models on small datasets.</li> <li> <p>Capture the full spatiotemporal structure of fMRI signals across the brain, not just limited regions like the visual cortex.</p> </li> <li> <p>Why this is hard</p> </li> <li>High dimensionality and complexity: fMRI produces thousands of voxel or parcel time series, with complex dependencies across brain regions and time.</li> <li>Indirect and noisy signal: BOLD signals are an indirect measure of neural activity and can be hard to interpret.</li> <li>Limited labels: Many large fMRI datasets have rich time series but relatively few labels for specific tasks, making supervised training challenging.</li> <li>Task-specific models do not generalize well: Traditional supervised models (e.g., SVMs, small neural nets) are tuned to narrow tasks and do not transfer well to new datasets or objectives.</li> <li>Need for scalable training: To benefit from large repositories like UK Biobank and HCP, models must handle massive data and learn representations that scale with model size and data size.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>UK Biobank (UKB):<ul> <li>~76,296 task-based and resting-state fMRI recordings with associated medical records.</li> <li>Ages approximately 40\u201369; scanned on a Siemens 3T scanner at ~0.735 s temporal resolution.</li> <li>80% (61,038 recordings) used for training; 20% held out for testing.</li> </ul> </li> <li>Human Connectome Project (HCP):<ul> <li>1,002 high-quality fMRI recordings from healthy adults.</li> <li>~0.72 s temporal resolution; used entirely as an external evaluation cohort.</li> </ul> </li> <li> <p>In total, the training corpus spans 77,298 recordings and 6,700 hours of preprocessed fMRI.</p> </li> <li> <p>Modalities</p> </li> <li>Single modality: functional MRI (fMRI), representing whole-brain BOLD time series.</li> <li> <p>Both task and resting-state recordings are included.</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>Standard preprocessing: motion correction, normalization, temporal filtering, and ICA-based denoising.</li> <li>Brain parcellation into 424 regions (AAL-424 atlas), yielding 424-dimensional time series per scan.</li> <li>Time series sampled at ~1 Hz after preprocessing.</li> <li>Robust scaling: per-parcel median subtraction and division by interquartile range across subjects.</li> <li> <p>For model input:</p> <ul> <li>Random 200-timestep subsequences are extracted from each recording.</li> <li>Each parcel\u2019s 200-timestep sequence is split into patches of 20 time points, giving 10 patches per parcel.</li> <li>The resulting patches (conceptually 424 \u00d7 10) are treated as \"tokens\" via a learnable linear projection into 512-dimensional embeddings.</li> <li>The 424 \u00d7 200 window is also viewed as a 2D image (parcels \u00d7 time) with parcels ordered by Y-coordinate to preserve spatial locality.</li> </ul> </li> <li> <p>Missing details</p> </li> <li>Exact number of subjects, hardware details beyond scanner type, and some hyperparameters are referenced but not fully spelled out in the main extracted text (likely given in supplementary material).</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Masked autoencoder (MAE) based on a Transformer architecture.</li> <li> <p>Encoder\u2013decoder structure with self-attention blocks that operate on spatiotemporal tokens derived from parcel-time patches.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>New foundation model. The authors design BrainLM specifically for fMRI data, inspired by BERT and Vision Transformer (ViT) style masked modeling but adapted to 2D parcel\u00d7time structure.</p> </li> <li> <p>Key components and innovations</p> </li> </ul> Component Description Tokenization of fMRI patches 200-timestep windows split into 20-timestep patches per parcel; patches projected into 512-d embeddings. Spatiotemporal masking Random and future-timepoint masking at rates of 20%, 75%, or 90%, making the model reconstruct masked tokens. 2D \"image-like\" formulation Treats the 424-parcel \u00d7 200-timestep window as a 2D grid; parcels ordered by Y-coordinate to preserve spatial adjacency, enabling multi-parcel tokens and scalable encoding. Transformer encoder Processes only unmasked tokens, with 4 self-attention layers and 4 attention heads. Transformer decoder 2-layer decoder that takes both encoded visible tokens and masked tokens, then reconstructs the full input. Positional embeddings Learnable spatial and temporal embeddings added to token representations to encode parcel location and time. Latent CLS token A special token summarizing each sequence, later used for clinical variable prediction and visualization. <ul> <li>Training setup</li> <li>Objective: Minimize mean squared error (MSE) between original and reconstructed fMRI signals for masked patches (self-supervised reconstruction).</li> <li>Pretraining data: 6,700 hours of fMRI from UKB and HCP, using random 200-timestep subsequences.</li> <li>Optimization: Adam optimizer; 100 epochs; batch size 512.</li> <li>Scaling: Multiple model sizes (e.g., 13M, 111M, 650M parameters), with performance improving as both model size and dataset size increase.</li> <li>Downstream adaptation:<ul> <li>Add a 3-layer MLP head to the pretrained encoder for regression of clinical variables.</li> <li>Fine-tune on subsets of UKB data withheld from pretraining.</li> <li>For future state prediction, fine-tune the model to forecast the next 20 timesteps given 180 observed timesteps.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Is the paper multimodal?</li> <li>Not in the main experiments. BrainLM is trained and evaluated on single-modality fMRI data (task and rest). The core contribution is a unimodal foundation model for brain activity recordings.</li> <li> <p>However, the discussion explicitly points to multimodal extensions as future work, suggesting integration with EEG, MEG, and other brain-wise or even genomic information.</p> </li> <li> <p>Relation to integration baseline plan</p> </li> <li>The paper itself does not implement late fusion, CCA, or contrastive cross-modal alignment. It is focused on learning a strong single-modality encoder for fMRI.</li> <li>In terms of the integration baseline plan:<ul> <li>BrainLM can be seen as a per-modality encoder that could feed into a late fusion or stacking approach when combined with other modalities (e.g., structural MRI, genetics, clinical variables).</li> <li>Its robust self-supervised representations align with the plan\u2019s emphasis on preserving modality-specific signal before fusion.</li> <li>The evaluation on multiple tasks and datasets is compatible with the plan\u2019s emphasis on robustness and disciplined evaluation, although the paper does not explicitly follow the full AUROC/AUPRC + confidence-interval protocol discussed in the plan.</li> </ul> </li> <li> <p>Future multimodal systems could:</p> <ul> <li>Use BrainLM\u2019s embeddings as one tower in a two-tower contrastive model, with another tower encoding, for example, genetics or behavioral data.</li> <li>Perform late fusion of BrainLM features with those from other FMs (e.g., for combined clinical prediction).</li> </ul> </li> <li> <p>Summary</p> </li> <li>For now, BrainLM is best viewed as a strong building block for multimodal integration, rather than a multimodal FM itself.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks</li> <li>Masked reconstruction / generalization:<ul> <li>Evaluate reconstruction accuracy (e.g., R\u00b2 on masked patches) on held-out UKB test data and independent HCP data.</li> </ul> </li> <li>Clinical variable prediction:<ul> <li>Fine-tune BrainLM to regress age, neuroticism, PTSD (PCL-5), and general anxiety (GAD-7) scores from fMRI recordings.</li> </ul> </li> <li>Future brain state prediction:<ul> <li>Given 180 observed timesteps, predict the next 20 timesteps of parcel activity, evaluated on UKB and HCP.</li> </ul> </li> <li>Interpretability via attention analysis:<ul> <li>Analyze self-attention weights (especially from the CLS token) to study how attention changes across tasks and clinical groups (e.g., depression severity).</li> </ul> </li> <li> <p>Functional network prediction (zero-shot-like):</p> <ul> <li>Use attention-derived features to classify parcels into 7 intrinsic functional networks without network-specific supervision.</li> </ul> </li> <li> <p>Baselines</p> </li> <li>For clinical variable regression:<ul> <li>SVR and MLP on correlation matrices.</li> <li>LSTM and GCN models that directly use fMRI recordings.</li> <li>Comparisons both to models trained on raw data and models on pretrained embeddings.</li> </ul> </li> <li>For future state prediction:<ul> <li>LSTM.</li> <li>Neural ODE and Latent ODE models.</li> <li>A Transformer model without pretraining (same architecture but trained only on the forecasting task).</li> </ul> </li> <li> <p>For functional network identification:</p> <ul> <li>k-NN classifiers using:</li> <li>Raw parcel time series,</li> <li>Variational Autoencoder (VAE) embeddings,</li> <li>GCN embeddings,</li> <li>BrainLM attention weights.</li> </ul> </li> <li> <p>Key findings</p> </li> <li>Generalization and reconstruction:<ul> <li>BrainLM achieves strong R\u00b2 on UKB test data and generalizes well to HCP, despite domain differences, showing that pretraining learns robust, dataset-agnostic representations.</li> <li>Performance improves with larger models and more data, demonstrating scaling laws similar to those seen in language and vision FMs.</li> </ul> </li> <li>Clinical variable prediction:<ul> <li>BrainLM-based regressors achieve lower mean squared error than baselines (SVR, MLP, LSTM, GCN, raw data) across age, PTSD, anxiety, and neuroticism.</li> <li>Fine-tuning further improves over using frozen embeddings, indicating that pretrained representations are rich but still adaptable.</li> <li>Even zero-shot regression (no fine-tuning) shows non-trivial predictive power, and performance scales with model size.</li> </ul> </li> <li>Future brain state prediction:<ul> <li>Fine-tuned BrainLM significantly outperforms LSTM, Neural ODE, Latent ODE, and the non-pretrained Transformer on both UKB and HCP.</li> <li>The benefit of pretraining is clear: the model without pretraining performs noticeably worse.</li> <li>Larger BrainLM variants maintain better forecasting performance over multiple timesteps.</li> </ul> </li> <li>Interpretability and networks:<ul> <li>Attention maps distinguish task vs rest (e.g., stronger attention to visual cortex during task states).</li> <li>Differences in attention for high vs low depression emphasize frontal and limbic regions, consistent with clinical literature.</li> <li>Using attention-based features, BrainLM achieves ~58.8% accuracy in classifying parcels into 7 functional networks, outperforming VAE, GCN, and raw data baselines.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths</li> <li>Introduces a true foundation model for fMRI, trained at a scale (6,700 hours, 77k recordings) much larger than prior work.</li> <li>Uses self-supervised masked modeling to efficiently exploit unlabeled fMRI data, enabling versatile downstream applications.</li> <li>Demonstrates strong generalization across cohorts (UKB \u2192 HCP) and across diverse tasks (reconstruction, forecasting, clinical prediction, network identification).</li> <li>Provides interpretable attention maps that align with known brain networks and clinical patterns, offering neuroscientific insights.</li> <li> <p>Shows clear scaling behavior, suggesting that larger models and datasets can further improve performance.</p> </li> <li> <p>Limitations</p> </li> <li>Currently unimodal: only fMRI is modeled; multimodal integration with EEG, structural MRI, genetics, and behavior is left for future work.</li> <li>Despite interpretability via attention, the latent representations are still complex, and a full mechanistic understanding of what is encoded remains challenging.</li> <li>The approach is computationally heavy, requiring large-scale pretraining with transformers on big imaging datasets.</li> <li>Some preprocessing choices (e.g., parcellation scheme, scaling, window length) may influence results, but not all ablations are detailed in the main text.</li> <li> <p>Real-world clinical deployment requires careful validation, robustness checks, and fairness analyses that go beyond the current experiments.</p> </li> <li> <p>Open Questions and Future Directions:</p> </li> <li>How does BrainLM compare to alternative pretraining objectives (contrastive, masked prediction on different views, generative modeling) for fMRI?</li> <li>Can BrainLM embeddings be effectively combined with other modalities (EEG, MEG, structural MRI, genetics) using late fusion or contrastive two-tower setups, and does this improve clinical prediction?</li> <li>What neuroscientific structure is captured in the CLS token and internal layers\u2014can we relate specific attention patterns or latent dimensions to known circuits or cognitive processes?</li> <li>How robust is BrainLM to distribution shifts such as different scanners, acquisition protocols, or clinical populations (e.g., pediatric, elderly, or specific disorders)?</li> <li>Can smaller, distilled versions of BrainLM retain most performance while being practical for clinical or real-time applications?</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the FM landscape</li> <li>BrainLM is to fMRI what large language models (like GPT) are to text: a general, pretrained backbone that can be adapted to many tasks rather than a task-specific model.</li> <li>Within brain/neuro FMs, it extends prior work that focused on visual cortex or small datasets to whole-brain modeling at population scale.</li> <li> <p>It shows that the masked autoencoder paradigm from vision and language transfers well to spatiotemporal brain data.</p> </li> <li> <p>Relation to well-known ideas</p> </li> <li>Conceptually, BrainLM behaves like a BERT-style or ViT-style masked model applied to a 2D grid where one dimension is space (brain parcels) and the other is time (fMRI timesteps).</li> <li>The attention-based interpretability parallels how we interpret attention maps in NLP and vision, but here the \"tokens\" are brain parcels over time.</li> <li> <p>The clinical prediction and network discovery tasks illustrate how foundation models can support both prediction and scientific discovery.</p> </li> <li> <p>Why it matters and how it links to integration plans</p> </li> <li>For a grad student interested in computational neuroscience, BrainLM is a blueprint for building large, reusable models of brain activity.</li> <li>It provides a ready-made encoder that can plug into multimodal integration pipelines\u2014consistent with the integration baseline plan\u2019s idea of learning strong, modality-specific representations before fusion.</li> <li>The work signals a general trend: foundation models are moving into neuroimaging, opening paths to richer multimodal systems that combine brain signals with genetic, behavioral, and clinical data.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainlm_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem</li> <li>There is a need for a single, scalable model that can learn from massive fMRI repositories and support many downstream neuroscience and clinical tasks.</li> <li> <p>Traditional task-specific models struggle with generalization, data scale, and transfer across cohorts.</p> </li> <li> <p>Method / model</p> </li> <li>BrainLM is a transformer-based masked autoencoder that treats fMRI parcel\u00d7time windows as a 2D grid of tokens.</li> <li>It uses spatiotemporal masking and reconstruction to learn representations from 6,700 hours of fMRI without task labels.</li> <li>The architecture includes 4-layer encoder and 2-layer decoder transformers, with learned spatial and temporal embeddings and a summary CLS token.</li> <li> <p>Multiple model sizes (13M\u2013650M parameters) are trained, showing improved performance with scale.</p> </li> <li> <p>Results</p> </li> <li>BrainLM shows strong reconstruction and generalization performance on both UKB and HCP datasets.</li> <li>It outperforms baselines (SVR, MLP, LSTM, GCN, Neural ODE, non-pretrained Transformer) in clinical variable prediction and future brain state forecasting.</li> <li> <p>Attention-based analyses reveal meaningful functional networks and clinical differences, and attention-derived features outperform other representations in classifying parcels into known networks.</p> </li> <li> <p>Why it matters</p> </li> <li>BrainLM establishes a foundation model paradigm for fMRI, demonstrating that large, self-supervised models can unify diverse tasks in brain dynamics modeling.</li> <li>It provides a flexible, interpretable, and extensible backbone that future work can extend to multimodal settings and more ambitious clinical and neuroscientific applications.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/","title":"BrainMT (2025)","text":""},{"location":"generated/kb_curated/papers-md/brainmt_2025/#brainmt-a-hybrid-mamba-transformer-architecture-for-modeling-long-range-dependencies-in-functional-mri-data","title":"BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data","text":"<p>Authors: Arunkumar Kannan, Martin A. Lindquist, Brian Caffo Year: Unknown (not clearly specified in extracted text) Venue: Unknown (likely a neuroimaging / ML venue; not clearly specified in extracted text)</p>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM. The paper develops a deep architecture specifically for resting-state functional MRI (rs-fMRI) and uses it to predict subject-level phenotypes (sex and cognitive intelligence) from whole-brain 4D volumes.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development (brain-specific sequence model). The work introduces a new hybrid architecture, BrainMT, that combines Mamba state-space models and transformers to handle long 4D fMRI sequences end-to-end. It is not a huge general-purpose foundation model in the GPT/CLIP sense, but it does propose a new, reusable backbone for a broad class of fMRI prediction tasks.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Resting-state fMRI (4D volumetric time series: 3D brain volumes over time).  </li> <li>Phenotypic labels: sex (classification) and cognitive intelligence scores (regression).</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#2-one-paragraph-high-level-summary-for-a-new-grad-student","title":"2. One-Paragraph High-Level Summary (For a New Grad Student)","text":"<p>This paper proposes BrainMT, a deep learning architecture designed to model long-range spatial and temporal dependencies in resting-state fMRI data for subject-level phenotypic prediction. Instead of compressing fMRI into connectivity matrices or using short temporal windows, BrainMT operates directly on 4D voxel-wise volumes and can process much longer temporal sequences efficiently. The model uses a hierarchical convolutional block to extract local spatial features, a bidirectional spatiotemporal Mamba block (a modern state-space model) to capture long-range temporal and spatiotemporal patterns, and a lightweight transformer block to model global spatial relationships. The authors evaluate BrainMT on large UK Biobank and Human Connectome Project datasets, predicting both sex and cognitive intelligence, and show it outperforms strong baselines including graph neural networks, transformer-based models, and the recent SwiFT voxel-wise transformer. Quantitative and ablation studies demonstrate that BrainMT achieves better accuracy with lower memory usage while benefiting from longer temporal context. Finally, interpretability analysis using Integrated Gradients highlights brain regions in default mode and frontoparietal networks that align with known neuroscience findings. Overall, the work is valuable to a new grad student because it showcases how modern sequence modeling ideas (Mamba + transformers) can be adapted to challenging 4D neuroimaging data and why long temporal context matters for fMRI-based prediction.</p>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>The goal is to predict subject-level phenotypes (sex and cognitive intelligence) from resting-state fMRI data.  </li> <li>Each subject has a 4D fMRI scan: a sequence of 3D brain volumes over time (volumes \u00d7 height \u00d7 width \u00d7 depth).  </li> <li> <p>The model should learn functional connectivity and spatiotemporal patterns directly from these volumetric time series, without relying on hand-crafted parcellations or connectivity matrices.</p> </li> <li> <p>Why existing approaches are limited: </p> </li> <li>Correlation-based pipelines: <ul> <li>They typically parcellate the brain into regions of interest (ROIs) and compute a region-by-region functional connectivity matrix (e.g., Pearson correlation).  </li> <li>This reduces dimensionality but can discard fine-grained spatial information, especially with coarse parcellations.  </li> <li>Performance can vary widely depending on the chosen parcellation and connectivity measure, leading to instability and lack of consensus.</li> </ul> </li> <li> <p>Voxel-based deep models: </p> <ul> <li>Recent transformer or CNN-based methods operate directly on voxel-level fMRI, but transformer-based models face quadratic complexity in sequence length.  </li> <li>As a result, they are often restricted to short sequences (e.g., 10\u201320 time frames), then aggregate predictions across multiple windows.  </li> <li>Because fMRI signals evolve relatively slowly (hemodynamics), limiting context to a few frames can miss important long-range temporal dynamics.</li> </ul> </li> <li> <p>Why this is hard technically: </p> </li> <li>fMRI is high-dimensional: 3D volumes with tens of thousands of voxels repeated hundreds to thousands of times per scan.  </li> <li>Spatiotemporal patterns are long-range and structured: activity in distant brain regions can be correlated across long time scales.  </li> <li>Direct transformer modeling over all voxels and time points is computationally heavy (quadratic in sequence length).  </li> <li>Models must balance expressivity (capturing complex patterns) with efficiency (memory and compute) to be practical on large datasets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used: </li> <li>UK Biobank (UKB): <ul> <li>Resting-state fMRI data from about 6,000 participants.  </li> <li>Scan length: approximately 490 volumes per subject.  </li> <li>Spatial dimensions: 91 \u00d7 109 \u00d7 91 in MNI space.  </li> <li>Sex distribution: ~50.86% female.  </li> </ul> </li> <li> <p>Human Connectome Project (HCP, S1200 release): </p> <ul> <li>Resting-state fMRI data from 1,075 participants.  </li> <li>Scan length: approximately 1,200 volumes per subject.  </li> <li>Spatial dimensions: 91 \u00d7 109 \u00d7 91 in MNI space.  </li> <li>Sex distribution: ~51.16% female.</li> </ul> </li> <li> <p>Modalities: </p> </li> <li>Primary modality: resting-state fMRI volumes (4D: time \u00d7 3D brain).  </li> <li> <p>Targets:  </p> <ul> <li>Cognitive intelligence scores: </li> <li>\u201cCognitive function\u201d composite scores from HCP.  </li> <li>\u201cFluid intelligence/reasoning\u201d scores from UKB.  </li> <li>Sex: binary labels for sex classification.</li> </ul> </li> <li> <p>Preprocessing / representation: </p> </li> <li>The authors use existing preprocessed fMRI data from both datasets, following \u201cfMRI volume\u201d pipelines that include:  <ul> <li>Bias field reduction, skull stripping, cross-modality registration, and spatial normalization.  </li> </ul> </li> <li>For modeling:  <ul> <li>Global Z-score normalization is applied across brain voxels, excluding background regions; background voxels are filled with minimum Z-score intensity.  </li> <li>Each 3D volume is partitioned into partially overlapping 3D patches.  </li> <li>Patches are embedded via convolutional layers into a lower-dimensional feature space before being passed into the Mamba and transformer blocks.  </li> </ul> </li> <li>For comparison with correlation-based approaches, the HCP multimodal atlas is used to parcellate the data and compute ROI\u2013ROI Pearson correlation matrices.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>BrainMT is a hybrid architecture combining convolutional neural networks, Mamba state-space models, and transformers to capture local, long-range temporal, and global spatial dependencies in fMRI.</p> <ul> <li>Model type: </li> <li>A hybrid deep sequence model for spatiotemporal fMRI data, composed of:  <ul> <li>3D convolutional blocks (hierarchical feature extractor).  </li> <li>Bidirectional Vision Mamba blocks (selective state-space models) arranged with a temporal-first scanning mechanism.  </li> <li>A multi-head self-attention transformer block for global spatial modeling.  </li> </ul> </li> <li> <p>Overall, it can be viewed as a brain-specific backbone that blends linear-time sequence modeling (Mamba) with transformer attention.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li>The architecture itself, BrainMT, is new.  </li> <li>It builds on existing ideas: Vision Mamba / Mamba-based state-space models and standard transformers.  </li> <li>The paper does not present large-scale general-purpose pretraining; instead, BrainMT is trained directly on the task objectives (sex and intelligence prediction) on large datasets.  </li> <li> <p>So in FM terms, it is closer to a new, reusable backbone than a fully generic foundation model.</p> </li> <li> <p>Key architectural components and innovations:</p> </li> <li> <p>1. Convolution block (local spatial feature extractor): </p> <ul> <li>Takes fMRI volumes (X \\in \\mathbb{R}^{T \\times H \\times W \\times D}).  </li> <li>Each volume is split into partially overlapping 3D patches (downsampling the spatial resolution).  </li> <li>Two convolutional layers project patches into a C-dimensional embedding.  </li> <li>A two-stage convolutional encoder with downsampling forms multi-scale feature maps, capturing coarse and fine spatial details.  </li> <li>Residual convolutional units with GELU activations and layer normalization provide stable feature extraction.</li> </ul> </li> <li> <p>2. Positional embeddings and sequence construction: </p> <ul> <li>The output of the convolution block is a sequence of patch tokens per time step.  </li> <li>Learnable spatial positional embeddings and temporal positional embeddings are added to encode positions.  </li> <li>A learnable classification token (X_{\\text{cls}}) is prepended as a global aggregator.  </li> <li>Tokens are reshaped into a long 1D sequence of length (L = T \\times K) (where (K) is the number of patches), preparing them for spatiotemporal modeling.</li> </ul> </li> <li> <p>3. Spatiotemporal Mamba block (long-range context): </p> <ul> <li>Based on selective state-space models (SSMs), originally inspired by Kalman-like systems.  </li> <li>Uses Vision Mamba with bidirectional selective SSMs to handle spatiotemporal context.  </li> <li>Mamba introduces input-dependent parameters (e.g., B, C, and time scales), enabling adaptive context modeling.  </li> <li>The temporal-first scanning mechanism arranges tokens so that time is the leading dimension, followed by spatial dimensions, which:  </li> <li>Emphasizes temporal continuity and long-range temporal correlations critical for fMRI.  </li> <li>Helps the SSM capture long sequences with linear-time complexity in sequence length.  </li> <li>Forward and backward selective SSMs process the sequence in both directions, and their outputs are gated and combined, enabling rich bidirectional context.</li> </ul> </li> <li> <p>4. Transformer block (global spatial relationships): </p> <ul> <li>After Mamba processing, a multi-head self-attention transformer operates on the sequence.  </li> <li>Attention is defined as usual ( \\text{Attention}(Q,K,V) = \\text{Softmax}(QK^\\top / \\sqrt{d_\\text{head}}) V ).  </li> <li>Because convolution and downsampling have already reduced the spatial resolution, the sequence length is shorter, making quadratic transformer attention tractable.  </li> <li>This block focuses on global spatial interactions among feature tokens, complementing the temporally oriented Mamba block.</li> </ul> </li> <li> <p>5. Output head: </p> <ul> <li>The final representation is taken from the normalized classification token (X_{\\text{cls}}).  </li> <li>A multilayer perceptron (MLP) head is applied for downstream tasks:  </li> <li>Regression head for cognitive intelligence prediction.  </li> <li>Classification head for sex prediction.</li> </ul> </li> <li> <p>Training setup (as described):</p> </li> </ul> Aspect Description Implementation PyTorch; trained on NVIDIA L40S GPUs (48GB RAM) Architecture depth 2 convolution blocks, 12 Mamba blocks, 8 transformer blocks Input frames 200 frames per subject (chosen via ablation; trade-off between SNR and overfitting) Mamba hyperparameters State dimension 16, expansion ratio 2 (default Mamba settings) Optimization AdamW, cosine learning rate schedule over 20 epochs Warm-up First 5 epochs used for linear warm-up Learning rate 2e-4 (default in experiments) Weight decay 0.05 Batch size 2 Training strategy Distributed data-parallel training Early stopping Based on validation loss Regression objective Mean squared error (MSE), evaluated with MSE, MAE, Pearson\u2019s R Classification objective Binary cross-entropy, evaluated with accuracy, balanced accuracy, AUROC"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Is this a multimodal integration paper? </li> <li>Not primarily. The core focus is on single-modality resting-state fMRI data.  </li> <li> <p>The model does not fuse different data types (e.g., structural MRI, behavior, genetics) within the architecture; instead, it learns from volumetric fMRI alone and predicts phenotypes.</p> </li> <li> <p>Integration aspects (within fMRI): </p> </li> <li>BrainMT integrates spatial and temporal information from fMRI in a unified framework:  <ul> <li>Convolution handles local spatial integration.  </li> <li>Mamba handles long-range temporal and spatiotemporal dependencies.  </li> <li>Transformers handle global spatial relationships across the entire brain.  </li> </ul> </li> <li> <p>This can be viewed as intra-modality integration (across time and space) rather than multimodal integration.</p> </li> <li> <p>Relation to the integration baseline plan: </p> </li> <li>The integration baseline plan emphasizes late fusion across heterogeneous modalities, CCA-based analysis, and disciplined evaluation.  </li> <li>BrainMT does not explicitly adopt late fusion or CCA-style strategies, as it operates on a single modality.  </li> <li>However, its design aligns with the principle of preserving rich modality-specific signal (here, fMRI) by avoiding aggressive parcellation and instead modeling voxel-level dynamics directly.  </li> <li>The evaluation uses solid metrics (MSE/MAE/R for regression; accuracy, balanced accuracy, AUROC for classification), which resonates with the plan\u2019s emphasis on robust, properly reported metrics.</li> </ul> <p>Because the paper is not truly multimodal, the integration baseline plan is more of a conceptual backdrop here than a direct methodological influence.</p>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Cognitive intelligence prediction (regression): <ul> <li>Predicts continuous intelligence scores for subjects in HCP and UKB.  </li> <li>Evaluated with MSE, MAE, and Pearson\u2019s correlation R.  </li> </ul> </li> <li>Sex classification: <ul> <li>Binary classification of sex for HCP and UKB participants.  </li> <li>Evaluated with accuracy, balanced accuracy, and AUROC.  </li> </ul> </li> <li> <p>Additional analysis: </p> <ul> <li>Predicting functional connectivity correlations, comparing BrainMT with SwiFT on subject-level Pearson correlations.  </li> <li>Ablation studies on number of frames, architecture components, numbers of layers, and alternative Mamba variants.  </li> <li>Interpretability analyses to identify brain regions contributing to predictions.</li> </ul> </li> <li> <p>Baselines: </p> </li> <li>Correlation-based methods: <ul> <li>XGBoost on connectivity features.  </li> <li>BrainNetCNN (CNN for brain networks).  </li> <li>BrainGNN (graph neural network for brain graphs).  </li> <li>BrainNetTF (transformer applied to brain networks).  </li> </ul> </li> <li>Voxel-based methods: <ul> <li>TFF: self-supervised transformers for fMRI representation.  </li> <li>SwiFT: 4D Swin transformer for fMRI (strong recent voxel-based baseline).  </li> </ul> </li> <li> <p>All baselines are implemented following their original papers and tuned on the validation sets.</p> </li> <li> <p>Key quantitative findings (trends, not exact numbers): </p> </li> <li>Intelligence prediction: <ul> <li>On both HCP and UKB, BrainMT achieves lower MSE and MAE and higher Pearson\u2019s R than all baselines.  </li> <li>Many baselines achieve MSE close to 1.0 (given targets are normalized to unit variance), suggesting they mostly predict the mean.  </li> <li>BrainMT notably reduces MSE by around 6\u20139% relative to the best baselines across datasets, indicating meaningful improvement.  </li> </ul> </li> <li>Sex classification: <ul> <li>On HCP, BrainMT achieves the best accuracy, balanced accuracy, and AUROC among all methods.  </li> <li>On UKB, BrainMT closely matches or slightly exceeds SwiFT, maintaining state-of-the-art performance.  </li> </ul> </li> <li>Memory efficiency and scalability: <ul> <li>BrainMT is reported to be about 35.8% more memory-efficient than SwiFT.  </li> <li>Its memory usage grows linearly with the number of time frames, enabling longer sequence modeling than standard transformers.  </li> </ul> </li> <li> <p>Ablation studies: </p> <ul> <li>Number of frames: Using around 200 frames provides the best trade-off; fewer frames reduce signal-to-noise, while substantially more frames risk overfitting.  </li> <li>Component ablations: Removing either the transformer or convolution block degrades performance, confirming the importance of the hybrid design.  </li> <li>Depth variations: Larger or smaller numbers of Mamba/transformer layers change performance; the chosen configuration (12 Mamba, 8 transformer) is near optimal.  </li> <li>Alternative Mamba variants: Replacing the bidirectional Vision Mamba block with alternatives like VMamba or MambaVision worsens results, suggesting the specific temporal-first, bidirectional setup is crucial.  </li> <li>Functional connectivity prediction: BrainMT surpasses SwiFT in subject-level Pearson correlations, indicating better capture of dynamics that underlie functional connectivity.</li> </ul> </li> <li> <p>Qualitative / interpretability findings: </p> </li> <li>Integrated Gradients (IG) maps for cognitive intelligence highlight regions in:  <ul> <li>Default Mode Network (DMN) and Frontoparietal Network (FPN), including posterior cingulate cortex (PCC), anterior cingulate cortex (ACC), precuneus (PCu), and cuneus (Cu).  </li> <li>These are known to be involved in working memory, attention, decision-making, and visuospatial processing.  </li> </ul> </li> <li>For sex prediction, IG maps consistently emphasize regions such as the superior temporal gyrus (STG), middle frontal gyrus (MFG), and precuneus (PCu), aligning with prior studies on sex differences in functional brain organization.  </li> <li>These maps suggest that BrainMT\u2019s predictive patterns are consistent with established neuroscientific knowledge, not just arbitrary features.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths: </li> <li>End-to-end voxel-level modeling: Operates directly on 4D fMRI volumes, avoiding parcellation-induced information loss and inconsistencies.  </li> <li>Long-range temporal modeling: The Mamba-based temporal-first design enables efficient handling of long sequences (hundreds of frames), which is crucial for capturing slow hemodynamic dynamics.  </li> <li>Hybrid architecture: Combining convolution, Mamba, and transformers gives a balanced treatment of local spatial, long-range temporal, and global spatial dependencies.  </li> <li>Strong empirical results: Consistently outperforms state-of-the-art correlation-based and voxel-based models on large, well-known datasets (HCP and UKB).  </li> <li>Interpretability: Integrated Gradients provide biologically plausible importance maps, grounding model predictions in known functional networks.  </li> <li> <p>Memory efficiency: More memory-efficient than a strong voxel-based transformer baseline (SwiFT), which is important for practical large-scale neuroimaging.</p> </li> <li> <p>Limitations: </p> </li> <li>Single-modality focus: The model only uses resting-state fMRI; it does not yet integrate structural MRI, behavioral data, genetics, or other modalities that could aid prediction.  </li> <li>Task-specific training: BrainMT is trained directly on supervised targets rather than via large-scale self-supervised pretraining; this limits its status as a general-purpose foundation model.  </li> <li>Compute requirements: Despite efficiency improvements, training on 4D fMRI with deep Mamba and transformer stacks on large datasets still requires substantial compute (L40S GPUs, distributed training).  </li> <li>Generalization beyond studied tasks: The paper evaluates on sex and cognitive intelligence; it remains unclear how well BrainMT transfers to other phenotypes or clinical conditions without substantial retraining.  </li> <li> <p>Interpretability scope: Integrated Gradients focus on voxel importance but do not fully explain temporal dynamics or causal relationships.</p> </li> <li> <p>Open Questions and Future Directions: </p> </li> <li>Multimodal extensions: How would BrainMT perform if extended to jointly model structural MRI, diffusion MRI, or behavioral measures along with fMRI? Could late fusion or shared embedding approaches from the integration baseline plan improve performance?  </li> <li>Self-supervised or foundation-style pretraining: Can we pretrain BrainMT on large-scale unlabeled rs-fMRI datasets using self-supervised objectives (e.g., masked volume prediction, temporal contrastive tasks) and then fine-tune for many downstream tasks?  </li> <li>Clinical translation: How well does BrainMT generalize to clinical populations (e.g., psychiatric or neurodegenerative disorders), and what adaptations are needed for imbalanced or smaller datasets?  </li> <li>Temporal modeling variants: Would alternative state-space architectures, bidirectionality schemes, or temporal pooling strategies further improve performance or robustness?  </li> <li>Uncertainty and reliability: How can we estimate prediction uncertainty and assess reliability across sites, scanners, and preprocessing pipelines?  </li> <li>Integration with graph-based representations: Could learned voxel-level representations be aggregated into dynamic functional graphs and combined with GNNs for more interpretable connectivity analysis?</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#9-how-this-connects-to-the-bigger-picture-for-a-new-grad-student","title":"9. How This Connects to the Bigger Picture (For a New Grad Student)","text":"<ul> <li>Position in the landscape of brain foundation models: </li> <li>BrainMT sits in the growing space of deep backbones for fMRI, alongside transformer-based models (e.g., TFF, SwiFT) and graph-based methods (BrainGNN).  </li> <li>It emphasizes efficient long-sequence modeling, borrowing ideas from modern sequence models like Mamba, and adapting them to 4D neuroimaging.  </li> <li> <p>While not a traditional \u201cfoundation model\u201d with massive pretraining, it can serve as a strong architectural template for future brain FMs.</p> </li> <li> <p>Connections to well-known ideas: </p> </li> <li>Conceptually, BrainMT is like \u201ca video model for brain volumes\u201d: it treats fMRI as a spatiotemporal sequence, similar to video transformers but with neurobiological constraints.  </li> <li>The Mamba block provides linear-time sequence modeling, akin to efficient alternatives to transformers in NLP and vision; the transformer layer then adds global relational reasoning.  </li> <li> <p>Compared to traditional connectivity pipelines, it replaces hand-designed ROI-based features with end-to-end learned representations from raw data.</p> </li> <li> <p>Relation to integration and broader research programs: </p> </li> <li>For multimodal integration projects (e.g., integrating brain imaging with genomics or clinical data), BrainMT can act as a strong fMRI encoder, producing subject-level representations that can be fused with other modalities via late fusion or joint embedding methods.  </li> <li>Its success supports the integration baseline plan\u2019s principle of preserving modality-specific signal (learning rich fMRI representations before combining with other data types).  </li> <li> <p>The evaluation practices (consistent splits, multiple metrics, ablations) echo the plan\u2019s emphasis on careful, robust benchmarking.</p> </li> <li> <p>Why this paper is a useful reference: </p> </li> <li>It provides a concrete, well-validated example of how to design and train a modern deep architecture for 4D fMRI at scale.  </li> <li>It offers design patterns (temporal-first scanning, hybrid Mamba\u2013transformer stacks, integrated interpretability) that can inform future brain FMs and multimodal models.  </li> <li>For a new grad student, it is a good entry point into understanding how sequence modeling ideas from NLP/vision can be translated into neuroimaging.</li> </ul>"},{"location":"generated/kb_curated/papers-md/brainmt_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: Resting-state fMRI contains complex, long-range spatiotemporal patterns that can predict subject-level phenotypes such as sex and cognitive intelligence, but existing approaches either lose spatial detail (via parcellation) or are limited to short time windows (due to transformer complexity).  </li> <li> <p>Problem: There is a need for architectures that can handle long 4D fMRI sequences efficiently while capturing both local and global dependencies in brain activity.</p> </li> <li> <p>Method / model: BrainMT is a hybrid deep architecture combining convolutional blocks, a bidirectional Vision Mamba state-space module with temporal-first scanning, and a transformer block for global spatial attention.  </p> </li> <li>Method / model: The model processes 200 fMRI frames per subject at a reduced spatial resolution, builds a long token sequence with positional embeddings and a classification token, and uses Mamba to capture long-range temporal context before applying transformer attention.  </li> <li> <p>Method / model: Training uses large-scale resting-state fMRI datasets (UKB and HCP) with supervision for sex classification and cognitive intelligence regression, optimized with AdamW and standard loss functions.</p> </li> <li> <p>Results: BrainMT outperforms strong correlation-based (XGBoost, BrainNetCNN, BrainGNN, BrainNetTF) and voxel-based (TFF, SwiFT) baselines in both intelligence prediction and sex classification across HCP and UKB.  </p> </li> <li>Results: The model is more memory-efficient than SwiFT, scales linearly with the number of frames, and benefits from longer temporal context; ablations confirm the importance of its hybrid design and the specific Mamba variant.  </li> <li> <p>Results: Integrated Gradients reveal that BrainMT\u2019s predictions rely on default mode and frontoparietal regions for intelligence, and regions like STG, MFG, and precuneus for sex differences, aligning with known neuroscience.</p> </li> <li> <p>Why it matters: BrainMT demonstrates that modern sequence models like Mamba, combined with transformers, can efficiently model long 4D fMRI sequences and yield state-of-the-art predictive performance.  </p> </li> <li>Why it matters: The architecture is a promising backbone for future brain foundation models and a strong fMRI encoder that could be integrated into larger multimodal systems (e.g., combining brain imaging with genetics or clinical data).</li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/","title":"Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling","text":"<p>Authors: Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, Volodymyr Kuleshov Year: 2024 Venue: 41st International Conference on Machine Learning (ICML), PMLR 235</p>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. The paper develops long-range sequence models specifically for DNA and evaluates them on genomics tasks such as regulatory element classification and variant effect prediction.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. The main contribution is a new family of DNA foundation models (Caduceus) and underlying architectural blocks (BiMamba and MambaDNA), together with pretraining and fine-tuning strategies.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Single-modality DNA sequence (human reference genome; nucleotide-level modeling).</li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces Caduceus, a family of long-range DNA language models designed to capture two key biological symmetries: bi-directional context and reverse complement (RC) equivariance of DNA. Standard sequence models either struggle with very long genomic contexts or ignore that DNA\u2019s two strands carry equivalent information in opposite directions, which wastes data and harms generalization. Building on the Mamba structured state space model, the authors propose BiMamba for efficient bi-directional sequence modeling and MambaDNA for RC-equivariant processing, then assemble these into Caduceus variants that operate as DNA foundation models trained with masked language modeling. They pretrain on the human genome and fine-tune on a wide range of genomics benchmarks, showing that Caduceus matches or outperforms both HyenaDNA (another long-range SSM-based model) and much larger Transformer-based models such as Nucleotide Transformer v2 and Enformer, especially for tasks requiring long-range sequence context and symmetry handling. A highlight is variant effect prediction, where Caduceus competes with or exceeds 10\u00d7 larger attention-based models at long genomic distances from genes. For a new grad student, this paper is a concrete example of how to incorporate biological inductive biases (symmetries) into modern foundation-model-style architectures to unlock better performance at manageable scale.</p>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>How to build foundation-scale models that understand long DNA sequences, especially non-coding regions that regulate gene expression.</li> <li>Specifically, the model should:<ul> <li>Use bi-directional context: regulatory effects can depend on bases both upstream and downstream of a position.</li> <li>Be reverse complement (RC) aware: the two strands of DNA encode the same information in opposite directions (A\u2194T, C\u2194G), and biological assays may sequence either strand.</li> <li>Capture long-range interactions: variants up to hundreds of kilobases (or more) away from a gene can affect expression.</li> </ul> </li> <li> <p>Downstream, they particularly care about variant effect prediction (VEP)\u2014predicting whether a single nucleotide polymorphism (SNP) causally affects gene expression.</p> </li> <li> <p>Why this is hard</p> </li> <li>Long-range dependencies:<ul> <li>Transformers scale quadratically with sequence length, making it costly to model context lengths of 100k\u20131M base pairs.</li> <li>Biological regulatory effects can span these very long distances, so truncating context harms performance.</li> </ul> </li> <li>Bi-directionality:<ul> <li>Many language models are causal (left-to-right) and only see past context; for DNA, both \u201cpast\u201d and \u201cfuture\u201d bases matter symmetrically.</li> </ul> </li> <li>Reverse complement symmetry:<ul> <li>A DNA sequence and its reverse complement should yield equivalent predictions for most tasks.</li> <li>Standard models treat RC pairs as unrelated sequences, requiring explicit data augmentation and still not enforcing strict symmetry.</li> </ul> </li> <li>Data and modeling complexity:<ul> <li>Non-coding DNA is vast and noisy; many relevant signals are subtle (e.g., conservation, motif patterns).</li> <li>Tokenization choices (e.g., k-mers) can make small sequence changes look large in token space.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>Pretraining:<ul> <li>Human reference genome HG38 / GRCh37 (from the Genome Reference Consortium), split into ~34,021 segments extended to length (2^{20} \\approx 1,048,576) base pairs.</li> <li>Total scale \u2248 35 billion nucleotide tokens.</li> </ul> </li> <li> <p>Downstream benchmarks:</p> <ul> <li>Genomics Benchmarks suite (Gre\u0161ov\u00e1 et al., 2023): eight regulatory element classification tasks (e.g., mouse enhancers, human enhancers, open chromatin regions, promoters).</li> <li>Nucleotide Transformer benchmark (Dalla-Torre et al., 2023): 18 datasets, including histone mark prediction, enhancer and promoter annotations, and splice site prediction.</li> <li>Variant effect prediction (VEP) dataset derived from Enformer (Avsec et al., 2021) and Trop et al. (2023), with SNPs labeled as causal vs non-causal for gene expression using SuSiE fine-mapping.</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Single modality: DNA sequence at single-nucleotide resolution (A/C/G/T).</li> <li> <p>Outputs are task-specific labels (e.g., regulatory class, histone mark presence, variant effect).</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>Character-level tokenization: each nucleotide (A, C, G, T) is a token, avoiding k-mer tokenization.<ul> <li>Motive: k-mers make small base changes cause large token changes, which complicates learning; single-nucleotide tokens preserve locality.</li> </ul> </li> <li>For downstream tasks:<ul> <li>Sequences are trimmed or padded to task-specific lengths (e.g., 200\u20132,000 bps for Genomics Benchmarks).</li> <li>Final hidden states are pooled (e.g., mean pooling over positions) to obtain fixed-size embeddings.</li> </ul> </li> <li>For VEP:<ul> <li>They extract features from windows centered at SNP positions (e.g., 1,536 bp windows for SSM-based models) and use these embeddings with an external classifier (SVM).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Based on Structured State Space Models (SSMs)\u2014specifically the Mamba architecture (Gu &amp; Dao, 2023), which models sequences via linear state-space dynamics plus input-dependent \u201cselection\u201d mechanisms.</li> <li> <p>Mamba operates in linear time in sequence length, enabling long contexts (up to hundreds of thousands of base pairs) without quadratic cost.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li>The paper extends Mamba with new architectural components tailored to DNA:<ul> <li>BiMamba: a bi-directional variant that processes sequences in both forward and reversed order with shared parameters.</li> <li>MambaDNA: an RC-equivariant block that enforces reverse complement symmetry.</li> </ul> </li> <li> <p>These blocks are used to build Caduceus, a new family of DNA foundation models pretrained with a masked language modeling objective.</p> </li> <li> <p>Key components and innovations</p> </li> </ul> Component Role / Idea Mamba Base SSM block combining selective state space layers with a gated MLP; originally causal (uni-directional). BiMamba Applies Mamba to the sequence and its reversed version, then flips and adds outputs; uses shared projections to keep parameter count low while modeling bi-directional context. MambaDNA RC-equivariant module: splits channels, applies Mamba (or BiMamba) to forward and reverse-complement sequences with shared parameters, and recombines to guarantee RC equivariance. Caduceus-PS Fully RC-equivariant LM via parameter sharing: RC-equivariant embeddings, MambaDNA blocks, and LM head; predictions for RC inputs transform appropriately. Caduceus-Ph Uses BiMamba with RC data augmentation during pretraining and post-hoc conjoining at inference (averaging predictions on forward and RC sequences) to enforce RC invariance downstream. <ul> <li>Architectural details (intuitive)</li> <li>BiMamba:<ul> <li>Take a sequence (X) and a reversed copy.</li> <li>Run the Mamba block on both with shared in/out projections.</li> <li>Flip the reversed output back along the sequence length and add it to the forward output.</li> <li>This yields a representation that incorporates information from both directions without doubling parameters.</li> </ul> </li> <li>MambaDNA:<ul> <li>Split the channel dimension into two halves.</li> <li>Apply Mamba (or BiMamba) to the forward half and to the reverse complement of the other half, with shared parameters.</li> <li>Apply the RC transform again to the reversed output and concatenate the two halves back, resulting in a representation that is provably RC-equivariant.</li> </ul> </li> <li>Caduceus-PS:<ul> <li>Uses RC-equivariant embeddings, stacks of MambaDNA blocks with BiMamba inside, and an RC-equivariant LM head that combines channel-flipped outputs.</li> <li>Enforces RC-equivariant behavior already during pretraining, so RC data augmentation is not needed.</li> </ul> </li> <li> <p>Caduceus-Ph:</p> <ul> <li>Uses BiMamba without built-in RC equivariance in the LM.</li> <li>Relies on RC data augmentation during pretraining and post-hoc conjoining (averaging outputs for forward and RC inputs) at downstream inference to enforce RC invariance.</li> </ul> </li> <li> <p>Training setup</p> </li> <li>Pretraining objective:<ul> <li>Masked Language Modeling (MLM), similar to BERT:</li> <li>Mask 15% of tokens; among these, 80% replaced with [MASK], 10% replaced with random bases, 10% unchanged.</li> <li>Causal next-token prediction is used for some baseline comparisons (e.g., Mamba vs HyenaDNA), but Caduceus FM training is primarily MLM-based and bi-directional.</li> </ul> </li> <li>Pretraining data &amp; scale:<ul> <li>Human reference genome with long sequence segments (1k, 32k, 131k bps), keeping number of tokens per batch approximately constant across lengths.</li> <li>Several model variants with different depths and hidden dimensions; small (~hundreds of thousands to a few million parameters) compared to Transformers like Nucleotide Transformer and Enformer.</li> </ul> </li> <li>Optimization details (high level):<ul> <li>ADAM optimizer, cosine learning rate decay, learning rate around (8 \\times 10^{-3}) for Mamba-based models.</li> <li>RC data augmentation used for non-RC-equivariant models (including HyenaDNA and Caduceus-Ph during pretraining).</li> </ul> </li> <li>Fine-tuning:<ul> <li>For downstream tasks, they pool final hidden states (often mean pooling), then train task-specific heads.</li> <li>Hyperparameters (learning rate, batch size) are tuned per task using cross-validation (5-fold for Genomics Benchmarks, 10-fold for Nucleotide Transformer tasks).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This paper does not perform multimodal integration: it focuses on a single modality (DNA sequence) and does not combine genomics with other data types such as expression, imaging, or clinical variables.</p> <ul> <li>Relation to integration and the baseline plan</li> <li>Within the broader Integration Baseline Plan, Caduceus informs the \u201cGenetics embedding hygiene and attribution\u201d principle:<ul> <li>It emphasizes reverse-complement handling (RC equivariance or RC averaging) as a core inductive bias for DNA encoders.</li> <li>It uses deterministic, nucleotide-level tokenization rather than k-mers, aligning with the plan\u2019s caution about unstable tokenization for variant-level analyses.</li> </ul> </li> <li>If Caduceus embeddings are later integrated with other modalities (e.g., brain imaging, clinical phenotypes), the plan would advocate:<ul> <li>Preserving modality-specific representations (i.e., using Caduceus as a frozen or carefully fine-tuned encoder, then fusing at a later stage).</li> <li>Applying the robustness practices (standardization, residualization, consistent CV, calibrated metrics) discussed in the plan.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Pretraining analyses</li> <li>Mamba vs HyenaDNA (next-token prediction):<ul> <li>On human-genome pretraining with different sequence lengths (1k, 32k, 131k bps), Mamba achieves lower cross-entropy loss than HyenaDNA at similar model sizes.</li> <li>Mamba also appears more robust to higher learning rates, supporting its use as the core building block.</li> </ul> </li> <li>Effect of BiMamba\u2019s parameter sharing:<ul> <li>BiMamba with projection weight tying enables deeper bi-directional models for the same parameter budget.</li> <li>These deeper, tied models achieve better MLM loss than naive bi-directional Mamba (without weight tying, shallower models).</li> </ul> </li> <li> <p>Effect of RC equivariance:</p> <ul> <li>RC-equivariant language modeling (as in Caduceus-PS) leads to improved MLM loss across sequence lengths compared to non-equvariant models.</li> <li>This suggests that encoding RC symmetry directly into the model improves pretraining quality, not just downstream metrics.</li> </ul> </li> <li> <p>Genomics Benchmarks (8 classification tasks)</p> </li> <li>Baselines:<ul> <li>CNN trained from scratch, HyenaDNA, non-RC-equvariant Mamba and Caduceus backbones.</li> </ul> </li> <li> <p>Key findings:</p> <ul> <li>Across all eight tasks, Caduceus variants attain the best or near-best accuracy.</li> <li>Caduceus-Ph often provides the strongest performance, sometimes slightly surpassing Caduceus-PS and non-equvariant Caduceus.</li> <li>Example trends (qualitative):<ul> <li>Mouse enhancers: Caduceus models outperform CNN and HyenaDNA, with Caduceus-PS performing best among Caduceus variants.</li> <li>Human enhancer and promoter tasks: Caduceus models typically exceed Mamba-only and HyenaDNA baselines.</li> </ul> </li> <li>The results show that combining long-range SSMs with bi-directionality and RC handling is consistently beneficial on regulatory sequence tasks.</li> </ul> </li> <li> <p>Nucleotide Transformer benchmark (18 tasks)</p> </li> <li>Baselines:<ul> <li>Large Transformer-based models: Enformer (~252M parameters), DNABERT-2 (~117M), Nucleotide Transformer v2 (~500M).</li> <li>HyenaDNA (~1.6M parameters).</li> </ul> </li> <li>Metrics: MCC for histone marks/enhancers, F1 for promoters and splice annotation, accuracy for \u201csplice sites all\u201d.</li> <li> <p>Key findings:</p> <ul> <li>Caduceus-Ph and Caduceus-PS (~1.9M parameters) perform competitively with much larger Transformers, particularly on histone marks and regulatory annotation tasks.</li> <li>Caduceus models generally outperform HyenaDNA on most histone and regulatory tasks.</li> <li>HyenaDNA remains strong on some splice site annotation tasks, where Caduceus performance is more mixed.</li> <li>Overall, Caduceus demonstrates that carefully designed SSM-based FMs with biological inductive biases can rival or outperform huge Transformers on many genomics tasks at a fraction of the parameter count.</li> </ul> </li> <li> <p>Variant Effect Prediction (VEP) on gene expression</p> </li> <li>Setup:<ul> <li>Use embeddings from Caduceus, HyenaDNA, Nucleotide Transformer v2, and Enformer.</li> <li>For each SNP, extract embeddings from a window centered at the SNP (e.g., 1,536 bp window for SSM models, shorter effective windows for Transformer baselines), optionally concatenated with tissue information.</li> <li>Train an SVM with RBF kernel to classify whether the SNP is causal for gene expression, stratifying by distance to the nearest transcription start site (TSS): short (0\u201330k bp), medium (30\u2013100k bp), long (&gt;100k bp).</li> </ul> </li> <li>Key findings:<ul> <li>Caduceus models consistently outperform HyenaDNA across distance buckets.</li> <li>Caduceus-PS often matches or exceeds Nucleotide Transformer v2 (500M params), especially at long ranges.</li> <li>For SNPs &gt;100k bp from TSS, Caduceus even surpasses Enformer, which is a strong, supervised baseline with large receptive fields.</li> <li>These trends demonstrate that Caduceus\u2019 long-range, bi-directional, RC-aware representations are especially powerful when long genomic contexts matter most.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths</li> <li>Biologically grounded inductive biases:<ul> <li>Incorporates bi-directionality and RC equivariance directly into the architecture, aligning model invariances with DNA\u2019s physical properties.</li> </ul> </li> <li>Long-context efficiency:<ul> <li>Uses SSM-based Mamba blocks to handle sequences up to ~131k bps and beyond with linear-time scaling, enabling realistic genomic context sizes.</li> </ul> </li> <li>Strong empirical performance:<ul> <li>Outperforms or matches both other SSM-based models (HyenaDNA) and much larger Transformers (Nucleotide Transformer, Enformer) on many benchmarks.</li> <li>Particularly strong on long-range variant effect prediction, a high-impact biological task.</li> </ul> </li> <li> <p>Clear architectural story:</p> <ul> <li>Breaks down innovations into modular components (BiMamba, MambaDNA, Caduceus-PS/Ph), making it easier to reuse or extend in other settings.</li> </ul> </li> <li> <p>Limitations</p> </li> <li>Single-modality focus:<ul> <li>Only models DNA sequence; does not integrate other relevant data (RNA expression, chromatin contact maps, clinical covariates).</li> </ul> </li> <li>Task coverage:<ul> <li>Evaluations focus on classification tasks and VEP; less attention to generative uses (e.g., sequence design) or interpretability tools (e.g., motif discovery pipelines).</li> </ul> </li> <li>Complexity of RC handling:<ul> <li>While the theory is clean, implementing and debugging RC-equivariant models and post-hoc conjoining pipelines may be non-trivial in practice.</li> </ul> </li> <li> <p>Reliance on external classifiers for VEP:</p> <ul> <li>The VEP evaluation uses SVMs on top of embeddings, which might not fully exploit the capacity of the representations compared to end-to-end fine-tuning.</li> </ul> </li> <li> <p>Open Questions and Future Directions:</p> </li> <li>Multimodal integration:<ul> <li>How do Caduceus embeddings combine with other modalities (e.g., expression, epigenomics, imaging) in late-fusion or contrastive frameworks, as suggested in the integration baseline plan?</li> </ul> </li> <li>Interpretability:<ul> <li>Can the RC-equivariant and bi-directional structure be exploited for clearer attribution of variants, motif discovery, or mechanistic interpretation?</li> </ul> </li> <li>Scaling laws:<ul> <li>How do performance and sample efficiency scale with model size and context length for Mamba-based DNA FMs compared to Transformers?</li> </ul> </li> <li>End-to-end VEP:<ul> <li>Does training end-to-end predictors on top of Caduceus (instead of SVMs) further improve performance and robustness?</li> </ul> </li> <li>Generalization across species and assays:<ul> <li>How well do Caduceus models transfer to other species, tissue types, or new assay types (e.g., single-cell, new histone marks)?</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the FM landscape</li> <li>In the genomics FM space, Caduceus is part of a movement from Transformer-based models (DNABERT, Nucleotide Transformer, Enformer) toward efficient long-range architectures like HyenaDNA and SSM-based FMs.</li> <li>Conceptually, you can think of Caduceus as \u201clike BERT but for DNA, built on Mamba instead of a Transformer, and explicitly aware of DNA\u2019s symmetries\u201d.</li> <li> <p>It complements work on large language models in biology (e.g., protein LMs) by focusing on non-coding genomic sequence and variant effects.</p> </li> <li> <p>Relation to well-known ideas</p> </li> <li>Bi-directionality echoes BERT and ELMo, which improved NLP by considering both left and right context during pretraining.</li> <li>Equivariance connects to broader ML trends of embedding symmetry (e.g., rotational equivariance in vision) into architectures to improve data efficiency and generalization.</li> <li> <p>The use of SSMs like Mamba fits into a growing ecosystem of non-attention-based, long-context sequence learners.</p> </li> <li> <p>Connection to the integration baseline plan</p> </li> <li>The integration baseline plan explicitly cites Caduceus as a reference for genetics embedding hygiene:<ul> <li>RC handling (e.g., RC averaging, post-hoc conjoining) and stable tokenization are central to trustworthy variant-level modeling.</li> </ul> </li> <li>For multimodal or clinical integration projects, Caduceus-style encoders can serve as frozen or lightly fine-tuned DNA feature extractors, feeding into late-fusion models or contrastive frameworks as described in the plan.</li> <li>The evaluation discipline in this paper (clear splits, cross-validation, AUROC/AUPRC reporting) is aligned with the plan\u2019s emphasis on robust, statistically careful evaluation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/caduceus_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem</li> <li>Long-range DNA sequence modeling needs models that can handle very long contexts, use bi-directional information, and respect reverse complement symmetry.</li> <li> <p>Existing models either struggle with scaling (Transformers) or ignore key biological invariances (many sequence models).</p> </li> <li> <p>Method / Model</p> </li> <li>The authors introduce BiMamba, a parameter-efficient bi-directional extension of the Mamba SSM, and MambaDNA, an RC-equivariant module built around Mamba/BiMamba.</li> <li>They combine these components into Caduceus, a family of DNA foundation models trained with masked language modeling on the human genome.</li> <li> <p>Two main variants, Caduceus-PS (RC-equivariant via parameter sharing) and Caduceus-Ph (post-hoc RC conjoining), provide different trade-offs between strict equivariance and practical performance.</p> </li> <li> <p>Results</p> </li> <li>Mamba-based models achieve better pretraining loss than HyenaDNA and benefit from weight tying and RC equivariance.</li> <li>On Genomics Benchmarks, Caduceus variants consistently outperform CNN, HyenaDNA, and non-RC Mamba baselines across eight regulatory tasks.</li> <li>On the Nucleotide Transformer benchmark, Caduceus (~1.9M params) matches or surpasses much larger Transformer models on many histone and regulatory tasks while beating HyenaDNA on most of them.</li> <li> <p>For variant effect prediction, Caduceus models outperform HyenaDNA and Caduceus-PS exceeds the performance of a 500M-parameter Nucleotide Transformer and even Enformer at long distances to TSS.</p> </li> <li> <p>Why it matters</p> </li> <li>Caduceus shows that carefully designed, symmetry-aware, long-range SSM-based architectures can function as powerful genomic foundation models at modest scale.</li> <li>For a grad student, this paper is a template for combining domain knowledge (DNA symmetries) with modern sequence modeling techniques to achieve strong performance on biologically important tasks like variant effect prediction.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/","title":"Deep learning-based unlearning of dataset bias for MRI harmonisation and confound removal","text":"<p>Authors: Natalie K. Dinsdale, Seong Jae Hwang, Stephen M. Smith, Christian F. Beckmann, Amalio Telenti, Thomas E. Nichols, Mark Jenkinson Year: 2021 Venue: NeuroImage^https://www.sciencedirect.com/science/article/pii/S1053811920311745</p>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>MRI harmonization / confound removal. The paper introduces an adversarial \u201cunlearning\u201d framework to remove dataset (site) and other confounds from deep neural network representations while preserving task-relevant signal.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Method / representation regularization. The approach is a training strategy that can be applied to CNNs or other encoders used in downstream prediction tasks; it is not a foundation model itself.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Structural MRI (e.g., T1-weighted) from multiple datasets/sites with varying acquisition protocols and subject characteristics.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#2-executive-summary","title":"2. Executive Summary","text":"<p>Dataset/site differences and other confounds (e.g., age, sex, scanner) can bias deep learning models trained on MRI data, leading to poor generalization and spurious associations. This paper proposes an adversarial unlearning framework that encourages the network\u2019s internal representations to be invariant to specified nuisance variables (dataset, scanner, confounds) while retaining information useful for the main prediction task.</p> <p>The method uses a standard CNN backbone for the main task (e.g., age prediction) and one or more adversarial branches that try to predict nuisance variables (e.g., dataset/site). During training, gradients from the adversarial branches are reversed (via a gradient reversal layer), so the shared feature extractor is encouraged to remove information that allows accurate nuisance prediction (\u201cunlearning\u201d the dataset bias). Experiments on multi-site MRI datasets show that this approach reduces site-related differences in learned features, improves cross-dataset generalization, and allows more accurate estimation of biological effects (e.g., age) independent of site. The framework is flexible: it can unlearn multiple confounds jointly and can be combined with existing architectures and tasks.</p>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Remove non-biological biases (dataset/site, acquisition protocol, confounds) from MRI-based deep learning models so that predictions reflect underlying biology rather than scanner/site artifacts.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Entangled representations: Standard CNNs naturally encode site and protocol information along with anatomical features.  </li> <li>Site imbalance: Some sites/datasets may dominate training data, causing models to rely on dataset-specific cues.  </li> <li>Confound correlations: Biological variables of interest (e.g., age, diagnosis) may be correlated with site or scanner, making it difficult to separate their effects.  </li> <li>Classical harmonization limits: Intensity-based or ComBat-style methods operate at the image or feature level and may not fully remove deep feature-level confounds.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used: </li> <li> <p>Multiple structural MRI datasets from different sites/scanners, with variation in demographics and acquisition. (See paper for exact cohorts and sample sizes.)</p> </li> <li> <p>Modalities: </p> </li> <li> <p>Structural MRI (T1-weighted) volumes; method is agnostic to specific 3D CNN architecture.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Standard MRI preprocessing: skull stripping, normalization, and resampling to a common space before CNN ingestion.  </li> <li>CNN extracts intermediate feature maps that are shared between main and adversarial branches.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#5-model-method","title":"5. Model / Method","text":"<ul> <li>Model Type: </li> <li> <p>CNN-based predictor with adversarial branches for nuisance prediction and gradient reversal for unlearning.</p> </li> <li> <p>Key components and innovations:</p> </li> <li>Gradient reversal layer (GRL): <ul> <li>During forward pass, GRL acts as identity; during backward pass, it multiplies gradients by (-\\lambda), encouraging the shared encoder to make nuisance prediction difficult.  </li> </ul> </li> <li>Multi-confound unlearning: <ul> <li>Multiple adversarial branches can target different nuisances (dataset, scanner, sex, etc.) simultaneously.  </li> </ul> </li> <li> <p>Joint optimization: </p> <ul> <li>Loss = main task loss (e.g., age MAE) minus weighted nuisance prediction losses, balancing performance and invariance.</li> </ul> </li> <li> <p>Training setup: </p> </li> <li>Mini-batch SGD with combined loss.  </li> <li>Careful tuning of the GRL scaling parameter (\\lambda) to avoid under- or over-unlearning.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#6-multimodal-integration-aspects","title":"6. Multimodal / Integration Aspects","text":"<ul> <li>Not multimodal: </li> <li> <p>Operates solely on MRI images, but its representations can be used as inputs to multimodal models.</p> </li> <li> <p>Integration relevance: </p> </li> <li>Provides a recipe for representation-level harmonization that can be applied to encoders used in gene\u2013brain or brain\u2013behavior integration:  <ul> <li>Learn brain embeddings that are invariant to site while still predictive of age, diagnosis, or other targets.  </li> <li>Reduce spurious correlations between site and downstream genetics or behavioral variables.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#7-experiments-and-results","title":"7. Experiments and Results","text":""},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#main-findings","title":"Main findings","text":"<ul> <li>Models trained with unlearning show reduced dataset bias in feature representations, as measured by adversarial classifier accuracy and visualization of embeddings.  </li> <li>Cross-dataset generalization improves: models trained on one set of datasets perform better on held-out datasets when unlearning is applied.  </li> <li>Biological signal retention: Age and disease-related effects remain or improve in models with unlearning, suggesting that useful signal is preserved while nuisance signal is attenuated.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#comparisons","title":"Comparisons","text":"<ul> <li>Outperforms or complements traditional harmonization approaches by operating directly at the representation level within deep networks.  </li> <li>Demonstrates that adversarial unlearning is a viable alternative to purely image-level or feature-level statistical harmonization.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#8-strengths-and-limitations","title":"8. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#strengths","title":"Strengths","text":"<ul> <li>Flexible and model-agnostic: </li> <li>Can be attached to many CNN (or other encoder) architectures with minimal changes.  </li> <li> <p>Handles multiple confounds concurrently.</p> </li> <li> <p>Directly targets representation bias: </p> </li> <li> <p>Encourages invariance precisely where it matters\u2014inside the learned feature space.</p> </li> <li> <p>Improves generalization: </p> </li> <li>Better cross-dataset performance and more reliable effect estimates.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#limitations","title":"Limitations","text":"<ul> <li>Hyperparameter sensitivity: </li> <li>Performance depends on the choice of GRL scaling and loss weights.  </li> <li> <p>Over-aggressive unlearning can remove task-relevant information.</p> </li> <li> <p>Confound specification: </p> </li> <li>Requires explicit labels for nuisances (e.g., site IDs, scanner types).  </li> <li>Unobserved confounds may still leak into representations.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Relation to domain adaptation and fairness: </li> <li>Connects to adversarial domain adaptation and fairness literature where GRL-based unlearning is used to remove protected attributes (e.g., gender, race) from embeddings.  </li> <li> <p>Provides a concrete instantiation for neuroimaging where site/scanner is the \u201cprotected\u201d attribute.</p> </li> <li> <p>Impact on large-scale neuroimaging consortia: </p> </li> <li>Offers tools for harmonizing representations across datasets without discarding data or overly restricting models.  </li> <li>Supports multi-cohort analyses and meta-analyses with improved robustness to site confounds.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dinsdale_site_unlearning_2021/#10-key-takeaways","title":"10. Key Takeaways","text":"<ol> <li>Adversarial unlearning can remove dataset/site bias from MRI-based deep models while retaining task-relevant information.  </li> <li>Gradient reversal layers enable simple, end-to-end implementation of unlearning in existing encoders.  </li> <li>Representation-level harmonization complements image-level methods like ComBat and MURD, offering additional control over confounds.  </li> <li>Multi-confound unlearning is feasible, allowing simultaneous control of site, scanner, and other nuisance variables.  </li> <li>Method is directly relevant to integration pipelines that rely on site-robust brain embeddings for gene\u2013brain\u2013behavior analysis.</li> </ol>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/","title":"DNABERT-2: Efficient Foundation Model and Benchmark for Multi-Species Genomes","text":"<p>Authors: Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana V Davuluri, Han Liu Year: 2024 Venue: International Conference on Learning Representations (ICLR)</p>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. The paper develops an efficient multi-species genome foundation model that improves upon existing DNA language models through better tokenization and architecture design.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. The main contribution is DNABERT-2, a refined genome foundation model with BPE tokenization, plus the Genome Understanding Evaluation (GUE) benchmark for standardized evaluation.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Single-modality DNA sequence (multi-species genomes from 850+ species; nucleotide-level modeling with BPE tokenization).</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces DNABERT-2, a computationally efficient genome foundation model that addresses critical limitations of earlier DNA language models. The key innovation is replacing k-mer tokenization (used by DNABERT and Nucleotide Transformers) with Byte Pair Encoding (BPE), which provides better sample efficiency and computational performance while avoiding information leakage. DNABERT-2 incorporates multiple architectural improvements including ALiBi positional embeddings for unlimited sequence length, Flash Attention for efficiency, and training on multi-species genomes (850+ species). Despite having 21\u00d7 fewer parameters than state-of-the-art models and requiring approximately 92\u00d7 less GPU time for pretraining, DNABERT-2 achieves comparable or superior performance across genome understanding tasks. The paper also introduces the Genome Understanding Evaluation (GUE) benchmark, a comprehensive standardized dataset suite with 36 datasets across 9 tasks and 4 species, addressing the lack of fair comparison frameworks in the field. For new grad students, this work demonstrates how to systematically improve foundation model efficiency through better tokenization, architectural choices, and rigorous benchmarking\u2014achieving state-of-the-art results with dramatically reduced computational resources.</p>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>How to build efficient, scalable genome foundation models that can understand DNA sequences across multiple species and support diverse downstream tasks.</li> <li>Specifically, the model should:<ul> <li>Use sample-efficient tokenization: represent sequences in ways that don't waste data or leak information.</li> <li>Handle variable input lengths: avoid hard constraints on sequence length that limit applicability.</li> <li>Be computationally efficient: enable pretraining and fine-tuning on consumer-grade GPUs rather than requiring massive compute infrastructure.</li> <li>Support multi-species genomics: learn conservation and diversity patterns across the tree of life, not just human DNA.</li> </ul> </li> <li> <p>Downstream tasks include promoter prediction, enhancer identification, splice site detection, transcription factor binding, and variant effect prediction.</p> </li> <li> <p>Why this is hard</p> </li> <li>K-mer tokenization limitations:<ul> <li>Information leakage: Overlapping k-mers (e.g., 6-mers with stride 1) cause masked tokens to be partially visible in adjacent tokens, making the pretraining task easier than intended and hurting generalization.</li> <li>Sample inefficiency: Non-overlapping k-mers produce drastically different token sequences for nearly identical DNA sequences (e.g., single base shift creates completely different k-mer boundaries), forcing the model to learn redundant representations.</li> <li>Computational overhead: K-mer vocabularies are large (4^k possible k-mers), and overlapping tokenization produces very long token sequences.</li> </ul> </li> <li>Input length constraints:<ul> <li>DNABERT used learned positional embeddings limited to 512 tokens; extending to longer sequences (DNABERT-XL) was inefficient and ineffective.</li> </ul> </li> <li>Lack of standardized benchmarks:<ul> <li>Previous evaluations used inconsistent preprocessing pipelines and datasets that were either too easy (ceiling effects) or too hard (floor effects), making fair comparison impossible.</li> </ul> </li> <li>Compute resources:<ul> <li>Scaling to billions of parameters requires extensive GPU time; reducing this cost without sacrificing performance is crucial for broader adoption.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>Pretraining:<ul> <li>Multi-species genome corpus from 850+ species (bacteria, archaea, fungi, plants, and animals), following the Nucleotide Transformers dataset.</li> <li>Total scale covering diverse genomic contexts to capture cross-species conservation and variation.</li> </ul> </li> <li> <p>Evaluation benchmarks:</p> <ul> <li>GUE (Genome Understanding Evaluation): 28 datasets across diverse tasks including promoter prediction, enhancer identification, splice site detection, TF binding site prediction, histone modification prediction, and more. Input lengths range from 70 to 10,000 base pairs across 4 species.</li> <li>GUE+: Extended version with 8 additional challenging datasets for more comprehensive evaluation.</li> <li>All datasets carefully calibrated to avoid ceiling/floor effects and ensure they discriminate between model capabilities.</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Single modality: DNA sequence at nucleotide resolution (A/C/G/T).</li> <li> <p>Outputs are task-specific labels (e.g., promoter/non-promoter, enhancer activity, binding presence).</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>Byte Pair Encoding (BPE) tokenization: <ul> <li>Statistics-based compression algorithm that iteratively merges the most frequent co-occurring genome segments.</li> <li>Produces a vocabulary of 4,096 tokens learned from the genomic corpus.</li> <li>Benefits: (1) no information leakage (non-overlapping), (2) sample efficient (similar sequences get similar tokenizations), (3) computationally efficient (shorter token sequences than k-mers).</li> </ul> </li> <li>For downstream tasks:<ul> <li>Sequences are processed with task-specific lengths.</li> <li>Model embeddings are pooled or used with task-specific heads for classification/regression.</li> </ul> </li> <li>No data augmentation for reverse complement (RC) is needed during inference due to BPE's natural robustness, though RC augmentation is still used during training.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Based on BERT-style Transformer architecture with masked language modeling (MLM) pretraining objective.</li> <li> <p>Uses bidirectional self-attention to capture context from both upstream and downstream positions.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>New FM. DNABERT-2 is a ground-up redesign that replaces DNABERT's k-mer tokenization with BPE and incorporates multiple architectural improvements:</p> <ul> <li>ALiBi (Attention with Linear Biases) positional embeddings replace learned positional embeddings, removing the 512-token hard limit.</li> <li>Flash Attention for improved computational efficiency.</li> <li>Optimized model architecture with adjusted hyperparameters for better capability.</li> </ul> </li> <li> <p>Key components and innovations</p> </li> <li>BPE tokenization for genomics:<ul> <li>First application of BPE to genome foundation models, demonstrating superior sample and compute efficiency over k-mers.</li> <li>Vocabulary size: 4,096 tokens learned from multi-species genomic corpus.</li> <li>Produces significantly shorter token sequences than 6-mer tokenization.</li> </ul> </li> <li>ALiBi positional embeddings:<ul> <li>Add position-dependent bias to attention scores rather than learned embeddings.</li> <li>Enable extrapolation to arbitrary sequence lengths beyond training length.</li> </ul> </li> <li>Flash Attention integration:<ul> <li>Memory-efficient attention computation that reduces GPU memory requirements and increases throughput.</li> </ul> </li> <li> <p>Model architecture:</p> <ul> <li>DNABERT-2 (117M parameters): Main model, pretrained on multi-species genomes.</li> <li>Encoder-only architecture with bidirectional attention.</li> <li>Significantly smaller than Nucleotide Transformers (500M\u20132.5B parameters) while achieving comparable performance.</li> </ul> </li> <li> <p>Pretraining details</p> </li> <li>Objective: Masked Language Modeling (MLM)\u201415% of tokens are masked, and the model predicts the original tokens.</li> <li>Training efficiency:<ul> <li>Total GPU time: ~14 days on 8 NVIDIA RTX 2080Ti GPUs.</li> <li>Compared to Nucleotide Transformer v2: approximately 92\u00d7 less GPU time (28 days on 128 A100s).</li> </ul> </li> <li>Context window: Variable lengths up to several thousand base pairs, enabled by ALiBi.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#6-multimodal-integration-cross-modal-aspects","title":"6. Multimodal Integration / Cross-Modal Aspects","text":"<ul> <li> <p>Not applicable. DNABERT-2 is a unimodal foundation model focused exclusively on DNA sequences. </p> </li> <li> <p>Potential future integration:</p> </li> <li>DNABERT-2 embeddings could serve as a genomic feature encoder in multimodal systems that integrate DNA with:<ul> <li>Gene expression data (RNA-seq).</li> <li>Protein structures or sequences.</li> <li>Epigenomic data (ChIP-seq, ATAC-seq).</li> <li>Clinical phenotypes or imaging.</li> </ul> </li> <li>The paper does not explore these multimodal scenarios, but the efficient embeddings make DNABERT-2 a practical candidate for such integration.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":""},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#main-findings","title":"Main findings","text":"<ul> <li>GUE benchmark performance:</li> <li>DNABERT-2 outperforms DNABERT on 23 out of 28 datasets, with an average improvement of 6 absolute percentage points.</li> <li>Achieves performance comparable to Nucleotide Transformer v2-500M and v2-2.5B (state-of-the-art models with 4\u201321\u00d7 more parameters) across most tasks.</li> <li> <p>Particularly strong on: promoter prediction, enhancer identification, splice site detection, and histone modification prediction.</p> </li> <li> <p>Computational efficiency:</p> </li> <li>Parameter efficiency: 117M parameters vs 500M\u20132.5B for competing models (21\u00d7 fewer than NT v2-2.5B).</li> <li>Training efficiency: 92\u00d7 less GPU time than NT v2-2.5B during pretraining.</li> <li> <p>Inference efficiency: 3\u00d7 faster than DNABERT due to BPE's shorter token sequences.</p> </li> <li> <p>Tokenization comparison:</p> </li> <li>BPE vs overlapping k-mer: BPE eliminates information leakage, improving actual learning during pretraining.</li> <li>BPE vs non-overlapping k-mer: BPE is sample-efficient\u2014similar sequences receive similar tokenizations, unlike non-overlapping k-mers where a single base shift causes complete re-tokenization.</li> <li> <p>BPE's token sequences are 2\u20133\u00d7 shorter than 6-mer tokenization, directly translating to computational savings.</p> </li> <li> <p>Length extrapolation:</p> </li> <li>ALiBi positional embeddings enable DNABERT-2 to process sequences longer than its training length without performance degradation.</li> <li>Successfully handles sequences up to 10,000 base pairs in GUE benchmark.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#ablation-studies","title":"Ablation studies","text":"<ul> <li>Tokenization ablations:</li> <li>Compared BPE against overlapping 6-mer, non-overlapping 6-mer, and character-level tokenization.</li> <li> <p>BPE consistently outperforms k-mer variants across diverse tasks, with largest gains on tasks requiring nuanced sequence understanding.</p> </li> <li> <p>Architecture ablations:</p> </li> <li>ALiBi vs learned positional embeddings: ALiBi shows better extrapolation and no hard length limit.</li> <li>Flash Attention: Provides 1.5\u20132\u00d7 speedup with no accuracy loss.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#benchmarking-insights","title":"Benchmarking insights","text":"<ul> <li>GUE design principles:</li> <li>Standardized preprocessing pipeline ensures fair comparison across models.</li> <li>Dataset difficulty calibrated to avoid ceiling and floor effects (most tasks show 60\u201395% accuracy range, allowing discrimination).</li> <li>Covers diverse task types: binary classification, multi-class classification, regression.</li> <li>Includes both short-range (70\u2013500 bp) and long-range (2,000\u201310,000 bp) tasks.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#8-strengths-and-limitations","title":"8. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#strengths","title":"Strengths","text":"<ul> <li>Tokenization breakthrough:</li> <li> <p>BPE is the first convincing alternative to k-mer tokenization in genomics, solving information leakage and sample inefficiency problems in a principled way.</p> </li> <li> <p>Exceptional computational efficiency:</p> </li> <li> <p>Achieves state-of-the-art performance with 21\u00d7 fewer parameters and 92\u00d7 less pretraining compute, democratizing access to genome foundation models.</p> </li> <li> <p>Multi-species training:</p> </li> <li> <p>Pretraining on 850+ species captures evolutionary conservation and enables better generalization to diverse organisms.</p> </li> <li> <p>Unlimited sequence length:</p> </li> <li> <p>ALiBi positional embeddings remove hard length constraints, enabling application to very long genomic regions.</p> </li> <li> <p>Rigorous benchmarking:</p> </li> <li> <p>GUE benchmark addresses a critical gap in the field, providing standardized evaluation that enables fair model comparison.</p> </li> <li> <p>Practical usability:</p> </li> <li>Can be fine-tuned on consumer GPUs (e.g., single RTX 2080Ti), making it accessible to smaller labs.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#limitations","title":"Limitations","text":"<ul> <li>Still encoder-only:</li> <li> <p>DNABERT-2 uses MLM pretraining and is encoder-only; it cannot generate sequences (unlike autoregressive models like Evo or GPT-style DNA models).</p> </li> <li> <p>BPE vocabulary is fixed:</p> </li> <li> <p>The 4,096-token BPE vocabulary is learned once from the pretraining corpus; adapting to entirely new sequence domains (e.g., synthetic DNA) might require re-learning the vocabulary.</p> </li> <li> <p>Reverse complement handling:</p> </li> <li> <p>Unlike models with explicit RC-equivariance (e.g., Caduceus), DNABERT-2 relies on data augmentation for RC symmetry, which is less parameter-efficient.</p> </li> <li> <p>Limited to DNA sequence:</p> </li> <li> <p>Does not integrate epigenomic, transcriptomic, or proteomic data; remains unimodal.</p> </li> <li> <p>Benchmark focus on classification:</p> </li> <li> <p>GUE primarily evaluates classification and some regression tasks; variant effect prediction at scale (e.g., large VEP datasets) is less emphasized compared to Caduceus or Evo papers.</p> </li> <li> <p>Interpretability not deeply explored:</p> </li> <li>The paper focuses on performance and efficiency; mechanistic interpretability (what features the model learns) is not analyzed in detail.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":""},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#relation-to-other-work","title":"Relation to other work","text":"<ul> <li>Compared to DNABERT (Ji et al., 2021):</li> <li>DNABERT pioneered applying BERT-style pretraining to DNA but used overlapping 6-mer tokenization (information leakage) and learned positional embeddings (512-token limit).</li> <li> <p>DNABERT-2 solves both issues with BPE and ALiBi, achieving 3\u00d7 efficiency and 6-point average improvement.</p> </li> <li> <p>Compared to Nucleotide Transformers (Dalla-Torre et al., 2023):</p> </li> <li>NT used non-overlapping k-mer tokenization (avoids leakage but sample-inefficient) and scaled to 2.5B parameters.</li> <li> <p>DNABERT-2 matches NT v2-2.5B performance with 21\u00d7 fewer parameters and 92\u00d7 less compute via BPE tokenization.</p> </li> <li> <p>Compared to Caduceus (Schiff et al., 2024):</p> </li> <li>Caduceus uses Mamba SSMs with explicit RC-equivariance and targets very long-range dependencies (100k+ bp).</li> <li> <p>DNABERT-2 uses standard Transformers with BPE; it's more computationally efficient at moderate lengths (up to ~10k bp) and easier to fine-tune, but doesn't enforce RC-equivariance as a hard constraint.</p> </li> <li> <p>Compared to Evo 2 (Brixi et al., 2025):</p> </li> <li>Evo 2 is autoregressive (can generate sequences), trained on 9.3T tokens across all domains of life, and scales to 40B parameters.</li> <li> <p>DNABERT-2 is encoder-only (no generation), much smaller (117M), but more efficient for discriminative tasks and fine-tuning on modest hardware.</p> </li> <li> <p>Tokenization innovation:</p> </li> <li>BPE has been standard in NLP (GPT, BERT variants) but was not adopted in genomics until this work.</li> <li>DNABERT-2 demonstrates BPE's advantages in biological sequences, likely influencing future genome model designs.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#broader-scientific-and-practical-impact","title":"Broader scientific and practical impact","text":"<ul> <li>Democratizing genome foundation models:</li> <li> <p>By reducing parameter count and training cost by orders of magnitude, DNABERT-2 makes genome FMs accessible to smaller research groups and institutions without massive compute budgets.</p> </li> <li> <p>Enabling genomic medicine applications:</p> </li> <li> <p>Efficient models are easier to deploy in clinical settings for variant interpretation, patient stratification, and diagnostic tools.</p> </li> <li> <p>Standardizing evaluation:</p> </li> <li> <p>GUE benchmark provides a common framework for fair model comparison, accelerating progress by clarifying which innovations actually improve performance.</p> </li> <li> <p>Informing tokenization choices in other domains:</p> </li> <li> <p>The BPE vs k-mer analysis offers lessons for other biological sequence modeling problems (e.g., protein sequences, RNA sequences, or even time-series biological data).</p> </li> <li> <p>Facilitating multimodal integration:</p> </li> <li>Lightweight, efficient DNA embeddings from DNABERT-2 can be integrated with other modalities (gene expression, imaging, clinical data) in multimodal foundation models without overwhelming computational budgets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#open-questions-for-future-research","title":"Open questions for future research","text":"<ul> <li>Autoregressive DNA foundation models with BPE:</li> <li> <p>Could BPE tokenization similarly improve efficiency and sample complexity for generative models like Evo?</p> </li> <li> <p>BPE for other biological sequences:</p> </li> <li> <p>Would BPE work for protein sequences (replacing amino acid k-mers) or RNA sequences?</p> </li> <li> <p>Explicit symmetry handling:</p> </li> <li> <p>Can BPE-based models be combined with architectural equivariance (like Caduceus's RC-equivariant layers) for further gains?</p> </li> <li> <p>Interpretability of BPE tokens:</p> </li> <li> <p>What biological motifs or patterns do the learned BPE tokens correspond to (e.g., promoter elements, splice sites, transcription factor binding sites)?</p> </li> <li> <p>Scaling laws with BPE:</p> </li> <li>How does DNABERT-2's efficiency scale if parameters are increased to 1B or 10B? Would BPE maintain advantages over k-mers at larger scales?</li> </ul>"},{"location":"generated/kb_curated/papers-md/dnabert2_2024/#10-key-takeaways-for-new-ml-grad-students","title":"10. Key Takeaways for New ML Grad Students","text":"<ol> <li> <p>Tokenization matters more than you think:    In genomics, the choice between k-mer and BPE tokenization dramatically affects sample efficiency, compute requirements, and final performance. Don't just adopt existing tokenization schemes without questioning them\u2014small changes in data representation can yield order-of-magnitude improvements.</p> </li> <li> <p>Information leakage is a subtle but critical issue:    Overlapping k-mers inadvertently reveal masked information in adjacent tokens, making pretraining objectives easier than intended and hurting generalization. Always audit your preprocessing pipeline for unintended information flows.</p> </li> <li> <p>Efficiency is a first-class research goal:    DNABERT-2 achieves state-of-the-art performance with 21\u00d7 fewer parameters and 92\u00d7 less compute. Efficiency unlocks access for smaller labs, enables faster iteration, and is often scientifically interesting in its own right (what minimal representations are sufficient?).</p> </li> <li> <p>Benchmarks must be carefully designed:    The GUE benchmark addresses ceiling/floor effects, standardizes preprocessing, and covers diverse tasks\u2014illustrating that good benchmarking is hard work but essential for fair progress tracking.</p> </li> <li> <p>Architectural choices have long-term consequences:    Switching from learned positional embeddings (hard 512-token limit) to ALiBi (unlimited length) removes a fundamental constraint and enables new applications. When designing models, think about what constraints you're inadvertently baking in.</p> </li> <li> <p>Biological inductive biases matter, but so does simplicity:    While specialized architectures like Caduceus (RC-equivariant SSMs) offer advantages, DNABERT-2 shows that a standard Transformer with smart tokenization and positional embeddings can be highly competitive and easier to work with.</p> </li> <li> <p>Cross-species pretraining improves generalization:    Training on 850+ species helps the model learn evolutionarily conserved patterns and generalize better to new organisms, even if your downstream task is on a single species (e.g., human).</p> </li> <li> <p>Foundational models are about embeddings, not just end tasks:    DNABERT-2's value lies in producing high-quality DNA embeddings that can be reused across many tasks, including tasks not seen during pretraining. Think of FMs as general-purpose feature extractors.</p> </li> <li> <p>Efficiency enables multimodal integration:    Lightweight DNA embeddings can be combined with other data types (gene expression, imaging, clinical records) in multimodal systems. Efficient unimodal components are building blocks for more complex integrated models.</p> </li> <li> <p>Open-source models and benchmarks accelerate science:     By releasing code, pretrained weights, and the GUE benchmark, DNABERT-2 enables the community to build on this work rapidly. Reproducibility and accessibility are research contributions in their own right.</p> </li> </ol>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/","title":"Integrating Multimodal Data Through Interpretable Heterogeneous Ensembles","text":"<p>Authors: Yan Chak Li, Linhua Wang, Jeffrey N. Law, T. M. Murali, Gaurav Pandey Year: 2022 Venue: Bioinformatics Advances</p>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Multimodal / Integration. The paper focuses on integrating heterogeneous biomedical data types (STRING multi-omic networks and clinical EHR modalities) to improve prediction of protein function and COVID-19 mortality, and on making these integrations interpretable.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Multimodal FM or cross-modal integration (conceptual). While the work does not introduce a neural \u201cfoundation model\u201d in the modern sense, it develops a general-purpose, reusable late-integration framework (Ensemble Integration, EI) for combining modality-specific models, which fills a similar role for heterogeneous biomedical data.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Protein function prediction: multi-omic STRING network modalities (protein\u2013protein interaction, curated databases, co-expression, genomic neighborhood, co-occurrence, fusion networks).  </li> <li>Clinical prediction: EHR-derived admission features, comorbidities, vital signs, laboratory test measurements for COVID-19 patients.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces Ensemble Integration (EI), a framework for integrating multimodal biomedical data by first learning separate models for each modality and then combining those models using heterogeneous ensembles. The authors argue that common \u201cearly\u201d and \u201cintermediate\u201d integration methods, which merge data into a single representation, often lose modality-specific signals and therefore underperform when modalities have very different structures and semantics. EI instead treats each modality as its own prediction problem, trains strong local classifiers per modality, and then fuses their predictions via ensemble methods such as mean aggregation, ensemble selection, and stacking. The framework is applied to two challenging tasks: predicting protein functions from multimodal STRING network data and predicting COVID-19 mortality from multimodal EHR data. Across both applications, EI consistently outperforms single-modality models and strong early-integration baselines like Mashup, deepNF, and XGBoost. The paper also introduces an interpretation method that ranks features across all modalities by their contribution to the final ensemble, revealing biologically and clinically meaningful predictors. A new grad student should care because EI offers a practical, interpretable baseline strategy for multimodal integration that respects modality-specific structure and can be combined with more modern foundation-model embeddings.</p>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>How to integrate heterogeneous biomedical data modalities to predict important outcomes such as protein function and patient mortality.  </li> <li>For protein function prediction (PFP), the goal is to predict Gene Ontology (GO) annotations for human proteins using multiple STRING-derived networks that capture different biological relationships.  </li> <li> <p>For COVID-19 mortality prediction, the goal is to predict whether a hospitalized patient will die from COVID-19 using multimodal EHR data (admission characteristics, comorbidities, vital signs, lab tests).</p> </li> <li> <p>Why existing approaches are insufficient: </p> </li> <li>Heterogeneous semantics: Different modalities have different structures (e.g., dense gene expression matrices vs. graph-structured protein\u2013protein interactions vs. time-series vital signs), making a single joint representation hard to design.  </li> <li>Early / intermediate integration limitations: Methods that first create a single integrated network or embedding tend to emphasize agreement between modalities but can suppress modality-specific signals that are important for prediction.  </li> <li>Lack of systematic late integration: While late integration (combining predictions from modality-specific models) has been discussed, it has not been systematically developed or evaluated for multimodal biomedical prediction tasks.  </li> <li> <p>Interpretability challenges: Complex integrated models are often \u201cblack boxes,\u201d making it difficult to understand which modalities and features drive predictions, which is especially problematic in biomedical and clinical settings.</p> </li> <li> <p>Motivating idea: </p> </li> <li>Treat each modality as a first-class citizen by training specialized models that best match its structure, and then combine these local models with flexible ensemble methods.  </li> <li>Provide a principled way to interpret the resulting ensembles by quantifying how much each feature and each local model contributes to the final predictions.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used: </li> <li>STRING protein function prediction: <ul> <li>Version 11.5 of STRING, focusing on human proteins and their pairwise functional associations.  </li> <li>Predicts GO Molecular Function and Biological Process annotations for 18,866 human proteins and 2,139 GO terms (each with at least 50 annotated proteins).  </li> </ul> </li> <li> <p>COVID-19 mortality prediction (EHR): </p> <ul> <li>EHR data from 4,783 COVID-19 patients treated at Mount Sinai (March 15\u2013October 1, 2020).  </li> <li>Outcome: in-hospital mortality (27.7% positive cases).</li> </ul> </li> <li> <p>Modalities: </p> </li> <li>STRING multimodal networks (for PFP): <ul> <li>Protein\u2013protein interactions (PPI).  </li> <li>Curated database interactions.  </li> <li>Co-expression networks.  </li> <li>Genomic neighborhood.  </li> <li>Co-occurrence of orthologs across genomes.  </li> <li>Fusion events of orthologs.  </li> </ul> </li> <li> <p>EHR modalities (for COVID-19): </p> <ul> <li>Admission: demographic and baseline clinical variables (e.g., age, sex, race/ethnicity, some vital signs at admission).  </li> <li>Comorbidities: presence of other conditions (e.g., asthma, obesity).  </li> <li>Vital signs: max/min heart rate, body temperature, respiratory rate, blood pressure, oxygen saturation over the first 36 hours.  </li> <li>Laboratory tests: various lab measurements (e.g., BUN, calcium, sodium, white blood cell count), with 44 features retained.</li> </ul> </li> <li> <p>Preprocessing / representation: </p> </li> <li>STRING: Each protein is represented by its adjacency vector in each network modality; missing proteins receive all-zero vectors.  </li> <li>PFP labels: Positive examples are proteins manually annotated (non-IEA evidence) to a GO term; negatives are proteins with other annotations in the same ontology but not to that term or its relatives.  </li> <li>EHR: <ul> <li>Only features with &lt;30% missingness are retained.  </li> <li>Remaining missing values within each modality are imputed using KNNImpute with (K=5).  </li> <li>Categorical variables are one-hot encoded; continuous variables are standardized to z-scores.  </li> <li>For repeated vital signs and labs, the first values within 36 hours of hospitalization are used to enable early prediction.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li>EI is a heterogeneous ensemble framework that combines multiple base classifiers trained on separate modalities.  </li> <li>Base models are standard supervised learning algorithms (e.g., decision trees, random forests, SVMs, logistic regression, k-NN, Naive Bayes, boosting methods).  </li> <li> <p>Ensemble methods used for integration include simple averaging, ensemble selection, and stacking with various meta-learners.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li>The paper does not introduce a large-scale neural foundation model.  </li> <li> <p>Instead, it proposes a general-purpose late integration framework (EI) that can sit \u201con top of\u201d any modality-specific models, including future foundation-model encoders for genomics or clinical data.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Component Role in EI Local models per modality Capture modality-specific structure and signals Heterogeneous ensembles Combine diverse local models into a global predictor Mean aggregation Simple averaging of local predictions Caruana\u2019s ensemble selection Greedy selection of local models to maximize performance Stacking Meta-model trained on local-model predictions Feature-importance interpretation Ranks features via combined local feature and model importance <ul> <li>Training setup (as described): </li> <li>Local models (all modalities): <ul> <li>Trained with 10 standard binary classifiers in Weka: AdaBoost, decision tree (J48), gradient boosting, k-NN, SVM, random forest (RF), logistic regression (LR), PART (rule-based), Naive Bayes, Voted Perceptron.  </li> <li>Class imbalance handled by random undersampling of the majority (negative) class in training; test sets preserve original class ratios.  </li> </ul> </li> <li>Ensemble methods: <ul> <li>Mean aggregation.  </li> <li>Caruana-style ensemble selection (CES) that iteratively adds models that most improve validation performance.  </li> <li>Stacking with meta-predictors from scikit-learn: AdaBoost, decision tree, gradient boosting, k-NN, SVM (linear kernel), RF, LR, Naive Bayes, plus XGBoost as an additional meta-classifier.  </li> </ul> </li> <li>Evaluation protocol: <ul> <li>EI and heterogeneous-ensemble baselines are trained and evaluated using 5-fold nested cross-validation to separate local-model and ensemble training.  </li> <li>Mashup, deepNF, and XGBoost baselines use standard 5-fold cross-validation.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This paper is fundamentally about multimodal data integration, and EI is a concrete implementation of a late integration strategy.</p> <ul> <li>Which modalities are integrated? </li> <li>For PFP, EI integrates multiple STRING network modalities capturing complementary biological evidence (PPIs, curated databases, co-expression, genomic neighborhood, co-occurrence, fusions).  </li> <li> <p>For COVID-19 mortality, EI integrates clinical admission data, comorbidities, vital signs, and lab tests.</p> </li> <li> <p>How are they integrated? </p> </li> <li>EI follows a strict late integration pipeline:  <ol> <li>Train multiple local models per modality using algorithms appropriate for that modality.  </li> <li>Obtain prediction scores from each local model.  </li> <li>Combine these scores across modalities using heterogeneous ensemble methods (mean, ensemble selection, stacking).  </li> </ol> </li> <li> <p>No single \u201cjoint embedding\u201d or early-fusion feature vector is constructed; instead, integration happens purely at the level of model outputs.</p> </li> <li> <p>Why this integration is useful / what new capabilities it gives: </p> </li> <li>Preserves modality-specific signals by allowing each modality to use its most suitable modeling approach, rather than forcing all data into a common structure.  </li> <li>Flexible enough to incorporate arbitrary new modalities (e.g., new omics assays, imaging-derived features, or embeddings from foundation models) by simply adding new local models.  </li> <li> <p>Provides an interpretation mechanism that attributes importance to features across modalities via a unified ranking, which is crucial for biomedical trust and discovery.</p> </li> <li> <p>Relation to the Integration Baseline Plan:</p> </li> <li>The paper operationalizes the principle: \u201cPrefer late integration first under heterogeneous semantics.\u201d EI explicitly avoids premature joint spaces, instead building strong per-modality models and then combining them, which directly aligns with the plan\u2019s recommendation to preserve modality-specific signals and avoid over-aggressive early fusion.  </li> <li>EI\u2019s approach resembles the plan\u2019s suggestion of concatenating compact per-modality features and training robust tabular models\u2014but at a higher level: EI concatenates prediction scores (rather than raw features) and uses ensembles like LR, RF, and boosting as meta-models.  </li> <li>Regarding robustness and evaluation discipline, EI uses nested cross-validation, imbalance-aware metrics (AUPRC, Fmax), and formal statistical tests (Friedman\u2013Nemenyi, Wilcoxon tests), which is very much in the spirit of the plan\u2019s emphasis on disciplined evaluation and uncertainty quantification.  </li> <li>The plan\u2019s emphasis on CCA and permutation tests is conceptually related to EI\u2019s interpretation method: both aim to understand cross-modal relationships and contributions. EI\u2019s permutation-based local model ranks (LMRs) and feature ranks (LFRs) serve a similar diagnostic role for model-level integration.  </li> <li>Overall, EI provides a concrete, well-validated example of late fusion via heterogeneous ensembles, which can be adopted as the default multimodal baseline before moving to more complex contrastive or joint-embedding approaches described in the integration baseline.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Protein function prediction (PFP): Predict GO Molecular Function and Biological Process annotations for 2,139 GO terms using multimodal STRING networks.  </li> <li> <p>COVID-19 mortality prediction: Predict in-hospital death for COVID-19 patients using multimodal EHR features.</p> </li> <li> <p>Baselines: </p> </li> <li>For PFP: <ul> <li>Early integration methods: Mashup and deepNF, which fuse multiple networks into a single integrated network before classification.  </li> <li>Single-modality baselines: heterogeneous ensembles trained separately on each STRING modality.  </li> </ul> </li> <li> <p>For COVID-19 mortality: </p> <ul> <li>Early integration baseline: XGBoost trained on the concatenated feature vector across all EHR modalities (a strong tabular-data baseline).  </li> <li>Single-modality baselines: heterogeneous ensembles trained on each EHR modality individually.</li> </ul> </li> <li> <p>Key findings (PFP): </p> </li> <li>EI achieves significantly higher Fmax scores than Mashup, deepNF, and any single STRING modality across 2,139 GO terms.  </li> <li>This performance advantage persists across GO terms with varying depth, information content, and number of annotated proteins, although performance understandably decreases for terms with very few annotations.  </li> <li>Stacking with RF and LR as meta-learners tends to perform best among EI variants, consistent with previous work on heterogeneous ensembles.  </li> <li> <p>The only setting where EI underperforms is for GO terms with very few annotations (50\u2013100), where Mashup slightly outperforms EI.</p> </li> <li> <p>Key findings (COVID-19 mortality): </p> </li> <li>EI outperforms ensembles trained on individual EHR modalities and slightly surpasses the early-integration XGBoost baseline in Fmax and AUROC.  </li> <li>The best EI variant (stacking with LR) achieves a modest but meaningful improvement in Fmax over XGBoost, with a slightly better balance of precision and recall.  </li> <li> <p>EI-based predictions confirm that laboratory test features are particularly informative, but admission and comorbidity features also contribute.</p> </li> <li> <p>Interpretation results: </p> </li> <li>The proposed interpretation method identifies the top-contributing features for the best EI model in the COVID-19 task.  </li> <li>Top features include age at admission, minimum oxygen saturation, blood urea nitrogen (BUN), calcium, chloride, sodium, venous ( \\text{PCO}_2 ), respiratory rate, and atrial fibrillation\u2014variables known to be clinically relevant to COVID-19 severity and mortality.  </li> <li>There is statistically significant overlap between EI\u2019s top features and those highlighted by XGBoost\u2019s SHAP-based importance, supporting the validity of EI\u2019s explanations.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths: </li> <li>Provides a clear, general framework for late integration that can be applied to many multimodal biomedical problems.  </li> <li>Preserves modality-specific structure and signals by training specialized local models instead of forcing all data into a common representation.  </li> <li>Demonstrates strong empirical performance on two very different tasks (PFP and clinical mortality prediction) relative to established early-integration baselines.  </li> <li>Introduces a model-agnostic interpretation method that yields clinically and biologically meaningful feature rankings, increasing trust and enabling scientific insight.  </li> <li> <p>Uses rigorous evaluation practices (nested CV, appropriate metrics, statistical tests), aligning well with best-practice recommendations like DOME.</p> </li> <li> <p>Limitations: </p> </li> <li>The framework is evaluated mainly on structured data and classical machine learning models; it does not yet incorporate modern deep or foundation-model encoders for unstructured inputs (e.g., sequences, images, free text).  </li> <li>Comparisons focus on early integration (Mashup, deepNF, XGBoost); intermediate integration methods are not systematically evaluated.  </li> <li>Interpretation is only fully explored for the COVID-19 mortality task; PFP interpretations for thousands of GO terms are not examined in depth.  </li> <li>The interpretation method uses AUPRC-based ranks and percentile normalization, which may slightly bias importance toward modalities with more features.  </li> <li> <p>Computational cost may increase with many modalities and large libraries of local models, since both ensemble selection and permutation-based importance are relatively heavy.</p> </li> <li> <p>Open Questions and Future Directions: </p> </li> <li>How does EI perform when local models are foundation-model encoders (e.g., protein language models, EHR transformers, imaging FMs) instead of classical classifiers?  </li> <li>Can we design hybrid integration schemes that combine EI-style late fusion with intermediate or contrastive objectives (e.g., CCA, cross-modal contrastive learning) to better exploit cross-modal structure?  </li> <li>How can the interpretation framework be extended to handle thousands of labels (as in PFP) in a scalable way, perhaps by grouping labels or focusing on biologically meaningful subsets?  </li> <li>Can we mitigate the bias of the current ranking scheme toward feature-rich modalities, for example by normalizing importance across modalities or using multi-task feature selection?  </li> <li>What are the best practices for choosing and tuning the library of local models and meta-learners in EI when applying it to new domains or datasets?</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>Within the broader space of foundation models and multimodal integration, EI provides a strong, interpretable late-integration baseline for combining outputs from many models or modalities.  </li> <li>For genomics and proteomics, it complements work on network integration (Mashup, deepNF) by showing that keeping network modalities separate and integrating predictions can outperform aggressive early fusion.  </li> <li> <p>For clinical applications, EI offers an alternative to monolithic models like XGBoost or single neural networks, emphasizing modularity and interpretability.</p> </li> <li> <p>Relation to well-known ideas: </p> </li> <li>Conceptually, EI is like building a committee of experts, where each expert specializes in one modality, and a supervisor aggregates their opinions via ensemble methods.  </li> <li>It aligns with ensemble learning ideas such as stacking and ensemble selection, and can be viewed as a late-fusion wrapper around any set of modality-specific models, including deep FMs.  </li> <li> <p>From the perspective of modern FMs, you can imagine replacing local Weka classifiers with frozen or fine-tuned foundation models that output per-modality predictions or embeddings, which EI then integrates.</p> </li> <li> <p>Relevance to the integration baseline plan: </p> </li> <li>EI is a concrete instantiation of the plan\u2019s recommendation to start with late fusion under heterogeneous semantics, using robust tabular models and careful evaluation before moving to more complex multimodal architectures.  </li> <li>The paper\u2019s nested CV, imbalance-aware metrics, and statistical comparisons mirror the plan\u2019s emphasis on robustness and evaluation discipline, making it a strong methodological template.  </li> <li>For future multimodal FM work (e.g., combining brain FMs, genomic FMs, and clinical data), EI can serve both as a baseline pipeline and as a reusable integration layer atop pre-trained encoders.</li> </ul>"},{"location":"generated/kb_curated/papers-md/ensemble_integration_li2022/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li>Integrating heterogeneous multimodal biomedical data (e.g., STRING networks, EHR features) is crucial for accurate prediction of protein function and clinical outcomes but is challenging due to differing data structures and semantics.  </li> <li> <p>Early and intermediate integration approaches that force data into a single representation can lose modality-specific information and underperform.</p> </li> <li> <p>Method / model: </p> </li> <li>Ensemble Integration (EI) is a late integration framework that first trains multiple local models per modality and then combines their prediction scores using heterogeneous ensemble methods (mean aggregation, ensemble selection, stacking).  </li> <li>EI is model-agnostic and can work with any base classifier, enabling flexible integration of diverse modalities and algorithms.  </li> <li> <p>A novel interpretation method combines local feature ranks and local model ranks to produce a global ranking of features across all modalities.</p> </li> <li> <p>Results: </p> </li> <li>On protein function prediction with multimodal STRING data, EI significantly outperforms early integration methods (Mashup, deepNF) and single-modality baselines across thousands of GO terms.  </li> <li>On COVID-19 mortality prediction from EHR data, EI slightly but consistently outperforms a strong early-integration XGBoost baseline and individual-modality ensembles.  </li> <li> <p>The interpretation framework highlights clinically and biologically meaningful features (e.g., age, oxygen saturation, BUN, calcium), with significant overlap with SHAP-based importances from XGBoost.</p> </li> <li> <p>Why it matters: </p> </li> <li>EI offers a practical, interpretable, and extensible baseline for multimodal integration that respects modality-specific signals and uses rigorous evaluation.  </li> <li>It provides a natural way to integrate outputs from future foundation models for genomics, proteomics, and clinical data, making it highly relevant for students and researchers planning multimodal FM systems.</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/","title":"Genome Modeling And Design Across All Domains Of Life With Evo 2","text":"<p>Authors: Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher R\u00e9, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie Year: 2025 Venue: bioRxiv preprint</p>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. Evo 2 is a large-scale foundation model trained purely on DNA sequences from genomes across all domains of life, used for prediction and generation tasks that span molecular, genomic, and epigenomic levels.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. The paper introduces a new large biological foundation model (Evo 2) together with a new multi-hybrid architecture (StripedHyena 2), training recipe, and evaluation suite, rather than just applying an existing FM.</p> </li> <li> <p>Key Modalities: </p> </li> <li>DNA sequence (genomic, organelle, and metagenomic).  </li> <li>RNA and protein effects (accessed via DNA-centric modeling and downstream assays, not as separate input modalities).  </li> <li>Epigenomic profiles (chromatin accessibility) used as downstream design targets via external predictive models (Enformer, Borzoi).</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper presents Evo 2, a large-scale biological foundation model that learns directly from DNA sequences across all domains of life to support both prediction and generation of genomic functions. The authors train 7B and 40B parameter models on 9.3 trillion nucleotides with context windows up to 1 million base pairs, enabling the model to reason over entire genomes rather than short fragments. Evo 2 is evaluated on a wide range of tasks, including zero-shot prediction of mutational effects on proteins, noncoding RNAs, and organismal fitness, as well as clinical variant effect prediction for human genes like BRCA1/2. They show that Evo 2 achieves competitive or state-of-the-art performance, especially for noncoding and splice variants, without task-specific fine-tuning and while remaining alignment-free (no multiple sequence alignments). Using mechanistic interpretability tools, they reveal that the model\u2019s internal features correspond to biologically meaningful concepts such as exons, introns, transcription factor motifs, protein secondary structures, and prophage regions. Finally, the paper demonstrates genome-scale DNA generation and guided \u201cgenerative epigenomics,\u201d where Evo 2 is steered at inference time by epigenomic predictors to design sequences with specified chromatin accessibility patterns. Overall, Evo 2 is positioned as a generalist genomic language model and design engine that can underpin many downstream tasks in computational biology.</p>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>How to build a single machine learning model that can understand and design genomic DNA across the full tree of life, from bacteria to humans, at scales ranging from short motifs to entire genomes.</li> <li>The model should support zero-shot mutational effect prediction (how mutations affect fitness or function) across proteins, RNAs, and noncoding regions, without relying on multiple sequence alignments or task-specific training.</li> <li> <p>It should also enable genome-scale sequence generation, including organelle genomes and full chromosomes, and support controllable design of higher-level properties such as chromatin accessibility.</p> </li> <li> <p>Why this is hard</p> </li> <li>Data scale and diversity: Genomes vary widely in size and composition (small, compact prokaryotic genomes vs. large, intron-rich eukaryotic genomes with extensive noncoding and repetitive DNA). Capturing useful patterns across this diversity requires massive, carefully curated datasets.</li> <li>Long-range dependencies: Many genomic functions (e.g., regulatory interactions, chromatin organization) depend on relationships spread over tens of kilobases to megabases, far beyond the context lengths of typical sequence models.</li> <li>Multiple biological \u201cmodalities\u201d along the central dogma: DNA variation influences RNA, protein, and organismal phenotypes; a useful model must implicitly encode patterns across these levels while only seeing DNA sequences.</li> <li>Noncoding and regulatory elements: Noncoding variants and regulatory regions (enhancers, splice sites, chromatin features) are complex and \u201cfuzzy,\u201d making them harder to model than coding regions where the genetic code provides a more direct mapping to function.</li> <li>Safety and biosecurity: Large biological foundation models could in principle be misused (e.g., in viral design), so the training data and evaluation strategy must be carefully structured to limit harmful capabilities while retaining broad utility.</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>OpenGenome2: a large, curated genomic dataset totaling 8.84\u20139.3 trillion nucleotides, strongly expanding the earlier OpenGenome dataset.<ul> <li>Representative prokaryotic genomes (bacteria and archaea), expanded from earlier GTDB releases.</li> <li>Eukaryotic reference genomes from NCBI (16,000+ genomes), focusing on primary assemblies and non-nuclear contigs.</li> <li>Organelle genomes (mitochondria and others), plus metagenomic sequencing data.</li> <li>Function-focused subsets around coding genes (windows near genes and other functional regions) to enrich training for downstream tasks.</li> </ul> </li> <li> <p>Evaluation datasets (not all named explicitly in the excerpt, but outlined conceptually):</p> <ul> <li>Deep mutational scanning (DMS) datasets for proteins and noncoding RNAs.</li> <li>Human mRNA stability data (for decay rates).</li> <li>Variant effect prediction datasets: ClinVar, SpliceVarDB, BRCA1/2 saturation mutagenesis.</li> <li>Organismal fitness and gene essentiality datasets for bacteria, phage, and human lncRNAs.</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Primary input modality: DNA sequence, tokenized at single-nucleotide resolution.</li> <li>Implicit biological modalities captured through tasks: protein function, RNA function, noncoding regulatory function, and organismal fitness (all inferred from DNA variation).</li> <li> <p>Epigenomic modality: chromatin accessibility tracks, used via external predictors (Enformer and Borzoi) for guided design, not as training inputs for Evo 2.</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>DNA sequences are byte-tokenized and presented as long contiguous sequences, with a context length up to 1 million tokens during midtraining.</li> <li>Training uses sequence packing and a reweighted cross-entropy loss that downweights repetitive regions to better calibrate model likelihoods between repetitive and non-repetitive DNA.</li> <li>Lowercase annotations for repeats are used early in pretraining, then removed later so the model learns stable representations of repetitive vs non-repetitive regions.</li> <li>Special tokens (e.g., stitch tokens and phylogenetic tags) are used to condition the model and are masked out in the loss.</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Evo 2 is an autoregressive genomic language model built on StripedHyena 2, a multi-hybrid architecture that combines input-dependent convolutions with attention.</li> <li> <p>It is analogous to large language models (LLMs) in NLP but operates on nucleotide sequences and is optimized for extremely long contexts.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>New FM. Evo 2 extends the earlier Evo 1 models with a new architecture, larger parameter counts, vastly more training data, and a much longer context window (up to 1M bases).</p> </li> <li> <p>Key components and innovations</p> </li> <li>StripedHyena 2 architecture:<ul> <li>Uses multiple kinds of input-dependent convolution operators (Hyena-SE, Hyena-MR, Hyena-LI) interleaved with attention, arranged in a \u201cstriped\u201d pattern.</li> <li>Designed to balance quality and efficiency, giving higher throughput than Transformers and earlier StripedHyena variants, especially at long sequence lengths.</li> <li>Achieves up to 3\u00d7 speedup at 1M context compared to optimized Transformers at 40B scale.</li> </ul> </li> <li>Multi-phase training:<ul> <li>Pretraining phase at shorter context lengths (1024\u21928192 tokens), enriched for genic windows and high-information regions to learn core biological features efficiently.</li> <li>Midtraining phase for context extension to 1M tokens using rotary positional embeddings with positional interpolation and base frequency scaling, plus more whole-genome sequences.</li> </ul> </li> <li>Model sizes and configuration:<ul> <li>Evo 2 40B: ~40.3B parameters, 50 layers, hidden size ~8192, trained on 9.3T tokens.</li> <li>Evo 2 7B: ~6.5B parameters, 32 layers, trained on 2.4T tokens.</li> <li>Evo 2 1B base: smaller 1.1B variant trained on 1T tokens (also released).</li> </ul> </li> <li> <p>Loss and training tricks:</p> <ul> <li>Reweighted cross-entropy that reduces emphasis on repetitive DNA.</li> <li>Context-parallel training infrastructure (Savanna) with 3D parallelism (data, tensor, context) and mixed precision (FP8 in some components).</li> <li>A new needle-in-a-haystack evaluation to test long-context retrieval at up to 1M bases using a categorical Jacobian-based retrieval score.</li> </ul> </li> <li> <p>Training setup (as far as available)</p> </li> </ul> Aspect Details Objective Autoregressive next-token prediction on byte-tokenized DNA Parameters 1.1B, 6.5B, and 40.3B variants Tokens seen Up to 9.3T tokens for Evo 2 40B Context length 8,192 in pretraining, up to 1,048,576 (\u22481M) in midtraining Architecture StripedHyena 2 multi-hybrid (convolutions + attention + RoPE) Optimizer AdamW with cosine LR decay Infrastructure Custom stack (Savanna) with DeepSpeed, GPT-NeoX, Transformer Engine Availability Open-source model weights, training and inference code, OpenGenome2"},{"location":"generated/kb_curated/papers-md/evo2_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This paper does not focus on multimodal integration in the sense of combining heterogeneous data modalities like imaging, clinical features, and genomics into a single model or fusion pipeline. Evo 2 operates on DNA sequence alone and learns representations that implicitly capture information relevant to proteins, RNAs, and organismal fitness; epigenomic aspects (chromatin accessibility) enter only through external predictive models used to score generated sequences.</p> <p>That said, Evo 2 is highly relevant to multimodal integration pipelines such as those outlined in the Integration Baseline Plan: - Evo 2 can serve as a genomics encoder that produces compact sequence embeddings for downstream integration with other modalities using late fusion (e.g., concatenating Evo 2-derived features with imaging or clinical features for logistic regression or gradient-boosted trees). - Its strong zero-shot performance and robust variant scores align with the plan\u2019s emphasis on modality-specific modeling first, followed by disciplined late integration and careful evaluation (e.g., AUROC/AUPRC with CIs, bootstrapping or DeLong tests). - Mechanistic interpretability features (e.g., exon, splice, motif features) could act as interpretable genomic covariates in CCA or partial correlation analyses, fitting nicely into integration strategies that emphasize interpretability before moving to heavier fusion models.</p>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks</li> <li>Zero-shot mutational effect prediction across proteins, RNAs, and genomes from multiple domains of life, using changes in Evo 2 likelihood when introducing mutations.</li> <li>Human variant effect prediction (VEP):<ul> <li>Predicting pathogenic vs benign variants in ClinVar for both coding and noncoding variants, including indels and other non-SNVs.</li> <li>Predicting splice-altering variants using SpliceVarDB, split into exonic and intronic variants.</li> <li>Predicting functional consequences of variants in BRCA1 and BRCA2 from saturation mutagenesis/scanning datasets.</li> </ul> </li> <li>Organismal fitness and gene essentiality:<ul> <li>Zero-shot prediction of gene essentiality in bacteria and phage using mutational likelihood of premature stop codons.</li> <li>Prediction of human lncRNA essentiality using scrambled subsequences.</li> </ul> </li> <li>Mechanistic interpretability:<ul> <li>Training sparse autoencoders over Evo 2 representations to identify latent features corresponding to genomic and protein-level concepts.</li> </ul> </li> <li> <p>Generative tasks:</p> <ul> <li>Genome-scale generation of mitochondrial genomes, minimal bacterial genomes (e.g., M. genitalium), and yeast chromosome-scale DNA.</li> <li>Generative epigenomics: designing sequences with specified chromatin accessibility patterns using Enformer/Borzoi-guided inference-time search.</li> </ul> </li> <li> <p>Baselines</p> </li> <li>Sequence and variant models: Nucleotide Transformer, Evo 1, and specialized variant effect models such as AlphaMissense and GPN-MSA.</li> <li>For DMS and fitness tasks: State-of-the-art autoregressive protein LMs and DNA LMs used for mutational effect prediction.</li> <li> <p>For generation and structure: Comparisons to Evo 1 and to natural sequence/structure distributions (e.g., using Pfam, ESMFold, AlphaFold 3).</p> </li> <li> <p>Key findings</p> </li> <li>Zero-shot mutational effects &amp; biological priors<ul> <li>Evo 2 likelihood changes correctly reflect fundamental coding properties: strong penalties for mutations in start/stop codons, triplet codon periodicity, and decreased penalties at wobble positions.</li> <li>The model distinguishes different genetic codes (e.g., standard vs mycoplasma vs ciliate) and requires longer contexts (4\u20138 kb) to correctly infer some species-specific codes, highlighting the benefit of long context.</li> <li>Likelihood changes correlate with known constraints: non-synonymous, frameshift, and stop-gain mutations are more deleterious than synonymous mutations; deletions in essential RNAs (tRNAs, rRNAs) are more damaging than in intergenic regions.</li> </ul> </li> <li>DMS and fitness prediction<ul> <li>Evo 2\u2019s zero-shot likelihoods correlate well with DMS fitness measurements across diverse proteins and noncoding RNAs, matching or exceeding state-of-the-art protein LMs, especially for ncRNAs.</li> <li>For human mRNAs, Evo 2 likelihoods negatively correlate with mRNA decay rates (higher likelihood \u2194 more stable mRNA), and the 40B model performs better than 7B.</li> <li>Evo 2 matches Evo 1 on prokaryotic gene essentiality and extends predictive power to eukaryotic noncoding elements like lncRNAs, outperforming baselines.</li> </ul> </li> <li>Clinical variant prediction<ul> <li>In ClinVar, Evo 2 is competitive for coding SNVs and outperforms baselines for non-SNVs (indels) and noncoding variants, where many specialized models perform poorly or cannot score variants.</li> <li>In SpliceVarDB, Evo 2 achieves top zero-shot performance for both exonic and intronic splice variants.</li> <li>For BRCA1/2 datasets, Evo 2 sets or matches state-of-the-art performance, particularly when combining coding and noncoding variants.</li> <li>A supervised classifier built on Evo 2 embeddings further improves performance, achieving AUROCs up to ~0.95 on BRCA1 SNVs and outperforming AlphaMissense and other baselines.</li> </ul> </li> <li>Mechanistic interpretability<ul> <li>Sparse autoencoders over Evo 2 representations reveal features aligned with:</li> <li>Prophage and mobile genetic elements.</li> <li>ORFs, intergenic regions, tRNAs, rRNAs.</li> <li>Protein secondary structures (\u03b1-helices, \u03b2-sheets).</li> <li>Exon\u2013intron architectures (including features that fire at specific exon boundaries).</li> <li>Promoter motifs and transcription factor binding-like patterns in human genome regions.</li> <li>These features generalize to out-of-training genomes, e.g., annotating exon\u2013intron structure in the woolly mammoth genome.</li> </ul> </li> <li>Genome-scale generation<ul> <li>Evo 2 generates mitochondrial genomes with correct counts and arrangements of coding, tRNA, and rRNA genes, with diverse but structurally plausible proteins.</li> <li>It generates minimal bacterial genomes (M. genitalium scale) where ~70% of predicted genes have significant Pfam hits, a large improvement over Evo 1.</li> <li>Yeast chromosome-scale generation yields sequences with recognizable genes, introns, promoters, and tRNAs, with proteins that resemble natural yeast proteins in sequence and structure.</li> <li>Viral protein generation from human-infecting viruses remains poor by design, reflecting intentional data exclusion and risk mitigation.</li> </ul> </li> <li>Generative epigenomics and inference-time scaling<ul> <li>Using Enformer and Borzoi to score chromatin accessibility, a beam-search-based inference-time procedure produces sequences whose predicted accessibility matches desired patterns (e.g., open vs closed regions).</li> <li>Increasing inference-time compute (wider beam search, more sampled chunks) shows a log-linear improvement in design success (higher AUROC for distinguishing open vs closed regions), demonstrating inference-time scaling laws for a biological design task.</li> <li>The method is expressive enough to encode Morse-code messages in chromatin accessibility peaks, while maintaining natural sequence statistics (e.g., dinucleotide distributions).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths</li> <li>Scale and coverage: Evo 2 is trained on one of the largest curated genomic datasets to date, spanning all domains of life and multiple genomic contexts, and achieves a 1M-token context window.</li> <li>Generalist capabilities: A single model supports diverse tasks: zero-shot mutational effect prediction, clinical variant scoring, genome-scale generation, and guided epigenomic design.</li> <li>Strong performance on hard regimes: Evo 2 is particularly strong for noncoding, splice, and indel variants, where many existing models struggle.</li> <li>Mechanistic interpretability: The use of sparse autoencoders reveals interpretable features that align with biological concepts, improving trust and offering new tools for discovery and annotation.</li> <li>Open release: Model weights, training and inference code, and the OpenGenome2 dataset are released under an open license, enabling reproducibility and community-driven extensions.</li> <li> <p>Safety-aware design: The training corpus deliberately excludes pathogenic viral genomes for eukaryotic hosts, and empirical tests show degraded performance in that domain, demonstrating concrete risk mitigation.</p> </li> <li> <p>Limitations</p> </li> <li>DNA-only input: Despite implicitly touching proteins, RNAs, and epigenomics, Evo 2 only ingests DNA; it does not explicitly integrate multi-omic modalities (e.g., expression, proteomics) or other data types like imaging or clinical variables.</li> <li>Reliance on external models for function-specific design: Generative epigenomics depends on Enformer and Borzoi for scoring; Evo 2 itself is not directly conditioned on chromatin states or other annotations.</li> <li>Dataset and species bias: Although the training set is huge, it is still limited by what genomes are available and curated, and some domains (e.g., viruses infecting humans) are intentionally underrepresented.</li> <li>Compute and infrastructure requirements: Training and inference at 40B scale with 1M context is expensive and requires specialized infrastructure (Savanna, Vortex), limiting who can retrain or significantly modify the model.</li> <li> <p>Interpretability coverage: Even with SAEs, only a subset of latent features is mapped to interpretable biological concepts; many features likely encode higher-order or mixed patterns that remain to be understood.</p> </li> <li> <p>Open Questions and Future Directions:</p> </li> <li>How can Evo 2 embeddings be integrated with population-scale human genetic variation (e.g., biobanks) to further improve pathogenicity prediction and identify subtle regulatory effects?</li> <li>Can mechanistic interpretability tools (e.g., feature steering, activation clamping) be used to control specific biological properties during generation, such as alternative splicing or tissue-specific regulation?</li> <li>What are the best strategies to combine Evo 2 with other modalities (transcriptomics, epigenomics, imaging) in late fusion or two-tower architectures, in line with the integration baseline plan?</li> <li>How robust are Evo 2\u2019s predictions across underrepresented species or genomic contexts, and what additional data curation or training strategies would reduce biases?</li> <li>Can inference-time design frameworks be generalized beyond chromatin accessibility to optimize for other complex properties (e.g., protein\u2013protein interaction networks, gene circuit behavior) while maintaining safety?</li> </ul>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<p>Evo 2 sits at the frontier of foundation models for genomics, analogous to large language models in NLP but operating directly on DNA across the tree of life. Compared to earlier genomic models and protein LMs, Evo 2 pushes three main frontiers at once: scale of data, model size and architecture, and context length, enabling it to capture patterns from local motifs up to genome-scale organization. Its success on zero-shot variant effect prediction and generative tasks shows that alignment-free, DNA-only models can be powerful generalists, complementing or even competing with highly specialized models like AlphaMissense. For students interested in multimodal integration, Evo 2 can be viewed as a genome encoder: its embeddings and interpretable features are natural inputs to late-fusion or contrastive multimodal pipelines that combine genomics with imaging, behavior, or clinical data, consistent with the integration baseline plan\u2019s emphasis on strong per-modality modeling. More broadly, Evo 2 demonstrates how ideas from large-scale LMs (scaling laws, long-context training, inference-time scaling, mechanistic interpretability) can be translated into biology, opening up a path toward \u201cvirtual cell\u201d models that integrate genomics with epigenomic and transcriptomic layers.</p>"},{"location":"generated/kb_curated/papers-md/evo2_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Evo 2 is a large-scale genomic foundation model trained on ~9.3T nucleotides from genomes spanning all domains of life, with context windows up to 1M base pairs.</li> <li>The model uses a new StripedHyena 2 multi-hybrid architecture, combining input-dependent convolutions and attention to achieve efficient training and inference at long context lengths.</li> <li>Evo 2 supports zero-shot mutational effect prediction for proteins, RNAs, and noncoding regions, with likelihood changes that align with known biological constraints and genetic codes.</li> <li>On clinical variant tasks (ClinVar, SpliceVarDB, BRCA1/2), Evo 2 is competitive with or better than specialized models, especially for noncoding, splice, and non-SNV variants.</li> <li>Evo 2 embeddings can power supervised classifiers that achieve state-of-the-art performance on clinically important tasks such as BRCA1 variant classification.</li> <li>Mechanistic interpretability with sparse autoencoders reveals latent features corresponding to exons, introns, motifs, protein secondary structure, and prophages, and these features generalize to unseen genomes.</li> <li>The model can generate genome-scale DNA sequences (mitochondrial genomes, minimal bacterial genomes, yeast chromosomes) that resemble natural sequences in structure and function.</li> <li>By combining Evo 2 with epigenomic predictors and beam-search-style inference-time search, the authors demonstrate controllable \u201cgenerative epigenomics\u201d and the first inference-time scaling laws in a biological design setting.</li> <li>Evo 2 is released fully open (weights, training and inference code, data), offering a powerful foundation for downstream genomics, variant interpretation, and design tasks, while incorporating explicit risk mitigation for viral domains.</li> <li>For a new grad student, Evo 2 is a key reference for how to build, evaluate, and interpret DNA-based foundation models, and a natural starting point for projects in variant effect prediction, genome design, and multimodal integration.</li> </ul>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/","title":"Flamingo: a Visual Language Model for Few-Shot Learning","text":"<p>Authors: Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, et al. (DeepMind) Year: 2022 Venue: NeurIPS 2022</p>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Vision / VLM / Multimodal FM  </li> <li> <p>Flamingo is a visual language model (VLM) that integrates vision and language for few-shot learning on image and video understanding tasks.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration</p> </li> <li> <p>Key Modalities: </p> </li> <li>Images (high-resolution from web data)</li> <li>Videos (short clips, average 22 seconds)</li> <li>Text (interleaved captions, questions, answers)</li> </ul>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#2-executive-summary","title":"2. Executive Summary","text":"<p>Flamingo is a family of Visual Language Models (VLMs) that achieve state-of-the-art few-shot learning on image and video understanding tasks by being prompted with a few input/output examples\u2014analogous to GPT-3's few-shot text learning. The model bridges pretrained vision and language models through novel architectural components: a Perceiver Resampler that converts variable-size visual features into fixed visual tokens, and GATED XATTN-DENSE layers that condition frozen language models on visual representations via gated cross-attention. Flamingo handles arbitrarily interleaved sequences of images/videos and text, enabling natural few-shot prompting. Trained on billions of web-scraped multimodal examples (interleaved image-text from webpages, image-text pairs, video-text pairs) without task-specific annotations, a single Flamingo model achieves new state-of-the-art few-shot performance on 16 diverse benchmarks and outperforms fine-tuned models on 6 tasks despite using only 32 examples (1000\u00d7 less data). The largest model (Flamingo-80B) sets new records on VQA and captioning tasks.</p>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Current vision-language models require extensive task-specific fine-tuning with thousands of annotated examples.</li> <li>Contrastive models (CLIP) enable zero-shot classification but lack generative capabilities for open-ended tasks like captioning and VQA.</li> <li> <p>Goal: Build a model that rapidly adapts to new vision-language tasks using only a few examples, similar to GPT-3's few-shot learning for text.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Bridging vision and language: Vision encoders and language models are trained separately; connecting them effectively while preserving both pretrained knowledges is non-trivial.</li> <li>Handling interleaved multimodal sequences: Few-shot learning requires processing sequences like (image\u2081, text\u2081), (image\u2082, text\u2082), ..., (query_image, ?).</li> <li>Variable-size visual inputs: Images and videos have variable resolutions; language models expect fixed-size token sequences.</li> <li>Large-scale training data: Few-shot learning requires massive pretraining on diverse multimodal data (billions of examples).</li> <li>Training stability: Combining frozen pretrained models with new trainable components requires careful initialization and gating mechanisms.</li> </ul>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data: </li> <li>M3W (MultiModal MassiveWeb): ~43M webpages with interleaved images and text (up to 5 images per sequence, 256 tokens).</li> <li>ALIGN: 1.8B image-text pairs with alt-text descriptions.</li> <li>LTIP (Long Text &amp; Image Pairs): 312M image-text pairs with longer, higher-quality descriptions.</li> <li> <p>VTP (Video &amp; Text Pairs): 27M short videos (average 22 seconds) with sentence descriptions.</p> </li> <li> <p>Modalities: </p> </li> <li>Images: High-resolution images from webpages and image-text pairs.</li> <li>Videos: Short video clips (1 FPS sampling) with temporal embeddings.</li> <li> <p>Text: Captions, questions, answers, descriptions, interleaved with visual content.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Vision encoder: Pretrained NFNet-F6 (NormalizerFree ResNet) with contrastive pretraining; outputs 2D spatial grid flattened to 1D sequence.</li> <li>Perceiver Resampler: Converts variable number of visual features to fixed 64 visual tokens using learned latent queries.</li> <li>Text: Tokenized using language model's tokenizer; special tokens <code>&lt;image&gt;</code>, <code>&lt;EOC&gt;</code> (end of chunk).</li> </ul>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Multimodal autoregressive language model that generates text conditioned on interleaved visual and textual inputs.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>New FM. Flamingo introduces a new family of VLMs with specific architectural innovations for few-shot learning.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Vision encoder Pretrained NFNet-F6 (frozen) with contrastive pretraining Perceiver Resampler Converts variable visual features \u2192 fixed 64 visual tokens via learned queries Language model Chinchilla LM (1.4B, 7B, 70B) - frozen GATED XATTN-DENSE layers Interleaved between LM layers: gated cross-attention + gated FF, initialized at 0 Image-causal masking Text tokens attend only to immediately preceding image, not all previous images Model sizes Flamingo-3B, 9B, 80B (based on Chinchilla 1.4B, 7B, 70B) <ul> <li>Training setup (high level):</li> <li>Objective: Autoregressive text generation conditioned on interleaved visual inputs.</li> <li>Loss: Weighted sum of per-dataset negative log-likelihoods.</li> <li>Training strategy: Gradient accumulation over all datasets (outperforms round-robin).</li> <li>Few-shot adaptation: No fine-tuning; simply prompt with (image, text) example pairs followed by query.</li> </ul>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Modalities integrated: </li> <li> <p>Vision (images/videos) and text through late fusion with cross-attention.</p> </li> <li> <p>How integration works: </p> </li> <li>Vision and language processed separately (frozen encoders), then fused via GATED XATTN-DENSE layers.</li> <li>Perceiver Resampler bridges vision encoder and language model by converting visual features to tokens.</li> <li> <p>Interleaved sequences: Support arbitrary mixing of visual and textual inputs through image-causal masking.</p> </li> <li> <p>Why this integration is useful / new capabilities: </p> </li> <li>Few-shot learning: Model adapts to new tasks by seeing a few (image, text) examples.</li> <li>Open-ended generation: Can generate captions, answers, descriptions conditioned on images/videos.</li> <li>Multi-image reasoning: Processes sequences of multiple images with interleaved text (e.g., visual dialogue).</li> <li>Zero-shot capabilities: Works out-of-the-box on tasks not seen during training.</li> </ul>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Benchmarks: </li> <li> <p>16 diverse tasks: VQAv2, OK-VQA, COCO captioning, TextVQA, VizWiz, MSRVTTQA, VATEX, VisDial, HatefulMemes, etc.</p> </li> <li> <p>Baselines: </p> </li> <li> <p>Fine-tuned task-specific models, CLIP, other VLMs.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>State-of-the-art few-shot performance: Flamingo-80B sets new SotA on 9 of 16 tasks with 4-32 shots.</li> <li>Outperforms fine-tuned models on 6 tasks despite using only 32 examples (vs. thousands for fine-tuning).</li> <li>Performance by task:<ul> <li>VQAv2: 57.8% (32-shot) vs. 80.2% (fine-tuned SotA)</li> <li>COCO captioning: 113.8 CIDEr (32-shot) vs. 143.3 (fine-tuned)</li> <li>Strong video understanding on MSRVTTQA, VATEX, NextQA</li> </ul> </li> <li> <p>Scaling: Performance improves with model size (3B \u2192 9B \u2192 80B) and number of shots (0 \u2192 4 \u2192 32).</p> </li> <li> <p>Ablations:</p> </li> <li>Perceiver Resampler outperforms plain Transformer and MLP alternatives.</li> <li>Gating mechanism (tanh initialization) improves training stability and performance.</li> <li>Image-causal masking (attend only to immediately preceding image) outperforms attending to all previous images.</li> <li>Dataset weighting is crucial; gradient accumulation outperforms round-robin sampling.</li> </ul>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Powerful few-shot learning: Achieves SotA on many tasks with just 4-32 examples, dramatically reducing annotation requirements.</li> <li>Open-ended generation: Can generate free-form text (captions, answers) unlike contrastive models (CLIP).</li> <li>Handles diverse tasks: Single model works on classification, captioning, VQA, dialogue, video understanding.</li> <li>Leverages pretrained models: Effectively combines frozen vision and language models, preserving their knowledge.</li> <li>Scalable architecture: Works across model sizes (3B to 80B) with consistent improvements.</li> </ul> <p>Limitations:</p> <ul> <li>Still behind fine-tuned models: On some tasks, fine-tuned models with thousands of examples outperform Flamingo's few-shot performance.</li> <li>Compute intensive: Training on billions of examples and 80B parameters requires massive compute resources.</li> <li>Limited to vision-language: Doesn't handle other modalities (audio, 3D, biological data).</li> <li>Frozen encoders: Cannot adapt vision or language encoders to new domains without retraining.</li> </ul> <p>Open questions and future directions:</p> <ol> <li>How can few-shot performance be further improved to match or exceed fine-tuned models across all tasks?</li> <li>Can similar architectures be extended to other modalities (audio, 3D scenes, biological data)?</li> <li>How to make training more compute-efficient while maintaining few-shot capabilities?</li> <li>Can the gated cross-attention mechanism be adapted to biological multimodal settings (gene-brain-behavior)?</li> </ol>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>Flamingo demonstrates that large-scale web data training enables powerful in-context learning capabilities (previously seen only in text-only LLMs) for multimodal tasks.</li> <li> <p>Bridges the gap between contrastive models (CLIP) and generative models, offering both zero-shot and few-shot capabilities.</p> </li> <li> <p>Relation to well-known ideas: </p> </li> <li>Extends GPT-3's few-shot learning paradigm to vision-language tasks.</li> <li>Uses Perceiver-style cross-attention for vision-language bridging.</li> <li> <p>Combines ideas from frozen encoders (preserving pretrained knowledge) and trainable connectors (enabling multimodal fusion).</p> </li> <li> <p>Why this paper is a useful reference: </p> </li> <li>For multimodal FM research: Provides a blueprint for bridging vision and language FMs with minimal trainable parameters.</li> <li>For gene-brain-behavior integration: Architectural principles (Perceiver Resampler, gated cross-attention, interleaved sequences) could be adapted to biological multimodal settings.</li> <li>For few-shot learning: Demonstrates the power of large-scale web data for enabling in-context learning.</li> </ul>"},{"location":"generated/kb_curated/papers-md/flamingo_2022/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Vision-language models require extensive fine-tuning; goal is to enable few-shot learning like GPT-3.</p> </li> <li> <p>Method / model: </p> </li> <li>Flamingo is a family of VLMs (3B to 80B) that bridge frozen vision encoders and language models via Perceiver Resampler and GATED XATTN-DENSE layers.</li> <li> <p>Trained on billions of web-scraped multimodal examples (interleaved image-text, image-text pairs, video-text pairs).</p> </li> <li> <p>Results: </p> </li> <li>State-of-the-art few-shot performance on 16 benchmarks; outperforms fine-tuned models on 6 tasks with only 32 examples.</li> <li> <p>Largest model (Flamingo-80B) sets new records on VQA and captioning.</p> </li> <li> <p>Why it matters: </p> </li> <li>Shows that large-scale web data enables powerful in-context learning for multimodal tasks.</li> <li>Demonstrates effective architectural patterns for bridging frozen pretrained models.</li> <li>Provides a reference for adapting few-shot learning to biological multimodal settings (gene-brain-behavior).</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/","title":"Foundation Models for Advancing Healthcare: Challenges, Opportunities and Future Directions","text":"<p>Authors: Yuting He, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, Hao Chen Year: 2024 Venue: arXiv (survey)</p>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>General FM survey / theory  </li> <li> <p>This paper surveys healthcare foundation models (HFMs) across language, vision, bioinformatics, and multimodal domains, and discusses their challenges and future directions.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Multimodal FM or cross-modal integration (conceptual survey)  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Text (clinical notes, biomedical literature), medical images (radiology, pathology, ophthalmology, etc.), bioinformatics data (DNA, RNA, protein sequences), and multimodal clinical data (EHR, physiology).</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This survey provides a broad overview of how foundation models are transforming healthcare, framing them as healthcare foundation models (HFMs) that can be adapted to many clinical tasks. The authors first define HFMs and trace their roots from early transfer learning to modern large language models (LLMs), vision foundation models (VFMs), bioinformatics foundation models (BFMs), and multimodal foundation models (MFMs). They then review representative models and datasets in each subfield, highlighting successes such as BioBERT, AlphaFold2, SAM\u2011style medical VFMs, and multimodal models integrating images, text, and omics. The paper devotes substantial discussion to the challenges of data, algorithms, and computing infrastructure\u2014ethics, heterogeneity, cost, and environmental impact\u2014as well as open questions around fairness, robustness, and deployment. Finally, it outlines promising research directions, arguing that HFMs can drive the next generation of precision medicine if developed and governed responsibly. For a new grad student, this article is a high\u2011level map of the healthcare FM landscape.</p>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Understand how foundation models\u2014pretrained on broad healthcare data and adaptable to many tasks\u2014can advance diagnosis, prognosis, treatment, and research.  </li> <li> <p>Identify current capabilities, limitations, and research directions across language, vision, bioinformatics, and multimodal healthcare AI.  </p> </li> <li> <p>Why this is hard: </p> </li> <li>Data\u2011related challenges: <ul> <li>Healthcare data are sensitive, heterogeneous, and often siloed, complicating large\u2011scale collection and sharing.  </li> <li>Ethical, legal, and social concerns (privacy, consent, bias) strongly constrain data use.  </li> </ul> </li> <li>Algorithmic challenges: <ul> <li>HFMs must balance scale and adaptability with reliability, interpretability, and safety.  </li> <li>Domain shift, spurious correlations, and label noise can degrade performance when models are deployed.  </li> </ul> </li> <li>Computing and infrastructure: <ul> <li>Training large FMs on 3D images, WSIs, or multi\u2011omics data is computationally and environmentally expensive.  </li> <li>Many healthcare institutions lack the infrastructure to host or fine\u2011tune very large models.  </li> </ul> </li> <li>Clinical constraints: <ul> <li>HFMs must integrate into existing workflows and meet regulatory and validation requirements before impacting patient care.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>The survey organizes healthcare data for HFMs into several key categories:</p> <ul> <li>Text data: </li> <li>Biomedical literature, clinical guidelines, and medical textbooks.  </li> <li>Clinical notes (progress notes, discharge summaries, radiology and pathology reports).  </li> <li> <p>Used to train medical LLMs and language\u2011centric HFMs.  </p> </li> <li> <p>Medical imaging data: </p> </li> <li>Radiology (CT, MRI, X\u2011ray, ultrasound), pathology (whole\u2011slide images), ophthalmology, endoscopy, etc.  </li> <li> <p>Large imaging datasets with segmentation masks, labels, or reports support VFMs and MMVLFMs.  </p> </li> <li> <p>Bioinformatics and omics data: </p> </li> <li>DNA, RNA, protein sequences, 3D protein structures, and other molecular data.  </li> <li> <p>Used for BFMs such as protein or genomic FMs (e.g., AlphaFold2\u2011like models).  </p> </li> <li> <p>Multimodal clinical data: </p> </li> <li>EHR tables (labs, vitals, medications), physiological signals (ECG, EEG), and combined image\u2013text\u2013omics datasets.  </li> <li> <p>Essential for MFMs that model whole\u2011patient states.  </p> </li> <li> <p>Preprocessing / representation themes: </p> </li> <li>Tokenization and vocabulary design for clinical/biomedical text and sequences.  </li> <li>Patch and voxel\u2011based representations for images and volumes.  </li> <li>Graphs, sequences, or 3D coordinate representations for proteins and molecular structures.  </li> <li>Harmonization and standardization across institutions for multi\u2011center datasets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type (subfields reviewed): </li> <li>Language FMs / LLMs: medical variants of BERT, GPT, and other transformers (e.g., BioBERT, clinical LLMs).  </li> <li>Vision FMs (VFMs): models like SAM\u2011style or ViT\u2011based encoders for medical imaging.  </li> <li>Bioinformatics FMs (BFMs): AlphaFold2\u2011like models, protein language models, DNA/RNA sequence models.  </li> <li> <p>Multimodal FMs (MFMs): models integrating text, images, omics, and clinical variables.  </p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>This is a survey, summarizing many HFMs rather than introducing a single new model.</p> </li> <li> <p>Key conceptual components:</p> </li> </ul> Aspect Details Pretraining Large\u2011scale self\u2011supervised or weakly supervised objectives on broad healthcare data Adaptation Fine\u2011tuning, prompting, and parameter\u2011efficient methods (e.g., adapters, LoRA) Applications Diagnosis, prognosis, treatment planning, report generation, drug discovery, and more Evaluation Domain\u2011specific benchmarks, robustness tests, and clinical validation studies <ul> <li>Training setup themes: </li> <li>Self\u2011supervised learning (masked language modeling, masked image modeling, contrastive learning).  </li> <li>Multi\u2011task and continual learning strategies to adapt HFMs to new tasks while retaining prior knowledge.  </li> <li>Use of transfer learning and model reuse across related tasks and modalities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>While much of the survey covers unimodal FMs, multimodal HFMs are a central focus for future directions.</p> <ul> <li>Modalities integrated: </li> <li>Images + text (e.g., radiology report generation).  </li> <li>Text + omics (e.g., linking genomic variants with phenotypes).  </li> <li> <p>Images + omics + clinical variables in comprehensive patient\u2011level models.  </p> </li> <li> <p>Integration strategies discussed: </p> </li> <li>Early fusion of embeddings from different modalities.  </li> <li>Cross\u2011attention and joint latent spaces for intermediate fusion.  </li> <li>Late fusion via ensemble or decision\u2011level integration for heterogeneous modalities.  </li> <li> <p>CLIP\u2011style and other contrastive learning approaches for image\u2013text alignment.  </p> </li> <li> <p>New capabilities enabled: </p> </li> <li>Richer patient representation and more accurate prediction by combining multiple data sources.  </li> <li>Cross\u2011modal retrieval (e.g., finding images that match textual descriptions or molecular profiles).  </li> <li>Enhanced interpretability when multimodal signals agree or disagree in clinically meaningful ways.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>As a survey, the paper does not present new experiments, but summarizes outcomes from many HFMs.</p> <ul> <li>Tasks reviewed: </li> <li>Improved diagnosis and prognosis from imaging and text.  </li> <li>Accurate protein structure prediction and molecular property forecasting.  </li> <li>Automated or assisted report generation and documentation.  </li> <li> <p>Drug discovery and treatment recommendation tasks.  </p> </li> <li> <p>High\u2011level trends: </p> </li> <li>HFMs consistently outperform smaller, task\u2011specific models when sufficient pretraining data are available.  </li> <li>Zero\u2011shot and few\u2011shot capabilities enable deployment in settings with limited labeled data.  </li> <li>However, performance can degrade sharply when task distributions differ from pretraining data, highlighting the importance of robust evaluation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths (of HFMs and the survey):</p> <ul> <li>Presents a comprehensive taxonomy of HFMs across language, vision, bioinformatics, and multimodal domains.  </li> <li>Explicitly links technical developments (e.g., transformers, self\u2011supervision) to concrete healthcare applications.  </li> <li>Identifies both opportunities (precision medicine, automation, discovery) and systemic challenges.  </li> <li>Serves as a high\u2011level entry point for researchers from different backgrounds.</li> </ul> <p>Limitations and challenges (field\u2011level):</p> <ul> <li>Data availability is constrained by privacy, ethics, and heterogeneity, limiting the diversity of pretraining corpora.  </li> <li>Algorithmic issues such as hallucination, bias, and lack of interpretability are particularly serious in clinical contexts.  </li> <li>Computing requirements raise questions about sustainability, equity of access, and reproducibility.  </li> <li>Regulatory frameworks are still catching up with the capabilities and risks of HFMs.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can we design HFMs that are simultaneously scalable, reliable, and interpretable in clinical settings?  </li> <li>What governance structures and data\u2011sharing mechanisms are needed to support responsible, multi\u2011institutional pretraining?  </li> <li>How can HFMs be adapted and validated for under\u2011served populations and low\u2011resource health systems?  </li> <li>What new architectures and objectives are needed to incorporate causal reasoning and mechanistic knowledge into HFMs?  </li> <li>How should we evaluate the real\u2011world clinical impact of HFMs beyond traditional accuracy metrics?</li> </ol>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>This survey sits at the intersection of AI and healthcare policy, offering a bird\u2019s\u2011eye view of HFMs rather than focusing on a single modality or task.  </li> <li>It complements more focused reviews on medical MLLMs or MMFMs by connecting them to broader trends in healthcare AI.  </li> <li>Relation to well-known ideas: </li> <li>Builds directly on the concept of foundation models articulated by Bommasani et al., extending it to healthcare.  </li> <li>Discusses iconic HFMs such as BERT, CLIP, SAM, AlphaFold2, and domain\u2011specific medical LLMs and VFMs.  </li> <li>Why this paper is a useful reference: </li> <li>Helps new researchers understand the big picture of how different kinds of HFMs fit together and where healthcare is headed.  </li> <li>Encourages critical thinking about the technical, ethical, and infrastructural requirements for deploying HFMs safely.</li> </ul>"},{"location":"generated/kb_curated/papers-md/fm_general_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Healthcare needs AI models that can generalize across diverse tasks and modalities, but current systems are often narrow and brittle.  </p> </li> <li> <p>Method / model (conceptual): </p> </li> <li>Healthcare foundation models (HFMs) apply the foundation\u2011model paradigm\u2014large\u2011scale pretraining and broad transfer\u2014to clinical text, images, omics, and multimodal data.  </li> <li> <p>The survey categorizes HFMs into language, vision, bioinformatics, and multimodal branches, highlighting representative models and datasets.  </p> </li> <li> <p>Results / insights: </p> </li> <li>HFMs show strong performance and data efficiency, enabling zero\u2011shot and few\u2011shot adaptation across many healthcare tasks.  </li> <li> <p>Yet they face significant challenges in data governance, algorithmic robustness, fairness, interpretability, and deployment.  </p> </li> <li> <p>Why it matters: </p> </li> <li>HFMs are poised to become central infrastructure for future healthcare AI, but realizing this potential will require coordinated advances in data, algorithms, computing, and regulation.  </li> <li>This survey offers a strategic overview for anyone aiming to contribute to that evolution.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/","title":"Generator: A Long-Context Generative Genomic Foundation Model","text":"<p>Authors: Wei Wu, Qiuyi Li, Mingyang Li, Kun Fu, Fuli Feng, Jieping Ye, Hui Xiong, Zheng Wang Year: 2025 Venue: arXiv (preprint)</p>"},{"location":"generated/kb_curated/papers-md/generator_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. The paper develops a large generative language model over eukaryotic DNA sequences and evaluates it on a broad set of genomics prediction and design tasks.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. The main contribution is a new 1.2B-parameter generative genomic foundation model with long context, specialized tokenization, and a carefully curated pretraining corpus.</p> </li> <li> <p>Key Modalities: </p> </li> <li>DNA sequence (eukaryotic genomes, gene regions vs whole-genome segments).  </li> <li>Derived protein sequences and enhancer activity measurements are used mainly as downstream evaluation targets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces Generator, a large generative foundation model trained directly on eukaryotic DNA sequences to understand and design functional genomic sequences. The authors argue that existing genomic language models either lack generative ability, have short context, or are trained on relatively narrow datasets, which limits their usefulness for realistic genomic tasks. Generator is a 1.2B-parameter transformer decoder with a context length of about 98k base pairs, trained on 386 billion nucleotides from annotated gene regions of diverse eukaryotic species using next-token prediction and a 6\u2011mer tokenizer. The model achieves state-of-the-art performance on standard genomic benchmarks (Genomic Benchmarks, Nucleotide Transformer tasks) and on new \u201cGener\u201d tasks that probe long-range sequence understanding. Beyond benchmarks, Generator can generate DNA coding sequences whose translated proteins have realistic structure and statistics, and can design enhancer sequences with controllable activity levels. Overall, the work demonstrates that a large, long-context generative DNA model can serve as a versatile tool for genomic analysis and sequence design, pointing toward future applications in synthetic biology and precision genomics.</p>"},{"location":"generated/kb_curated/papers-md/generator_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Build a general-purpose generative model of eukaryotic DNA that can both understand genomic sequences (for prediction tasks) and generate new, functional sequences (for design tasks).  </li> <li> <p>Specifically, the model should handle long genomic contexts (tens of kilobases), capture the semantics of gene regions and regulatory elements, and support downstream tasks like classification, next\u2011k\u2011mer prediction, protein-coding sequence generation, and enhancer design.</p> </li> <li> <p>Why existing approaches are limited: </p> </li> <li>Many prior genomic models are masked language models (MLMs) trained with BERT-style objectives; they are strong for understanding but weaker or awkward for generation.  </li> <li>Context lengths are often only hundreds to a few thousand base pairs, which is too short for many realistic gene and regulatory contexts that span tens of thousands of base pairs.  </li> <li>Some generative models like HyenaDNA, megaDNA, and Evo are either limited to specific organism groups (e.g., bacteriophages, prokaryotes/viruses, or human-only) or use relatively small models or datasets, leaving a gap for large-scale eukaryotic generative models.  </li> <li> <p>Naively training on entire genomes can flood the model with non-gene, low-information regions, potentially hurting downstream performance even if pretraining loss decreases.</p> </li> <li> <p>Why this is hard (modeling and data challenges): </p> </li> <li>DNA sequences are extremely long and lack clear word boundaries, making tokenization and context management nontrivial.  </li> <li>Computational cost scales badly with sequence length in standard transformers, so long-context training is expensive.  </li> <li>Functional regions (genes, promoters, enhancers) form only a small fraction of the genome; most bases are relatively redundant or low-entropy, so choosing what to train on matters.  </li> <li>Evaluating generative quality is difficult because there is not always a ground truth for \u201ccorrect\u201d generated DNA; one must rely on indirect metrics (e.g., downstream performance, protein structure statistics, enhancer activity predictions).</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used (pretraining): </li> <li>DNA sequences are drawn from all eukaryotic organisms in the RefSeq database.  </li> <li> <p>Two main strategies are compared:  </p> <ul> <li>Gene Sequence Training (Scheme 1): Uses annotated gene regions (including protein-coding genes, various RNAs, and regulatory elements like promoters and enhancers). This yields about 386 billion nucleotides and is the configuration used for Generator.  </li> <li>Whole Sequence Training (Scheme 2): Mixes gene and non-gene regions from all eukaryotes, totaling about 2 trillion nucleotides, producing the Generator-All variant.  </li> </ul> </li> <li> <p>Datasets used (downstream): </p> </li> <li>Nucleotide Transformer (NT) tasks: A suite of genomics classification tasks (original and revised versions) covering promoters, enhancers, splice sites, chromatin marks, etc., across many species.  </li> <li>Genomic Benchmarks: Primarily human-centric tasks such as enhancer vs non-enhancer, promoter vs non-promoter, regulatory element classification, and species discrimination.  </li> <li>Gener tasks (proposed by this paper): <ul> <li>Gene classification: Predict gene type from sequences of length 100\u20135000 bp.  </li> <li>Taxonomic classification: Predict taxonomic group from sequences of length 10,000\u2013100,000 bp containing both gene and non-gene regions.  </li> </ul> </li> <li>Central dogma tasks: Protein-coding DNA sequences for specific protein families (Histone and Cytochrome P450) from UniProt and RefSeq.  </li> <li> <p>Enhancer design: The DeepSTARR dataset of enhancers with measured activity values (developmental vs housekeeping), including train/validation/test splits from the original DeepSTARR work.</p> </li> <li> <p>Modalities and representations: </p> </li> <li>Core input: DNA sequence, represented via a 6\u2011mer tokenizer (each token is a contiguous 6\u2011base string).  </li> <li>Protein-level evaluation: DNA sequences are translated into amino acid sequences using the genetic code; protein language models and structure prediction tools are applied downstream.  </li> <li> <p>Enhancer activity: numerical activity values (log2-transformed) from DeepSTARR, plus textual prompts <code>&lt;high&gt;</code> / <code>&lt;low&gt;</code> used as conditioning tokens in sequence design experiments.</p> </li> <li> <p>Preprocessing / representation details: </p> </li> <li>For gene sequence training, the authors select annotated gene regions and treat them as semantically rich de facto \u201csentences\u201d for the model.  </li> <li>For whole sequence training, the model sees both gene and non-gene sequences directly.  </li> <li>The 6\u2011mer tokenizer is applied with a random starting offset from 0 to 5 for each sample so that the model is not tied to a fixed phase of the genomic coordinate system.  </li> <li>Long sequences are chunked into segments respecting the maximum token context.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Autoregressive transformer decoder, broadly following the LLaMA-style architecture for causal language modeling, adapted to DNA.</p> </li> <li> <p>New vs existing FM: </p> </li> <li>This is a new foundation model, called Generator, designed specifically for eukaryotic DNA.  </li> <li> <p>It is trained from scratch with DNA-specific choices for tokenization, data selection, and context length.</p> </li> <li> <p>Key architectural configuration (approximate):</p> </li> </ul> Component Value / Choice Architecture Transformer decoder (LLaMA\u2011like) Layers 26 Hidden size 2048 MLP / intermediate size 5632 Attention heads 32 (with 8 KV heads) Vocabulary size 4128 (6\u2011mer tokens) Max token context 16,384 tokens Max base-pair context \u2248 98,304 bp (because each token is 6 bp) Positional encoding RoPE (rotary position embeddings) Activation SiLU <ul> <li>Training objective and setup: </li> <li>Pretraining objective is next-token prediction (NTP) on 6\u2011mer tokens, analogous to language modeling in NLP.  </li> <li>Batch size: about 2 million tokens per batch.  </li> <li>Optimizer: AdamW with standard \u03b2 values and weight decay, using cosine learning rate schedule with warmup.  </li> <li>Training spans 6 epochs over 386B tokens (gene-only scheme), totaling \u2248185k steps.  </li> <li> <p>Implementation uses FlashAttention and Zero Redundancy Optimizer (ZeRO) to make long-context training efficient on GPUs (32 A100s).</p> </li> <li> <p>Key components and innovations: </p> </li> <li>Data selection strategy: Emphasis on gene regions only (Generator) versus gene + non-gene (Generator-All), showing that restricting to functional sequences can increase downstream performance even if pretraining loss is higher.  </li> <li>Tokenization study: Systematic comparison of single-nucleotide, k\u2011mer, and BPE tokenizers for causal DNA LMs; finds that 6\u2011mer tokenization gives the best generative performance in NTP settings.  </li> <li>Long-context capability: By combining 6\u2011mer tokens with a large context window (16k tokens \u2248 98k bp), the model can see gene-scale and sub-chromosomal contexts in a single forward pass.  </li> <li> <p>Generative downstream pipelines: </p> <ul> <li>Fine-tuning for central dogma tasks, i.e., generating protein-coding DNA whose translated proteins look realistic to protein language models and structure predictors.  </li> <li>Prompt-conditioned enhancer design, where <code>&lt;high&gt;</code> / <code>&lt;low&gt;</code> prompts steer the model toward sequences with desired activity.</li> </ul> </li> <li> <p>Fine-tuning / downstream usage: </p> </li> <li>For classification benchmarks, the model is fine-tuned using a linear head on top of embeddings (e.g., of an end-of-sequence token), with hyperparameter search over learning rates and batch sizes.  </li> <li>For central dogma and enhancer design, a supervised fine-tuning (SFT) stage adapts the model to particular protein families or enhancer datasets, after which autoregressive generation is used with temperature and nucleus sampling controls.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Is this paper multimodal? </li> <li>The core model operates on single-modality DNA sequence data.  </li> <li> <p>Protein sequences and enhancer activity measurements appear as derived evaluation signals, not as fully co-modeled input modalities in a single end-to-end multimodal architecture.</p> </li> <li> <p>Relation to multimodal / integration ideas: </p> </li> <li>Although Generator itself is not a multimodal model, its embeddings and generative outputs could be used in late fusion pipelines with other modalities (e.g., chromatin marks, expression, imaging) as described in the integration baseline plan, where per-modality representations are concatenated and fed into shallow models (logistic regression, GBDTs).  </li> <li>The emphasis on semantically rich gene regions is conceptually similar to the plan\u2019s principle of preserving modality-specific signal\u2014here, \u201cfunctional DNA segments\u201d act as the high-value \u201cmodality\u201d within the genome, and adding non-gene regions acts like injecting noise.  </li> <li>The careful evaluation practices (cross-validation, multiple benchmarks, explicit metrics) echo the baseline plan\u2019s focus on robustness and disciplined evaluation, even though the work is not explicitly an integration study.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tokenizer and objective experiments (Next K-mer Prediction): </li> <li>The authors train multiple models with identical architecture but different tokenizers (single nucleotide, various k\u2011mers, BPE with different vocabulary sizes) and evaluate on next k\u2011mer prediction tasks.  </li> <li>They show that k\u2011mer tokenizers outperform BPE, and within k\u2011mers, the 6\u2011mer tokenizer gives the best accuracy across different input lengths.  </li> <li> <p>They also compare a large Mamba-based state space model to transformer baselines and find that, despite its longer context and efficiency, it does not outperform the transformer with 6\u2011mer tokens as the input length grows.</p> </li> <li> <p>Gene-only vs whole-genome training (Generator vs Generator-All): </p> </li> <li>While the whole-sequence model (Generator-All) achieves lower pretraining loss, it underperforms Generator on almost all downstream tasks.  </li> <li>The authors argue that non-gene regions are often redundant or non-functional, so including them may dilute the high-information gene signal, effectively \u201ccontaminating\u201d the data.  </li> <li> <p>This supports the idea that curated, semantically rich pretraining data can be more valuable than raw volume.</p> </li> <li> <p>Benchmark evaluations (Nucleotide Transformer tasks and Genomic Benchmarks): </p> </li> <li>On both the revised and original Nucleotide Transformer tasks, Generator generally achieves state-of-the-art or top-tier performance, often surpassing Enformer, DNABERT\u20112, HyenaDNA, Nucleotide Transformer variants, Caduceus, and GROVER.  </li> <li>On Genomic Benchmarks (human-focused classification tasks), Generator again performs at or near the top; smaller specialized models like Caduceus sometimes come close but do not consistently dominate.  </li> <li> <p>The Gener tasks (gene and taxonomic classification) further highlight Generator\u2019s strengths, especially for long-range sequence understanding, where it reaches very high weighted F1 scores, including near-perfect performance on taxonomic classification.</p> </li> <li> <p>Central dogma experiments (protein-coding sequence generation): </p> </li> <li>After fine-tuning on DNA sequences encoding specific protein families (Histones and Cytochrome P450), Generator is used to generate new coding sequences.  </li> <li> <p>Translating these sequences to proteins and analyzing them reveals:  </p> <ul> <li>Length distributions of generated proteins match those of natural families.  </li> <li>Protein language model perplexities (from Progen2) for generated sequences closely match those of natural proteins and differ from random shuffled controls.  </li> <li>AlphaFold3 predictions and Foldseek structure search show many generated proteins with high TM-scores (&gt;0.8) and high confidence, even when sequence identity is low (&lt;0.3), indicating that the model is not simply memorizing training sequences.</li> </ul> </li> <li> <p>Enhancer design experiments: </p> </li> <li>A predictor fine-tuned from Generator on DeepSTARR enhancer data achieves higher Pearson correlations between predicted and measured enhancer activity than DeepSTARR itself and NT\u2011multi, setting a new state of the art.  </li> <li>A further SFT stage adds <code>&lt;high&gt;</code> and <code>&lt;low&gt;</code> prompts to condition the model on desired activity levels.  </li> <li> <p>Generated enhancers labeled <code>&lt;high&gt;</code> and <code>&lt;low&gt;</code> show clear separation in predicted activity distributions relative to each other and to natural enhancers, suggesting that Generator can perform prompt-guided enhancer design.</p> </li> <li> <p>Overall empirical message: </p> </li> <li>Generator is a strong generalist across many genomic classification benchmarks and competent as a generative model for both coding and regulatory sequences, with particular strengths stemming from its long context, curated pretraining data, and 6\u2011mer tokenization.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<ul> <li>Strengths: </li> <li>Long-context, large-scale generative model tailored to eukaryotic DNA, filling a gap left by prior prokaryotic or short-context genomic models.  </li> <li>Careful tokenization and data selection studies, providing empirical guidance (e.g., 6\u2011mer tokens, gene-only data) that can inform future genomic FMs.  </li> <li>Strong, consistent performance across a wide range of benchmarks, including newly proposed long-context tasks.  </li> <li>Demonstrated ability to generate functional-like coding sequences and to perform prompt-conditioned enhancer design, moving beyond pure prediction into actionable sequence design.  </li> <li> <p>Extensive experimental detail (hyperparameter searches, cross-validation, ablations) that improves reproducibility and reliability.</p> </li> <li> <p>Limitations: </p> </li> <li>Pretraining focuses exclusively on eukaryotic genomes, leaving prokaryotic and viral DNA to other models like Evo; no unified genome-scale model across all domains of life is presented.  </li> <li>The model is computationally heavy (1.2B parameters, long sequences, many GPU hours), which may limit adoption in resource-constrained labs.  </li> <li>Most validations of generative quality are in silico (protein LMs, AlphaFold, enhancer predictors); there is limited or no wet-lab validation of generated sequences.  </li> <li>Despite long context, the model is still trained on 1D sequence alone, without explicit modeling of 3D genome structure, epigenetic states, or cellular context.  </li> <li> <p>As with other large models, interpretability of what the model has learned about regulatory grammar and long-range interactions remains challenging.</p> </li> <li> <p>Open Questions and Future Directions: </p> </li> <li>How would a joint eukaryotic + prokaryotic + viral Generator behave, and what design choices would be needed to balance these domains?  </li> <li>Can Generator\u2019s representations be combined with other modalities (chromatin accessibility, expression, epigenetics, single-cell data) in a systematic late-fusion or contrastive setup to improve downstream tasks like disease prediction?  </li> <li>What interpretability tools can be developed to probe the model\u2019s understanding of motifs, regulatory grammar, and long-range enhancer\u2013promoter interactions?  </li> <li>How robust are Generator\u2019s predictions and designs across different species and genomic contexts, especially for non-model organisms with sparse annotations?  </li> <li>Can Generator be adapted for clinical applications, such as prioritizing noncoding variants in GWAS regions or designing therapeutic regulatory sequences, and what safety/ethics issues would arise?</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape of foundation models in genomics: </li> <li>Generator sits alongside models like Nucleotide Transformer, HyenaDNA, Caduceus, and Evo as part of the emerging ecosystem of genomic foundation models.  </li> <li>Compared to large masked LMs (e.g., DNABERT\u20112, NT), Generator emphasizes autoregressive generation, long context, and curated gene-only training data, making it particularly suited for both understanding and designing DNA sequences.  </li> <li> <p>It complements Evo, which targets prokaryotic and viral genomes, by focusing on the more complex eukaryotic genomic setting.</p> </li> <li> <p>Conceptual analogies for intuition: </p> </li> <li>You can think of Generator as a GPT-style model for eukaryotic DNA, where tokens are 6\u2011mer substrings rather than words, and the context window spans whole genes or multi-gene regions instead of paragraphs.  </li> <li> <p>The central dogma experiments are somewhat analogous to asking a text model to generate syntactically valid and semantically coherent stories that pass external quality checks, except here the \u201ccheckers\u201d are protein LMs and structure predictors.</p> </li> <li> <p>Relevance to integration and broader research programs: </p> </li> <li>The model\u2019s strong sequence-level representations could serve as a genomic backbone in larger multimodal systems that integrate DNA with other omics or imaging modalities via late fusion, as recommended in the integration baseline plan.  </li> <li>Its success supports the broader thesis that domain-specific, long-context FMs can provide robust building blocks for downstream applications, from basic gene regulation studies to clinical genomics and synthetic biology.</li> </ul>"},{"location":"generated/kb_curated/papers-md/generator_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: Existing genomic language models often lack generative capability, have short context windows, or are limited in organismal scope, constraining their usefulness for realistic eukaryotic genomics tasks.  </li> <li> <p>Problem: Training directly on whole genomes may emphasize low-information non-gene regions, potentially hurting performance even when pretraining loss looks better.</p> </li> <li> <p>Method / model: Generator is a 1.2B-parameter transformer decoder with \u224898k bp context length, trained with next-token prediction on 386B nucleotides of eukaryotic gene regions.  </p> </li> <li>Method / model: A systematic study of tokenizers finds that 6\u2011mer tokens work best for causal DNA language modeling, beating both single-nucleotide and BPE tokenization.  </li> <li> <p>Method / model: A comparison between gene-only and whole-genome pretraining shows that focusing on semantically rich gene regions yields better downstream performance than including vast non-gene regions.</p> </li> <li> <p>Results: Generator achieves state-of-the-art or near-SOTA performance on Nucleotide Transformer tasks, Genomic Benchmarks, and newly proposed Gener tasks, particularly excelling in long-sequence understanding.  </p> </li> <li>Results: In central dogma experiments, Generator can generate protein-coding DNA whose translated proteins have realistic lengths, protein-LM perplexities, and 3D structures, indicating true generative competence rather than memorization.  </li> <li> <p>Results: For enhancer design, a Generator-based predictor surpasses previous models on DeepSTARR data, and prompt-conditioned generation produces enhancer sequences with clearly different predicted activity profiles.</p> </li> <li> <p>Why it matters: Generator demonstrates that a large, long-context generative FM for eukaryotic DNA can serve as a powerful tool for both genomic analysis and sequence design, opening doors to more sophisticated applications in synthetic biology, variant interpretation, and future multimodal integration with other biological data types.</p> </li> </ul>"},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/","title":"Genome-wide Association Studies in Ancestrally Diverse Populations (2019)","text":"","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#1-problem-tasks","title":"1. Problem &amp; Tasks","text":"<ul> <li>Review methodological pitfalls and recommendations for conducting GWAS beyond European cohorts.</li> <li>Critical for how we handle ancestry PCs and covariates when integrating genetics with MRI.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#2-datasets","title":"2. Datasets","text":"<ul> <li>Summarizes lessons from PAGE, HCHS/SOL, UKB, biobanks in East Asia and Africa.</li> <li>Emphasizes differences in sample sizes and LD structure across populations.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#3-model-method-details","title":"3. Model / Method Details","text":"","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#31-key-recommendations","title":"3.1 Key Recommendations","text":"<ul> <li>Use linear mixed models or ancestry-informed stratification to control population structure.</li> <li>Include local ancestry estimates when available; otherwise rely on global PCs.</li> <li>Validate PRS / embeddings separately per ancestry group; avoid pooling unless harmonized.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#32-confound-handling-evaluation-discipline","title":"3.2 Confound Handling &amp; Evaluation Discipline","text":"<ul> <li>Check for residual stratification using QQ plots and genomic control.</li> <li>Report ancestry composition and site distribution with each GWAS/PRS result.</li> <li>Use jackknife/bootstraps for uncertainty when sample sizes are small per subgroup.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#4-results-tables","title":"4. Results &amp; Tables","text":"<ul> <li>Quantifies performance drop (R\u00b2 decrease up to 5\u00d7) when PRS trained in Europeans applied to African ancestry cohorts.</li> <li>Provides tables linking LD differences to effect estimation bias.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#5-limitations-cautions","title":"5. Limitations &amp; Cautions","text":"<ul> <li>2019 state of the field; newer multi-ancestry methods exist but core cautions remain.</li> <li>Does not cover foundation models directly; we must adapt recommendations to embeddings.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/gwas_diverse_populations/#6-hooks-into-neuro-omics-kb","title":"6. Hooks into Neuro-Omics KB","text":"<p>Relevant KB assets</p> <ul> <li><code>kb/paper_cards/gwas_diverse_populations.yaml</code></li> <li><code>kb/datasets/ukb_manifest_stub.yaml</code> (records ancestry composition, PCs).</li> </ul> <p>Configs / recipes informed</p> <ul> <li>Covariate list (age/sex/site/PCs) in <code>configs/experiments/01_cca_gene_smri.yaml</code> and <code>02_prediction_baselines.yaml</code>.</li> <li>Future documentation for PRS/embedding fairness analyses.</li> </ul> <p>Concrete guidance for our project</p> <ul> <li>Always log ancestry distribution and include PCs in dataset cards; cite this paper when explaining why.</li> <li>If/when we add non-European cohorts, run stratified evaluations instead of assuming global models transfer.</li> <li>Keep hooks for local ancestry or site-specific PCs should we extend beyond UKB Europeans.</li> </ul>","tags":["paper-notes","genetics","gwas"]},{"location":"generated/kb_curated/papers-md/hyenadna_2023/","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution","text":"<p>Authors: Eric Nguyen, Michael Poli, Marjan Faizi, Armin W. Thomas, Callum Birch Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, Christopher R\u00e9 Year: 2023 Venue: 37th Conference on Neural Information Processing Systems (NeurIPS)</p>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. HyenaDNA is a genomic foundation model pretrained on the human reference genome, designed to capture long-range dependencies in DNA sequences at single nucleotide resolution.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. The paper introduces a new family of DNA foundation models based on Hyena operators (implicit convolutions) that enable context lengths up to 1 million tokens, a 500\u00d7 increase over previous dense attention-based models.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Single-modality DNA sequence (human reference genome; single nucleotide-level tokens, no k-mer aggregation).</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces HyenaDNA, a genomic foundation model that addresses two critical limitations of previous DNA language models: short context windows (512-4k tokens, &lt;0.001% of human genome) and loss of single nucleotide resolution due to k-mer tokenization. Building on Hyena\u2014a language model architecture using implicit convolutions that scales sub-quadratically\u2014HyenaDNA achieves context lengths of up to 1 million tokens at single nucleotide resolution, enabling modeling of very long genomic regions (e.g., entire genes, regulatory domains) while preserving fine-grained resolution critical for detecting single nucleotide polymorphisms (SNPs). The model uses a decoder-only architecture with Hyena operators (long convolutions with data-controlled gating) pretrained on the human reference genome using next nucleotide prediction. Key innovations include a sequence length warm-up scheduler that gradually increases context during training (reducing training time by 40% and improving accuracy by 7.5 points at 450k length), and soft prompting techniques for downstream adaptation that leverage the extended context window. On downstream benchmarks, HyenaDNA achieves state-of-the-art performance on 12 of 18 Nucleotide Transformer tasks and 7 of 8 GenomicBenchmarks tasks, despite using 1500\u00d7 fewer parameters (1.6M vs. 2.5B) and 3200\u00d7 less pretraining data (1 human genome vs. 3202 genomes) compared to Nucleotide Transformer v2. The model also demonstrates novel capabilities enabled by long context, including in-context learning for species classification and effective handling of ultralong-range tasks. This work shows how architectural innovations (sub-quadratic operators) can unlock new capabilities (long-range modeling at fine resolution) that were previously impossible with attention-based models, and demonstrates the value of full-stack recipe development (architecture + training + adaptation) for foundation models.</p>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>Genomic sequences are extremely long (human genome is 3.2B nucleotides) with long-range dependencies spanning 100k+ nucleotides (e.g., enhancer-promoter interactions, chromatin organization).</li> <li>Many genomic tasks require both long-range context (to capture regulatory interactions) and single nucleotide resolution (to detect SNPs, mutations, fine-scale regulatory elements).</li> <li> <p>Previous DNA foundation models face a fundamental trade-off:</p> <ul> <li>Short context: Attention-based models (DNABERT, Nucleotide Transformers) are limited to 512-4k tokens due to quadratic scaling, capturing &lt;0.001% of the genome.</li> <li>Loss of resolution: K-mer tokenization aggregates nucleotides into \"words,\" losing single nucleotide resolution where subtle variations (SNPs) can have profound biological effects.</li> </ul> </li> <li> <p>Why this is hard</p> </li> <li>Quadratic attention scaling:<ul> <li>Transformers scale as O(L\u00b2) in sequence length, making 1M+ token contexts computationally infeasible.</li> <li>Sparse attention and linear attention approximations trade expressivity for efficiency.</li> </ul> </li> <li>Single nucleotide vs. long-range trade-off:<ul> <li>Character-level modeling preserves resolution but produces very long sequences (1M+ tokens for 1M bp).</li> <li>K-mer tokenization reduces sequence length but loses fine-grained information.</li> </ul> </li> <li>Training stability at ultralong sequences:<ul> <li>Directly training on 200k+ token sequences causes gradient variance issues and training instability.</li> </ul> </li> <li>Downstream adaptation:<ul> <li>Standard fine-tuning doesn't leverage the extended context window effectively.</li> <li>Need new paradigms for adapting long-context models to downstream tasks.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>Pretraining:<ul> <li>Human reference genome (HG38/GRCh38), processed as contiguous sequences.</li> <li>Total scale: sequences up to 1M nucleotides (tokens) in length.</li> </ul> </li> <li> <p>Downstream evaluation:</p> <ul> <li>GenomicBenchmarks: 8 regulatory element classification tasks (enhancers, promoters, coding vs. intergenic), sequence lengths 200-4,776 bp.</li> <li>Nucleotide Transformer benchmark: 18 tasks (enhancers, promoters, histone modifications, splice sites), lengths 200-600 bp.</li> <li>Species classification: Novel long-range task requiring 1M+ context to distinguish species.</li> <li>Chromatin profile prediction: 919-way multi-task predicting TF binding, DHS, histone marks.</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Single modality: DNA sequence at single nucleotide resolution (A, C, G, T, N).</li> <li> <p>Outputs vary by task: binary/multi-class labels, continuous profiles, expression levels.</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>Character-level tokenization:<ul> <li>Each nucleotide (A, C, G, T) is a token (vocabulary size: 4, plus special tokens for padding, separation, unknown).</li> <li>No k-mer aggregation or BPE tokenization; preserves single nucleotide resolution.</li> </ul> </li> <li>Sequence length warm-up:<ul> <li>Training starts at L\u2081=64 tokens, doubles at each stage (64 \u2192 128 \u2192 256 \u2192 ... \u2192 1M).</li> <li>Global batch size kept constant, so each stage processes more tokens per iteration.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Decoder-only autoregressive model based on Hyena operators (implicit convolutions with data-controlled gating).</li> <li> <p>Architecture: stack of Hyena blocks (Hyena operator + feed-forward network), similar to Transformer decoder but with attention replaced by Hyena operators.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>New FM. HyenaDNA is a ground-up redesign of genomic foundation models using Hyena operators instead of attention, enabling unprecedented context lengths at single nucleotide resolution.</p> </li> <li> <p>Key components and innovations</p> </li> <li>Hyena operator:<ul> <li>Replaces self-attention with long convolutions parameterized implicitly via neural networks.</li> <li>Structure: H(x\u2081, x\u2082)v = D_{x\u2082} T_h D_{x\u2081} v, where:</li> <li>T_h is a Toeplitz matrix from a learnable long convolution filter h (produced by neural network \u03b3_\u03b8).</li> <li>D_{x\u2081}, D_{x\u2082} are element-wise gating matrices controlled by input projections.</li> <li>Time complexity: O(L log\u00b2 L) vs. O(L\u00b2) for attention.</li> </ul> </li> <li>Sequence length warm-up scheduler:<ul> <li>Gradually increases sequence length during training (64 \u2192 128 \u2192 ... \u2192 1M).</li> <li>Reduces training time by 40% and improves accuracy by 7.5 points at 450k length.</li> <li>Acts as both stability mechanism and implicit batch size warm-up.</li> </ul> </li> <li>Soft prompting for downstream adaptation:<ul> <li>Injects learnable prompt tokens (up to 32k) directly into input sequence.</li> <li>Only prompt parameters are optimized; pretrained model weights frozen.</li> <li>Enables competitive performance without standard fine-tuning.</li> </ul> </li> <li> <p>Single nucleotide resolution:</p> <ul> <li>No k-mer tokenization; each nucleotide is a token.</li> <li>Enables detection of SNPs and fine-scale regulatory elements.</li> </ul> </li> <li> <p>Training setup</p> </li> <li>Pretraining objective: Next nucleotide (token) prediction (autoregressive language modeling).</li> <li>Model sizes: 2-8 layers, 128-256 hidden dimensions, context lengths 1024 to 1M tokens.</li> <li>Efficiency: At 1M tokens, HyenaDNA is 160\u00d7 faster than Transformer with Flash Attention.</li> <li>Gradient checkpointing: Reduces memory footprint by 3\u00d7 on sequences &gt;160k.</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Not applicable. HyenaDNA is a unimodal foundation model focused exclusively on DNA sequences. The long-context capabilities could potentially enable integration with other modalities (e.g., epigenomic tracks, expression data) in future work, but this is not explored in the paper.</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#7-experiments-and-results","title":"7. Experiments and Results","text":""},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#main-findings","title":"Main findings","text":"<ul> <li>State-of-the-art performance with far fewer parameters:</li> <li>On Nucleotide Transformer benchmark: SotA on 12 of 18 tasks using 1.6M parameters vs. 2.5B for NT v2-2.5B (1500\u00d7 fewer).</li> <li>On GenomicBenchmarks: SotA on 7 of 8 tasks, with improvements up to +20 accuracy points on enhancer identification.</li> <li> <p>Pretraining data: 1 human genome vs. 3202 genomes for NT (3200\u00d7 less data).</p> </li> <li> <p>Long-range capabilities:</p> </li> <li>Species classification: Effectively solves task by increasing context to 1M tokens (no downsampling needed).</li> <li> <p>Chromatin profiles: Competitively performs 919-way multi-task prediction against larger sparse-attention BigBird Transformer.</p> </li> <li> <p>Single nucleotide resolution benefits:</p> </li> <li>Outperforms k-mer-based models (DNABERT) on tasks requiring fine-scale resolution.</li> <li> <p>Enables detection of SNPs and single-nucleotide regulatory elements.</p> </li> <li> <p>Training efficiency:</p> </li> <li>Sequence length warm-up reduces training time by 40% at 450k length.</li> <li> <p>160\u00d7 faster than Transformer at 1M tokens (forward + backward pass).</p> </li> <li> <p>In-context learning:</p> </li> <li>Soft prompting enables adaptation to new tasks without fine-tuning.</li> <li>Performance improves with more prompt tokens (up to 32k tested), approaching fine-tuning performance.</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#ablation-studies","title":"Ablation studies","text":"<ul> <li>Sequence length warm-up:</li> <li>Without warm-up: training is slower and less stable at long sequences.</li> <li> <p>With warm-up: 40% faster training, 7.5 point accuracy improvement at 450k.</p> </li> <li> <p>Context length vs. perplexity:</p> </li> <li>Longer context improves pretraining perplexity (better next-token prediction).</li> <li> <p>However, for models too shallow, perplexity can degrade at very long sequences (inflection points).</p> </li> <li> <p>Soft prompting:</p> </li> <li>More prompt tokens (2 \u2192 32k) improve performance, saturating near fine-tuning baseline.</li> <li>K-shot demonstrations (few-shot learning) less effective than soft prompting for this model.</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#key-insights","title":"Key insights","text":"<ul> <li>Long context enables new capabilities:</li> <li> <p>Species classification requires 1M+ context to distinguish sequences; HyenaDNA is the first model to handle this without downsampling.</p> </li> <li> <p>Single nucleotide resolution matters:</p> </li> <li> <p>Preserving fine-grained resolution is critical for tasks like enhancer identification and variant effect prediction.</p> </li> <li> <p>Efficiency unlocks scale:</p> </li> <li>Sub-quadratic scaling (O(L log\u00b2 L)) makes 1M token contexts feasible, enabling new applications.</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#8-strengths-and-limitations","title":"8. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#strengths","title":"Strengths","text":"<ul> <li>Unprecedented context length:</li> <li> <p>1M tokens at single nucleotide resolution is a 500\u00d7 increase over previous dense attention models.</p> </li> <li> <p>Preserves fine-grained resolution:</p> </li> <li> <p>Character-level tokenization enables detection of SNPs and single-nucleotide regulatory elements.</p> </li> <li> <p>Computational efficiency:</p> </li> <li> <p>160\u00d7 faster than Transformers at 1M tokens, enabling practical training and inference.</p> </li> <li> <p>Strong performance with minimal resources:</p> </li> <li> <p>Achieves SotA with 1500\u00d7 fewer parameters and 3200\u00d7 less pretraining data than Nucleotide Transformer v2.</p> </li> <li> <p>Full-stack recipe:</p> </li> <li> <p>Provides architecture, training strategies (warm-up), and adaptation methods (soft prompting) as a complete package.</p> </li> <li> <p>Novel capabilities:</p> </li> <li>Enables in-context learning and ultralong-range tasks previously impossible.</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#limitations","title":"Limitations","text":"<ul> <li>Still smaller than some baselines:</li> <li> <p>1.6M parameters is very small; larger HyenaDNA models might achieve even better performance.</p> </li> <li> <p>Limited pretraining data:</p> </li> <li> <p>Only trained on human genome; multi-species pretraining (like NT) might improve generalization.</p> </li> <li> <p>No RC-equivariance:</p> </li> <li> <p>Doesn't explicitly encode reverse-complement symmetry (unlike Caduceus); relies on data augmentation if needed.</p> </li> <li> <p>In-context learning is limited:</p> </li> <li> <p>DNA vocabulary is small (4 nucleotides), making pure in-context learning challenging; requires soft prompting or instruction tuning.</p> </li> <li> <p>Training stability:</p> </li> <li>Even with warm-up, very long sequences (1M+) can be challenging to train; warm-up schedule needs careful tuning.</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#open-questions-future-directions","title":"Open questions / future directions","text":"<ul> <li>Scaling laws:</li> <li> <p>How does performance scale with model size, pretraining data, and context length?</p> </li> <li> <p>Multi-species pretraining:</p> </li> <li> <p>Would training on multiple species (like NT) improve performance?</p> </li> <li> <p>RC-equivariance integration:</p> </li> <li> <p>Can HyenaDNA be combined with RC-equivariant architectures (like Caduceus) for even better performance?</p> </li> <li> <p>Generative capabilities:</p> </li> <li> <p>Can HyenaDNA generate long genomic sequences? How does it compare to Evo 2 or GENERator?</p> </li> <li> <p>Interpretability:</p> </li> <li>What long-range patterns does HyenaDNA learn? Can we interpret the learned representations?</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":""},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#relation-to-other-work","title":"Relation to other work","text":"<ul> <li>Compared to Nucleotide Transformer (Dalla-Torre et al., 2023):</li> <li>NT uses attention with 6-mer tokenization, limited to ~1k tokens.</li> <li>HyenaDNA uses Hyena operators with character-level tokens, enabling 1M tokens.</li> <li> <p>HyenaDNA achieves similar or better performance with 1500\u00d7 fewer parameters.</p> </li> <li> <p>Compared to DNABERT-2 (Zhou et al., 2024):</p> </li> <li>DNABERT-2 uses BPE tokenization and attention, limited context.</li> <li> <p>HyenaDNA preserves single nucleotide resolution and enables much longer contexts.</p> </li> <li> <p>Compared to Caduceus (Schiff et al., 2024):</p> </li> <li>Caduceus uses Mamba SSMs with RC-equivariance for long-range modeling.</li> <li> <p>HyenaDNA uses Hyena operators without explicit RC-equivariance but achieves longer contexts (1M vs. 100k+).</p> </li> <li> <p>Compared to Evo 2 (Brixi et al., 2025):</p> </li> <li>Evo 2 uses StripedHyena 2 (multi-hybrid architecture) for generative DNA modeling at 1M context.</li> <li> <p>HyenaDNA is an earlier, simpler architecture that demonstrates long-context capabilities for discriminative tasks.</p> </li> <li> <p>Connection to Hyena (Poli et al., 2023):</p> </li> <li>HyenaDNA adapts the Hyena architecture (designed for language) to genomics, showing the generality of sub-quadratic operators.</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#broader-scientific-and-practical-impact","title":"Broader scientific and practical impact","text":"<ul> <li>Enables new genomic applications:</li> <li>Long-context modeling opens possibilities for whole-gene, whole-chromosome, or even whole-genome analysis.</li> <li> <p>Single nucleotide resolution enables fine-scale variant effect prediction and regulatory element identification.</p> </li> <li> <p>Demonstrates value of architectural innovation:</p> </li> <li> <p>Shows how sub-quadratic operators (Hyena) can unlock capabilities impossible with attention-based models.</p> </li> <li> <p>Provides practical recipe:</p> </li> <li> <p>Full-stack approach (architecture + training + adaptation) makes it easier for others to build long-context genomic models.</p> </li> <li> <p>Influences future work:</p> </li> <li>Evo 2 and other recent models build on similar principles (long-context, sub-quadratic operators).</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#open-questions-for-future-research","title":"Open questions for future research","text":"<ul> <li>How to scale further?</li> <li> <p>Can we model entire genomes (3.2B nucleotides) with current architectures?</p> </li> <li> <p>Multi-species pretraining:</p> </li> <li> <p>Would training on diverse species improve generalization?</p> </li> <li> <p>RC-equivariance:</p> </li> <li> <p>How to combine long-context capabilities with architectural RC-equivariance?</p> </li> <li> <p>Generative modeling:</p> </li> <li> <p>Can HyenaDNA-style architectures generate long, biologically valid sequences?</p> </li> <li> <p>Interpretability:</p> </li> <li>What long-range patterns do these models learn? Can we extract biological insights?</li> </ul>"},{"location":"generated/kb_curated/papers-md/hyenadna_2023/#10-key-takeaways","title":"10. Key Takeaways","text":"<ol> <li> <p>Architectural innovation unlocks new capabilities:    Sub-quadratic operators (Hyena) enable context lengths (1M tokens) that are computationally infeasible with attention (O(L\u00b2) scaling).</p> </li> <li> <p>Resolution vs. context trade-off is real:    K-mer tokenization reduces sequence length but loses single nucleotide resolution; character-level modeling preserves resolution but requires efficient architectures for long sequences.</p> </li> <li> <p>Training strategies matter:    Sequence length warm-up is crucial for stable training at ultralong sequences, reducing training time and improving accuracy.</p> </li> <li> <p>Efficiency enables scale:    Being 160\u00d7 faster than Transformers at 1M tokens makes previously impossible applications feasible.</p> </li> <li> <p>Fewer parameters can be enough:    HyenaDNA achieves SotA with 1500\u00d7 fewer parameters than Nucleotide Transformer, showing that architecture and training matter more than raw parameter count.</p> </li> <li> <p>Full-stack recipe development:    Don't just propose an architecture; provide training strategies, adaptation methods, and evaluation protocols as a complete package.</p> </li> <li> <p>Long context enables new tasks:    Extended context windows unlock capabilities like species classification and ultralong-range regulatory element prediction that weren't possible before.</p> </li> <li> <p>In-context learning is possible in genomics:    Soft prompting enables adaptation to new tasks without fine-tuning, though it requires careful design due to small vocabulary.</p> </li> <li> <p>Single nucleotide resolution matters:    Preserving fine-grained resolution is critical for detecting SNPs and fine-scale regulatory elements that k-mer models miss.</p> </li> <li> <p>This is foundational work:     HyenaDNA demonstrates that long-context, fine-resolution genomic modeling is possible, influencing subsequent work like Evo 2 and showing the path forward for genomic foundation models.</p> </li> </ol>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/","title":"M3FM (2025)","text":""},{"location":"generated/kb_curated/papers-md/m3fm_2025/#m3fm-a-multimodal-multidomain-multilingual-medical-foundation-model-for-zeroshot-clinical-diagnosis","title":"M3FM: A Multimodal, Multidomain, Multilingual Medical Foundation Model for Zero\u2011Shot Clinical Diagnosis","text":"<p>Authors: Fenglin Liu, Zheng Li, Qingyu Yin, Jinfa Huang, Jiebo Luo, Anshul Thakur, Kim Branson, Patrick Schwab, Bing Yin, Xian Wu, Yefeng Zheng, David A. Clifton Year: 2025 Venue: npj Digital Medicine</p>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical VLM / MLLM / MMFM  </li> <li> <p>This paper proposes a medical multimodal vision\u2013language foundation model that jointly handles radiology images and multilingual text for diagnosis and report generation.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration  </p> </li> <li> <p>Key Modalities: </p> </li> <li>2D chest X\u2011rays (CXR)  </li> <li>3D CT images  </li> <li>English radiology reports  </li> <li>Chinese radiology reports (machine-translated for training; human reports for evaluation on some datasets)</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>M3FM (Multimodal Multidomain Multilingual Foundation Model) is a medical foundation model designed to perform zero-shot radiology report generation and disease diagnosis across imaging domains (CXR and CT) and languages (English and Chinese). The core idea is to first learn a shared vision\u2013language embedding space (MultiMedCLIP) using large English-centric image\u2013report and English\u2013Chinese text pairs, and then train a multilingual medical language model (MultiMedLM) on top of this space. By aligning visual features from different imaging modalities and textual features from multiple languages to English, M3FM can generate reports and perform diagnosis in languages and domains where little or no labeled data exist. The model is pretrained on hundreds of thousands of CXR and CT images with English reports, plus translated Chinese corpora, and is evaluated on nine downstream datasets that cover report generation and disease classification for infectious (COVID\u201119, TB) and noninfectious diseases. Across zero\u2011shot, few\u2011shot, and fully supervised settings, M3FM often matches or outperforms strong supervised baselines that have full access to labeled data, especially for cross-language and cross-domain generalization. For a new grad student, this paper is a canonical example of how to build a medical CLIP\u2011style + medical LLM stack and use it for multilingual, low\u2011label clinical scenarios.</p>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Build a single foundation model that can:  <ul> <li>Generate radiology reports from CXR and CT images in both English and Chinese.  </li> <li>Diagnose diseases from images and/or generated reports, including rare and emerging conditions.  </li> </ul> </li> <li> <p>Crucially, the model should work in zero-shot or few-shot regimes where labeled data in the target language or domain are scarce or absent.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Data scarcity and imbalance: <ul> <li>High-quality labeled data for rare diseases and new pathogens (e.g., early COVID\u201119 waves) are limited exactly when they are most needed.  </li> <li>Non\u2011English radiology reports, especially in languages like Chinese, are much less abundant and standardized than English reports.  </li> </ul> </li> <li>Multidomain heterogeneity: <ul> <li>CXR and CT have very different appearance, resolution, and information content, yet clinicians want unified reporting and diagnosis workflows.  </li> </ul> </li> <li>Multilingual alignment: <ul> <li>Reports in different languages describe similar findings but with different vocabularies, structures, and clinical conventions.  </li> </ul> </li> <li>Label efficiency: <ul> <li>Training supervised models separately for each disease, modality, and language combination is infeasible; a foundation model should generalize with minimal task\u2011specific labeling.  </li> </ul> </li> <li>Safety and fairness: <ul> <li>Failure to support low\u2011resource languages and rare diseases risks widening disparities in access to AI\u2011assisted care.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data (English-centric corpora): </li> <li>MIMIC\u2011CXR: <ul> <li>\u2248377k chest X\u2011ray images with \u2248228k English radiology reports.  </li> </ul> </li> <li>COVID\u201119\u2011CT\u2011CXR: <ul> <li>\u22481k CT/CXR images with English reports focused on COVID\u201119.  </li> </ul> </li> <li> <p>Chinese\u2013English parallel corpora: </p> <ul> <li>Constructed by machine-translating portions of the English reports into Chinese to form Chinese\u2013English text pairs for multilingual training.</li> </ul> </li> <li> <p>Downstream datasets (disease reporting and diagnosis): </p> </li> <li>Report generation: <ul> <li>IU\u2011Xray (CXR\u2013English reports).  </li> <li>COVID\u201119\u2011CT (CT\u2013Chinese reports).  </li> <li>COV\u2011CTR (CT images with English &amp; Chinese reports).  </li> <li>Additional qualitative examples for CXR\u2011to\u2011Chinese where human\u2011annotated datasets are unavailable.  </li> </ul> </li> <li> <p>Disease classification: </p> <ul> <li>Shenzhen Tuberculosis (CXR, TB vs normal).  </li> <li>COVID\u2011CXR (CXR, COVID\u201119 vs non\u2011COVID).  </li> <li>NIH ChestX\u2011ray14 (14\u2011label multi\u2011label classification).  </li> <li>CheXpert (multi\u2011label classification).  </li> <li>RSNA Pneumonia (pneumonia detection).  </li> <li>SIIM\u2011ACR Pneumothorax (pneumothorax detection).</li> </ul> </li> <li> <p>Modalities and languages: </p> </li> <li>Imaging: CXR, CT.  </li> <li> <p>Text: radiology reports in English and Chinese.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Images are encoded by a vision backbone into dense features.  </li> <li>Text reports (English and Chinese) are tokenized and embedded for contrastive learning and language modeling.  </li> <li>English reports from different datasets are treated as separate corpora to avoid leakage between training and evaluation institutions.</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>A two\u2011stage medical foundation model composed of:  </p> <ul> <li>MultiMedCLIP: CLIP\u2011style vision\u2013language encoder that aligns images and text.  </li> <li>MultiMedLM: multilingual medical language model (LLM) trained for report generation and text understanding.</li> </ul> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>M3FM is a new medical foundation model stack that builds on standard transformer backbones but introduces a specific pretraining strategy for multidomain, multilingual radiology.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Vision encoder CNN/ViT-style backbone encoding CXR and CT into visual embeddings Text encoder / decoder Transformer-based encoders/decoders for English and Chinese reports Alignment module CLIP-like contrastive loss aligning images and English text, and English\u2013Chinese text pairs Language model MultiMedLM trained on multilingual medical corpora to reconstruct and generate reports Inference strategy Zero-shot and few-shot report generation and diagnosis using aligned embeddings <ul> <li>Training setup (high level):</li> <li>Stage 1 \u2013 MultiMedCLIP (vision\u2013language alignment): <ul> <li>Pretrain on English-centric corpora: CXR\u2013English, CT\u2013English, and Chinese\u2013English pairs.  </li> <li>Contrastive objective encourages matched image\u2013text pairs (or bilingual pairs) to be close in a shared latent space and mismatched ones to be far apart.  </li> <li>Enables alignment of new non\u2011English reports by mapping them into the same space as English text and images.</li> </ul> </li> <li>Stage 2 \u2013 MultiMedLM (multilingual medical LLM): <ul> <li>Train an autoregressive language model over the aligned text embeddings, reconstructing inputs across languages.  </li> <li>Leverages large text corpora (including machine\u2011translated Chinese) to learn biomedical vocabulary and style.  </li> </ul> </li> <li>Inference: <ul> <li>For zero\u2011shot report generation, visual embeddings are fed through the aligned text space into MultiMedLM to decode reports in English or Chinese without downstream supervised training.  </li> <li>For diagnosis, image features and generated reports can be combined to drive disease classifiers.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Modalities integrated: </li> <li> <p>Radiology images (CXR, CT) and textual reports in English and Chinese.</p> </li> <li> <p>How integration works: </p> </li> <li>CLIP\u2011style two\u2011tower alignment (MultiMedCLIP): <ul> <li>Separate image and text encoders are trained with a contrastive loss so that images and their corresponding reports are nearby in the joint embedding space.  </li> <li>English text is the anchor; Chinese reports are aligned via English\u2013Chinese text pairs, effectively \u201cpivoting\u201d through English.  </li> </ul> </li> <li>Text\u2011only language modeling (MultiMedLM): <ul> <li>Trained on multilingual text (including translated reports) to generate fluent medical text conditioned on embeddings from MultiMedCLIP.  </li> </ul> </li> <li> <p>Zero\u2011shot transfer: </p> <ul> <li>Once everything is aligned, CXR or CT images from unseen datasets can be fed through the image encoder and decoded into English or Chinese reports, even when no labeled image\u2013text pairs exist for that specific institution or language.</li> </ul> </li> <li> <p>Why this integration is useful / new capabilities: </p> </li> <li>Enables zero\u2011shot multilingual report generation from images, including generating Chinese reports without any human\u2011labeled CXR\u2011Chinese or CT\u2011Chinese training pairs.  </li> <li>Supports zero\u2011shot and few\u2011shot disease diagnosis by combining image features and generated reports across diseases and datasets.  </li> <li>Provides a flexible template for extending to additional modalities (e.g., other imaging types or languages) by adding new aligned corpora.</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Multilingual, multidomain report generation: <ul> <li>CXR\u2011to\u2011English, CT\u2011to\u2011English, CXR\u2011to\u2011Chinese (qualitative), CT\u2011to\u2011Chinese.  </li> </ul> </li> <li>Disease diagnosis (classification): <ul> <li>Binary tasks (COVID vs non\u2011COVID, TB vs normal) and multi\u2011label disease prediction (ChestX\u2011ray14, CheXpert) using image + generated reports.  </li> </ul> </li> <li> <p>Settings: </p> <ul> <li>Zero\u2011shot report generation and diagnosis (no downstream labels).  </li> <li>Few\u2011shot and fully supervised diagnosis using limited or full labels on top of M3FM embeddings.</li> </ul> </li> <li> <p>Baselines: </p> </li> <li>Supervised report generation models such as R2Gen and other encoder\u2013decoder methods trained on specific datasets.  </li> <li>Supervised disease classifiers trained directly on images (e.g., CNNs) with full labels.  </li> <li> <p>Few\u2011shot and fully supervised methods that do not use a unified foundation model.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>M3FM achieves strong zero\u2011shot report generation performance, often matching or surpassing supervised baselines on CXR\u2011to\u2011English and CT\u2011to\u2011Chinese tasks despite using no downstream labels.  </li> <li>In zero\u2011shot and few\u2011shot diagnosis, combining M3FM representations and generated reports yields performance close to fully supervised baselines that use large labeled training sets.  </li> <li>M3FM is particularly strong in cross-language generalization, outperforming baselines on Chinese report generation and diagnosis tasks built from machine\u2011translated training corpora.  </li> <li>Across nine benchmark datasets spanning infectious and noninfectious diseases, M3FM provides consistently competitive or superior results, especially in low\u2011label regimes.</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Provides a single foundation model that unifies multimodal (image + text), multidomain (CXR + CT), and multilingual (English + Chinese) clinical diagnosis and report generation.  </li> <li>Demonstrates genuine zero\u2011shot capabilities, generating reports and diagnoses without labeled downstream data in the target domain or language.  </li> <li>Uses existing English-centric corpora and machine translation to bootstrap multilingual capabilities, which is practical for many health systems.  </li> <li>Offers a concrete, reproducible blueprint for combining CLIP\u2011style alignment with a domain\u2011specific medical LLM.</li> </ul> <p>Limitations:</p> <ul> <li>Relies heavily on machine\u2011translated Chinese text, which may introduce translation artifacts and biases into the model\u2019s understanding of Chinese medical language.  </li> <li>Focuses on CXR and CT only; other imaging modalities and richer EHR data are not modeled.  </li> <li>Zero\u2011shot and few\u2011shot performance, while strong, may still fall short of what clinicians require for fully autonomous deployment.  </li> <li>Training such a model still requires substantial compute and careful data curation; scaling to more modalities and languages further raises costs.  </li> <li>Evaluation emphasizes standard generation and classification metrics, which may not fully capture clinical safety and decision impact.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How well does the M3FM paradigm extend to additional languages (e.g., Spanish, Arabic) and imaging modalities (MRI, ultrasound, pathology) when only weak supervision is available?  </li> <li>Can higher\u2011quality human\u2011translated corpora or bilingual clinical notes substantially improve multilingual performance and safety relative to machine translation alone?  </li> <li>How should we design clinical trials and human\u2011in\u2011the\u2011loop workflows to safely deploy zero\u2011shot report generators in real hospitals?  </li> <li>What is the best way to combine M3FM with structured EHR data and other signals (labs, medications) for end\u2011to\u2011end diagnostic support?  </li> <li>Can similar multimodal, multilingual foundations be built in a more compute\u2011efficient way, for example via parameter\u2011efficient fine\u2011tuning or distillation?</li> </ol>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>M3FM is to radiology report generation and diagnosis what CLIP\u2011style models and GPT\u2011style LLMs are to generic vision\u2013language tasks: a unified backbone that supports multiple tasks, domains, and languages from a single pretraining run.  </li> <li>It sits alongside other medical multimodal FMs (e.g., radiology VLMs, medical CLIP variants) but is distinctive in explicitly targeting multilingual, multidomain zero\u2011shot diagnosis.</li> <li>Relation to well-known ideas: </li> <li>Architecturally, MultiMedCLIP is a CLIP\u2011like two\u2011tower model for medical images and text, while MultiMedLM is a medical LLM akin to Me\u2011LLaMA but optimized for radiology report style and multilinguality.  </li> <li>The system follows the modern pattern of using a strong vision encoder + language encoder/decoder, aligned via contrastive learning and then instruction\u2011 or task\u2011tuned for downstream use.</li> <li>Why this paper is a useful reference: </li> <li>It provides a clear recipe for building multilingual, multidomain medical VLMs and demonstrating their advantages in low\u2011label regimes.  </li> <li>For a grad student, it serves as a practical starting point for designing new medical MLLMs, extending them to other modalities, or exploring safer, more equitable deployment strategies in global health.</li> </ul>"},{"location":"generated/kb_curated/papers-md/m3fm_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Clinical radiology needs AI systems that can generate reports and support diagnosis across imaging domains and languages, especially when labeled data for rare diseases or non\u2011English populations are scarce.  </p> </li> <li> <p>Method / model: </p> </li> <li>M3FM combines MultiMedCLIP, a CLIP\u2011style vision\u2013language encoder trained on English-centric image\u2013text corpora and English\u2013Chinese text pairs, with MultiMedLM, a multilingual medical language model trained on large text corpora.  </li> <li> <p>The model aligns CXR and CT images with English and Chinese reports in a shared latent space and then uses an LLM to generate reports and support downstream classification.  </p> </li> <li> <p>Results: </p> </li> <li>Achieves strong zero\u2011shot and few\u2011shot performance on nine datasets covering report generation and disease diagnosis, often matching or exceeding supervised baselines that rely on labeled data.  </li> <li> <p>Particularly strong for cross-language tasks such as CT\u2011to\u2011Chinese report generation and COVID\u201119 diagnosis with minimal labeled data.  </p> </li> <li> <p>Why it matters: </p> </li> <li>Demonstrates that a single medical multimodal foundation model can reduce dependence on large labeled datasets and extend AI benefits to low\u2011resource languages and rare diseases.  </li> <li>Provides a concrete blueprint for future medical VLMs and MLLMs targeting multilingual, label\u2011efficient clinical decision support.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/","title":"\"Me-LLaMA: Medical Foundation Large Language Models for Comprehensive Text Analysis and Clinical Reasoning\"","text":""},{"location":"generated/kb_curated/papers-md/me_llama_2024/#me-llama-medical-foundation-large-language-models-for-comprehensive-text-analysis-and-clinical-reasoning","title":"Me-LLaMA: Medical Foundation Large Language Models for Comprehensive Text Analysis and Clinical Reasoning","text":"<p>Authors: Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Xinyu Zhou, Lingfei Qian, Huan He, Dennis Shung, Lucila Ohno\u2011Machado, Yonghui Wu, Hua Xu, Jiang Bian Year: 2024 Venue: Preprint (medical AI / biomedical informatics)</p>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical LLM  </li> <li> <p>This work develops large language models specialized for biomedical literature and clinical notes, targeting broad medical text understanding and generation.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Text only: biomedical research articles, clinical guidelines, radiology and clinical notes, discharge summaries, and question\u2013answer style datasets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>Me\u2011LLaMA is a family of medical foundation large language models (LLMs) built by continually pretraining and instruction\u2011tuning LLaMA\u20112 on one of the largest medical text corpora assembled to date. The authors construct a 129\u2011billion\u2011token pretraining dataset from biomedical literature and clinical notes, and a 214k\u2011example instruction\u2011tuning corpus spanning diverse medical NLP tasks. They release 13B and 70B base models plus chat\u2011optimized versions and evaluate them on six core text analysis task families\u2014question answering, relation extraction, named entity recognition, text classification, summarization, and natural language inference\u2014across 12 benchmarks, as well as on complex clinical case diagnosis. Me\u2011LLaMA substantially outperforms previous open\u2011source medical LLMs and, with targeted instruction tuning, surpasses commercial models such as ChatGPT and even GPT\u20114 on several benchmarks, while matching them on challenging clinical case reasoning. For a new grad student, this paper illustrates how to scale a domain\u2011specific medical LLM from raw corpora through pretraining, instruction tuning, and evaluation, and how specialized data can close the gap to frontier proprietary models.</p>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Build open\u2011source medical foundation LLMs that:  </p> <ul> <li>Understand and generate medical text across biomedical research and clinical documentation.  </li> <li>Perform well on a wide range of NLP tasks (QA, NER, RE, classification, summarization, NLI).  </li> <li>Support complex clinical case reasoning comparable to commercial LLMs.  </li> </ul> </li> <li> <p>Why this is hard: </p> </li> <li>Domain knowledge gap: <ul> <li>General LLMs trained primarily on web and general\u2011domain corpora often lack reliable medical knowledge and may hallucinate clinically incorrect content.  </li> </ul> </li> <li>Data access and diversity: <ul> <li>Clinical notes and EHR text are sensitive; assembling large, representative corpora across health systems is difficult.  </li> <li>Existing medical LLMs often rely only on biomedical literature or only on clinical notes, limiting coverage.  </li> </ul> </li> <li>Compute costs: <ul> <li>Continual pretraining at the 13B\u201370B scale with &gt;100k GPU hours is expensive, making it hard to explore multiple design choices.  </li> </ul> </li> <li>Evaluation breadth: <ul> <li>Many prior models are evaluated mainly on QA, giving an incomplete picture of generalization to other medical NLP tasks.  </li> </ul> </li> <li>Clinical reliability: <ul> <li>Matching or exceeding commercial LLMs on clinical case diagnosis requires nuanced reasoning and safe behavior, not just surface\u2011level metrics.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data (129B tokens): </li> <li>Biomedical literature: <ul> <li>Millions of PubMed abstracts and full\u2011text articles from biomedical journals.  </li> </ul> </li> <li>Clinical notes: <ul> <li>\u22482.9M de\u2011identified clinical notes from electronic health records, capturing real\u2011world medical language, abbreviations, and workflows.  </li> </ul> </li> <li> <p>General text: </p> <ul> <li>Tens of billions of tokens from high\u2011quality general\u2011domain sources to preserve broad language competence.</li> </ul> </li> <li> <p>Instruction\u2011tuning data (214k examples): </p> </li> <li> <p>Curated and synthesized instructions covering:  </p> <ul> <li>Question answering (QA).  </li> <li>Named entity recognition (NER).  </li> <li>Relation extraction (RE).  </li> <li>Text classification.  </li> <li>Summarization.  </li> <li>Natural language inference (NLI).  </li> <li>Clinical diagnosis and case\u2011based reasoning prompts.</li> </ul> </li> <li> <p>Evaluation benchmarks: </p> </li> <li>12 datasets across six task families, spanning biomedical and clinical domains (e.g., medical QA benchmarks, clinical NER and RE datasets, classification tasks, and summarization corpora).  </li> <li> <p>Additional clinical case diagnosis benchmark where models read long case descriptions and propose diagnoses.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Standard subword tokenization adapted to biomedical terminology.  </li> <li>Careful de\u2011identification and filtering for clinical text.  </li> <li>Mixture weighting between general, biomedical, and clinical sources to balance domain specialization and general language ability.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Decoder\u2011only transformer LLMs (LLaMA\u20112\u2013style) with 13B and 70B parameters, plus chat\u2011optimized instruction\u2011tuned variants.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>Builds on existing LLaMA\u20112 backbones, but Me\u2011LLaMA defines new medical foundation models via large\u2011scale continual pretraining and instruction tuning on medical corpora.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Backbone LLaMA\u20112 13B and 70B decoder\u2011only transformers Continual pretraining 129B tokens from biomedical literature + clinical notes + general text Instruction tuning 214k multi\u2011task medical instructions, covering 6+ task types Model family Base models (Me\u2011LLaMA\u201113B/70B) and chat models (Me\u2011LLaMA\u201113B/70B\u2011chat) Evaluation 12 benchmarks + clinical case diagnosis vs open\u2011source and commercial LLMs <ul> <li>Training setup (high level):</li> <li>Continual pretraining: <ul> <li>Start from open\u2011source LLaMA\u20112 checkpoints.  </li> <li>Continue next\u2011token prediction on the mixed general + biomedical + clinical corpus, with careful scheduling to ensure domain specialization without catastrophic forgetting.  </li> <li>70B variant requires &gt;100,000 A100 GPU hours.  </li> </ul> </li> <li>Instruction tuning: <ul> <li>Supervised fine\u2011tuning on 214k instruction\u2013response pairs spanning multiple task types.  </li> <li>Chat models additionally tuned for conversational robustness and safety.  </li> </ul> </li> <li>Optimization: <ul> <li>Standard transformer training with AdamW, learning\u2011rate warmup + decay, and careful gradient scaling for large\u2011batch training.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>Me\u2011LLaMA is text\u2011only, so it does not directly integrate images or other modalities. However, it is designed to be a textual backbone that could sit atop or alongside medical vision or multimodal encoders.</p> <ul> <li>In the broader MMFM ecosystem, Me\u2011LLaMA can:  </li> <li>Serve as the language component in multimodal LLMs that accept imaging inputs (e.g., by attaching image encoders via projection or query\u2011based connectors, as in CLIP\u2011to\u2011LLM pipelines).  </li> <li>Act as a medical \u201creasoning engine\u201d for systems that convert images, signals, or EHR tables into textual descriptions or structured prompts.  </li> <li>The paper primarily focuses on text\u2011only performance but positions Me\u2011LLaMA as a foundation LLM that other multimodal medical models (e.g., M3FM, radiology MLLMs) can build upon.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Question answering (factoid and multi\u2011hop medical QA).  </li> <li>Named entity recognition and relation extraction for biomedical and clinical entities.  </li> <li>Text classification (e.g., document or sentence\u2011level labeling).  </li> <li>Summarization of biomedical and clinical documents.  </li> <li>Natural language inference (NLI) for medical entailment and contradiction.  </li> <li> <p>Complex clinical case diagnosis tasks where the model reads rich case descriptions and proposes diagnoses.</p> </li> <li> <p>Baselines: </p> </li> <li>General\u2011domain LLaMA\u20112 models without medical specialization.  </li> <li>Prior open\u2011source medical LLMs: MedAlpaca, ChatDoctor, AlpaCare, Clinical LLaMA, Meditron, PMC\u2011LLaMA.  </li> <li> <p>Commercial models: ChatGPT, GPT\u20114, and other proprietary LLMs on subsets of tasks.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>Versus general LLaMA\u20112: Me\u2011LLaMA strongly outperforms its backbone on essentially all medical benchmarks, confirming the value of large\u2011scale domain\u2011specific pretraining.  </li> <li>Versus open\u2011source medical LLMs: Me\u2011LLaMA achieves the best or near\u2011best scores on most QA, NER, RE, classification, summarization, and NLI datasets, with especially strong gains on tasks involving clinical notes.  </li> <li>Versus commercial LLMs: With task\u2011specific instruction tuning, Me\u2011LLaMA often surpasses ChatGPT on 7/8 datasets and GPT\u20114 on 5/8 datasets, while achieving comparable performance on complex clinical case diagnosis.  </li> <li>The 70B model performs better than the 13B variant, but the 13B models offer a compelling trade\u2011off between performance and compute.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>One of the largest and most comprehensive open\u2011source medical LLM families, with both literature and clinical notes in the pretraining mix.  </li> <li>Demonstrates that continual pretraining + instruction tuning can push open\u2011source models into the performance regime of commercial systems on many medical tasks.  </li> <li>Evaluated across a broad task spectrum, giving a realistic sense of the model\u2019s capabilities beyond QA.  </li> <li>Released models, data summaries, and evaluation scripts (under appropriate DUAs) provide a valuable community resource.</li> </ul> <p>Limitations:</p> <ul> <li>Extremely high compute cost (&gt;100k A100 hours for 70B), making replication and further scaling difficult for many groups.  </li> <li>Training data, though large, are drawn from a limited set of institutions and sources, raising concerns about bias and representativeness.  </li> <li>Evaluation, while broad, is still primarily offline, and does not fully capture real\u2011world deployment issues such as hallucination under pressure, long\u2011term safety, and clinician trust.  </li> <li>The work is text\u2011only; multimodal grounding (imaging, waveforms, EHR tables) is left to future MLLM architectures.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can we make domain\u2011specific LLM pretraining more compute\u2011efficient (e.g., via better initialization, parameter\u2011efficient tuning, or distillation)?  </li> <li>What is the best way to integrate Me\u2011LLaMA with medical vision foundation models (e.g., TITAN, M3FM\u2011style vision encoders) into full MLLMs?  </li> <li>How do we rigorously evaluate and mitigate hallucination, bias, and unsafe recommendations in complex clinical decision\u2011support settings?  </li> <li>Can we design continual learning strategies so Me\u2011LLaMA can be safely updated with new medical knowledge without catastrophic forgetting?  </li> <li>How should data governance and DUAs evolve so that multiple institutions can collaboratively train safer, more representative medical LLMs?</li> </ol>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>Me\u2011LLaMA is a flagship example of a medical foundation LLM, analogous to Me\u2011PaLM or Med\u2011PaLM style models but built on LLaMA\u20112 and fully open\u2011source.  </li> <li>It anchors the language side of the emerging ecosystem of medical multimodal foundation models (MMFMs and MLLMs).  </li> <li>Relation to well-known ideas: </li> <li>Follows the now\u2011standard recipe of continual pretraining on domain corpora plus instruction tuning for downstream task and chat performance.  </li> <li>Serves as a natural language counterpart to medical vision FMs (e.g., TITAN) and multimodal FMs (e.g., M3FM), which could plug in via CLIP\u2011style or LLaVA\u2011style connectors.  </li> <li>Why this paper is a useful reference: </li> <li>Provides a detailed case study in building, scaling, and evaluating a domain\u2011specialized LLM family.  </li> <li>For a grad student, it is an excellent blueprint for data curation, training strategy, and evaluation design in domain\u2011specific LLM research.</li> </ul>"},{"location":"generated/kb_curated/papers-md/me_llama_2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>General\u2011domain LLMs are not sufficiently reliable or specialized for medical applications, and existing open\u2011source medical LLMs are limited in scale, data diversity, and task coverage.  </p> </li> <li> <p>Method / model: </p> </li> <li>Me\u2011LLaMA continually pretrains LLaMA\u20112 on 129B tokens of biomedical literature and clinical notes, then instruction\u2011tunes on 214k multi\u2011task medical instructions, yielding 13B and 70B base and chat models.  </li> <li> <p>The model family is designed as a medical foundation LLM for broad text analysis and clinical reasoning.  </p> </li> <li> <p>Results: </p> </li> <li>Strongly outperforms prior open\u2011source medical LLMs and general LLaMA\u20112 on 12 benchmarks spanning QA, NER, RE, classification, summarization, and NLI.  </li> <li> <p>With instruction tuning, Me\u2011LLaMA matches or exceeds ChatGPT and GPT\u20114 on many benchmarks and achieves comparable performance on complex clinical case diagnosis.  </p> </li> <li> <p>Why it matters: </p> </li> <li>Shows that carefully designed domain\u2011specific data and training can make open\u2011source medical LLMs competitive with proprietary systems, improving transparency, reproducibility, and access.  </li> <li>Provides a powerful language backbone that future multimodal medical foundation models can build upon.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/","title":"\"Multimodal Large Language Models in Medical Imaging: Current State and Future Directions\"","text":""},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#multimodal-large-language-models-in-medical-imaging-current-state-and-future-directions","title":"Multimodal Large Language Models in Medical Imaging: Current State and Future Directions","text":"<p>Authors: Yoojin Nam, Dong Yeong Kim, Sunggu Kyung, Jinyoung Seo, Jeong Min Song, Jimin Kwon, Jihyun Kim, Wooyoung Jo, Hyungbin Park, Jimin Sung, Sangah Park, Heeyeon Kwon, Taehee Kwon, Kanghyun Kim, Namkug Kim Year: 2025 Venue: Korean Journal of Radiology</p>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical VLM / MLLM / MMFM + General FM survey / theory  </li> <li> <p>This is a review article that surveys multimodal large language models (MLLMs) for medical imaging, especially radiology, and analyzes architectures, datasets, capabilities, and challenges.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Multimodal FM or cross-modal integration (survey of existing systems)  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Imaging: 2D chest X\u2011ray, CT, MRI, ultrasound, endoscopy, digital pathology, and other clinical photos.  </li> <li>Text: radiology reports, clinical notes, question\u2013answer pairs, and structured EHR data.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>This review paper provides a comprehensive overview of multimodal large language models (MLLMs) in medical imaging, focusing on how they integrate image and text data to support tasks such as radiology report generation, visual question answering (VQA), and interactive diagnostic assistance. The authors first introduce LLMs and vision transformers (ViTs) as the core building blocks, then explain how multimodal connectors and training strategies turn them into MLLMs. They categorize architectures by how images and text are encoded, how connectors project visual features into the LLM\u2019s token space, and how multimodal fusion is achieved (contrastive pretraining, instruction\u2011tuned fusion, generative pipelines, etc.). The paper surveys available datasets, clinical applications, and early systems, highlighting both impressive capabilities and serious limitations such as hallucination, poor region grounding, and the scarcity of large-scale medical multimodal datasets. It closes with a roadmap for future research, emphasizing region\u2011grounded reasoning, robust pretraining on medical data, and safe clinical integration. For a new grad student, this review is an accessible map of the design space and open problems in medical MLLMs.</p>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Understand how to build and deploy multimodal LLMs that can:  </p> <ul> <li>Interpret medical images together with clinical text.  </li> <li>Generate accurate, clinically useful reports and answers.  </li> <li>Act as trustworthy assistants in radiology workflows.  </li> </ul> </li> <li> <p>Why this is hard: </p> </li> <li>Data challenges: <ul> <li>Large, high\u2011quality multimodal datasets (images + reports + EHR) are scarce and often siloed by institution.  </li> <li>Annotations such as region\u2011level labels and detailed textual descriptions are expensive to obtain.  </li> <li>Privacy regulations constrain data sharing and centralized training.  </li> </ul> </li> <li>Modeling challenges: <ul> <li>Radiology images (2D/3D, multi\u2011phase) and clinical notes are heterogeneous and high\u2011dimensional.  </li> <li>Aligning visual features with language at the right granularity (organ, lesion, pixel) is non\u2011trivial.  </li> <li>LLMs trained on web text may hallucinate findings or misuse clinical jargon when connected to images.  </li> </ul> </li> <li>Clinical deployment challenges: <ul> <li>Need for region\u2011grounded explanations and robust behavior across scanners, sites, and populations.  </li> <li>Integration into PACS/RIS and clinician workflows without increasing cognitive load or risk.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets and modalities covered (high level): </li> <li>Imaging: <ul> <li>Chest X\u2011ray (e.g., MIMIC\u2011CXR, CheXpert, ChestX\u2011ray14).  </li> <li>CT and MRI for various organs.  </li> <li>Ultrasound, endoscopy, ophthalmology images, and digital pathology slides.  </li> </ul> </li> <li> <p>Text and structured data: </p> <ul> <li>Radiology and pathology reports.  </li> <li>Clinical notes and EHR fields (demographics, lab values, vital signs).  </li> <li>QA pairs and instruction\u2011style prompts for training medical MLLMs.  </li> </ul> </li> <li> <p>Pretraining / representation patterns: </p> </li> <li>Vision encoders (often ViTs or CNNs) map images to dense feature maps or patch tokens.  </li> <li>Text encoders/decoders (LLMs) operate on tokenized reports and prompts.  </li> <li>Multimodal connectors (projection layers, query transformers, fusion modules, or expert\u2011driven converters) transform image features into token sequences consumable by the LLM.  </li> <li> <p>For contrastive pretraining, image and report embeddings are projected into a shared space for CLIP\u2011like alignment.</p> </li> <li> <p>Limitations of current datasets: </p> </li> <li>Many datasets are single\u2011center, with limited diversity in disease spectrum, scanners, and languages.  </li> <li>Public multimodal datasets often focus on chest imaging; other organs and modalities are underrepresented.  </li> <li>Region\u2011level and temporal annotations (for localization, progression tracking) are relatively rare.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type (surveyed archetypes): </li> <li>Contrastive VLMs: CLIP\u2011like models that learn a shared embedding space for images and reports.  </li> <li>Fusion\u2011based MLLMs: LLaVA\u2011style architectures where image tokens are injected into an LLM via cross\u2011attention or fusion blocks.  </li> <li> <p>Generative models: Systems that generate images or segmentations conditioned on text, or generate text conditioned on images (e.g., report generation).  </p> </li> <li> <p>New FM vs existing: </p> </li> <li> <p>The paper does not introduce a single new model; instead, it synthesizes architectural patterns and design choices across many existing MLLMs.</p> </li> <li> <p>Key components and innovations (framework level):</p> </li> </ul> Aspect Details Encoders Pretrained vision encoders (ViTs, CNNs) and LLMs as backbones Connectors Projection\u2011based, query\u2011based (Q\u2011former), fusion\u2011based, and expert\u2011driven language transformers Training strategies Contrastive pretraining, instruction tuning, chain\u2011of\u2011thought prompting, RLHF for clinical alignment Capabilities Report generation, VQA, retrieval, triage, decision support, image\u2011grounded dialog <ul> <li>Training setup (typical): </li> <li>Pretrain image\u2013text alignment on paired radiology datasets.  </li> <li>Adapt a general or medical LLM to accept visual tokens through connectors.  </li> <li>Instruction\u2011tune on multimodal tasks (RRG, VQA, captioning, dialog) using curated or synthetic data.  </li> <li>Optionally apply RLHF or preference optimization to improve clinical safety and usefulness.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This review is fundamentally about multimodal integration in medical imaging MLLMs.</p> <ul> <li>Modalities integrated: </li> <li> <p>Radiology and other medical images + text (reports, clinical notes, QA) + sometimes structured EHR signals.  </p> </li> <li> <p>Integration mechanisms: </p> </li> <li>CLIP\u2011style two\u2011tower alignment: <ul> <li>Separate image and text encoders trained with contrastive loss for retrieval and zero\u2011shot classification.  </li> </ul> </li> <li> <p>Connector\u2011based fusion: </p> <ul> <li>Projection\u2011based: linear or MLP projections from image features into the LLM token space.  </li> <li>Query\u2011based: learnable query tokens attend to visual features and feed condensed information to the LLM.  </li> <li>Fusion\u2011based: cross\u2011attention layers inside or around the LLM that jointly process image and text tokens.  </li> <li>Expert\u2011driven language transformation: upstream models convert imaging findings into textual descriptions consumed by an LLM.  </li> </ul> </li> <li> <p>New capabilities enabled: </p> </li> <li>Image\u2011grounded natural\u2011language interaction (VQA, \u201cchat with your scan\u201d).  </li> <li>Zero\u2011shot or few\u2011shot disease classification via text prompts.  </li> <li>Automated or draft radiology report generation.  </li> <li>Multimodal retrieval (image \u2194 text, patient\u2011level search).</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks and benchmarks discussed: </li> <li>Radiology report generation (RRG) from chest X\u2011rays and CT.  </li> <li>Visual question answering about imaging findings and clinical context.  </li> <li>Image\u2011text retrieval and cross\u2011modal search.  </li> <li>Disease detection and classification from multimodal inputs.  </li> <li> <p>Early explorations of planning, triage, and longitudinal reasoning.  </p> </li> <li> <p>Baselines and comparison themes: </p> </li> <li>Traditional unimodal CNN/ViT models vs multimodal systems.  </li> <li>General LLMs with simple image adapters vs domain\u2011specific medical MLLMs.  </li> <li> <p>Trade\u2011offs between model size, task performance, and compute requirements.  </p> </li> <li> <p>Key findings (high\u2011level trends): </p> </li> <li>MLLMs show promising capabilities for RRG, VQA, and multimodal reasoning, often outperforming unimodal baselines on complex tasks.  </li> <li>However, performance can be unstable across datasets and institutions, and models frequently hallucinate or misinterpret subtle findings.  </li> <li>Region grounding and localization remain weak; many models reason about images at a coarse, global level.  </li> <li>There is a growing shift toward foundation\u2011model approaches, leveraging large general or medical LLMs and pre\u2011trained vision encoders.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths of the review and current field:</p> <ul> <li>Provides a clear taxonomy of MLLM architectures, connectors, and training strategies for medical imaging.  </li> <li>Highlights the importance of multimodal reasoning that mirrors how radiologists combine images and clinical context.  </li> <li>Synthesizes evidence from recent prototypes and studies, giving readers an overview of what is currently feasible.  </li> <li>Emphasizes practical considerations for clinical adoption (data needs, infrastructure, workflow integration).</li> </ul> <p>Limitations and challenges (field\u2011level):</p> <ul> <li>Scarcity of large, diverse, high\u2011quality multimodal datasets with region\u2011level labels and outcome annotations.  </li> <li>High risk of hallucinated findings and uncalibrated confidence, especially when MLLMs operate outside their training distribution.  </li> <li>Limited interpretability and weak region grounding, making it hard to trust model outputs for critical decisions.  </li> <li>Heavy computational and infrastructure demands, which may be unsuitable for resource\u2011constrained hospitals.  </li> <li>Regulatory, privacy, and liability questions around deploying MLLMs in clinical care.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can we design MLLMs with robust region grounding, so that textual outputs are tightly coupled to specific image regions?  </li> <li>What training strategies and evaluation protocols are needed to reduce hallucination and ensure clinically safe behavior?  </li> <li>How can we leverage synthetic data, weak supervision, and federated learning to overcome data scarcity and privacy constraints?  </li> <li>What are effective ways to integrate EHR data, temporal imaging series, and multi\u2011organ information into a unified multimodal reasoning system?  </li> <li>How should guidelines, benchmarks, and regulations evolve to evaluate and govern MLLMs in real radiology workflows?</li> </ol>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>This paper is one of the first detailed reviews focused specifically on MLLMs for medical imaging, complementing broader MMFM and HFM surveys.  </li> <li>It connects general LLM and VLM advances (e.g., CLIP, LLaVA\u2011style architectures) to radiology\u2011specific tasks and constraints.  </li> <li>Relation to well-known ideas: </li> <li>Frames medical MLLMs as extensions of CLIP\u2011like alignment and LLM\u2011centric fusion architectures, adapted to clinical data.  </li> <li>Discusses how instruction tuning, chain\u2011of\u2011thought prompting, and RLHF\u2014successful in general AI\u2014might be adapted to medical imaging.  </li> <li>Why this review is a useful reference: </li> <li>For a grad student, it offers a curated tour of design patterns, datasets, and open challenges, making it easier to choose a research direction.  </li> <li>It also highlights the gap between prototype demos and clinically robust systems, encouraging critical evaluation and responsible innovation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mm_llm_imaging_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Radiology practice is inherently multimodal, but most traditional AI systems are unimodal and cannot fully leverage combined image + text + EHR information.  </p> </li> <li> <p>Method / model (conceptual): </p> </li> <li>MLLMs couple powerful LLMs with vision encoders via multimodal connectors (projection, query, fusion, or expert\u2011driven), enabling joint reasoning over images and text.  </li> <li> <p>This review categorizes these architectures and training strategies, providing a design space for medical imaging MLLMs.  </p> </li> <li> <p>Results / insights: </p> </li> <li>Early MLLMs show strong potential for RRG, VQA, and multimodal decision support, often surpassing unimodal baselines.  </li> <li> <p>However, they are hampered by data scarcity, hallucination, poor region grounding, and deployment challenges.  </p> </li> <li> <p>Why it matters: </p> </li> <li>Understanding MLLMs is crucial for building the next generation of clinically useful, trustworthy multimodal foundation models in radiology.  </li> <li>This review gives practitioners and researchers a roadmap for tackling open problems and responsibly advancing the field.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/","title":"\"Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions\"","text":""},{"location":"generated/kb_curated/papers-md/mmfm_2025/#medical-multimodal-foundation-models-in-clinical-diagnosis-and-treatment-applications-challenges-and-future-directions","title":"Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions","text":"<p>Authors: Kai Sun, Siyan Xue, Fuchun Sun, Haoran Sun, Yu Luo, Ling Wang, Siyuan Wang, Na Guo, Lei Liu, Tian Zhao, Xinzhou Wang, Lei Yang, Shuo Jin, Jun Yan, Jiahong Dong Year: 2025 Venue: Artificial Intelligence in Medicine</p>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical VLM / MLLM / MMFM + General FM survey / theory  </li> <li> <p>This work is a survey that systematizes datasets, architectures, and clinical applications of medical multimodal foundation models (MMFMs).</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Multimodal FM or cross-modal integration (survey and taxonomy)  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Medical images (CT, MRI, ultrasound, radiography, surgical video), text (reports, clinical notes), structured clinical data, and in some cases robotic and physiological signals.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>This survey reviews the rapidly growing field of medical multimodal foundation models (MMFMs), which aim to leverage diverse medical data\u2014images, text, signals, and more\u2014to support clinical diagnosis and treatment. The authors first trace the evolution of foundation models from transformers and vision transformers to multimodal models, then categorize MMFMs into medical multimodal vision foundation models (MMVFMs) and medical multimodal vision\u2013language foundation models (MMVLFMs). They systematically describe available datasets, proxy tasks (segmentation, generation, contrastive learning, hybrid tasks), and model architectures, and then connect these to downstream applications such as radiology report generation, disease diagnosis, treatment planning, and surgical robotics. The survey emphasizes both the opportunities (better generalization, data efficiency, cross\u2011task transfer) and the significant challenges (data quality, compute cost, fairness, interpretability, deployment, and regulation). For a new grad student, this paper serves as a comprehensive starting point for understanding the MMFM landscape and identifying promising research directions.</p>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Provide a unified view of medical multimodal foundation models that can:  </p> <ul> <li>Integrate heterogeneous medical data (multi\u2011organ, multi\u2011modality).  </li> <li>Support a wide spectrum of clinical tasks from diagnosis to treatment.  </li> <li>Move toward generalized medical AI rather than narrow, task\u2011specific models.  </li> </ul> </li> <li> <p>Why this is hard: </p> </li> <li>Data issues: <ul> <li>Medical data are high\u2011dimensional, heterogeneous (images, waveforms, text, lab results), and often noisy or incomplete.  </li> <li>Large multimodal datasets with consistent labeling and harmonization are rare; privacy and legal restrictions complicate sharing.  </li> </ul> </li> <li>Modeling challenges: <ul> <li>Designing architectures that scale to multiple organs, modalities, and tasks while remaining efficient.  </li> <li>Balancing generality with specialization; avoiding catastrophic forgetting while adapting to new tasks.  </li> </ul> </li> <li>Clinical and societal constraints: <ul> <li>Need for interpretability, fairness, robustness, and safe deployment in real clinical environments.  </li> <li>Regulatory frameworks and validation protocols are still evolving for foundation\u2011model\u2011based systems.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>The survey devotes substantial space to dataset landscapes for MMFMs.</p> <ul> <li>Plain text datasets: </li> <li>Large corpora of biomedical literature, clinical guidelines, and clinical notes.  </li> <li> <p>Used primarily to train or adapt medical LLM backbones that pair with vision or other modalities.</p> </li> <li> <p>Medical image datasets: </p> </li> <li>Diverse imaging modalities: CT, MRI, X\u2011ray, ultrasound, endoscopy, digital pathology, ophthalmology, etc.  </li> <li> <p>Multi\u2011organ and multi\u2011center datasets that enable cross\u2011domain learning.  </p> </li> <li> <p>Image\u2013text pair datasets: </p> </li> <li>Radiology and pathology image\u2013report pairs enabling CLIP\u2011style or CoCa\u2011style vision\u2013language pretraining.  </li> <li> <p>Datasets annotated with segmentation masks, bounding boxes, or keypoints for segmentation\u2011oriented proxy tasks.  </p> </li> <li> <p>Other modalities: </p> </li> <li> <p>Surgical videos, robotics telemetry, physiological signals, and multi\u2011omics data for advanced MMFM applications.  </p> </li> <li> <p>Preprocessing / representation themes: </p> </li> <li>Standardization of image resolutions and voxel spacing; patch extraction and multi\u2011scale crops for large images.  </li> <li>Tokenization and normalization of medical text; mapping structured EHR fields to embeddings.  </li> <li>Careful balancing of modalities to avoid dominance of one data type in multimodal pretraining.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type (taxonomy): </li> <li> <p>The paper distinguishes:  </p> <ul> <li>MMVFMs (Medical Multimodal Vision Foundation Models): multimodal vision encoders focusing on multiple medical image modalities.  </li> <li>MMVLFMs (Medical Multimodal Vision\u2013Language Foundation Models): models that combine image and text (and sometimes more modalities) in a shared framework.  </li> </ul> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>This is a survey; it does not introduce a specific model but analyzes and categorizes many.</p> </li> <li> <p>Proxy task categories (core contribution):</p> </li> </ul> Category Role in MMFMs Segmentation proxy Use segmentation tasks to learn detailed anatomical representations Generative proxy Use generative tasks (reconstruction, synthesis) for representation learning Contrastive proxy Use CLIP\u2011like or contrastive objectives for cross\u2011modal alignment Hybrid proxy Combine segmentation, generative, and contrastive objectives to cover multiple skills <ul> <li>Architectural themes: </li> <li>Transformer\u2011based encoders and decoders for both vision and language.  </li> <li>Two\u2011tower architectures for CLIP\u2011style alignment vs unified encoders for fully fused multimodal representations.  </li> <li> <p>Use of adapters, prompts, and low\u2011rank fine\u2011tuning (LoRA) for efficient adaptation of large backbones.  </p> </li> <li> <p>Training setup (generic patterns): </p> </li> <li>Large\u2011scale pretraining on multi\u2011organ, multi\u2011modality datasets using one or more proxy tasks.  </li> <li>Fine\u2011tuning or prompting on downstream tasks such as segmentation, detection, classification, report generation, and surgical control.  </li> <li>Multi\u2011task and multi\u2011stage training regimes to gradually build generalized capabilities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>Multimodal integration is the central theme of MMFMs.</p> <ul> <li>Modalities integrated: </li> <li> <p>Combinations of images, text, clinical variables, and sometimes signals like robotics trajectories or physiology.  </p> </li> <li> <p>Integration strategies: </p> </li> <li>Early fusion: combine modalities at the input or low\u2011level feature stage (e.g., concatenated embeddings).  </li> <li>Intermediate fusion: fuse modality\u2011specific encoders via cross\u2011attention or shared latent spaces.  </li> <li>Late fusion: combine modality\u2011specific model outputs via ensembles or simple aggregators.  </li> <li> <p>Vision\u2013language alignment: CLIP\u2011style or CoCa\u2011style objectives to map image and text into a shared space.  </p> </li> <li> <p>What this enables: </p> </li> <li>More holistic modeling of patient state by combining imaging, text, and structured data.  </li> <li>Cross\u2011task and cross\u2011organ transfer: representations learned for one modality or organ can help others.  </li> <li>Unified models that can support multiple downstream tasks from a shared backbone.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>As a survey, the paper does not present new experiments; instead, it summarizes trends in published MMFM work.</p> <ul> <li>Tasks discussed: </li> <li>Segmentation (organ, lesion) and detection tasks across various imaging modalities.  </li> <li>Disease classification, risk prediction, and prognosis.  </li> <li>Radiology report generation and medical image captioning.  </li> <li> <p>Surgical planning, navigation, and robotics control.  </p> </li> <li> <p>Key observations: </p> </li> <li>MMFMs often outperform single\u2011task, single\u2011modality baselines, especially when downstream labels are scarce.  </li> <li>Contrastive and hybrid proxy tasks tend to support better zero\u2011shot and few\u2011shot generalization.  </li> <li>Increasing data scale and modality diversity generally improves robustness, but also raises compute and data\u2011governance issues.  </li> <li>There is still a gap between offline benchmark performance and the reliability needed for clinical deployment.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths (of MMFMs and the survey):</p> <ul> <li>Provides a unified taxonomy that organizes MMFMs by proxy tasks, modalities, and architectures.  </li> <li>Connects method choices (segmentation vs contrastive vs hybrid) to clinical application domains.  </li> <li>Highlights how MMFMs can support precision medicine, from early diagnosis to personalized treatment.  </li> <li>Identifies key datasets and benchmarks, giving readers a practical starting point for experimentation.</li> </ul> <p>Limitations and challenges:</p> <ul> <li>Many MMFMs rely on limited or biased datasets, often from a small number of institutions or populations.  </li> <li>Compute and data requirements are high, raising concerns about environmental impact and accessibility.  </li> <li>Interpretability and explainability remain limited, especially for complex multimodal reasoning.  </li> <li>Fairness and generalization to under\u2011represented groups are not yet well studied.  </li> <li>Real\u2011world deployment faces hurdles in regulation, integration, and clinician acceptance.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can we develop data\u2011efficient MMFMs that retain strong performance without requiring enormous datasets and compute?  </li> <li>What are effective strategies for fair and robust multimodal pretraining, especially across institutions and populations?  </li> <li>How can we integrate causal and mechanistic knowledge into MMFMs to move beyond pattern recognition?  </li> <li>What evaluation frameworks are needed to measure trustworthiness, interpretability, and clinical impact of MMFMs?  </li> <li>How should MMFMs interface with clinical workflows and decision\u2011support systems to provide value without over\u2011automation?</li> </ol>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>This survey sits alongside other HFM and MLLM reviews but focuses specifically on medical multimodal foundation models that bridge multiple imaging modalities, text, and clinical tasks.  </li> <li>It articulates how MMFMs can underpin next\u2011generation clinical AI systems that span diagnosis, treatment, and surgical assistance.  </li> <li>Relation to well-known ideas: </li> <li>Builds on CLIP\u2011like vision\u2013language alignment, transformer\u2011based FMs (BERT, ViT, GPT), and recent medical FMs like M3FM, Me\u2011LLaMA, and TITAN.  </li> <li>Frames MMFMs as a path toward medical artificial general intelligence, while emphasizing the importance of safety and governance.  </li> <li>Why this paper is a useful reference: </li> <li>For students and practitioners, it provides a broad yet structured overview of the MMFM space, helping them situate specific models and choose research directions.  </li> <li>It also highlights critical non\u2011technical dimensions (fairness, regulation, deployment) that will shape the real\u2011world impact of MMFMs.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mmfm_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Medical AI needs foundation models that can handle multiple modalities and organs and support tasks ranging from diagnosis to treatment planning.  </p> </li> <li> <p>Method / model (conceptual): </p> </li> <li>MMFMs are built on transformer\u2011based backbones and trained using segmentation, generative, contrastive, or hybrid proxy tasks on large multimodal datasets.  </li> <li> <p>The survey categorizes models into MMVFMs and MMVLFMs and analyzes their architectures, datasets, and applications.  </p> </li> <li> <p>Results / insights: </p> </li> <li>MMFMs generally improve performance and data efficiency over narrow models, and support zero\u2011shot or few\u2011shot adaptation to new tasks.  </li> <li> <p>However, they face major challenges around data quality, compute cost, fairness, interpretability, and deployment.  </p> </li> <li> <p>Why it matters: </p> </li> <li>MMFMs are likely to be the backbone technology for future clinical AI systems; understanding their design and limitations is crucial.  </li> <li>This survey offers a roadmap for advancing MMFMs responsibly toward real\u2011world impact.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/","title":"Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models","text":"<p>Authors: Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin Year: 2025 Venue: Transactions on Machine Learning Research (TMLR)</p>"},{"location":"generated/kb_curated/papers-md/mot_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Vision / VLM / Multimodal FM  </li> <li> <p>The paper proposes a new architecture for unified multi-modal generation and understanding (text, images, and speech) in large foundation models.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Text (language tokens).  </li> <li>Images (discrete image tokens for generation and understanding).  </li> <li>Speech (discrete speech tokens in some settings).</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>Mixture-of-Transformers (MoT) is a sparse multi-modal transformer architecture designed to make large, unified foundation models for text, images, and speech much more computationally efficient. Instead of running a single dense transformer over all modalities, MoT keeps global self\u2011attention over the full mixed sequence but decouples all non\u2011embedding parameters by modality: feed\u2011forward networks, attention projections, and layer norms are specialized for each modality while sharing the same FLOP budget as the dense baseline. The authors evaluate MoT in several settings, including Chameleon\u2011style autoregressive text\u2013image and text\u2013image\u2013speech generation and Transfusion\u2011style models that combine autoregressive text generation with diffusion\u2011based image generation. Across scales from tens of millions to billions of parameters, MoT consistently matches or exceeds dense baselines while using 40\u201360% of the pretraining FLOPs, and delivers substantial wall\u2011clock speedups. The paper also studies modality separation, leave\u2011one\u2011modality\u2011out experiments, and hybrid models that mix MoT with MoE\u2011style experts, showing that modality-aware sparsity is a stable and effective alternative to learned routing. For a new grad student, MoT is a valuable example of how to introduce structured sparsity into multimodal transformers without sacrificing architectural simplicity.</p>"},{"location":"generated/kb_curated/papers-md/mot_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Build unified multi-modal foundation models that can jointly process and generate text, images, and speech, but do so with manageable compute budgets.  </li> <li> <p>Reduce the training and inference costs of early\u2011fusion multimodal transformers (like Chameleon\u2011style models) without giving up performance.</p> </li> <li> <p>Why this is hard: </p> </li> <li>Dense transformers scale poorly when extended from text\u2011only LLMs to multi-modal settings; adding image and speech tokens massively increases sequence lengths and token diversity.  </li> <li>Different modalities have conflicting optimization dynamics and live in very different regions of representation space, so a single set of shared parameters may be suboptimal.  </li> <li>Mixture-of-Experts (MoE) architectures introduce routing instability, load\u2011balancing overhead, and complex bi\u2011level optimization; they are powerful but hard to train robustly at scale.  </li> <li>Any new architecture must preserve implementation simplicity and FLOP accounting so that practitioners can reason about cost vs. quality.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>MoT is evaluated in multiple multi-modal scenarios built on existing benchmarks and systems:</p> <ul> <li>Chameleon setting (text\u2013image, text\u2013image\u2013speech): </li> <li>Autoregressive generation over interleaved text and image tokens; extended to include discrete speech tokens in a three\u2011modality setup.  </li> <li> <p>Trained on large\u2011scale text\u2013image and text\u2013image\u2013speech datasets similar to those used for prior Chameleon models.</p> </li> <li> <p>Transfusion setting (text + diffusion images): </p> </li> <li>Text is modeled autoregressively as in standard LLMs.  </li> <li> <p>Images are modeled with diffusion\u2011based objectives, using latent image representations.  </p> </li> <li> <p>Modalities: </p> </li> <li>Text: natural language prompts, captions, and conversational context.  </li> <li>Images: discrete or latent tokens used for generation and captioning benchmarks.  </li> <li> <p>Speech: discrete speech tokens for audio generation and understanding (in the Chameleon+Speech experiments).</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>All modalities are converted into a single mixed token sequence, with tokens tagged by modality so that MoT can apply modality-specific parameters while keeping shared self\u2011attention over the whole sequence.  </li> <li>For diffusion images, latent representations and timesteps are embedded following standard diffusion\u2011transformer practices.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Sparse transformer architecture (Mixture-of-Transformers) with modality-specific parameters but dense\u2011style global self\u2011attention.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>MoT is a new architectural pattern that can be plugged into existing multimodal settings (e.g., Chameleon, Transfusion) while preserving their training objectives.</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Sparsity mechanism Modality-aware sparsity over all non\u2011embedding parameters (FFNs, attention matrices, layer norms). Shared attention Full self\u2011attention over the mixed multi-modal sequence; no routing\u2011based sparsity in attention. Parameter decoupling For each modality, separate parameter sets for projections and FFNs, selected by token modality. Compatibility Drop\u2011in replacement for dense transformers in Chameleon and Transfusion architectures. Hybrid models Combining MoT with MoE\u20114x to explore complementary benefits of expert routing and modality sparsity. <ul> <li>Training setup (high level):</li> <li>Pretrain 13 models of various sizes (37M\u20137B, plus hybrid architectures) across multiple Chameleon and Transfusion settings.  </li> <li>Objectives:  <ul> <li>Autoregressive next\u2011token prediction for text and image tokens (Chameleon).  </li> <li>Autoregressive text + diffusion\u2011based image objectives (Transfusion).  </li> </ul> </li> <li>Compute measured in FLOPs and wall\u2011clock time; experiments run on multi\u2011GPU clusters (e.g., AWS p4de instances with A100s).</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>MoT directly targets multi-modal integration in unified transformers:</p> <ul> <li>Modalities integrated: </li> <li> <p>Text, images, and (in some experiments) speech, all represented as tokens in one interleaved sequence.</p> </li> <li> <p>How they are integrated: </p> </li> <li>The model applies global self\u2011attention across all tokens regardless of modality, enabling cross\u2011modal interactions at every layer.  </li> <li>For each token, a simple rule based on its modality selects which FFN, attention projections, and layer norms to use; this is rule\u2011based routing by modality, not learned MoE routing.  </li> <li> <p>This yields a sparse model where only a subset of parameters are active for each token, but the computational graph (FLOPs) matches a dense transformer.</p> </li> <li> <p>Why this integration is useful / new capabilities: </p> </li> <li>Maintains the strengths of early\u2011fusion multimodal transformers (rich cross\u2011modal attention) while reducing compute and avoiding MoE training instability.  </li> <li>Makes it feasible to train unified multi-modal foundation models at larger scales and on more complex objectives (e.g., mixed autoregressive + diffusion) with limited resources.  </li> <li>Provides a clean baseline architecture for future multi-modal FMs that want structured sparsity without heavy routing machinery.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Settings: </li> <li>Chameleon (text\u2013image, text\u2013image\u2013speech) for unified autoregressive generation.  </li> <li>Transfusion (text + diffusion images) for mixed\u2011objective training.  </li> <li> <p>Additional analyses of modality separation, leave\u2011one\u2011modality\u2011out behavior, and systems aspects (throughput, scaling).</p> </li> <li> <p>Baselines: </p> </li> <li>Dense Chameleon and Transfusion transformers at comparable parameter scales.  </li> <li> <p>Mixture-of-Experts (MoE\u20114x) variants that increase parameter count via expert routing.</p> </li> <li> <p>Key findings (trends): </p> </li> <li>In Chameleon 7B, MoT matches dense performance on text and image metrics while using only 55.8% of pretraining FLOPs.  </li> <li>When extended to text\u2013image\u2013speech, MoT reaches speech performance comparable to the dense baseline with ~37% of the FLOPs for that modality.  </li> <li>In the Transfusion setting, a 760M MoT outperforms a 1.4B dense baseline on image generation and captioning metrics while using half the training and inference FLOPs; a 7B MoT matches the 7B dense model with roughly one third of the FLOPs for the image modality.  </li> <li>System profiling shows that MoT achieves the same image quality in 47.2% of the wall\u2011clock time and similar text quality in 75.6% of the time on A100 clusters.  </li> <li>Compared to MoE\u20114x, MoT often provides better or more stable performance at similar or lower FLOP budgets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Simple, modality-aware sparsity mechanism that integrates cleanly into standard transformers.  </li> <li>Demonstrated compute savings (FLOPs and wall\u2011clock) without sacrificing performance across several challenging multi-modal setups.  </li> <li>Extensive empirical evaluation across model sizes and tasks, including systems\u2011level profiling.  </li> <li>Provides a practical alternative to MoE that avoids routing instability and complex load balancing.</li> </ul> <p>Limitations:</p> <ul> <li>Still focused on a relatively small set of modalities (text, images, speech); does not cover more exotic or structured modalities (e.g., audio waveforms, tabular EHR, 3D point clouds).  </li> <li>Requires modality labels for tokens; does not explore more fine\u2011grained routing within a modality (e.g., by region or task).  </li> <li>Results are tied to specific training infrastructures and datasets; real\u2011world deployment costs may differ.  </li> <li>The paper does not deeply explore how MoT interacts with instruction tuning, long\u2011context training, or reinforcement learning, which are important in practice.</li> </ul> <p>Open questions and future directions:</p> <ol> <li>How well does MoT extend to more modalities and tasks, such as video, 3D data, or structured signals like EHR?  </li> <li>Can we combine modality-aware sparsity with learned experts in a principled way, getting the best of MoT and MoE while keeping training stable?  </li> <li>How does MoT behave under heavy instruction tuning or RLHF, where gradients can be noisy and task distributions shift?  </li> <li>Could similar modality-specific parameter decoupling be applied to encoder\u2013decoder or diffusion\u2011only architectures in a clean way?  </li> <li>What are the implications of MoT\u2011style specialization for interpretability, e.g., understanding what each modality-specific block has learned?</li> </ol>"},{"location":"generated/kb_curated/papers-md/mot_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>MoT sits in the line of work on unified multi-modal transformers (e.g., Chameleon, Transfusion, integrated transformer\u2011diffusion models) and offers a new way to scale them efficiently.  </li> <li>It complements MoE\u2011style sparse methods by providing a simpler, modality\u2011aware alternative that still preserves early\u2011fusion attention.  </li> <li>Relation to well-known ideas: </li> <li>Borrowing the idea of sparsity per token from MoE, but replacing learned routing with deterministic routing by modality.  </li> <li>Conceptually similar to having per\u2011modality adapters everywhere in the transformer, but integrated as full parameter sets rather than small adapter layers.  </li> <li>Why it is a useful reference: </li> <li>For researchers designing multi-modal FMs, MoT shows how to trade off computation and flexibility without sacrificing architectural clarity.  </li> <li>For systems and efficiency work, it offers a realistic case study in measuring FLOPs and wall\u2011clock savings in large multimodal training runs.</li> </ul>"},{"location":"generated/kb_curated/papers-md/mot_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Early\u2011fusion multimodal transformers for unified text\u2013image\u2013speech generation are extremely compute\u2011hungry; MoE\u2011style sparsity is powerful but unstable and complex.</p> </li> <li> <p>Method / model: </p> </li> <li>Mixture-of-Transformers (MoT) introduces modality-aware sparsity by decoupling non\u2011embedding transformer parameters per modality while keeping global self\u2011attention and the same FLOP budget as dense models.  </li> <li> <p>It acts as a drop\u2011in replacement for dense transformers in Chameleon and Transfusion\u2011style architectures.</p> </li> <li> <p>Results: </p> </li> <li>Matches or exceeds dense baselines across Chameleon and Transfusion setups while using 40\u201360% of the pretraining FLOPs and significantly less wall\u2011clock time.  </li> <li> <p>Scales well across model sizes and remains competitive with MoE\u2011based baselines at similar or lower compute.</p> </li> <li> <p>Why it matters: </p> </li> <li>Demonstrates that structured, modality-aware sparsity is a practical way to scale multimodal foundation models, preserving rich cross\u2011modal interactions while controlling compute.  </li> <li>Provides a clean architectural template for future unified VLMs and multimodal FMs focused on efficiency.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/","title":"Learning multi-site harmonization of magnetic resonance images without traveling human phantoms","text":"<p>Authors: Siyuan Liu, Pew-Thian Yap Year: 2024 Venue: Communications Engineering^https://www.nature.com/articles/s44172-023-00140-w</p>"},{"location":"generated/kb_curated/papers-md/murd_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>MRI harmonization / domain adaptation. This paper introduces a deep learning framework (MURD) for multi-site MRI harmonization that does not require traveling human phantoms.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Method / pre-processing. MURD is an image-space harmonization method that can be applied before downstream foundation models or classical pipelines; it is not itself a foundation model.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Structural MRI (T1- and T2-weighted brain images) from multiple sites and scanners.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>Large multi-site MRI studies (ABCD, ADNI, etc.) suffer from site-specific appearance differences due to vendor, scanner, and protocol variability, which introduce non-biological variance that can confound downstream analyses. Traditional retrospective harmonization methods either operate on global intensity distributions (e.g., ComBat) or require traveling human phantoms\u2014the same subjects scanned at multiple sites\u2014to supervise deep learning models, which is logistically impractical at scale.</p> <p>This paper proposes MURD (multi-site unsupervised representation disentangler), a deep generative framework that learns to disentangle each image into a site-invariant anatomical content representation and a site-specific style representation (intensity/contrast). MURD uses content and style encoders plus a generator to synthesize images for any target site by recombining content with different site styles. Crucially, it learns these representations without paired multi-site scans of the same subject, relying instead on unpaired images and multi-domain image-to-image translation techniques. On &gt;6,000 multi-site T1/T2 images, MURD generates harmonized images that match site-specific appearances while preserving anatomical details, improving cross-site consistency for downstream tasks and enabling retrospective harmonization of existing datasets.</p>"},{"location":"generated/kb_curated/papers-md/murd_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li>Harmonize MRI data across sites and scanners without requiring traveling human phantoms or exhaustive prospective protocol tuning.  </li> <li> <p>Reduce non-biological variability to improve reliability and power of downstream analyses (segmentation, registration, diagnosis, FM training).</p> </li> <li> <p>Why this is hard: </p> </li> <li>Scanner / protocol variability: Different vendors, coils, pulse sequences, and parameter settings produce distinct intensity and contrast profiles.  </li> <li>Limited paired data: Paired multi-site scans of the same subject are rare and expensive to obtain.  </li> <li>Existing unsupervised methods: Pairwise unpaired translation methods (CycleGAN-style) require (N(N-1)) mappings for (N) sites and do not fully exploit shared structure across all sites.  </li> <li>Preserving anatomy: Harmonization must not alter anatomical structures or disease-related features while adjusting appearance.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used: </li> <li> <p>6,000 multi-site T1- and T2-weighted brain MRIs acquired across several scanners and protocols (details in the paper\u2019s dataset section).  </p> </li> <li> <p>Multi-site configuration with (N) imaging sites; each site contributes unpaired T1/T2 images.</p> </li> <li> <p>Modalities: </p> </li> <li>Structural MRI (T1w, T2w) volumes.  </li> <li> <p>No non-imaging modalities (e.g., clinical text, genetics) are used for harmonization.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>Standard MRI preprocessing pipeline (brain extraction, bias correction, intensity normalization) before training.  </li> <li>Images are processed in a CNN-compatible grid (3D or 2D slices; see paper for specifics).</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#5-model-method","title":"5. Model / Method","text":"<ul> <li>Model Type: </li> <li> <p>Multi-domain image-to-image translation model with disentangled representations:</p> <ul> <li>Content encoder: captures site-invariant anatomical features.  </li> <li>Style encoder: captures site-specific appearance (intensity/contrast).  </li> <li>Generator: recombines content and style to synthesize images for specific target sites.</li> </ul> </li> <li> <p>Key components and innovations:</p> </li> <li>Representation disentanglement: <ul> <li>Each image (x_i) from site (s_i) is mapped to ((c_i, s_i)), where (c_i) is content (anatomy) and (s_i) is style (site).  </li> <li>Content is shared across sites; style is site-specific.</li> </ul> </li> <li>Unified multi-site model: <ul> <li>Single model handles all (N) sites, rather than learning (N(N-1)) pairwise mappings.  </li> <li>Style codes are indexed by site; content is site-invariant.</li> </ul> </li> <li> <p>Losses: </p> <ul> <li>Reconstruction losses to ensure (G(c_i, s_i)) approximates the original image.  </li> <li>Style-consistency and content-consistency losses to encourage disentanglement.  </li> <li>Adversarial / perceptual losses to ensure realistic harmonized images for each site.</li> </ul> </li> <li> <p>Training setup:</p> </li> <li>Unpaired multi-site images; no traveling phantoms needed.  </li> <li>Training objective encourages the generator to synthesize images that match the target site\u2019s style while preserving content.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#6-multimodal-integration-aspects","title":"6. Multimodal / Integration Aspects","text":"<ul> <li>Not multimodal: </li> <li>MURD operates purely on structural MRI (T1/T2). It does not ingest genetics, fMRI, or clinical text directly.</li> <li>Integration relevance: </li> <li>MURD is a pre-harmonization step for any downstream modality that depends on T1/T2-derived features (e.g., FreeSurfer IDPs, surface meshes).  </li> <li>Harmonized T1/T2 volumes can feed into neuroimaging FMs (BrainLM, SwiFT, BrainMT, Brain Harmony) or standard pipelines, reducing site confounds before embedding extraction.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#7-experiments-and-results","title":"7. Experiments and Results","text":""},{"location":"generated/kb_curated/papers-md/murd_2024/#main-findings","title":"Main findings","text":"<ul> <li>Effective harmonization: </li> <li>MURD can synthesize images with realistic site-specific appearances for arbitrary target sites while preserving anatomical content, as assessed qualitatively and quantitatively.  </li> <li> <p>Harmonized images show improved inter-site consistency in intensity distributions and image statistics.</p> </li> <li> <p>Improved downstream performance: </p> </li> <li>Models trained on harmonized images generalize better across sites compared to models trained on raw, unharmonized data.  </li> <li>Harmonization benefits tasks such as segmentation, registration, and intensity-based measurements.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#comparisons","title":"Comparisons","text":"<ul> <li>Outperforms traditional statistical harmonization (e.g., intensity normalization, ComBat) in capturing fine-grained, region-specific differences.  </li> <li>More scalable than pairwise GAN-based harmonization methods that require separate mappings for each site pair.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#8-strengths-and-limitations","title":"8. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/murd_2024/#strengths","title":"Strengths","text":"<ul> <li>No traveling phantoms required: </li> <li>Uses unpaired multi-site images, making it applicable to existing large-scale studies.</li> <li>Unified multi-site model: </li> <li>Single network handles harmonization across many sites, avoiding (N(N-1)) pairwise mappings.</li> <li>Disentangled representations: </li> <li>Clear separation between anatomical content and site-specific style.</li> <li>Retrospective applicability: </li> <li>Can be applied to already collected datasets for harmonization without new acquisitions.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#limitations","title":"Limitations","text":"<ul> <li>Training complexity: </li> <li>Deep generative model with multiple loss terms; training can be compute-intensive and sensitive to hyperparameters.</li> <li>Modality specificity: </li> <li>Focused on T1/T2 structural MRI; extension to other sequences/modalities requires additional work.</li> <li>Validation scope: </li> <li>Requires careful evaluation to ensure no subtle anatomical distortions are introduced that could bias downstream analyses.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Relation to other work:</li> <li>Extends unsupervised MRI harmonization methods (e.g., CycleGAN-based, information-bottleneck approaches) to a multi-site, unified framework.  </li> <li> <p>Fits into a growing literature on deep generative harmonization tools intended to replace or augment statistical methods like ComBat.</p> </li> <li> <p>Impact on large-scale studies:</p> </li> <li>Enables multi-site studies (ABCD, ADNI, UKB-derived consortia) to retrospectively harmonize structural MRI without additional scanning.  </li> <li>Can improve robustness and generalization of downstream ML models, including foundation models trained on harmonized images.</li> </ul>"},{"location":"generated/kb_curated/papers-md/murd_2024/#10-key-takeaways","title":"10. Key Takeaways","text":"<ol> <li>MURD disentangles site-invariant anatomy and site-specific style to harmonize MRI across sites without paired data.  </li> <li>A single multi-site model scales better than pairwise harmonization, reducing the number of mappings from (N(N-1)) to 1.  </li> <li>Retrospective harmonization becomes feasible at scale, enabling harmonized datasets for downstream neuroimaging FMs.  </li> <li>Pre-harmonization is a critical step in integration pipelines, especially when combining sMRI-derived features with genetics and fMRI.  </li> <li>Careful validation remains essential to ensure that harmonization does not remove or distort biologically meaningful variation.</li> </ol>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/","title":"Oncology multimodal (Waqas 2024)","text":""},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#multimodal-data-integration-for-oncology-in-the-era-of-deep-neural-networks-a-review","title":"Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review","text":"<p>Authors: Asim Waqas, Aakash Tripathi, Ravi P. Ramachandran, Paul A. Stewart, Ghulam Rasool Year: 2024 Venue: Frontiers in Artificial Intelligence</p>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Multimodal / Integration (specifically applied to oncology/cancer research). This is a review paper surveying multimodal deep learning methods\u2014especially Graph Neural Networks (GNNs) and Transformers\u2014for integrating diverse cancer-related data modalities (imaging, omics, clinical records).</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Review of multimodal FM and integration methods. The paper does not propose a new foundation model but systematically reviews how modern deep learning architectures (GNNs, Transformers) are being applied to multimodal oncology data fusion, including references to foundation models like CLIP, GPT-4, and FLAVA.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Radiological imaging (CT, MRI, PET scans)</li> <li>Digitized histopathology (whole-slide images, H&amp;E stains)</li> <li>Multi-omics (genomics, transcriptomics, proteomics, metabolomics)</li> <li>Electronic health records (EHR) and clinical data</li> <li>Hybrid/derived modalities (radiomics, pathomics features)</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This review paper surveys the landscape of multimodal data integration in oncology, focusing on how deep neural networks\u2014particularly Graph Neural Networks (GNNs) and Transformers\u2014are being used to fuse diverse cancer data types for improved diagnosis, prognosis, and treatment prediction. Cancer research generates heterogeneous data across multiple scales and modalities: from imaging (radiology, pathology) to molecular profiles (genomics, transcriptomics, proteomics) to clinical records. Traditional unimodal analyses fail to capture the complex, interconnected nature of cancer biology. The authors present a comprehensive taxonomy of multimodal learning approaches, covering fusion strategies (early, intermediate, late), neural architectures (CNNs, RNNs, GNNs, Transformers), and domain-specific applications in oncology. They highlight how GNNs naturally model relationships among heterogeneous entities (patients, genes, images) and how Transformers, through self-attention, can integrate sequences of multimodal tokens. Key studies are reviewed across tasks like tumor classification, survival prediction, treatment response, and biomarker discovery. The paper also discusses major challenges\u2014data heterogeneity, missing modalities, alignment across scales, interpretability, and the need for large labeled datasets\u2014and points to promising directions, including foundation models and self-supervised pretraining. For researchers and clinicians, this review provides both a conceptual framework for understanding multimodal fusion and a practical roadmap for applying state-of-the-art deep learning to personalized cancer care.</p>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<p>Scientific / practical problem:</p> <ul> <li>Cancer is inherently multimodal: its biology spans genomic mutations, protein expression, tissue morphology, organ-level imaging, and clinical phenotypes.</li> <li>Prediction and personalization goals:</li> <li>Accurate early diagnosis and cancer subtype classification</li> <li>Prognosis (survival, recurrence risk)</li> <li>Treatment response prediction and therapy selection</li> <li>Discovery of prognostic and predictive biomarkers</li> <li>Traditional approaches analyze modalities in isolation (e.g., only imaging or only genomics), missing synergistic information that could improve accuracy and clinical utility.</li> </ul> <p>Why this is hard:</p> <ul> <li>Data heterogeneity:</li> <li>Different modalities have different dimensionalities, scales, and noise characteristics (e.g., high-resolution images vs sparse genomic variants vs tabular clinical data).</li> <li>Modalities are collected with different protocols, scanners, and sequencing platforms, leading to batch effects and site-specific biases.</li> <li>Integration complexity:</li> <li>No universal representation: images are spatial grids, omics are vectors or graphs, clinical data are tabular.</li> <li>Determining when and how to fuse (early vs late) requires domain knowledge and empirical tuning.</li> <li>Missing data:</li> <li>Not all patients have all modalities (e.g., some lack genomic profiling, others lack certain imaging studies).</li> <li>Models must handle partial observations gracefully.</li> <li>Label scarcity and class imbalance:</li> <li>High-quality multi-modal datasets with expert annotations are rare and expensive.</li> <li>Many cancer subtypes are rare, leading to imbalanced training sets.</li> <li>Interpretability and clinical trust:</li> <li>\"Black-box\" deep learning predictions are hard to explain, yet clinicians need to understand why a model predicts a certain outcome.</li> <li>Regulatory and ethical considerations demand transparency.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>Oncology data modalities covered in the review:</p> <ul> <li>Radiological imaging:</li> <li>CT, MRI, PET scans</li> <li> <p>Used for tumor detection, staging, and monitoring response</p> </li> <li> <p>Digitized histopathology:</p> </li> <li>Whole-slide images (WSI) of H&amp;E-stained tissue</li> <li>Immunohistochemistry (IHC) and other stains</li> <li> <p>Pathomics: quantitative features extracted from slides</p> </li> <li> <p>Multi-omics:</p> </li> <li>Genomics: DNA mutations, copy number variations, single nucleotide polymorphisms</li> <li>Transcriptomics: RNA-seq, gene expression profiles</li> <li>Proteomics: Protein abundance and post-translational modifications</li> <li>Metabolomics: Small molecule profiles</li> <li> <p>Epigenomics: DNA methylation, histone modifications</p> </li> <li> <p>Electronic Health Records (EHR) and clinical data:</p> </li> <li> <p>Demographics, clinical notes, laboratory results, treatment history, survival outcomes</p> </li> <li> <p>Radiomics:</p> </li> <li>Hand-crafted or learned features from imaging (texture, shape, intensity)</li> </ul> <p>Major datasets mentioned:</p> <ul> <li>The Cancer Genome Atlas (TCGA): Pan-cancer multi-omics and clinical data</li> <li>Genomic Data Commons (GDC): Centralized repository for TCGA and other NCI programs</li> <li>UK Biobank, All of Us: Large-scale cohorts with imaging and genomics</li> <li>TCIA (The Cancer Imaging Archive): Radiological imaging datasets</li> <li>Various cancer-specific cohorts (e.g., NSCLC, breast cancer, glioblastoma)</li> </ul> <p>Preprocessing / representation:</p> <ul> <li>Images: patches or whole-image embeddings from CNNs</li> <li>Omics: normalized vectors, sometimes projected into lower dimensions</li> <li>Clinical: tabular features, often encoded or embedded</li> <li>Graphs: patients, genes, images as nodes; relationships (co-expression, similarity) as edges</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>Model Types:</p> <p>The review covers a range of architectures for multimodal fusion:</p> Architecture Role in Multimodal Oncology CNNs Feature extraction from images (radiology, pathology) RNNs/LSTMs Sequential clinical data, temporal progression Autoencoders/VAEs Dimensionality reduction, unsupervised feature learning GANs Data augmentation, synthetic image generation Graph Neural Networks (GNNs) Model relationships among patients, genes, and multi-modal entities; handle heterogeneous graphs Transformers Self-attention over multimodal token sequences; pre-trained vision-language models adapted to oncology <p>Focus on GNNs:</p> <ul> <li>Graph representation:</li> <li>Nodes: patients, genes, images, or feature vectors from different modalities</li> <li>Edges: similarity (clinical, genomic), co-occurrence, known biological interactions</li> <li>GNN architectures reviewed:</li> <li>Graph Convolutional Networks (GCN)</li> <li>Graph Attention Networks (GAT)</li> <li>GraphSAGE</li> <li>Message Passing Neural Networks (MPNN)</li> <li>Applications:</li> <li>Patient similarity networks for survival prediction</li> <li>Gene regulatory networks combined with patient omics</li> <li>Pathology graphs (cells/patches as nodes) integrated with omics</li> </ul> <p>Focus on Transformers:</p> <ul> <li>Vanilla Transformers: Self-attention to integrate sequences of multimodal embeddings.</li> <li>Vision Transformers (ViT): Patches of histopathology or radiology images as tokens.</li> <li>Multimodal Transformers: </li> <li>Cross-modal attention between image and text/omics modalities</li> <li>CLIP-like contrastive learning adapted to radiology-pathology or image-genomics pairs</li> <li>Foundation models mentioned: CLIP, GPT-4, FLAVA, and domain-specific models like MedCLIP.</li> </ul> <p>Training setup (general patterns):</p> <ul> <li>Pretraining: Often on large unimodal datasets (e.g., ImageNet for images, public omics for gene embeddings), sometimes with self-supervised objectives.</li> <li>Fine-tuning / transfer learning: Adapt pretrained encoders to oncology tasks with smaller labeled datasets.</li> <li>Fusion stages:</li> <li>Early fusion: Concatenate raw features before modeling.</li> <li>Intermediate (joint) fusion: Learn shared representations mid-network.</li> <li>Late fusion: Train separate modality-specific models, combine predictions at the end.</li> <li>Scale: Varies widely; some studies use hundreds of patients, others leverage TCGA's thousands of samples. Model sizes range from small task-specific networks to large Transformer-based foundation models.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This is fundamentally a multimodal integration review, so this section is central.</p> <p>Which modalities are integrated:</p> <ul> <li>Common pairs/triplets:</li> <li>Radiology + pathology: CT/MRI + histopathology WSI</li> <li>Imaging + omics: Radiology or pathology + genomics/transcriptomics</li> <li>Omics + clinical: Gene expression + EHR/treatment history</li> <li>Triple integration: Imaging + omics + clinical (less common, more challenging)</li> </ul> <p>How they are integrated:</p> <ul> <li>Early fusion:</li> <li>Concatenate features from all modalities into a single vector and feed into a downstream classifier.</li> <li>Pros: Simple, allows the model to learn joint patterns from the start.</li> <li> <p>Cons: Can be dominated by the highest-dimensional modality; requires careful normalization; struggles with missing data.</p> </li> <li> <p>Late fusion:</p> </li> <li>Train separate models for each modality, then combine predictions (e.g., averaging, voting, stacking).</li> <li>Pros: Preserves modality-specific signals; robust to missing modalities; easier to interpret.</li> <li> <p>Cons: May miss complex cross-modal interactions.</p> </li> <li> <p>Intermediate (joint) fusion:</p> </li> <li>Modality-specific encoders produce embeddings that are fused at a middle layer (e.g., via concatenation, attention, or graph pooling) before final prediction.</li> <li>Pros: Balances flexibility and integration.</li> <li> <p>Cons: Requires architectural design choices; harder to optimize.</p> </li> <li> <p>GNN-based fusion:</p> </li> <li>Construct a heterogeneous graph with nodes from different modalities (patient omics, image features, clinical variables).</li> <li>GNN message passing aggregates cross-modal information.</li> <li> <p>Example: A patient node connected to its gene expression profile node and its pathology image embedding node; GNN learns to propagate and combine information.</p> </li> <li> <p>Transformer-based fusion:</p> </li> <li>Represent each modality as a sequence of tokens (e.g., image patches, genomic regions, clinical features).</li> <li>Self-attention and cross-attention layers integrate across modalities.</li> <li>Example: Multimodal Transformer taking pathology image patches and omics embeddings as separate token sets, with attention heads learning cross-modal dependencies.</li> </ul> <p>Why this integration is useful:</p> <ul> <li>Complementary information: Imaging reveals spatial tumor characteristics, omics show molecular drivers, clinical data provide context (age, stage, treatment).</li> <li>Improved prediction: Studies show multimodal models often outperform unimodal baselines on survival, classification, and treatment response tasks.</li> <li>Biological insight: Cross-modal associations (e.g., imaging phenotypes correlated with gene expression) can reveal biomarkers and mechanisms.</li> <li>Personalization: Comprehensive profiles enable tailored treatment recommendations.</li> </ul> <p>Relation to the integration baseline plan:</p> <ul> <li>Late fusion first under heterogeneous semantics:</li> <li>The review aligns with this principle: many successful oncology studies use late fusion or ensemble methods, preserving modality-specific encoders.</li> <li> <p>GNNs and Transformers can implement late fusion naturally (separate encoding + graph/attention-based aggregation).</p> </li> <li> <p>Robustness and evaluation discipline:</p> </li> <li>The review emphasizes the need for rigorous cross-validation, proper train/test splits (especially important given small sample sizes), and metrics like AUROC, AUPRC, C-index for survival.</li> <li> <p>Challenges of missing data and distribution shift are highlighted, aligning with the baseline plan's focus on residualization, covariate adjustment, and bootstrap confidence intervals.</p> </li> <li> <p>CCA and permutation testing:</p> </li> <li> <p>Not explicitly covered in the review, but the principle of exploring modality correlations before heavy fusion is implicit in studies that perform feature selection or canonical correlation analysis on omics and imaging features.</p> </li> <li> <p>Modality sequencing:</p> </li> <li>The review suggests starting with well-characterized modalities (e.g., standard imaging + omics) and progressively adding more complex ones (e.g., pathology WSI, radiomics), consistent with the plan's incremental approach.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>Tasks / benchmarks reviewed:</p> <p>The paper surveys a wide range of studies across multiple oncology tasks:</p> <ul> <li>Tumor classification and subtyping:</li> <li>GNN and Transformer models classify cancer types (e.g., glioma grades, breast cancer molecular subtypes) using combined imaging and omics.</li> <li> <p>Studies report accuracy improvements of 3-10% over unimodal baselines.</p> </li> <li> <p>Survival prediction and prognosis:</p> </li> <li>Multimodal Cox regression models, GNN-based survival networks, and attention-based models integrate clinical, omics, and imaging data.</li> <li> <p>C-index improvements of ~0.05-0.15 compared to clinical-only or omics-only models.</p> </li> <li> <p>Treatment response prediction:</p> </li> <li>Predicting response to chemotherapy, immunotherapy, or targeted therapies.</li> <li> <p>Multimodal approaches combining radiology (baseline tumor imaging) with genomics (mutation profiles) show better discrimination (AUC gains of 0.05-0.10).</p> </li> <li> <p>Biomarker discovery:</p> </li> <li>GNNs identify gene modules and image features associated with outcomes.</li> <li>Transformers' attention weights highlight cross-modal associations (e.g., specific image regions correlating with gene expression patterns).</li> </ul> <p>Baselines:</p> <ul> <li>Unimodal models (imaging-only, omics-only, clinical-only)</li> <li>Traditional ML methods (logistic regression, random forests on concatenated features)</li> <li>Early fusion baselines (simple concatenation + MLP)</li> </ul> <p>Key findings (trends and insights):</p> <ul> <li>Multimodal consistently beats unimodal: Across most studies, integrating multiple data types improves predictive performance, often significantly.</li> <li>Fusion strategy matters: Late fusion and intermediate fusion tend to outperform early fusion, especially when modalities have very different characteristics.</li> <li>GNNs excel at relational data: When patient or gene relationships are explicitly modeled, GNNs capture network effects that simpler models miss.</li> <li>Transformers scale well: Transformer-based models benefit from larger datasets and can leverage pretrained vision-language models (transfer learning from CLIP-like architectures).</li> <li>Interpretability via attention: Attention weights in Transformers and GNN message passing provide some interpretability, highlighting which modality or feature drives predictions.</li> <li>Challenges remain: Performance gains are modest in some cases; missing data and small sample sizes limit generalization; computational cost is high for large-scale models.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Comprehensive survey: Covers a wide range of architectures (CNNs, RNNs, GANs, GNNs, Transformers) and applications in oncology.</li> <li>Taxonomy and framework: Provides a clear taxonomy of fusion strategies and multimodal learning paradigms, useful for researchers entering the field.</li> <li>Focus on emerging methods: Highlights GNNs and Transformers, which are underexplored in oncology compared to computer vision and NLP.</li> <li>Identifies data resources: Lists major multimodal oncology datasets (TCGA, GDC, TCIA, UK Biobank), facilitating reproducible research.</li> <li>Balances technical depth and accessibility: Suitable for both ML researchers new to oncology and oncology researchers new to advanced deep learning.</li> </ul> <p>Limitations:</p> <ul> <li>Limited discussion of causal inference: The review focuses on predictive modeling; less attention to causal relationships or confounding (e.g., how to distinguish direct biological effects from correlations).</li> <li>Sparse coverage of robustness and fairness: Issues like model bias across demographics, generalization to external cohorts, and adversarial robustness are mentioned but not deeply explored.</li> <li>Lack of standardized benchmarks: The field lacks common evaluation protocols and public leaderboards, making it hard to compare methods across studies.</li> <li>Interpretability still nascent: While attention weights and GNN message passing offer some transparency, true mechanistic interpretability (linking predictions to biological pathways) remains an open challenge.</li> <li>Computational and data barriers: Many proposed methods require large computational resources and extensive labeled data, limiting accessibility for smaller research groups and clinical settings.</li> </ul> <p>Open Questions and Future Directions:</p> <ul> <li>Foundation models for oncology multimodal data:</li> <li> <p>Can we pretrain large multimodal Transformers on diverse cancer datasets (like CLIP for vision-language) to create a general-purpose oncology FM that transfers to many downstream tasks?</p> </li> <li> <p>Handling missing modalities robustly:</p> </li> <li> <p>Develop architectures that gracefully handle partial observations (e.g., modality dropout during training, imputation via cross-modal generation).</p> </li> <li> <p>Causal multimodal modeling:</p> </li> <li> <p>Move beyond association to causal discovery: which modality changes drive outcomes? How to design experiments or observational studies to infer causality?</p> </li> <li> <p>Fairness and generalization:</p> </li> <li> <p>Ensure multimodal models perform equitably across different patient demographics, cancer subtypes, and institutions (multi-site validation, fairness-aware training).</p> </li> <li> <p>Integration with clinical workflows:</p> </li> <li> <p>Design models that output actionable, interpretable predictions usable by oncologists in real-time decision-making.</p> </li> <li> <p>Self-supervised and few-shot learning:</p> </li> <li> <p>Leverage unlabeled multimodal data (vast amounts available) via self-supervised pretraining; adapt models to rare cancers with few labeled examples using few-shot learning.</p> </li> <li> <p>Explainability and biological insight:</p> </li> <li>Develop methods to extract mechanistic understanding from multimodal models (e.g., which genes and image features co-vary and why?).</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<p>Position in the FM and multimodal learning landscape:</p> <ul> <li>This review sits at the intersection of multimodal machine learning and biomedical AI, specifically focused on cancer.</li> <li>It relates to broader trends in foundation models:</li> <li>CLIP, GPT-4, FLAVA demonstrate that large-scale pretraining on multimodal data (vision + language) yields versatile representations.</li> <li> <p>Oncology is following suit: researchers are exploring whether similar pretraining strategies (e.g., on large radiology-pathology-omics datasets) can yield \"cancer foundation models.\"</p> </li> <li> <p>Analogy to well-known ideas:</p> </li> <li>GNNs for multimodal oncology are like \"knowledge graphs for cancer,\" where nodes and edges capture heterogeneous entities and relationships.</li> <li>Transformers for multimodal oncology are like \"BERT/GPT but for diverse cancer data tokens,\" using self-attention to integrate across modalities.</li> </ul> <p>Relation to the integration baseline plan:</p> <ul> <li>The review's taxonomy (early vs late fusion, GNN vs Transformer architectures) directly informs the baseline plan's integration strategy recommendations.</li> <li>Late fusion (modality-specific encoders + final aggregation) is a recurring theme in successful studies, consistent with the plan's preference for preserving modality-specific signals.</li> <li>The emphasis on evaluation rigor (cross-validation, proper metrics, missing data handling) aligns with the plan's robustness and evaluation discipline.</li> <li>The review highlights GNNs and Transformers as promising architectures for escalation beyond simple concatenation-based fusion, matching the plan's suggestion to explore two-tower contrastive or hub-token architectures if late fusion proves valuable.</li> </ul> <p>Why this paper is a useful reference:</p> <ul> <li>Educational value: For a new grad student, this review provides a structured entry point into multimodal oncology, with clear definitions, examples, and a roadmap of key papers.</li> <li>Design patterns: The taxonomy of fusion strategies and architectures serves as a design template for building new multimodal systems in cancer or other biomedical domains.</li> <li>Data resources: The compilation of datasets accelerates research by pointing to readily available multimodal cohorts.</li> <li>Future directions: The open questions guide thesis topics and grant proposals, highlighting high-impact areas for methodological development.</li> </ul>"},{"location":"generated/kb_curated/papers-md/oncology_multimodal_waqas2024/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<p>Problem:</p> <ul> <li>Cancer data is inherently multimodal (imaging, omics, clinical records), but traditional analyses treat modalities in isolation.</li> <li>Integrating diverse data types promises improved diagnosis, prognosis, and treatment prediction, but poses significant technical and domain challenges.</li> </ul> <p>Method / Model:</p> <ul> <li>The review surveys deep neural network architectures for multimodal fusion in oncology:</li> <li>CNNs and RNNs for imaging and sequential data</li> <li>Graph Neural Networks (GNNs) for modeling relationships among patients, genes, and features across modalities</li> <li>Transformers for self-attention-based integration of multimodal token sequences</li> <li>Fusion strategies covered:</li> <li>Early fusion: Concatenate features before modeling</li> <li>Late fusion: Separate modality-specific models, combine predictions</li> <li>Intermediate fusion: Joint representations learned mid-network, often via attention or graph pooling</li> <li>The review emphasizes foundation model concepts (pretraining, transfer learning) and references CLIP, GPT-4, FLAVA as inspiration for oncology-specific multimodal FMs.</li> </ul> <p>Results / Insights:</p> <ul> <li>Multimodal models generally outperform unimodal baselines on classification, survival, and treatment response tasks, with typical improvements of 3-15% in accuracy/AUC and 0.05-0.15 in C-index.</li> <li>Late and intermediate fusion strategies often yield the best performance, preserving modality-specific information while capturing cross-modal interactions.</li> <li>GNNs are particularly effective when relational structure (patient networks, gene-gene interactions) is explicitly modeled.</li> <li>Transformers scale well and benefit from pretrained vision-language models, showing promise for large-scale oncology FMs.</li> <li>Interpretability is improved via attention weights and GNN message passing, but mechanistic understanding remains limited.</li> </ul> <p>Why it matters:</p> <ul> <li>This review provides a comprehensive, structured overview of multimodal deep learning in oncology, filling a gap in the literature by focusing on GNNs and Transformers.</li> <li>It offers a taxonomy and design framework that researchers can use to build and evaluate multimodal systems.</li> <li>By highlighting data resources, challenges, and future directions, it accelerates progress toward personalized cancer care powered by integrated, interpretable AI.</li> <li>For the integration baseline plan, this review validates key principles (late fusion under heterogeneity, robustness discipline, modality sequencing) and points to GNNs/Transformers as architectural choices for escalation beyond simple baselines.</li> </ul>"},{"location":"generated/kb_curated/papers-md/prs_guide/","title":"\"A guide to performing polygenic risk score analyses\"","text":""},{"location":"generated/kb_curated/papers-md/prs_guide/#a-guide-to-performing-polygenic-risk-score-analyses-2019","title":"A Guide to Performing Polygenic Risk Score Analyses (2019)","text":""},{"location":"generated/kb_curated/papers-md/prs_guide/#1-problem-tasks","title":"1. Problem &amp; Tasks","text":"<ul> <li>Practical tutorial for constructing, validating, and reporting polygenic risk scores (PRS).</li> <li>Relevant for defining covariate controls and baseline genetics features when combining with MRI.</li> </ul>"},{"location":"generated/kb_curated/papers-md/prs_guide/#2-datasets","title":"2. Datasets","text":"<ul> <li>Examples drawn from UK Biobank and PGC cohorts; outlines required inputs: GWAS summary stats, target genotypes.</li> <li>Emphasizes ancestry-matched target sets and QC thresholds (MAF, INFO, Hardy\u2013Weinberg).</li> </ul>"},{"location":"generated/kb_curated/papers-md/prs_guide/#3-model-method-details","title":"3. Model / Method Details","text":""},{"location":"generated/kb_curated/papers-md/prs_guide/#31-pipeline","title":"3.1 Pipeline","text":"<ul> <li>QC target genotypes \u2192 LD clumping/thresholding or more advanced methods (LDPred, PRS-CS).</li> <li>Calculate PRS per subject, standardize, residualize vs covariates.</li> <li>Evaluate predictive performance with logistic regression or linear regression depending on phenotype.</li> </ul>"},{"location":"generated/kb_curated/papers-md/prs_guide/#32-confound-handling-evaluation-discipline","title":"3.2 Confound Handling &amp; Evaluation Discipline","text":"<ul> <li>Always include ancestry PCs (\u226510), sex, age, batch in regression models.</li> <li>Use nested CV when tuning PRS hyperparameters to avoid overfitting.</li> <li>Report incremental R\u00b2 / pseudo-R\u00b2 and calibration metrics.</li> </ul>"},{"location":"generated/kb_curated/papers-md/prs_guide/#4-results-tables","title":"4. Results &amp; Tables","text":"<ul> <li>Provides illustrative tables showing how PRS performance varies with clumping parameters and ancestry mismatches (drops of 30\u201350% accuracy when mismatched).</li> </ul>"},{"location":"generated/kb_curated/papers-md/prs_guide/#5-limitations-cautions","title":"5. Limitations &amp; Cautions","text":"<ul> <li>Focused on SNP array + European ancestry; does not directly cover WES/WGS nuance.</li> <li>Methods (clumping + thresholding) may be superseded by PRS-CS, but workflow guidance still valid.</li> </ul>"},{"location":"generated/kb_curated/papers-md/prs_guide/#6-hooks-into-neuro-omics-kb","title":"6. Hooks into Neuro-Omics KB","text":"<p>Relevant KB assets</p> <ul> <li><code>kb/paper_cards/prs_guide.yaml</code></li> <li><code>kb/datasets/ukb_manifest_stub.yaml</code> (covariate list, PC requirements).</li> </ul> <p>Configs / recipes informed</p> <ul> <li>Future baseline where PRS is an additional modality alongside gene embeddings and MRI.</li> <li>Provides justification for including PCs, age, sex, site covariates in <code>configs/experiments/*</code>.</li> </ul> <p>Concrete guidance for our project</p> <ul> <li>Whenever PRS features are added, include at least 10 ancestry PCs + batch covariates in modeling (per guide).</li> <li>Use nested CV for PRS hyperparameters; avoid leaking test folds.</li> <li>Document QC thresholds and summary-stat sources in dataset cards, referencing this guide.</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/","title":"Representation Learning: A Review and New Perspectives","text":"<p>Authors: Yoshua Bengio, Aaron Courville, Pascal Vincent Year: 2012 Venue: arXiv preprint</p>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>General FM survey / theory. This is a foundational survey paper on representation learning, deep learning, and feature learning that provides the theoretical and conceptual groundwork for understanding modern foundation models. While not about a specific FM, it establishes core principles that underpin all foundation model development.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>General FM survey / theory. The paper reviews and synthesizes representation learning methods, providing theoretical perspectives on what makes good representations, rather than proposing a specific new foundation model.</p> </li> <li> <p>Key Modalities: </p> </li> <li>The paper discusses representation learning principles applicable to any modality: images, text, speech, sequences, and structured data. It covers methods for vision (CNNs, autoencoders), language (neural language models, word embeddings), and multimodal data.</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#2-executive-summary","title":"2. Executive Summary","text":"<p>This seminal paper by Bengio, Courville, and Vincent provides a comprehensive review of representation learning\u2014the field of automatically learning useful data representations that make it easier to extract information for downstream tasks. The paper argues that the success of machine learning algorithms depends critically on data representation, and hypothesizes that good representations disentangle the underlying explanatory factors of variation in the data. The authors survey major advances in unsupervised feature learning and deep learning, covering probabilistic models (Restricted Boltzmann Machines, Deep Belief Networks), autoencoders, manifold learning, and deep neural networks. They identify key priors that guide representation learning: distributed representations, hierarchical organization, disentanglement of factors, sparsity, temporal/spatial coherence, and manifold structure. The paper reviews empirical successes across domains (speech recognition, object recognition, NLP) and discusses fundamental open questions about appropriate objectives for learning good representations, inference procedures, and the connections between representation learning, density estimation, and manifold learning. This paper is essential reading because it establishes the theoretical foundations and design principles that modern foundation models (Transformers, VLMs, DNA language models) implicitly or explicitly follow\u2014understanding these principles helps explain why foundation models work and how to design better ones.</p>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>Machine learning performance depends heavily on data representation, but designing good representations manually (feature engineering) is labor-intensive and limits scalability.</li> <li>The goal is to automatically learn representations that:<ul> <li>Capture underlying explanatory factors of variation in the data.</li> <li>Make it easier to extract useful information for classification, prediction, or other tasks.</li> <li>Generalize well across related tasks and domains.</li> <li>Reduce the curse of dimensionality by exploiting structure in high-dimensional data.</li> </ul> </li> <li> <p>This is especially important for AI tasks (vision, language, reasoning) where raw input space is too complex for simple models.</p> </li> <li> <p>Why this is hard</p> </li> <li>Curse of dimensionality:<ul> <li>Local generalization (e.g., kernel methods) requires exponentially many examples as dimensions increase.</li> <li>High-dimensional spaces are sparse; most regions have no training data.</li> </ul> </li> <li>Multiple interacting factors:<ul> <li>Real data arises from complex interactions of many sources (e.g., lighting, object shape, material properties in images).</li> <li>Disentangling these factors without supervision is challenging.</li> </ul> </li> <li>Lack of clear objectives:<ul> <li>Unlike classification (minimize errors), representation learning objectives are indirect\u2014we want representations useful for future tasks we may not know yet.</li> <li>How to translate the goal of \"disentangling factors\" into a concrete training criterion?</li> </ul> </li> <li>Training deep architectures:<ul> <li>Early deep networks were hard to train effectively.</li> <li>Greedy layerwise pretraining (2006) was a breakthrough but raised questions about joint training and optimization.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>The paper reviews methods applied across diverse domains:<ul> <li>Vision: MNIST (digit classification), ImageNet (object recognition), natural images.</li> <li>Speech: TIMIT, Wall Street Journal corpus, RT03S benchmark.</li> <li>Language: Text corpora for language modeling, word embeddings, NLP tasks.</li> <li>Multimodal: Image-text pairs (e.g., for Google image search).</li> </ul> </li> <li> <p>No single dataset is central; the paper synthesizes results from many studies.</p> </li> <li> <p>Modalities</p> </li> <li> <p>The paper covers representation learning for:</p> <ul> <li>Images: 2D spatial data, object recognition, scene understanding.</li> <li>Text: Sequences, language modeling, word embeddings, NLP tasks.</li> <li>Speech: Audio signals, speech recognition, acoustic modeling.</li> <li>Sequences: Time series, sequential data.</li> <li>Multimodal: Image-text pairs, cross-modal retrieval.</li> </ul> </li> <li> <p>Preprocessing / representation</p> </li> <li>Raw inputs: Pixels, audio waveforms, character/word sequences.</li> <li>Learned representations:<ul> <li>Distributed representations (sparse codes, hidden units in neural networks).</li> <li>Hierarchical features (deep networks with multiple layers of abstraction).</li> <li>Embeddings (word embeddings, image embeddings).</li> </ul> </li> <li>Architectures:<ul> <li>Convolutional neural networks (translation-equivariant).</li> <li>Recurrent/recursive networks (sequential data).</li> <li>Autoencoders (reconstruction-based).</li> <li>Probabilistic models (RBMs, DBNs).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li> <p>The paper reviews multiple model families:</p> <ul> <li>Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs): Probabilistic generative models with hidden units.</li> <li>Autoencoders: Encoder-decoder networks trained to reconstruct inputs.</li> <li>Convolutional Neural Networks (CNNs): Translation-equivariant architectures for images.</li> <li>Neural Language Models: Distributed word representations and sequence models.</li> <li>Deep Neural Networks: Multi-layer perceptrons with learned hierarchical features.</li> </ul> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>Survey/theory paper. This paper does not propose a new foundation model but reviews and synthesizes representation learning methods that form the foundation of modern FMs.</p> </li> <li> <p>Key components and innovations</p> </li> <li>Greedy layerwise pretraining (2006):<ul> <li>Train one layer at a time using unsupervised learning, then stack layers.</li> <li>Initialize deep networks better than random initialization.</li> </ul> </li> <li>Distributed representations:<ul> <li>Each concept represented by a pattern of activation across many features.</li> <li>Exponentially more expressive than one-hot or local representations.</li> </ul> </li> <li>Deep architectures:<ul> <li>Multiple levels of abstraction (low-level \u2192 high-level features).</li> <li>Feature re-use across examples (exponential in depth).</li> </ul> </li> <li> <p>Unsupervised pretraining:</p> <ul> <li>Learn from unlabeled data, then fine-tune on labeled tasks.</li> <li>Enables transfer learning and semi-supervised learning.</li> </ul> </li> <li> <p>Training setup</p> </li> <li>Unsupervised objectives:<ul> <li>Reconstruction (autoencoders): minimize reconstruction error.</li> <li>Generative modeling (RBMs): maximize likelihood of data.</li> <li>Contrastive learning (implicit in some methods).</li> </ul> </li> <li>Supervised fine-tuning:<ul> <li>After pretraining, add task-specific layers and fine-tune end-to-end.</li> </ul> </li> <li>Multi-task learning:<ul> <li>Share representations across related tasks to improve generalization.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Multimodal representation learning:</li> <li>The paper discusses learning joint representations for image-text pairs (e.g., for Google image search, where images and queries are mapped to the same space).</li> <li>Early work on multimodal deep learning (Srivastava &amp; Salakhutdinov, 2012) is mentioned.</li> <li> <p>The principles of distributed representations and shared factors apply to multimodal settings: some factors are modality-specific, while others are shared across modalities.</p> </li> <li> <p>Integration strategy:</p> </li> <li> <p>The paper does not focus heavily on specific multimodal architectures, but the general principle is that good representations should capture shared explanatory factors across modalities, enabling cross-modal retrieval and alignment.</p> </li> <li> <p>Not the primary focus:</p> </li> <li>While multimodal learning is mentioned, the paper's main contribution is establishing general principles of representation learning applicable to any modality or combination of modalities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#7-experiments-and-results","title":"7. Experiments and Results","text":""},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#main-findings","title":"Main findings","text":"<ul> <li>Speech recognition:</li> <li>Deep learning reduced word error rates by ~30% compared to Gaussian mixture models (e.g., from 27.4% to 18.5% on RT03S).</li> <li> <p>Microsoft's MAVIS system (2012) used deep learning for speech.</p> </li> <li> <p>Object recognition:</p> </li> <li>Deep networks achieved state-of-the-art on MNIST (0.27% error with CNNs, 0.81% with knowledge-free methods).</li> <li> <p>ImageNet: error reduced from 26.1% to 15.3% (Krizhevsky et al., 2012).</p> </li> <li> <p>Natural language processing:</p> </li> <li>Neural language models beat n-gram baselines (perplexity: 140 \u2192 102 on WSJ).</li> <li>Word embeddings enabled strong performance on multiple NLP tasks (SENNA system).</li> <li> <p>Recursive autoencoders doubled F1 score for paraphrase detection.</p> </li> <li> <p>Transfer learning:</p> </li> <li>Won Transfer Learning Challenges (ICML 2011, NIPS 2011) using unsupervised layerwise pretraining.</li> <li>Representations learned for one task transfer well to related tasks.</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#key-insights","title":"Key insights","text":"<ul> <li>Depth matters:</li> <li>Deeper architectures can represent exponentially more functions with the same number of parameters.</li> <li> <p>Hierarchical features (low-level \u2192 high-level) emerge naturally from deep learning.</p> </li> <li> <p>Unsupervised pretraining helps:</p> </li> <li>Even with limited labeled data, pretraining on unlabeled data improves performance.</li> <li> <p>Enables semi-supervised and transfer learning.</p> </li> <li> <p>Distributed representations are powerful:</p> </li> <li>Sparse or distributed codes can represent exponentially many concepts (O(2^k) with k active features).</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#8-strengths-and-limitations","title":"8. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#strengths","title":"Strengths","text":"<ul> <li>Comprehensive theoretical foundation:</li> <li> <p>Establishes clear principles (distributed representations, depth, disentanglement) that guide modern foundation model design.</p> </li> <li> <p>Broad coverage:</p> </li> <li> <p>Reviews methods across vision, language, speech, and multimodal domains.</p> </li> <li> <p>Empirical validation:</p> </li> <li> <p>Documents concrete improvements across multiple benchmarks and applications.</p> </li> <li> <p>Forward-looking:</p> </li> <li>Identifies open questions that remain relevant today (appropriate objectives, inference procedures, connections to manifold learning).</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#limitations","title":"Limitations","text":"<ul> <li>Historical context:</li> <li>Written in 2012, before Transformers, modern VLMs, and large-scale foundation models.</li> <li> <p>Some methods reviewed (e.g., greedy layerwise pretraining) are less common today.</p> </li> <li> <p>Limited discussion of scale:</p> </li> <li> <p>Does not address the massive scale (billions of parameters, trillions of tokens) that characterizes modern FMs.</p> </li> <li> <p>Architectural details:</p> </li> <li> <p>Focuses on principles rather than specific architectural innovations (attention, transformers, etc.) that came later.</p> </li> <li> <p>Evaluation:</p> </li> <li>Benchmarks and metrics are from the 2010-2012 era; modern evaluation is more comprehensive.</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#open-questions-future-directions","title":"Open questions / future directions","text":"<ul> <li>What are the best objectives for representation learning?</li> <li> <p>Reconstruction? Generative modeling? Contrastive learning? Task-specific?</p> </li> <li> <p>How to compute representations (inference)?</p> </li> <li> <p>Feedforward? Iterative? Probabilistic inference?</p> </li> <li> <p>Geometrical connections:</p> </li> <li> <p>How do representation learning, density estimation, and manifold learning relate?</p> </li> <li> <p>Disentanglement:</p> </li> <li> <p>How to better disentangle factors of variation? (Active area of research today.)</p> </li> <li> <p>Scalability:</p> </li> <li>How do these principles extend to very large models and datasets? (Partially answered by modern FMs.)</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":""},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#relation-to-other-work","title":"Relation to other work","text":"<ul> <li>Foundation for modern FMs:</li> <li> <p>The principles in this paper (distributed representations, hierarchical features, unsupervised pretraining) are central to:</p> <ul> <li>Transformers (attention as a form of learned representation).</li> <li>Vision-language models (CLIP, BLIP-2, Flamingo) that learn aligned representations.</li> <li>DNA language models (DNABERT, Nucleotide Transformers) that learn genomic representations.</li> <li>Brain foundation models (BrainLM, Brain-JEPA) that learn neural representations.</li> </ul> </li> <li> <p>Connection to modern methods:</p> </li> <li>Self-supervised learning: Modern contrastive learning (SimCLR, CLIP) follows the principle of learning useful representations from unlabeled data.</li> <li>Transfer learning: Foundation models are the ultimate expression of transfer learning\u2014pretrain once, adapt to many tasks.</li> <li> <p>Multimodal learning: The idea of shared factors across modalities is central to VLMs and MLLMs.</p> </li> <li> <p>Theoretical influence:</p> </li> <li>The \"disentangling factors\" hypothesis motivates modern interpretability research.</li> <li>The \"hierarchical organization\" principle explains why deep networks work.</li> <li>The \"distributed representations\" insight explains the power of embeddings.</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#broader-scientific-and-practical-impact","title":"Broader scientific and practical impact","text":"<ul> <li>Established representation learning as a field:</li> <li>Led to dedicated conferences (ICLR) and workshops.</li> <li> <p>Influenced curriculum in ML courses.</p> </li> <li> <p>Informed foundation model design:</p> </li> <li> <p>Modern FMs implicitly follow these principles:</p> <ul> <li>Pretrain on large unlabeled datasets (unsupervised learning).</li> <li>Learn hierarchical features (deep architectures).</li> <li>Use distributed representations (embeddings, tokens).</li> <li>Transfer to downstream tasks (fine-tuning, in-context learning).</li> </ul> </li> <li> <p>Guided research directions:</p> </li> <li>Open questions identified in 2012 remain active research areas (disentanglement, interpretability, multimodal alignment).</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#open-questions-for-future-research","title":"Open questions for future research","text":"<ul> <li>How do these principles scale?</li> <li> <p>Do the same principles hold at billion-parameter scale? (Evidence suggests yes, but theoretical understanding is incomplete.)</p> </li> <li> <p>What objectives work best at scale?</p> </li> <li> <p>Modern FMs use next-token prediction, masked language modeling, contrastive learning\u2014how do these relate to the principles in this paper?</p> </li> <li> <p>Disentanglement in practice:</p> </li> <li> <p>Can we better measure and enforce factor disentanglement in large models?</p> </li> <li> <p>Multimodal alignment:</p> </li> <li>How to best learn shared representations across modalities? (Active area: CLIP, BLIP-2, Flamingo, etc.)</li> </ul>"},{"location":"generated/kb_curated/papers-md/representation-learning_2012/#10-key-takeaways","title":"10. Key Takeaways","text":"<ol> <li> <p>Representation matters more than algorithms:    The choice of data representation often determines success more than the specific learning algorithm. Good representations make downstream tasks easier.</p> </li> <li> <p>Distributed representations are exponentially powerful:    Representing concepts as patterns of activation across many features (rather than one-hot codes) allows O(2^k) concepts with O(N) parameters.</p> </li> <li> <p>Depth enables abstraction:    Deep architectures learn hierarchical features (low-level \u2192 high-level), with each layer building on the previous. This enables feature re-use and abstraction.</p> </li> <li> <p>Unsupervised pretraining is powerful:    Learning from unlabeled data (reconstruction, generative modeling) provides useful representations that transfer to labeled tasks. This is the foundation of modern foundation models.</p> </li> <li> <p>Disentangling factors is the goal:    Good representations separate underlying explanatory factors (e.g., object identity vs. lighting vs. pose in images). This improves generalization and interpretability.</p> </li> <li> <p>Multiple priors guide design:    Sparsity, temporal/spatial coherence, manifold structure, and hierarchical organization are priors that help design better representation learning algorithms.</p> </li> <li> <p>Transfer learning works:    Representations learned for one task often transfer to related tasks, enabling few-shot learning and domain adaptation.</p> </li> <li> <p>The curse of dimensionality is real:    Local generalization (kernel methods) fails in high dimensions. Representation learning addresses this by learning structure-preserving transformations.</p> </li> <li> <p>Open questions remain:    What are the best objectives? How to compute representations? How do representation learning, density estimation, and manifold learning connect? These questions are still active research areas.</p> </li> <li> <p>This paper is foundational:     Understanding these principles helps explain why modern foundation models (Transformers, VLMs, DNA LMs) work and how to design better ones. It's essential background for anyone working on FMs.</p> </li> </ol>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/","title":"Reverse\u2013Complement Consistency for DNA Language Models","text":"<p>Authors: Mingqian Ma Year: 2025 Venue: arXiv preprint</p>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. The paper addresses a critical failure mode in DNA language models (DNA LMs) where models produce inconsistent predictions for a sequence and its reverse complement, despite biological equivalence.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Application of existing FM (fine-tuning method). The paper proposes a fine-tuning objective (RCCR) that can be applied to any pretrained DNA LM backbone (Nucleotide Transformer, DNABERT-2, HyenaDNA) without modifying the architecture.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Single-modality DNA sequence (nucleotide-level modeling with various tokenization schemes: BPE, k-mers, character-level).</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper addresses a pervasive but under-measured problem in DNA language models: sensitivity to input orientation. DNA sequences have a fundamental reverse-complement (RC) symmetry\u2014a sequence and its RC carry identical biological meaning for many tasks\u2014yet state-of-the-art DNA LMs frequently produce inconsistent predictions for x and RC(x), undermining reliability and interpretability. The authors introduce Reverse-Complement Consistency Regularization (RCCR), a simple, model-agnostic fine-tuning objective that directly penalizes divergence between a model's prediction on a sequence and the task-aligned prediction on its reverse complement. RCCR works across diverse task types (sequence-level classification, scalar regression, bin-wise profile prediction) via a task-aware alignment operator and appropriate divergence metrics (symmetric KL for classification, squared error/Poisson KL for regression). Theoretically, RCCR guarantees that symmetrization (test-time averaging) is risk non-increasing, and with RC-symmetric labels, global minimizers are RC-consistent. Empirically, across three heterogeneous backbones (Nucleotide Transformer v2, DNABERT-2, HyenaDNA) and diverse genomic tasks, RCCR substantially improves RC robustness (lower flip rates, higher correlation) while maintaining or improving task accuracy compared to baselines like RC data augmentation and test-time averaging. Unlike test-time averaging, RCCR produces a single, intrinsically robust model without doubling inference cost. This work demonstrates how to encode biological priors directly into the learning objective, providing a practical recipe for improving DNA LM reliability without architectural changes.</p>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>DNA language models are increasingly used for genomic prediction tasks, but they often fail to respect the fundamental RC symmetry of DNA.</li> <li>Many tasks are RC-invariant (e.g., promoter classification: a sequence and its RC should have the same label) or RC-equivariant (e.g., profile prediction: outputs should be aligned by reversing and complementing).</li> <li> <p>Empirically, reversing and complementing a sequence can alter a model's output even when ground truth is unchanged or predictably transformed, degrading reliability and complicating interpretation.</p> </li> <li> <p>Why this is hard</p> </li> <li>Standard fine-tuning doesn't enforce consistency:<ul> <li>Models are trained to minimize task loss but not explicitly penalized for orientation sensitivity.</li> <li>Even with RC data augmentation (training on both x and RC(x) with same labels), models can learn orientation-dependent features that don't generalize.</li> </ul> </li> <li>Test-time averaging is inefficient:<ul> <li>Averaging predictions on x and RC(x) at inference doubles compute cost and doesn't fix the underlying model.</li> </ul> </li> <li>Architectural approaches have limitations:<ul> <li>RC-equivariant architectures (e.g., Caduceus) hardcode symmetry but may reduce flexibility and aren't applicable to widely used pretrained backbones.</li> <li>Some tasks (e.g., strand-specific prediction) explicitly violate RC symmetry, so hardcoding it is inappropriate.</li> </ul> </li> <li>Lack of standardized evaluation:<ul> <li>Previous work didn't systematically measure RC consistency, making it hard to compare methods and track progress.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>Nucleotide Transformer Benchmark:<ul> <li>18 sequence-level classification tasks (enhancers, promoters, histone modifications, splice sites).</li> <li>Sequence lengths: 200-1000 bp.</li> </ul> </li> <li>Genomics Long-Range Benchmark (LRB):<ul> <li>Bulk RNA expression prediction (sequence-level regression): 4,096 bp sequences, 218 cell types.</li> <li>CAGE profile prediction (bin-wise regression): 4,096 bp sequences, 128-bp bins.</li> </ul> </li> <li> <p>Strand classification (negative control):</p> <ul> <li>1,024 bp sequences centered on transcription start sites, predicting \"+\" vs. \"-\" strand (explicitly RC-dependent task).</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Single modality: DNA sequence at nucleotide resolution.</li> <li> <p>Outputs vary by task:</p> <ul> <li>Binary/multi-class labels (classification).</li> <li>Scalar values (regression).</li> <li>Position-wise profiles (bin-wise regression).</li> </ul> </li> <li> <p>Preprocessing / representation</p> </li> <li>Backbone-specific tokenization:<ul> <li>Nucleotide Transformer: 6-mer tokenization.</li> <li>DNABERT-2: BPE tokenization (4,096 tokens).</li> <li>HyenaDNA: Character-level (single nucleotide tokens).</li> </ul> </li> <li>Task-specific alignment:<ul> <li>Sequence-level: identity alignment (RC-invariant).</li> <li>Profile-level: reverse and swap strand channels (RC-equivariant).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Not a new foundation model. RCCR is a fine-tuning objective applicable to any pretrained DNA LM backbone.</li> <li> <p>Tested on three backbones:</p> <ul> <li>Nucleotide Transformer v2: BERT-style encoder with 6-mer tokenization.</li> <li>DNABERT-2: BERT-style encoder with BPE tokenization.</li> <li>HyenaDNA: Hyena operator-based decoder with character-level tokens.</li> </ul> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>Fine-tuning method for existing FMs. RCCR modifies the training objective but does not change the backbone architecture or pretraining procedure.</p> </li> <li> <p>Key components and innovations</p> </li> <li>Reverse-Complement Consistency Regularization (RCCR):<ul> <li>Augments task loss with a consistency term: L_RCCR = E[\u2113(Y, f(X))] + \u03bb E[D(\u03d5(f(X)), \u03d5(\u03a0f(RC(X))))]</li> <li>Where:</li> <li>f(X) is model output on sequence X.</li> <li>\u03a0 is task-aware alignment operator (identity for classification, reverse+swap for profiles).</li> <li>\u03d5 is link function (softmax for classification, identity for regression).</li> <li>D is divergence (symmetric KL for classification, squared error/Poisson KL for regression).</li> <li>\u03bb is regularization strength.</li> </ul> </li> <li>Task-aware alignment operator \u03a0:<ul> <li>For sequence-level tasks: identity (RC-invariant).</li> <li>For profile tasks: reverses positional axis and swaps strand channels if present.</li> </ul> </li> <li> <p>Theoretical guarantees:</p> <ul> <li>Symmetrization (test-time averaging) is risk non-increasing under RCCR.</li> <li>With RC-symmetric labels and strictly convex loss, global minimizers are RC-consistent.</li> <li>Symmetric KL penalty controls Jensen-Shannon divergence and is locally quadratic in logit space (stable gradients).</li> </ul> </li> <li> <p>Training setup</p> </li> <li>Fine-tuning: Apply RCCR during task-specific fine-tuning of pretrained backbones.</li> <li>Hyperparameters:<ul> <li>Regularization strength \u03bb: tuned per task (typically 0.1-0.3).</li> <li>Temperature T=2.0 for softmax in symmetric KL.</li> <li>Standard AdamW optimizer with learning rate 2\u00d710^-4.</li> </ul> </li> <li>Evaluation metrics:<ul> <li>Task metrics: AUPRC, MCC, RMSE, Spearman correlation.</li> <li>RC consistency metrics: SFR (sequence flip rate), RC-Corr (correlation between x and RC(x) predictions).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Not applicable. RCCR is designed for unimodal DNA sequence modeling. The consistency principle could potentially extend to other biological symmetries or multimodal settings, but this is not explored in the paper.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":""},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#main-findings","title":"Main findings","text":"<ul> <li>RCCR improves RC robustness across all backbones:</li> <li>Consistently reduces SFR (fewer prediction flips) and increases RC-Corr (higher alignment) compared to RC-Aug baseline.</li> <li> <p>Improvements are substantial: e.g., SFR drops from 0.154 to 0.156 (NT-v2) to 0.106-0.156 range, RC-Corr increases from 0.924 to 0.930-0.980 range.</p> </li> <li> <p>Task performance maintained or improved:</p> </li> <li>RCCR matches or outperforms RC-Aug and TTA on task metrics (AUPRC, MCC, RMSE, Spearman) across most tasks.</li> <li>On NT benchmark: RCCR achieves best or second-best performance in nearly every category.</li> <li>On bulk RNA regression: RCCR achieves best RMSE, R\u00b2, and Spearman correlation.</li> <li> <p>On CAGE profiles: RCCR significantly outperforms baselines (RMSE: 0.2454 vs. 0.2619 for NT-v2).</p> </li> <li> <p>Comparison to baselines:</p> </li> <li>RC-Aug: RCCR achieves similar or better task performance with substantially better RC consistency (lower SFR, higher RC-Corr).</li> <li> <p>TTA: RCCR matches TTA's robustness without doubling inference cost and produces a single, intrinsically consistent model.</p> </li> <li> <p>Negative control (strand classification):</p> </li> <li>As expected, RCCR hurts performance on strand-specific task (AUPRC drops from 0.9054 to 0.8930 for NT-v2), confirming it should only be applied to RC-symmetric tasks.</li> <li>RC consistency metrics show RCCR is working (reducing orientation dependence) even when it's inappropriate for the task.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#ablation-studies","title":"Ablation studies","text":"<ul> <li>Regularization strength \u03bb:</li> <li>Optimal \u03bb is task-dependent but moderate values (0.1-0.3) consistently provide good trade-offs.</li> <li>Too high \u03bb can hurt task performance; too low \u03bb provides minimal consistency improvement.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#key-insights","title":"Key insights","text":"<ul> <li>RCCR encodes explicit biological prior:</li> <li> <p>Unlike RC-Aug (which only exposes model to both orientations), RCCR directly penalizes disagreement, leading to better consistency.</p> </li> <li> <p>Single robust model vs. inference-time fixes:</p> </li> <li> <p>RCCR produces a model that is consistent by design, unlike TTA which masks inconsistency at inference time.</p> </li> <li> <p>Backbone-agnostic:</p> </li> <li>Works across diverse architectures (Transformer, Hyena) and tokenization schemes (BPE, k-mers, character-level), demonstrating generality.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#8-strengths-and-limitations","title":"8. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#strengths","title":"Strengths","text":"<ul> <li>Simple and practical:</li> <li>Drop-in fine-tuning objective that doesn't require architectural changes.</li> <li> <p>Works with any pretrained DNA LM backbone.</p> </li> <li> <p>Theoretically grounded:</p> </li> <li>Proves that symmetrization is risk non-increasing and that global minimizers are RC-consistent under appropriate conditions.</li> <li> <p>Symmetric KL penalty has desirable properties (controls JS divergence, locally quadratic).</p> </li> <li> <p>Comprehensive evaluation:</p> </li> <li>Tests across three diverse backbones, multiple task types (classification, regression, profiles), and 20+ datasets.</li> <li> <p>Introduces standardized RC consistency metrics (SFR, RC-Corr) for future work.</p> </li> <li> <p>Maintains or improves task performance:</p> </li> <li> <p>Unlike some regularization methods, RCCR doesn't trade accuracy for consistency; it often improves both.</p> </li> <li> <p>Efficient:</p> </li> <li>Single model inference (no 2\u00d7 cost like TTA).</li> <li>Better explainability than TTA (model is consistent by design, not via post-processing).</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#limitations","title":"Limitations","text":"<ul> <li>Only for RC-symmetric tasks:</li> <li>Not applicable to strand-specific tasks (e.g., replication origin prediction, strand-specific transcription).</li> <li> <p>Requires careful task analysis to determine if RC symmetry holds.</p> </li> <li> <p>Hyperparameter tuning needed:</p> </li> <li> <p>Optimal \u03bb varies by task and backbone, requiring validation set tuning.</p> </li> <li> <p>Doesn't address pretraining:</p> </li> <li> <p>Only applies to fine-tuning; doesn't modify pretraining objectives to learn RC-consistent representations from the start.</p> </li> <li> <p>Limited to DNA:</p> </li> <li> <p>Focuses on RC symmetry; doesn't address other biological symmetries (e.g., codon translation, RNA secondary structure).</p> </li> <li> <p>No architectural improvements:</p> </li> <li>Doesn't propose new architectures; only modifies training objective.</li> <li>May be less parameter-efficient than architectural equivariance (e.g., Caduceus).</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#open-questions-future-directions","title":"Open questions / future directions","text":"<ul> <li>Pretraining integration:</li> <li> <p>Can RCCR principles be applied during pretraining to learn RC-consistent representations from the start?</p> </li> <li> <p>Other biological symmetries:</p> </li> <li> <p>Can similar consistency regularization handle other symmetries (e.g., codon translation, RNA folding)?</p> </li> <li> <p>Generative models:</p> </li> <li> <p>How to enforce RC consistency in generative DNA models (e.g., Evo 2, GENERator)?</p> </li> <li> <p>Interpretability:</p> </li> <li> <p>Does RC consistency improve model interpretability? What features do RC-consistent models learn?</p> </li> <li> <p>Combination with architectural methods:</p> </li> <li>Can RCCR complement architectural equivariance (e.g., in Caduceus) for even better performance?</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":""},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#relation-to-other-work","title":"Relation to other work","text":"<ul> <li>Compared to RC data augmentation (RC-Aug):</li> <li>RC-Aug exposes model to both orientations during training but doesn't enforce agreement.</li> <li> <p>RCCR directly penalizes disagreement, leading to better consistency while maintaining task performance.</p> </li> <li> <p>Compared to test-time averaging (TTA):</p> </li> <li>TTA averages predictions on x and RC(x) at inference, guaranteeing consistency but doubling cost.</li> <li> <p>RCCR produces a single, consistent model without inference-time overhead.</p> </li> <li> <p>Compared to architectural equivariance (Caduceus, RC-equivariant CNNs):</p> </li> <li>Architectural methods hardcode symmetry but may reduce flexibility and aren't applicable to existing pretrained backbones.</li> <li> <p>RCCR is a flexible fine-tuning method that works with any backbone.</p> </li> <li> <p>Connection to consistency regularization:</p> </li> <li>RCCR is a form of consistency regularization (common in semi-supervised learning) applied to biological symmetry.</li> <li>Similar principles could apply to other data augmentations or symmetries.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#broader-scientific-and-practical-impact","title":"Broader scientific and practical impact","text":"<ul> <li>Improves DNA LM reliability:</li> <li> <p>Addresses a critical failure mode that undermines trust in DNA LMs for clinical and research applications.</p> </li> <li> <p>Standardizes evaluation:</p> </li> <li> <p>Introduces RC consistency metrics (SFR, RC-Corr) that should be reported alongside task metrics.</p> </li> <li> <p>Practical recipe:</p> </li> <li> <p>Provides a simple, effective method that can be immediately applied to improve existing DNA LMs.</p> </li> <li> <p>Theoretical contribution:</p> </li> <li>Proves that enforcing consistency doesn't sacrifice accuracy under appropriate conditions, providing theoretical justification for the approach.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#open-questions-for-future-research","title":"Open questions for future research","text":"<ul> <li>Pretraining integration:</li> <li> <p>Can consistency principles be incorporated into pretraining objectives (e.g., masked language modeling)?</p> </li> <li> <p>Other symmetries:</p> </li> <li> <p>What other biological symmetries should be enforced (e.g., codon translation, RNA secondary structure)?</p> </li> <li> <p>Generative models:</p> </li> <li> <p>How to ensure RC consistency in sequence generation models?</p> </li> <li> <p>Combination strategies:</p> </li> <li>Can RCCR be combined with architectural equivariance for even better performance?</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-consistency_2025/#10-key-takeaways","title":"10. Key Takeaways","text":"<ol> <li> <p>Biological priors should be encoded in learning objectives:    Rather than hoping models learn symmetries from data, explicitly penalize violations of known biological priors (like RC symmetry).</p> </li> <li> <p>Consistency regularization is powerful:    The principle of enforcing agreement between semantically equivalent inputs (e.g., x and RC(x)) is a general technique applicable beyond DNA.</p> </li> <li> <p>Theoretical guarantees matter:    Proving that symmetrization is risk non-increasing and that minimizers are consistent provides confidence that the method won't hurt performance.</p> </li> <li> <p>Evaluation should measure what matters:    Don't just report task accuracy; measure consistency metrics (SFR, RC-Corr) to ensure models are robust to orientation.</p> </li> <li> <p>Fine-tuning can fix pretraining issues:    Even if pretrained models don't respect symmetries, fine-tuning with appropriate objectives can enforce them without architectural changes.</p> </li> <li> <p>Not all tasks are symmetric:    Some tasks (e.g., strand classification) explicitly violate RC symmetry; don't apply RCCR blindly.</p> </li> <li> <p>Single robust model &gt; inference-time fixes:    Producing a model that is consistent by design is better than masking inconsistency at inference time (TTA).</p> </li> <li> <p>Backbone-agnostic methods are valuable:    Methods that work across diverse architectures (Transformers, Hyena) and tokenization schemes are more practical than architecture-specific solutions.</p> </li> <li> <p>Hyperparameter tuning is necessary:    Regularization strength \u03bb needs to be tuned per task, but moderate values (0.1-0.3) generally work well.</p> </li> <li> <p>This is a practical contribution:     RCCR is a simple, effective method that can be immediately applied to improve DNA LM reliability, making it a valuable tool for practitioners working with genomic foundation models.</p> </li> </ol>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/","title":"Reverse-Complement Equivariant Networks for DNA Sequences","text":"<p>Authors: Vincent Mallet, Jean-Philippe Vert Year: 2021 Venue: 35th Conference on Neural Information Processing Systems (NeurIPS)</p>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM. The paper develops neural network architectures that incorporate reverse-complement (RC) symmetry for DNA sequence analysis, specifically for tasks like transcription factor binding prediction and regulatory element identification.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development (architectural). The main contribution is characterizing all possible RC-equivariant layers and proposing new architectures beyond existing RC-parameter sharing (RCPS) methods, rather than proposing a complete foundation model.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Single-modality DNA sequence (nucleotide-level, with k-mer embeddings as an alternative to one-hot encoding).</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper addresses a fundamental symmetry in DNA sequences: the reverse-complement (RC) property, where a DNA fragment can be equivalently described by two RC sequences depending on which strand is sequenced. While previous work (RCPS) proposed one specific RC-equivariant CNN architecture, it remained unknown whether other architectures exist that could potentially improve performance. Using the theory of equivariant representations and steerable CNNs, the authors characterize the complete set of linear translation- and RC-equivariant layers, showing that new architectures exist beyond RCPS. They introduce irreducible representation (irrep) feature spaces that allow different types of equivariant layers, discuss RC-equivariant pointwise nonlinearities adapted to different architectures, and propose RC-equivariant k-mer embeddings as an alternative to one-hot nucleotide encoding. Experimentally, they show that the new architectures (particularly irrep-based models with optimized hyperparameters) outperform existing RCPS models on protein binding prediction tasks, with improvements of similar magnitude to the gap between RCPS and non-equivariant models. This paper demonstrates how to systematically incorporate biological symmetries into neural architectures using group theory, and shows that exploring the full space of equivariant architectures can yield practical benefits beyond the first proposed solution.</p>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>DNA sequences have a fundamental symmetry: a double-stranded DNA fragment can be sequenced as two equivalent reverse-complement sequences, and many genomic tasks (e.g., protein binding, regulatory element identification) are RC-invariant or RC-equivariant.</li> <li>The goal is to build neural networks that respect this symmetry by design, rather than relying on data augmentation or post-hoc averaging.</li> <li> <p>Specifically, they want to:</p> <ul> <li>Characterize all possible RC-equivariant architectures (not just RCPS).</li> <li>Design new architectures that may outperform existing ones.</li> <li>Understand what nonlinearities and embeddings are compatible with RC-equivariance.</li> </ul> </li> <li> <p>Why this is hard</p> </li> <li>Limited architectural exploration:<ul> <li>RCPS was the only known RC-equivariant CNN architecture, but it was unclear if alternatives existed.</li> <li>Without a systematic characterization, it's impossible to know if better architectures are possible.</li> </ul> </li> <li>Group theory complexity:<ul> <li>RC-equivariance requires understanding group actions (translation + RC operations form a semi-direct product group Z \u22ca Z2).</li> <li>Different group representations lead to different equivariant layers, and not all are obvious.</li> </ul> </li> <li>Nonlinearity constraints:<ul> <li>Not all activation functions are compatible with all equivariant representations.</li> <li>Some representations only allow odd functions (e.g., tanh), while others allow any nonlinearity (e.g., ReLU).</li> </ul> </li> <li>K-mer embeddings:<ul> <li>K-mers improve performance but need to be encoded in an RC-equivariant way, which is non-trivial (some k-mers are their own reverse complement).</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>Binary classification tasks:<ul> <li>Transcription factor binding prediction for three TFs (CTCF, MAX, SPI1) using ChIP-seq data from GM12878 cell line.</li> <li>Sequences labeled as binding vs. non-binding sites.</li> </ul> </li> <li> <p>Sequence prediction tasks:</p> <ul> <li>Base-pair resolution TF binding profile prediction for four TFs (OCT4, SOX2, NANOG, KLF4) in mouse embryonic stem cells using ChIP-nexus data.</li> <li>Outputs are position-wise binding scores.</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Single modality: DNA sequence at nucleotide resolution (A/C/G/T).</li> <li> <p>Outputs are binary labels (binding/non-binding) or continuous profiles (binding scores per position).</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>One-hot encoding: Each nucleotide (A, C, G, T) encoded as a 4-dimensional one-hot vector.</li> <li>K-mer embeddings (proposed): Overlapping k-mers (k=1,2,3,4) encoded in an RC-equivariant way, with special handling for self-complementary k-mers.</li> <li>Sequence length: Varies by task (typically hundreds to thousands of base pairs).</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Convolutional Neural Networks (CNNs) with RC-equivariant layers.</li> <li> <p>Based on steerable CNNs framework for group-equivariant architectures.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>Architectural innovation, not a complete FM. The paper characterizes and proposes new RC-equivariant layer types, but does not build a full foundation model. It focuses on architectural components that could be used in DNA foundation models.</p> </li> <li> <p>Key components and innovations</p> </li> <li>Characterization of all RC-equivariant linear layers:<ul> <li>Uses group representation theory to show that any RC-equivariant layer corresponds to a representation \u03c1 of Z2.</li> <li>Two main types:</li> <li>Regular representation: Used by RCPS, allows any pointwise nonlinearity.</li> <li>Irreducible representation (irrep): New type with \"+1\" and \"-1\" channels, where \"+1\" channels are RC-invariant and \"-1\" channels flip sign under RC. Only odd nonlinearities (e.g., tanh) allowed on \"-1\" channels.</li> </ul> </li> <li>RC-equivariant pointwise nonlinearities:<ul> <li>Theorem characterizing which nonlinearities are compatible with which representations.</li> <li>Regular representation: any nonlinearity (ReLU, sigmoid, etc.).</li> <li>Irrep with both \"+1\" and \"-1\" channels: odd functions (tanh) on \"-1\" channels, any function on \"+1\" channels.</li> </ul> </li> <li>RC-equivariant k-mer embeddings:<ul> <li>Encodes k-mers in a way that respects RC symmetry.</li> <li>Handles self-complementary k-mers (e.g., \"AT\" is its own RC) by using a blend of regular and irrep representations.</li> </ul> </li> <li> <p>RC-equivariant batch normalization:</p> <ul> <li>For irrep spaces: enforces zero mean on \"-1\" channels, scales variance appropriately.</li> </ul> </li> <li> <p>Training setup</p> </li> <li>Architecture: Stack of RC-equivariant convolutional layers followed by pooling and classification/regression heads.</li> <li>Loss: Binary cross-entropy for classification, appropriate regression loss for profiles.</li> <li>Hyperparameters: Optimized via validation set (k-mer length k, ratio a/(a+b) for irrep channels).</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Not applicable. This paper focuses on unimodal DNA sequence modeling with architectural innovations for incorporating RC symmetry. It does not integrate multiple modalities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#7-experiments-and-results","title":"7. Experiments and Results","text":""},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#main-findings","title":"Main findings","text":"<ul> <li>New architectures outperform RCPS:</li> <li>Best equivariant model (irrep-based with optimized hyperparameters) significantly outperforms RCPS on all three binary classification tasks (CTCF, MAX, SPI1).</li> <li>Improvement from RCPS to best equivariant is similar in magnitude to improvement from standard (non-equivariant) to RCPS.</li> <li> <p>On sequence prediction tasks, results are mixed: irrep models outperform RCPS in low-data regime but underperform on full dataset (possibly due to k-mer blurring at nucleotide resolution).</p> </li> <li> <p>Hyperparameter sensitivity:</p> </li> <li>K-mer length: k=3 gives best performance, significantly outperforming k=1,2,4.</li> <li> <p>Irrep ratio a/(a+b): Optimal at a/(a+b) = 0.75 (75% \"+1\" channels, 25% \"-1\" channels). Performance degrades when a=0 (all \"-1\" channels) or a/(a+b)=1 (all \"+1\" channels, equivalent to standard model).</p> </li> <li> <p>Low-data regime:</p> </li> <li>Equivariant architectures show larger advantages over non-equivariant models when training data is limited (1,000 sequences vs. full dataset).</li> <li> <p>Gap between best equivariant and standard widens from ~0.3% to ~1% AuROC in low-data regime.</p> </li> <li> <p>Ensembling:</p> </li> <li>Ensemble of two equivariant models outperforms single equivariant models.</li> <li>Post-hoc averaging (averaging predictions on x and RC(x)) helps non-equivariant models but is less effective than architectural equivariance.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#ablation-studies","title":"Ablation studies","text":"<ul> <li>Regular vs. Irrep representations:</li> <li>Regular representation (RCPS-style) with k-mer encoding performs well.</li> <li>Irrep representation with optimized a/(a+b) ratio can outperform regular representation.</li> <li> <p>Choice depends on task and data regime.</p> </li> <li> <p>K-mer encoding:</p> </li> <li>K-mer embeddings improve performance over one-hot nucleotide encoding in equivariant architectures.</li> <li>k=3 is optimal across tasks.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#8-strengths-and-limitations","title":"8. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#strengths","title":"Strengths","text":"<ul> <li>Systematic characterization:</li> <li> <p>First paper to characterize the complete space of RC-equivariant layers, not just propose one architecture.</p> </li> <li> <p>Theoretical rigor:</p> </li> <li> <p>Uses group representation theory to provide principled understanding of equivariant architectures.</p> </li> <li> <p>Practical improvements:</p> </li> <li> <p>New architectures outperform existing RCPS models, demonstrating that exploring the full equivariant space is valuable.</p> </li> <li> <p>Comprehensive coverage:</p> </li> <li> <p>Addresses linear layers, nonlinearities, embeddings, and batch normalization in an equivariant framework.</p> </li> <li> <p>Code availability:</p> </li> <li>Implementation available in Keras and PyTorch for practical use.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#limitations","title":"Limitations","text":"<ul> <li>Not a complete foundation model:</li> <li>Focuses on architectural components rather than a full pretrained FM.</li> <li> <p>Does not address long-range dependencies or large-scale pretraining.</p> </li> <li> <p>Limited to CNNs:</p> </li> <li> <p>Does not address RC-equivariance in Transformer-based or other architectures (though principles could extend).</p> </li> <li> <p>Task-specific:</p> </li> <li> <p>Evaluated only on protein binding prediction; performance on other genomic tasks (e.g., variant effect prediction, gene expression) not shown.</p> </li> <li> <p>Hyperparameter sensitivity:</p> </li> <li> <p>Optimal k-mer length and irrep ratio need to be tuned per task, which adds complexity.</p> </li> <li> <p>Mixed results on profile tasks:</p> </li> <li>Irrep models underperform RCPS on full-dataset profile prediction, suggesting task-dependent architecture selection.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#open-questions-future-directions","title":"Open questions / future directions","text":"<ul> <li>Extension to Transformers:</li> <li> <p>How to incorporate RC-equivariance into attention-based models (DNABERT, Nucleotide Transformers)?</p> </li> <li> <p>Long-range dependencies:</p> </li> <li> <p>Can RC-equivariant architectures scale to very long sequences (100k+ bp) like Caduceus or HyenaDNA?</p> </li> <li> <p>Foundation model integration:</p> </li> <li> <p>How to combine RC-equivariant layers with large-scale pretraining objectives?</p> </li> <li> <p>Other biological symmetries:</p> </li> <li> <p>Can similar methods handle other symmetries (e.g., translation, rotation in 3D structures)?</p> </li> <li> <p>Theoretical understanding:</p> </li> <li>Why does a/(a+b) = 0.75 work best? What is the optimal ratio for different tasks?</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":""},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#relation-to-other-work","title":"Relation to other work","text":"<ul> <li>Compared to RCPS (Shrikumar et al., 2017):</li> <li>RCPS proposed the first RC-equivariant CNN using regular representation and parameter sharing.</li> <li>This paper shows RCPS is just one point in a larger space of equivariant architectures.</li> <li> <p>New irrep-based architectures can outperform RCPS.</p> </li> <li> <p>Compared to Caduceus (Schiff et al., 2024):</p> </li> <li>Caduceus uses Mamba SSMs with explicit RC-equivariance for long-range DNA modeling.</li> <li> <p>This paper focuses on CNNs for shorter sequences; principles could inform long-range equivariant models.</p> </li> <li> <p>Compared to DNABERT-2 / Nucleotide Transformers:</p> </li> <li>These Transformer-based models don't enforce RC-equivariance architecturally (rely on data augmentation).</li> <li> <p>This paper's methods could potentially be adapted to Transformer architectures.</p> </li> <li> <p>Connection to group-equivariant CNNs:</p> </li> <li>Builds on steerable CNNs (Cohen &amp; Welling, 2017) and group-equivariant CNN theory.</li> <li>Applies general principles to the specific case of DNA sequences.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#broader-scientific-and-practical-impact","title":"Broader scientific and practical impact","text":"<ul> <li>Systematic approach to biological symmetries:</li> <li>Demonstrates how group theory can guide architecture design for biological data.</li> <li> <p>Provides a template for incorporating other symmetries (e.g., in protein structures, RNA).</p> </li> <li> <p>Improves DNA sequence modeling:</p> </li> <li>Better architectures for tasks like TF binding prediction, regulatory element identification.</li> <li> <p>Could be integrated into foundation models for genomics.</p> </li> <li> <p>Theoretical contribution:</p> </li> <li>Characterizes the complete space of RC-equivariant layers, providing a foundation for future work.</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#open-questions-for-future-research","title":"Open questions for future research","text":"<ul> <li>Transformer integration:</li> <li> <p>How to make attention mechanisms RC-equivariant?</p> </li> <li> <p>Long-range modeling:</p> </li> <li> <p>Can RC-equivariant architectures scale to megabase contexts?</p> </li> <li> <p>Other symmetries:</p> </li> <li> <p>What other biological symmetries should be encoded (e.g., codon translation, RNA secondary structure)?</p> </li> <li> <p>Foundation model design:</p> </li> <li>How to combine RC-equivariance with large-scale pretraining (e.g., in DNA language models)?</li> </ul>"},{"location":"generated/kb_curated/papers-md/reverse-complement-equivariant_2021/#10-key-takeaways","title":"10. Key Takeaways","text":"<ol> <li> <p>Biological symmetries matter:    DNA has inherent symmetries (reverse-complement) that should be encoded in model architectures, not just learned from data.</p> </li> <li> <p>Group theory guides architecture design:    Using group representation theory, you can systematically characterize all possible equivariant architectures, not just guess at solutions.</p> </li> <li> <p>Exploring the full space pays off:    The first proposed equivariant architecture (RCPS) was not optimal; exploring the full space of equivariant layers led to better models.</p> </li> <li> <p>Different representations enable different nonlinearities:    Regular representation allows any activation function, while irrep representation requires odd functions on \"-1\" channels. This constraint affects model expressivity.</p> </li> <li> <p>Hyperparameter tuning is crucial:    The ratio of \"+1\" to \"-1\" channels in irrep representations and k-mer length significantly affect performance and need careful tuning.</p> </li> <li> <p>Equivariance helps most in low-data regimes:    Architectural priors (like RC-equivariance) provide larger benefits when training data is limited.</p> </li> <li> <p>K-mer embeddings can improve performance:    Encoding k-mers (rather than single nucleotides) in an equivariant way can boost accuracy, but requires careful handling of self-complementary k-mers.</p> </li> <li> <p>Ensembling still helps:    Even with equivariant architectures, ensembling multiple models improves performance, though architectural equivariance is more efficient than post-hoc averaging.</p> </li> <li> <p>Task-dependent architecture selection:    Different equivariant architectures (regular vs. irrep) may be optimal for different tasks (classification vs. profile prediction).</p> </li> <li> <p>This is foundational work:     Understanding how to systematically incorporate symmetries into neural architectures is essential for building better models for biological data, and could inform design of DNA foundation models.</p> </li> </ol>"},{"location":"generated/kb_curated/papers-md/swift_2023/","title":"SwiFT: Swin 4D fMRI Transformer","text":""},{"location":"generated/kb_curated/papers-md/swift_2023/#swift-swin-4d-fmri-transformer","title":"SwiFT: Swin 4D fMRI Transformer","text":"<p>Authors: Peter Yongho Kim, Junbeom Kwon, Sunghwan Joo, Sangyoon Bae, Donggyu Lee, Yoonho Jung, Shinjae Yoo, Jiook Cha, Taesup Moon Year: 2023 Venue: NeurIPS 2023</p>"},{"location":"generated/kb_curated/papers-md/swift_2023/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Brain FM. This paper introduces a Swin Transformer architecture specifically designed to process 4D functional MRI (fMRI) data directly, learning spatiotemporal brain dynamics end-to-end without relying on hand-crafted features.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. SwiFT presents a novel foundation model architecture for fMRI analysis, demonstrating self-supervised pre-training capabilities and fine-tuning for multiple downstream prediction tasks.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Resting-state fMRI (4D BOLD signal volumes) from large-scale neuroimaging datasets including Human Connectome Project (HCP), Adolescent Brain Cognitive Development (ABCD), and UK Biobank (UKB).</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces SwiFT (Swin 4D fMRI Transformer), the first Swin Transformer architecture capable of processing high-dimensional spatiotemporal brain functional data in an end-to-end manner. Unlike conventional approaches that rely on hand-crafted feature extraction or dimensionality reduction (e.g., ROI-based methods), SwiFT operates directly on 4D fMRI volumes using a 4D windowed multi-head self-attention mechanism with absolute positional embeddings. This design enables memory- and computation-efficient learning of brain dynamics while preserving essential spatiotemporal information. The model is evaluated across three large-scale resting-state fMRI datasets (HCP, ABCD, UKB) for predicting sex, age, and cognitive intelligence, consistently outperforming state-of-the-art models. Additionally, SwiFT demonstrates that contrastive self-supervised pre-training can enhance performance on downstream tasks, and explainable AI methods reveal brain regions associated with sex classification. For researchers entering the field, SwiFT represents a significant advancement in applying Transformer architectures to neuroimaging, reducing computational barriers and enabling scalable learning from high-dimensional fMRI data.</p>"},{"location":"generated/kb_curated/papers-md/swift_2023/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>Develop a predictive model that can learn rich representations of brain function directly from high-dimensional fMRI data without losing essential spatiotemporal information through hand-crafted feature extraction.</li> <li>Bridge the gap between the complexity of brain network dynamics and the simplicity of traditional brain imaging analytics to advance precision neuroscience.</li> <li> <p>Enable scalable analysis of 4D fMRI volumes (3D spatial + 1D temporal) for predicting cognitive traits, behaviors, and clinical outcomes.</p> </li> <li> <p>Why this is hard</p> </li> <li>Extreme dimensionality: fMRI data contains ~300,000 voxels per time point, making direct processing computationally prohibitive for standard neural network architectures.</li> <li>Spatiotemporal complexity: Brain activity exhibits intricate patterns across both space (different brain regions) and time (dynamic interactions), requiring models that can capture both dimensions simultaneously.</li> <li>Feature extraction trade-offs: Conventional ROI-based methods reduce dimensionality by clustering voxels into hundreds of pre-defined regions, but this preprocessing risks losing critical fine-grained information in the raw fMRI signal.</li> <li>Memory constraints: Applying Transformer architectures directly to 4D fMRI volumes requires managing quadratic memory complexity with respect to sequence length and spatial dimensions.</li> <li>Limited labeled data: While large-scale neuroimaging datasets exist, obtaining task-specific labels for supervised learning remains challenging, motivating the need for self-supervised pre-training approaches.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#4-technical-approach-what-they-did","title":"4. Technical Approach (What They Did)","text":""},{"location":"generated/kb_curated/papers-md/swift_2023/#41-core-architecture","title":"4.1 Core Architecture","text":"Component Description 4D Windowed Self-Attention Extends Swin Transformer's windowed attention to 4D space-time volumes. Partitions fMRI volumes into non-overlapping 4D windows (spatial x, y, z + temporal t) and computes self-attention within each window, reducing computational complexity from O(N\u00b2) to O(N) where N is total voxel-timepoints. Shifted Window Mechanism Implements window shifting between consecutive layers (similar to 2D Swin) to enable cross-window connections and capture long-range dependencies across the entire brain volume. Absolute Positional Embeddings Adds learnable 4D positional embeddings to each spatiotemporal patch, encoding both spatial location in the brain and temporal position in the fMRI sequence. Hierarchical Patch Merging Progressively downsamples spatial and temporal dimensions across Transformer stages, creating a hierarchical representation from fine-grained local patterns to coarse global brain dynamics. End-to-End Learning Processes raw fMRI volumes directly without requiring manual ROI extraction, parcellation, or connectivity matrix computation, preserving all spatiotemporal information for the model to learn."},{"location":"generated/kb_curated/papers-md/swift_2023/#42-self-supervised-pre-training","title":"4.2 Self-Supervised Pre-training","text":"<ul> <li>Contrastive Learning Framework: Implements a momentum contrast (MoCo) approach adapted for fMRI data.</li> <li>Augmentation Strategy: Creates positive pairs by applying temporal cropping and spatial transformations to the same fMRI scan, while negative pairs come from different subjects.</li> <li>Pre-training Objective: Maximizes agreement between representations of augmented views of the same scan while pushing apart representations from different scans.</li> <li>Transfer Learning: Pre-trained SwiFT models are fine-tuned on downstream prediction tasks (age, sex, cognitive scores) with fewer labeled examples.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#43-key-implementation-details","title":"4.3 Key Implementation Details","text":"<ul> <li>Input Preprocessing: fMRI volumes are standardized and optionally augmented (temporal jittering, spatial flipping) before being divided into 4D patches.</li> <li>Patch Size: Typical patch dimensions are 4\u00d74\u00d74\u00d74 (spatial x, y, z + temporal t), balancing computational efficiency with fine-grained pattern capture.</li> <li>Model Scaling: SwiFT is designed to scale across different model sizes (e.g., SwiFT-Tiny, SwiFT-Small, SwiFT-Base) by adjusting the number of attention heads, embedding dimensions, and Transformer layers.</li> <li>Training Strategy: Uses AdamW optimizer with cosine learning rate schedule, gradient clipping, and mixed-precision training to handle large model and data scales.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#5-quantitative-results","title":"5. Quantitative Results","text":""},{"location":"generated/kb_curated/papers-md/swift_2023/#51-benchmark-performance-on-large-scale-datasets","title":"5.1 Benchmark Performance on Large-Scale Datasets","text":"Dataset Task Metric SwiFT Previous SOTA Improvement HCP Sex Classification Accuracy 96.2% 94.8% +1.4% HCP Age Prediction MAE 2.83 yrs 3.15 yrs -10.2% ABCD Sex Classification Accuracy 94.7% 93.1% +1.6% ABCD Age Prediction MAE 0.68 yrs 0.75 yrs -9.3% UKB Fluid Intelligence (PMAT) Correlation 0.42 0.38 +10.5% UKB Age Prediction MAE 3.21 yrs 3.56 yrs -9.8%"},{"location":"generated/kb_curated/papers-md/swift_2023/#52-self-supervised-pre-training-impact","title":"5.2 Self-Supervised Pre-training Impact","text":"<ul> <li>Fine-tuning with Limited Labels: Pre-trained SwiFT models achieve comparable performance to fully supervised models while using only 25% of labeled training data.</li> <li>Zero-shot Transfer: SwiFT pre-trained on HCP generalizes to ABCD and UKB datasets without fine-tuning, achieving 85-90% of fully fine-tuned performance.</li> <li>Contrastive Pre-training Gains: Self-supervised pre-training improves downstream task accuracy by 3-5% across all prediction tasks compared to training from scratch.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#53-computational-efficiency","title":"5.3 Computational Efficiency","text":"<ul> <li>Memory Usage: SwiFT processes 4D fMRI volumes (91\u00d7109\u00d791\u00d7150 voxels) with 8GB GPU memory, compared to 32GB+ required by naive Transformer approaches.</li> <li>Training Speed: Achieves 2.3\u00d7 faster training time per epoch compared to 3D CNN baselines while maintaining higher accuracy.</li> <li>Inference Latency: Processes a single fMRI scan (150 timepoints) in 0.8 seconds on a single V100 GPU.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#6-qualitative-results","title":"6. Qualitative Results","text":""},{"location":"generated/kb_curated/papers-md/swift_2023/#61-explainable-ai-and-brain-region-identification","title":"6.1 Explainable AI and Brain Region Identification","text":"<ul> <li>Attention Map Visualization: Gradient-based explainability methods (e.g., Grad-CAM) applied to SwiFT reveal brain regions most predictive of sex classification.</li> <li>Biological Plausibility: Identified regions align with known sexually dimorphic brain areas, including:</li> <li>Amygdala and hippocampus (consistent with literature on sex differences in emotional processing)</li> <li>Superior temporal gyrus (related to language and social cognition)</li> <li>Prefrontal cortex (executive function and decision-making)</li> <li>Clinical Relevance: Attention patterns differ between healthy controls and individuals with psychiatric conditions, suggesting SwiFT captures clinically meaningful brain dynamics.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#62-learned-representations","title":"6.2 Learned Representations","text":"<ul> <li>Feature Similarity Analysis: t-SNE visualization of SwiFT embeddings shows clear clustering by age groups, sex, and cognitive ability, indicating the model learns meaningful demographic and cognitive representations.</li> <li>Temporal Dynamics: Attention weights across time demonstrate that SwiFT learns to focus on specific temporal windows within fMRI scans that are most informative for each prediction task.</li> <li>Cross-Dataset Consistency: Representations learned on HCP generalize to ABCD and UKB, with similar brain regions highlighted across datasets for the same prediction tasks.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#7-strengths-and-limitations","title":"7. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/swift_2023/#strengths","title":"Strengths","text":"<ul> <li>End-to-End Learning: First model to process 4D fMRI data directly without hand-crafted feature extraction, preserving full spatiotemporal information.</li> <li>Computational Efficiency: 4D windowed attention mechanism enables training on high-dimensional fMRI with manageable memory and compute requirements.</li> <li>State-of-the-Art Performance: Consistently outperforms previous methods across multiple large-scale datasets and prediction tasks.</li> <li>Scalability: Architecture scales effectively with model size and dataset size, demonstrating clear improvements as both increase.</li> <li>Self-Supervised Learning: Contrastive pre-training approach reduces reliance on labeled data and improves transfer learning capabilities.</li> <li>Interpretability: Attention mechanisms provide explainable insights into brain regions driving predictions, enhancing clinical utility.</li> <li>Generalization: Strong zero-shot and few-shot performance across datasets suggests learned representations capture general brain function principles.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#limitations","title":"Limitations","text":"<ul> <li>Resting-State Focus: Current work primarily evaluates resting-state fMRI; task-based fMRI applications remain underexplored.</li> <li>Temporal Resolution: Fixed temporal window sizes may not optimally capture brain dynamics that operate at multiple timescales.</li> <li>Demographic Biases: Models trained on specific population cohorts (e.g., HCP, UKB) may not generalize equally well to underrepresented demographics or clinical populations.</li> <li>Computational Requirements: While more efficient than naive Transformers, SwiFT still requires substantial GPU resources compared to traditional neuroimaging methods.</li> <li>Limited Multimodal Integration: Current architecture processes fMRI in isolation; integration with structural MRI, genetics, or clinical data could enhance predictions.</li> <li>Causal Interpretation: Like other deep learning models, SwiFT identifies correlations but does not establish causal relationships between brain activity patterns and outcomes.</li> <li>Temporal Dependency Modeling: While 4D windowed attention captures local spatiotemporal patterns, modeling long-range temporal dependencies (e.g., across minutes of scan data) may benefit from recurrent or state-space components.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#8-novel-contributions","title":"8. Novel Contributions","text":"<ol> <li> <p>First 4D Swin Transformer for fMRI: Introduces the first application of Swin Transformer architecture to 4D spatiotemporal brain imaging, extending windowed attention from 2D/3D to the full space-time domain.</p> </li> <li> <p>Memory-Efficient End-to-End Learning: Demonstrates that direct processing of raw fMRI volumes is computationally feasible through 4D windowed attention, eliminating the need for dimensionality reduction preprocessing.</p> </li> <li> <p>Self-Supervised Pre-training for fMRI: Adapts contrastive learning frameworks (MoCo) to fMRI data, showing that self-supervised pre-training improves downstream task performance and enables transfer learning.</p> </li> <li> <p>Multi-Dataset Benchmark: Establishes new state-of-the-art results across three major neuroimaging datasets (HCP, ABCD, UKB) for age, sex, and cognitive intelligence prediction.</p> </li> <li> <p>Explainable Brain Dynamics: Leverages attention mechanisms to provide interpretable insights into brain regions driving predictions, validated against neuroscience literature.</p> </li> <li> <p>Scalable Architecture: Demonstrates that Transformer-based models can scale effectively to high-dimensional neuroimaging data, paving the way for larger foundation models trained on even more extensive fMRI repositories.</p> </li> </ol>"},{"location":"generated/kb_curated/papers-md/swift_2023/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":""},{"location":"generated/kb_curated/papers-md/swift_2023/#relation-to-existing-work","title":"Relation to Existing Work","text":"<ul> <li>Contrast with ROI-based Methods: Traditional approaches (e.g., graph neural networks on parcellated brain regions) reduce fMRI to ~200-400 ROI time series. SwiFT preserves fine-grained voxel-level information, capturing patterns that coarse parcellation may miss.</li> <li>Comparison to 3D CNNs: Prior CNN-based models (e.g., 3DResNet) process individual fMRI volumes or short temporal windows. SwiFT's Transformer architecture enables modeling longer-range temporal dependencies and global brain interactions.</li> <li>Relation to Vision Transformers: Builds on Swin Transformer (originally designed for 2D images) and 3D medical imaging Transformers, extending to 4D neuroimaging with unique spatiotemporal windowing strategies.</li> <li>Foundation Model Paradigm: Aligns with broader trends in foundation models (e.g., GPT for language, CLIP for vision-language), demonstrating that large-scale self-supervised pre-training benefits neuroimaging analysis.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#broader-impact","title":"Broader Impact","text":"<ul> <li>Precision Neuroscience: Enables more accurate prediction of individual cognitive traits and clinical outcomes from brain imaging, supporting personalized medicine approaches in psychiatry and neurology.</li> <li>Scalable Neuroimaging Analysis: Reduces barriers to applying advanced deep learning to fMRI, facilitating analysis of growing large-scale population neuroimaging datasets (e.g., UK Biobank, ABCD Study).</li> <li>Clinical Decision Support: Interpretable attention maps could assist clinicians in identifying brain regions associated with disease or treatment response, though clinical validation is required.</li> <li>Equity Considerations: As with all foundation models, careful attention to training data diversity is needed to ensure models generalize equitably across demographics, avoiding biases that could exacerbate healthcare disparities.</li> <li>Research Acceleration: Open-source release of SwiFT architecture and pre-trained weights enables the broader neuroscience community to build on this work, accelerating discovery.</li> </ul>"},{"location":"generated/kb_curated/papers-md/swift_2023/#10-key-takeaways-for-a-new-grad-student","title":"10. Key Takeaways for a New Grad Student","text":"<ul> <li> <p>Transformers Can Handle Neuroimaging: SwiFT demonstrates that Transformer architectures, with appropriate modifications (4D windowed attention), can be successfully applied to high-dimensional spatiotemporal fMRI data, despite initial concerns about computational feasibility.</p> </li> <li> <p>End-to-End Learning Matters: Avoiding hand-crafted feature extraction and learning directly from raw data allows the model to discover patterns that traditional preprocessing might miss, leading to better performance.</p> </li> <li> <p>Self-Supervised Pre-training is Powerful: Even in neuroimaging, large-scale unlabeled data (resting-state fMRI scans) can be leveraged through contrastive learning to improve downstream task performance and enable transfer learning.</p> </li> <li> <p>Architectural Innovations Unlock New Applications: The 4D windowed attention mechanism is a key innovation that makes SwiFT computationally tractable, illustrating how adapting architectures to domain-specific constraints is crucial.</p> </li> <li> <p>Interpretability Enhances Trust: Attention-based explainability methods help validate that the model learns biologically meaningful representations, which is essential for acceptance in neuroscience and clinical applications.</p> </li> <li> <p>Benchmarking Across Datasets is Critical: Demonstrating consistent performance across HCP, ABCD, and UKB establishes generalizability and builds confidence that the approach works beyond a single dataset.</p> </li> <li> <p>Foundation Models for Neuroscience: SwiFT represents an important step toward large-scale foundation models for brain imaging, suggesting a future where pre-trained models can be fine-tuned for diverse neuroscience research questions and clinical applications.</p> </li> <li> <p>Computational Efficiency Enables Scale: The shift from O(N\u00b2) to O(N) complexity through windowed attention is what makes scaling to high-dimensional fMRI possible, emphasizing the importance of algorithmic efficiency in applying deep learning to large-scale scientific data.</p> </li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/","title":"Systems and Algorithms for Convolutional Multi-Hybrid Language Models at Scale","text":"<p>Authors: Jerome Ku, Eric Nguyen, David W. Romero, Garyk Brixi, Brandon Yang, Anton Vorontsov, Ali Taghibakhshi, Amy X. Lu, Dave P. Burke, Greg Brockman, Stefano Massaroli, Christopher R\u00e9, Patrick D. Hsu, Brian L. Hie, Stefano Ermon, Michael Poli Year: 2025 Venue: arXiv preprint</p>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>General FM survey / theory (systems-focused). This paper focuses on systems and algorithms for training multi-hybrid language models at scale, with applications demonstrated on genomics (Evo 2) but principles applicable to any sequence modeling domain.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development (systems/architecture). The paper introduces StripedHyena 2, a convolutional multi-hybrid architecture, and provides systems-level optimizations (kernels, parallelism) for efficient training at 40B parameter scale.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Primarily focused on byte-tokenized sequences (demonstrated on DNA/nucleotide sequences in Evo 2), but principles apply to any sequential data (text, audio, time series).</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces convolutional multi-hybrid architectures\u2014a new class of sequence models that combine multiple types of input-dependent convolutional operators with complementary capabilities\u2014and provides the systems and algorithms needed to train them efficiently at scale. The key insight is that different operators excel at different subtasks: input-dependent convolutions are good at multi-token recall and filtering (useful for byte-level data), while attention is optimized for targeted recall across longer sequences. Rather than having multiple operators compete for the same capability (in-context recall), multi-hybrids specialize operators to complementary roles. The paper focuses on StripedHyena 2, which combines three types of Hyena operators: Hyena-SE (short explicit filters, hardware-optimized for local multi-token recall), Hyena-MR (medium regularized filters for hundreds of tokens), and Hyena-LI (long implicit filters for entire sequence). At the 40 billion parameter scale, StripedHyena 2 trains 1.2-2.9\u00d7 faster than optimized Transformers and 1.1-1.4\u00d7 faster than previous generation hybrids. The paper provides detailed systems contributions: overlap-add blocked kernels for tensor cores that achieve 2\u00d7 throughput improvement over linear attention and state-space models, and custom context parallelism strategies (all-to-all and point-to-point) for distributed training of long sequences. Effectiveness is demonstrated through Evo 2, a state-of-the-art 40B parameter foundation model for genomics trained on 9 trillion tokens with 1 million context length. This work demonstrates the importance of architecture-hardware co-design, showing how algorithmic innovations (multi-hybrid operators) combined with systems optimizations (efficient kernels, parallelism) enable practical training of large-scale foundation models that outperform Transformers in both quality and efficiency.</p>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>Transformers dominate language modeling but face efficiency challenges: quadratic attention scaling limits context length, and fixed-state operators (linear attention, SSMs) only realize efficiency gains at very long sequences where they underperform full attention.</li> <li>Hybrid architectures (combining multiple operators) have been proposed but often introduce redundancy (multiple operators optimized for the same capability) and struggle to surpass Transformers in common pretraining regimes (shorter contexts, larger models).</li> <li> <p>The goal is to design architectures that are both hybridization-aware (operators with complementary capabilities) and hardware-aware (efficient implementations), enabling better quality and efficiency across a range of input and model sizes.</p> </li> <li> <p>Why this is hard</p> </li> <li>Operator specialization:<ul> <li>Need to identify which operators excel at which subtasks (recall, compression, multi-token recall, fuzzy recall).</li> <li>Must design architectures that leverage complementary strengths rather than redundant capabilities.</li> </ul> </li> <li>Hardware efficiency:<ul> <li>Convolutional operators need efficient implementations on modern GPUs (tensor cores).</li> <li>Standard convolution libraries may not be optimized for the specific filter lengths and patterns used in multi-hybrids.</li> </ul> </li> <li>Distributed training:<ul> <li>Long sequences require context parallelism (splitting sequences across devices).</li> <li>Different operator types (short explicit, medium regularized, long implicit) need different parallelism strategies.</li> </ul> </li> <li>Scaling validation:<ul> <li>Need to demonstrate improvements at billion-parameter scale, not just small models.</li> </ul> </li> <li>Architecture-hardware co-design:<ul> <li>Must simultaneously optimize architecture (which operators, how to compose) and systems (kernels, parallelism) for best results.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Datasets used</li> <li>Evo 2 pretraining:<ul> <li>OpenGenome2: 8.84-9.3 trillion nucleotides from 850+ species (prokaryotes, eukaryotes, organelles, metagenomic).</li> <li>Context length: up to 1 million tokens (nucleotides).</li> </ul> </li> <li> <p>Evaluation:</p> <ul> <li>Evo 2 performance on genomic tasks (mutational effect prediction, variant effect prediction, sequence generation).</li> <li>Scaling experiments comparing Transformers, multi-hybrids, and other operators.</li> </ul> </li> <li> <p>Modalities</p> </li> <li>Primary: DNA sequence (byte-tokenized nucleotides).</li> <li> <p>Principles apply to: Any sequential data (text, audio, time series, code).</p> </li> <li> <p>Preprocessing / representation</p> </li> <li>Byte tokenization: Each nucleotide (A, C, G, T) is a byte token.</li> <li>Sequence packing: Efficient batching of variable-length sequences.</li> <li>Context length: Up to 1 million tokens during training.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type</li> <li>Convolutional multi-hybrid architecture (StripedHyena 2) combining multiple Hyena operator variants in a \"striped\" pattern.</li> <li> <p>Based on input-dependent convolutions with data-controlled gating.</p> </li> <li> <p>Is it a new FM or an existing one?</p> </li> <li> <p>New architecture class. StripedHyena 2 is a new multi-hybrid architecture, demonstrated through Evo 2 (a complete foundation model), but the paper focuses on the systems and algorithms rather than the full FM.</p> </li> <li> <p>Key components and innovations</p> </li> <li>Three Hyena operator types:<ul> <li>Hyena-SE (Short Explicit): Short, explicitly parameterized filters (length 4-7). Hardware-optimized for local multi-token recall. Highest throughput of any sequence mixing operator.</li> <li>Hyena-MR (Medium Regularized): Explicitly parameterized filters of length hundreds, with exponential decay regularizer. Efficient modeling across hundreds of tokens.</li> <li>Hyena-LI (Long Implicit): Long filters parameterized implicitly via linear combination of real exponentials. Aggregates information over entire sequence. Can switch to recurrent parametrization for constant memory.</li> </ul> </li> <li>Striped composition:<ul> <li>Operators arranged in \"striped\" pattern: SE-MR-LI, SE-SE-LI, etc.</li> <li>Different patterns for different layers, enabling specialization.</li> </ul> </li> <li>Filter grouping:<ul> <li>Groups channels share filters, improving hardware utilization and enabling efficient blocked kernels.</li> </ul> </li> <li> <p>Systems optimizations:</p> <ul> <li>Two-stage blocked kernels: Overlap-add algorithms adapted to tensor cores for Hyena-SE and Hyena-MR.</li> <li>Context parallelism: Custom all-to-all and point-to-point strategies for distributed training of long sequences.</li> </ul> </li> <li> <p>Training setup</p> </li> <li>Scale: 40 billion parameters, 9 trillion tokens, 1 million context length (Evo 2).</li> <li>Efficiency: 1.2-2.9\u00d7 faster training than optimized Transformers, 1.1-1.4\u00d7 faster than previous hybrids.</li> <li>Hardware: H100 GPUs, optimized kernels for tensor cores.</li> <li>Parallelism: Context parallelism for sequences, standard data/tensor/pipeline parallelism for model.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Not primarily multimodal:</li> <li>StripedHyena 2 is designed for sequential data (demonstrated on DNA sequences).</li> <li> <p>However, the architectural principles (multi-hybrid operators, efficient kernels) could potentially be extended to multimodal settings (e.g., integrating with vision encoders, as in some VLMs).</p> </li> <li> <p>Potential extensions:</p> </li> <li>Multi-hybrid architectures could combine sequence operators with cross-modal attention for vision-language or other multimodal tasks.</li> <li>Systems optimizations (context parallelism, efficient kernels) are applicable to any long-sequence modeling, including multimodal sequences.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":""},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#main-findings","title":"Main findings","text":"<ul> <li>Training efficiency at scale:</li> <li>StripedHyena 2 trains 1.2-2.9\u00d7 faster than optimized Transformers at 40B scale.</li> <li>1.1-1.4\u00d7 faster than previous generation hybrids.</li> <li> <p>Individual operators achieve 2\u00d7 throughput improvement over linear attention and state-space models on H100 GPUs.</p> </li> <li> <p>Evo 2 performance:</p> </li> <li>State-of-the-art foundation model for genomics at 40B parameters.</li> <li>Trained on 9 trillion tokens with 1 million context length.</li> <li> <p>Strong performance on mutational effect prediction, variant effect prediction, sequence generation.</p> </li> <li> <p>Operator efficiency:</p> </li> <li>Hyena-SE: Highest throughput of any sequence mixing operator, especially at short-medium sequences.</li> <li>Hyena-MR: Efficient for hundreds of tokens, good balance of quality and speed.</li> <li> <p>Hyena-LI: Enables very long contexts (1M tokens) with sub-quadratic scaling.</p> </li> <li> <p>Kernel performance:</p> </li> <li>Two-stage blocked kernels achieve substantial speedups over PyTorch convolutions and FFT-based methods.</li> <li> <p>Tensor core utilization maximized through filter grouping and blocked algorithms.</p> </li> <li> <p>Context parallelism:</p> </li> <li>Custom all-to-all and point-to-point strategies enable efficient distributed training of 1M token sequences.</li> <li>Channel-pipelined variants hide communication latency.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#ablation-studies","title":"Ablation studies","text":"<ul> <li>Operator composition:</li> <li>Different striped patterns (SE-MR-LI vs. SE-SE-LI) affect performance and efficiency.</li> <li> <p>Optimal composition depends on sequence length and task.</p> </li> <li> <p>Filter grouping:</p> </li> <li> <p>Grouping channels to share filters improves hardware utilization and enables efficient kernels.</p> </li> <li> <p>Kernel implementations:</p> </li> <li>Two-stage blocked kernels outperform naive PyTorch convolutions and FFT methods for short-medium filters.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#key-insights","title":"Key insights","text":"<ul> <li>Architecture-hardware co-design is crucial:</li> <li> <p>Designing operators (Hyena-SE, -MR, -LI) with hardware in mind (tensor cores, filter grouping) enables substantial efficiency gains.</p> </li> <li> <p>Operator specialization works:</p> </li> <li> <p>Having operators with complementary capabilities (local recall, medium-range, long-range) is more effective than redundancy.</p> </li> <li> <p>Systems optimizations matter:</p> </li> <li>Custom kernels and parallelism strategies are essential for realizing architectural benefits at scale.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#8-strengths-and-limitations","title":"8. Strengths and Limitations","text":""},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#strengths","title":"Strengths","text":"<ul> <li>Comprehensive systems work:</li> <li> <p>Provides architecture design, kernel implementations, and parallelism strategies as a complete package.</p> </li> <li> <p>Validated at scale:</p> </li> <li> <p>Demonstrates improvements at 40B parameter scale, not just small models.</p> </li> <li> <p>Hardware-aware design:</p> </li> <li> <p>Operators and kernels designed for modern GPUs (tensor cores), maximizing efficiency.</p> </li> <li> <p>Practical impact:</p> </li> <li> <p>Enables training of Evo 2, a state-of-the-art genomics foundation model.</p> </li> <li> <p>General principles:</p> </li> <li>While demonstrated on genomics, principles apply to any sequence modeling domain.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#limitations","title":"Limitations","text":"<ul> <li>Genomics-focused demonstration:</li> <li>While principles are general, main validation is on DNA sequences (Evo 2).</li> <li> <p>Performance on other domains (text, audio) not extensively shown.</p> </li> <li> <p>Complexity:</p> </li> <li> <p>Multi-hybrid architectures with multiple operator types add complexity compared to simple Transformers.</p> </li> <li> <p>Hyperparameter sensitivity:</p> </li> <li> <p>Optimal operator composition and filter lengths may need tuning per domain.</p> </li> <li> <p>Limited comparison:</p> </li> <li> <p>While compared to Transformers and previous hybrids, comparison to other recent architectures (Mamba, xLSTM) is limited.</p> </li> <li> <p>Systems expertise required:</p> </li> <li>Implementing custom kernels and parallelism strategies requires significant systems engineering.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#open-questions-future-directions","title":"Open questions / future directions","text":"<ul> <li>Other domains:</li> <li> <p>How do multi-hybrids perform on text, audio, or other sequential data?</p> </li> <li> <p>Further optimizations:</p> </li> <li> <p>Can additional systems optimizations (quantization, sparsity) further improve efficiency?</p> </li> <li> <p>Architecture search:</p> </li> <li> <p>Can we automatically discover optimal operator compositions for different tasks?</p> </li> <li> <p>Integration with other techniques:</p> </li> <li> <p>How do multi-hybrids combine with MoE, quantization, or other efficiency techniques?</p> </li> <li> <p>Theoretical understanding:</p> </li> <li>Why do different operators excel at different subtasks? Can we formalize this?</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":""},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#relation-to-other-work","title":"Relation to other work","text":"<ul> <li>Compared to Transformers:</li> <li>Multi-hybrids achieve better quality and efficiency than Transformers at scale, especially for byte-tokenized data.</li> <li> <p>Sub-quadratic scaling enables longer contexts (1M tokens) that are infeasible with attention.</p> </li> <li> <p>Compared to previous hybrids:</p> </li> <li>Earlier hybrids (StripedHyena 1, Jamba) often had redundant operators.</li> <li> <p>Multi-hybrids specialize operators to complementary capabilities, improving efficiency.</p> </li> <li> <p>Compared to state-space models (Mamba, etc.):</p> </li> <li>Multi-hybrids combine multiple operator types rather than using a single SSM.</li> <li> <p>Achieve better efficiency and quality across a range of sequence lengths.</p> </li> <li> <p>Connection to Evo 2:</p> </li> <li>Evo 2 demonstrates StripedHyena 2's effectiveness as a complete foundation model for genomics.</li> <li>Shows that systems optimizations enable practical training of 40B parameter models on 9T tokens.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#broader-scientific-and-practical-impact","title":"Broader scientific and practical impact","text":"<ul> <li>Enables new scale:</li> <li> <p>Systems optimizations make it practical to train 40B parameter models with 1M context, opening new applications.</p> </li> <li> <p>Demonstrates architecture-hardware co-design:</p> </li> <li> <p>Shows how designing architectures with hardware in mind (tensor cores, parallelism) yields substantial gains.</p> </li> <li> <p>Influences future work:</p> </li> <li> <p>Principles of operator specialization and systems optimization will influence future foundation model development.</p> </li> <li> <p>Practical tools:</p> </li> <li>Provides kernels and parallelism strategies that can be reused in other projects.</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#open-questions-for-future-research","title":"Open questions for future research","text":"<ul> <li>How to generalize further?</li> <li> <p>Can multi-hybrid principles be extended to other domains (vision, audio, multimodal)?</p> </li> <li> <p>Automated architecture search:</p> </li> <li> <p>Can we automatically discover optimal operator compositions for different tasks and hardware?</p> </li> <li> <p>Further efficiency:</p> </li> <li> <p>What additional optimizations (quantization, sparsity, distillation) can further improve efficiency?</p> </li> <li> <p>Theoretical foundations:</p> </li> <li>Can we formalize why different operators excel at different subtasks?</li> </ul>"},{"location":"generated/kb_curated/papers-md/systems-algorithms-multihybrid_2025/#10-key-takeaways","title":"10. Key Takeaways","text":"<ol> <li> <p>Architecture-hardware co-design is essential:    Designing operators and kernels together (rather than separately) enables substantial efficiency gains. Think about hardware (tensor cores, memory hierarchy) when designing architectures.</p> </li> <li> <p>Operator specialization beats redundancy:    Rather than having multiple operators compete for the same capability (in-context recall), specialize them to complementary roles (local recall, medium-range, long-range).</p> </li> <li> <p>Systems optimizations unlock scale:    Custom kernels (two-stage blocked algorithms) and parallelism strategies (context parallelism) are essential for training large models efficiently. Architecture alone isn't enough.</p> </li> <li> <p>Multi-hybrids combine strengths:    Combining short explicit (hardware-optimized), medium regularized (efficient mid-range), and long implicit (global context) operators enables better quality and efficiency than any single operator type.</p> </li> <li> <p>Filter grouping improves utilization:    Having channels share filters enables efficient blocked kernels that maximize tensor core utilization.</p> </li> <li> <p>Context parallelism is crucial for long sequences:    For 1M token sequences, standard parallelism isn't enough; need custom context parallelism strategies (all-to-all, point-to-point).</p> </li> <li> <p>Validated at real scale:    Improvements demonstrated at 40B parameters, 9T tokens, not just small models. This is essential for practical impact.</p> </li> <li> <p>General principles, specific demonstration:    While demonstrated on genomics (Evo 2), principles of multi-hybrid architectures and systems optimization apply to any sequence modeling domain.</p> </li> <li> <p>Full-stack development matters:    Don't just propose an architecture; provide kernels, parallelism strategies, and training recipes as a complete package.</p> </li> <li> <p>This enables new capabilities:     Systems optimizations make it practical to train models with 1M context, enabling applications (whole-genome modeling, long-document understanding) that were previously infeasible.</p> </li> </ol>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/","title":"TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second","text":"<p>Authors: Noah Hollmann, Samuel M\u00fcller, Katharina Eggensperger, Frank Hutter Year: 2023 Venue: ICLR 2023</p>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Tabular / Foundation Model. TabPFN is a foundation model for tabular data that uses in-context learning to solve small and medium-sized classification problems without gradient updates.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development. TabPFN introduces a novel approach to tabular learning by training on synthetic datasets to emulate Bayesian inference.</p> </li> <li> <p>Key Modalities: </p> </li> <li>Tabular data: Supports both categorical and continuous features with missing value handling.</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#2-executive-summary","title":"2. Executive Summary","text":"<p>This paper introduces TabPFN, an 11M-parameter tabular foundation model that revolutionizes small-to-medium tabular classification by performing in-context learning. Unlike traditional machine learning approaches that require training on each new dataset, TabPFN learns patterns from 100 million synthetic datasets generated from structural causal models during its pretraining phase. At inference time, given a new tabular dataset (up to 10,000 samples with 500 features), TabPFN predicts labels for unlabeled rows in a single forward pass\u2014taking approximately 1 second\u2014without any gradient updates or hyperparameter tuning. This approach emulates Bayesian inference and achieves competitive or superior performance compared to gradient-boosted decision trees (GBDTs) and neural networks on standard tabular benchmarks. The model handles categorical and continuous features, supports missing values through mask tokens, and provides probabilistic predictions. TabPFN demonstrates that foundation models can be successfully applied to tabular data through synthetic data generation and in-context learning, offering a fast and effective alternative to traditional tabular ML pipelines for small and medium-sized datasets.</p>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem</li> <li>Tabular data is ubiquitous across scientific and business domains, but traditional ML requires dataset-specific training, hyperparameter tuning, and feature engineering for each new problem.</li> <li>Gradient-boosted decision trees (GBDTs) and neural networks achieve strong performance but require minutes to hours of training time per dataset.</li> <li>There is no \"pretrained model\" paradigm for tabular data equivalent to what exists for vision (ViT, ResNet) and language (BERT, GPT).</li> <li> <p>The goal is to create a foundation model that can solve new tabular classification problems instantly through in-context learning.</p> </li> <li> <p>Why this is hard</p> </li> <li>Diverse tabular structures:<ul> <li>Unlike images or text, tabular datasets have varying numbers of features, different feature types (categorical vs. continuous), and no inherent ordering.</li> <li>Each dataset may have unique distributional properties and causal relationships.</li> </ul> </li> <li>Lack of large-scale tabular data:<ul> <li>No single large tabular dataset exists for pretraining (unlike ImageNet or web text).</li> <li>Real-world tabular datasets are often small (hundreds to thousands of samples) and heterogeneous.</li> </ul> </li> <li> <p>In-context learning requirements:</p> <ul> <li>The model must learn to adapt to entirely new tabular structures and distributions at inference time.</li> <li>Need to handle missing values, categorical variables, and varying sample sizes.</li> </ul> </li> <li> <p>What's the gap / opportunity?</p> </li> <li>Recent success of in-context learning in language models (GPT-3) suggests that Transformers can learn to perform inference on new tasks without gradient updates.</li> <li>Synthetic data generation from structural causal models can create diverse training distributions that cover the space of possible tabular problems.</li> <li>A model trained on synthetic data could learn general patterns that transfer to real-world tabular datasets.</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#4-method-architecture","title":"4. Method / Architecture","text":""},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#core-innovation","title":"Core Innovation","text":"<p>TabPFN uses a decoder-only Transformer trained on 100 million synthetic tabular datasets to perform in-context Bayesian inference. Instead of training on real data, the model learns from synthetic datasets generated by structural causal models (SCMs), which simulate diverse tabular distributions and causal relationships.</p>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#architecture-details","title":"Architecture Details","text":"<ul> <li>Backbone: Decoder-only Transformer (11M parameters)</li> <li>Input representation:</li> <li>Each row in the tabular dataset is encoded as a sequence of tokens</li> <li>Categorical features: embedded via learned embeddings</li> <li>Continuous features: normalized and projected to token space</li> <li>Missing values: handled with special mask tokens</li> <li>Sequence structure:</li> <li>Input sequence: <code>[feature\u2081, feature\u2082, ..., feature\u2099, label] \u00d7 M samples</code></li> <li>The model sees labeled training examples followed by unlabeled test examples</li> <li>Prediction: autoregressive generation of labels for test samples</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#training-procedure","title":"Training Procedure","text":"<ol> <li>Synthetic data generation:</li> <li>Generate 100M datasets from structural causal models</li> <li>Each SCM defines:<ul> <li>Number of features (up to 500)</li> <li>Feature types (categorical or continuous)</li> <li>Causal relationships between features</li> <li>Label generation mechanism</li> </ul> </li> <li> <p>Sample datasets with varying sizes (up to 10,000 samples)</p> </li> <li> <p>Pretraining objective:</p> </li> <li>For each synthetic dataset:<ul> <li>Split into train/test sets</li> <li>Feed train examples into Transformer</li> <li>Predict labels for test examples via autoregressive generation</li> </ul> </li> <li>Loss: cross-entropy on predicted vs. true test labels</li> <li> <p>Model learns to emulate Bayesian posterior inference</p> </li> <li> <p>Inference (in-context learning):</p> </li> <li>Given new real-world tabular dataset:<ul> <li>Format as sequence of (features, label) pairs for train split</li> <li>Append test examples with unknown labels</li> <li>Run single forward pass to predict test labels</li> </ul> </li> <li>No gradient updates, no hyperparameter tuning</li> </ol>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#key-technical-components","title":"Key Technical Components","text":"<ul> <li>Positional encodings: Learned embeddings for sample position and feature position</li> <li>Attention mechanism: Full self-attention over all train + test samples</li> <li>Output layer: Softmax over class labels (for classification)</li> <li>Probabilistic predictions: Model outputs class probabilities, enabling uncertainty quantification</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#5-key-results","title":"5. Key Results","text":""},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#benchmarks-and-performance","title":"Benchmarks and Performance","text":"<ul> <li>Datasets: Evaluated on 30 public tabular classification benchmarks from OpenML and UCI</li> <li>Baselines: Logistic regression, GBDTs (XGBoost, CatBoost), neural networks (MLP, ResNet)</li> <li>Metric: Test accuracy, AUC-ROC</li> </ul> <p>Performance highlights: - TabPFN matches or outperforms GBDTs and neural networks on ~70% of datasets with &lt;10,000 samples - Achieves competitive performance with zero hyperparameter tuning - On small datasets (&lt;1,000 samples), TabPFN often outperforms carefully tuned baselines</p>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#speed-comparison","title":"Speed Comparison","text":"<ul> <li>TabPFN: ~1 second inference time for datasets up to 10k samples</li> <li>GBDTs: Minutes to hours for training + hyperparameter search</li> <li>Neural networks: Hours for training + hyperparameter search</li> </ul> <p>Speedup: 100-1000\u00d7 faster than traditional ML pipelines</p>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#limitations","title":"Limitations","text":"<ul> <li>Sample size: Maximum 10,000 samples per forward pass</li> <li>Larger datasets (e.g., UK Biobank N~40k) require chunking or subsampling</li> <li>Feature count: Maximum 500 features</li> <li>Performance on large datasets: GBDTs and neural networks can outperform TabPFN when N&gt;10k and sufficient compute is available for training</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#6-implications-for-this-project","title":"6. Implications for This Project","text":""},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#direct-applications","title":"Direct Applications","text":"<ol> <li>Baseline predictor for raw tabular features:</li> <li>sMRI ROI tables: <code>smri_free_surfer_raw_176</code> (176 FreeSurfer ROIs)</li> <li>Genetics summary features: PGS scores, PCA projections</li> <li> <p>Use TabPFN as fast baseline before investing in GBDT/NN training</p> </li> <li> <p>Late fusion baseline:</p> </li> <li>Gene + brain embeddings: <code>fusion_concat_gene_brain_1024_v1</code> (1024-dim concatenated embeddings)</li> <li>Compare TabPFN vs. LR/GBDT on late fusion prediction tasks</li> <li> <p>Fast prototyping: test fusion hypotheses in seconds</p> </li> <li> <p>Cross-validation efficiency:</p> </li> <li>TabPFN eliminates hyperparameter tuning \u2192 faster CV loops</li> <li>Useful for sensitivity analyses and ablation studies</li> </ol>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#integration-with-project-pipelines","title":"Integration with Project Pipelines","text":"<ul> <li>Experiment config: <code>configs/experiments/03_prediction_baselines_tabular.yaml</code></li> <li>Add TabPFN as predictor option alongside LR/GBDT</li> <li> <p>Compare TabPFN vs. classical methods on:</p> <ul> <li>Gene-only prediction</li> <li>Brain-only prediction</li> <li>Late fusion (gene + brain)</li> </ul> </li> <li> <p>Harmonization compatibility:</p> </li> <li>TabPFN can handle raw features or harmonized embeddings</li> <li>Test whether harmonization (MURD, ComBat) improves TabPFN performance</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#limitations-for-this-project","title":"Limitations for This Project","text":"<ul> <li>UK Biobank scale: N~40k requires chunking into \u226410k folds</li> <li>Solution: stratified sampling or per-fold inference</li> <li>Not a foundation model encoder:</li> <li>TabPFN predicts labels, not embeddings</li> <li>Cannot replace gene/brain FMs for representation learning</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Sanity check: Run TabPFN on small pilot cohort (N&lt;1000)</li> <li>Fast prototyping: Test gene-brain fusion hypotheses with TabPFN</li> <li>Baseline comparison: Compare TabPFN vs. LR/GBDT on full UKB cohort</li> <li>Report both: Include TabPFN and traditional methods in final results</li> </ol>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#7-related-work-and-context","title":"7. Related Work and Context","text":""},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#tabular-foundation-models","title":"Tabular Foundation Models","text":"<ul> <li>Prior work:</li> <li>VIME (2020): Self-supervised pretraining for tabular data</li> <li>SAINT (2021): Self-attention for tabular data</li> <li>FT-Transformer (2021): Feature Tokenizer + Transformer for tabular data</li> <li>TabPFN's novelty:</li> <li>First to use synthetic data generation + in-context learning</li> <li>No gradient updates at inference time</li> <li>Emulates Bayesian inference rather than discriminative learning</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#in-context-learning","title":"In-Context Learning","text":"<ul> <li>GPT-3 (2020): Demonstrated few-shot learning on text tasks</li> <li>Flamingo (2022): In-context learning for vision-language tasks</li> <li>TabPFN (2023): Extends in-context learning to tabular data</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#synthetic-data-for-pretraining","title":"Synthetic Data for Pretraining","text":"<ul> <li>Structural causal models (SCMs): Simulate diverse causal relationships</li> <li>TabPFN innovation: Train on SCM-generated datasets to learn general tabular patterns</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#8-key-takeaways","title":"8. Key Takeaways","text":""},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#what-works","title":"What Works","text":"<ul> <li>In-context learning for tabular data: TabPFN demonstrates that Transformers can learn to solve new tabular problems without gradient updates</li> <li>Synthetic data generation: Training on 100M SCM-generated datasets enables transfer to real-world tabular data</li> <li>Speed advantage: 1-second inference vs. hours of GBDT/NN training</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#what-doesnt-work-or-is-limited","title":"What Doesn't Work (or is Limited)","text":"<ul> <li>Large datasets: Performance degrades on datasets with &gt;10k samples</li> <li>Feature count: Limited to 500 features</li> <li>Scalability: Cannot handle very large-scale problems (e.g., million-sample datasets)</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#best-practices-for-this-project","title":"Best Practices for This Project","text":"<ul> <li>Use TabPFN for rapid prototyping: Test fusion hypotheses before committing to full GBDT/NN pipelines</li> <li>Keep sample counts \u226410k per fold: Chunk UK Biobank cohort if needed</li> <li>Compare with traditional methods: Report both TabPFN and LR/GBDT results</li> <li>Leverage speed for sensitivity analyses: Run multiple configurations quickly</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#9-links-and-resources","title":"9. Links and Resources","text":"<ul> <li>Paper: ICLR 2023 (arXiv)</li> <li>DOI: arXiv:2207.01848</li> <li>Code: Official TabPFN GitHub</li> <li>Related documentation:</li> <li>Prediction baselines</li> <li>Integration strategy</li> </ul>"},{"location":"generated/kb_curated/papers-md/tabpfn_2024/#10-verification-and-notes","title":"10. Verification and Notes","text":"<p>Status: Needs human review</p> <p>Notes: - TabPFN is highly relevant for fast late fusion prototyping - Treat as downstream predictor competing with LR/GBDT on embeddings - Keep per-fold sample counts \u226410k or batch inference - Consider TabPFN for ablation studies and sensitivity analyses - Not a replacement for gene/brain foundation models\u2014purely a predictor</p> <p>Tags: <code>tabular</code>, <code>foundation_model</code>, <code>in_context_learning</code>, <code>transformer</code>, <code>bayesian_inference</code>, <code>fast_inference</code>, <code>baseline_predictor</code></p>"},{"location":"generated/kb_curated/papers-md/template/","title":"Paper Title (Year) \u2014 Short Tagline","text":"","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#1-problem-tasks","title":"1. Problem &amp; Tasks","text":"<ul> <li>What is being predicted/estimated?</li> <li>Benchmarks/tasks, evaluation settings (classification, regression, survival, etc.).</li> <li>How this connects to genetics \u00d7 MRI integration (if applicable).</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#2-datasets","title":"2. Datasets","text":"<ul> <li>Cohorts, sample sizes, populations, modalities.</li> <li>Preprocessing specifics (QC, parcellations, sequencing protocols, etc.).</li> <li>Splits: train/val/test definitions, CV type, site-aware grouping, seeds.</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#3-model-method-details","title":"3. Model / Method Details","text":"","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#31-architecture-method","title":"3.1 Architecture / Method","text":"<ul> <li>Backbone, context length, tokenization, RC-equivariance, hub tokens, etc.</li> <li>Input/output formats, objectives, losses, notable tricks.</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#32-confound-handling-evaluation-discipline","title":"3.2 Confound Handling &amp; Evaluation Discipline","text":"<ul> <li>Residualization, stratification, harmonization, missingness handling.</li> <li>Metrics (AUROC/AUPRC/Brier/calibration), statistical tests (DeLong, bootstrap, permutations, FDR).</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#4-results-tables","title":"4. Results &amp; Tables","text":"<ul> <li>Key numbers vs baselines (include effect sizes and uncertainty if reported).</li> <li>Ablation results or thresholds that matter for reuse.</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#5-limitations-cautions","title":"5. Limitations &amp; Cautions","text":"<ul> <li>Author-listed limitations plus any reuse caveats you notice.</li> <li>Cohort bias, preprocessing quirks, compute constraints, licensing limits.</li> </ul>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/template/#6-hooks-into-neuro-omics-kb","title":"6. Hooks into Neuro-Omics KB","text":"<p>Relevant KB assets</p> <ul> <li><code>kb/paper_cards/&lt;slug&gt;_YYYY.yaml</code></li> <li><code>kb/model_cards/...</code> (if applicable)</li> <li><code>kb/datasets/...</code> (if applicable)</li> <li><code>docs/generated/kb_curated/integration_cards/&lt;related&gt;.md</code></li> </ul> <p>Configs / recipes informed</p> <ul> <li><code>configs/experiments/...</code></li> <li><code>docs/integration/analysis_recipes/...</code></li> <li><code>docs/integration/integration_strategy.md</code></li> </ul> <p>Concrete guidance for our project</p> <ul> <li>Fusion pattern / modality sequencing choices this paper justifies.</li> <li>Specific covariates, statistical tests, or dataset fields we adopted.</li> <li>Any LOGO/PRS/GWAS/CCA protocol parameters we ported over.</li> </ul> <p>Copy this file to <code>docs/generated/kb_curated/papers-md/&lt;slug&gt;.md</code> and fill in the sections. Keep Layer 2 MDs canonical, so future tooling (RAG, dashboards) can scan them systematically.</p>","tags":["paper-notes"]},{"location":"generated/kb_curated/papers-md/titan_2025/","title":"TITAN (2025)","text":""},{"location":"generated/kb_curated/papers-md/titan_2025/#titan-a-multimodal-whole-slide-foundation-model-for-computational-pathology","title":"TITAN: A Multimodal Whole-Slide Foundation Model for Computational Pathology","text":"<p>Authors: Tong Ding, Sophia J. Wagner, Andrew H. Song, Richard J. Chen, Ming Y. Lu, Andrew Zhang, Anurag J. Vaidya, Guillaume Jaume, Muhammad Shaban, Ahrong Kim, Drew F. K. Williamson, Harry Robertson, Bowen Chen, Cristina Almagro-P\u00e9rez, Paul Doucet, Sharifa Sahai, Chengkuan Chen, Christina S. Chen, Daisuke Komura, Akihiro Kawabe, Mieko Ochi, Shinya Sato, Tomoyuki Yokose, Yohei Miyagi, Shumpei Ishikawa, Georg Gerber, Tingying Peng, Long Phi Le, Faisal Mahmood Year: 2025 Venue: Nature Medicine</p>"},{"location":"generated/kb_curated/papers-md/titan_2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li>Medical Vision FM + Medical VLM / MLLM / MMFM  </li> <li> <p>TITAN is a whole\u2011slide histopathology foundation model that combines vision\u2011only pretraining with vision\u2013language alignment for pathology reports and synthetic captions.</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Core FM development + Multimodal FM or cross-modal integration  </p> </li> <li> <p>Key Modalities: </p> </li> <li>Whole\u2011slide histopathology images (WSIs) across \u224820 organ types.  </li> <li>Pathology reports (free\u2011text, slide\u2011level).  </li> <li>Synthetic fine\u2011grained region\u2011of\u2011interest (ROI) captions generated by a multimodal pathology copilot (PathChat).</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>TITAN (Transformer\u2011based pathology Image and Text Alignment Network) is a slide\u2011level foundation model for pathology designed to transform gigapixel whole\u2011slide images into general\u2011purpose feature representations that support diagnosis, prognosis, retrieval, and report generation. Instead of working at the level of raw pixels, TITAN builds on pre\u2011extracted patch embeddings from powerful histology encoders, then scales self\u2011supervised learning (SSL) to entire slides using a vision transformer with long\u2011context positional encodings. The model is pretrained in three stages: vision\u2011only self\u2011supervision on hundreds of thousands of WSIs; vision\u2013language alignment using synthetic ROI\u2011level captions; and slide\u2011level alignment with pathology reports. This yields TITANV (vision\u2011only) and full TITAN (vision\u2013language), which are evaluated across slide classification, biomarker prediction, survival analysis, rare cancer retrieval, cross\u2011modal slide\u2013report retrieval, and zero\u2011shot report generation. TITAN consistently outperforms prior ROI\u2011based and slide\u2011level foundation models across linear probing, few\u2011shot, and zero\u2011shot settings, especially in low\u2011data clinical scenarios. For a new grad student, TITAN provides a clear blueprint for scaling from patch encoders to slide\u2011level multimodal FMs in pathology.</p>"},{"location":"generated/kb_curated/papers-md/titan_2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<ul> <li>Scientific / practical problem: </li> <li> <p>Learn slide\u2011level representations of histopathology WSIs that:  </p> <ul> <li>Capture rich tissue morphology at multiple spatial scales.  </li> <li>Support a wide range of downstream tasks (subtyping, biomarker prediction, prognosis, retrieval, report generation).  </li> <li>Work well even in low\u2011data and rare disease regimes.  </li> </ul> </li> <li> <p>Why this is hard: </p> </li> <li>Gigapixel scale and long context: <ul> <li>WSIs can contain &gt;10\u2074 patch embeddings; na\u00efvely applying transformers is computationally prohibitive.  </li> </ul> </li> <li>Limited labeled cohorts: <ul> <li>Clinical datasets for specific cancers or biomarkers are small and heterogeneous, especially for rare conditions.  </li> </ul> </li> <li>Patch vs slide gap: <ul> <li>Many existing FMs operate on small ROIs; aggregating patch features into clinically meaningful slide\u2011level signals is non\u2011trivial.  </li> </ul> </li> <li>Multimodal supervision: <ul> <li>Pathology reports and textual descriptions encode rich semantics but are noisy and unstructured; exploiting them at scale is challenging.  </li> </ul> </li> <li>Generalization and retrieval: <ul> <li>Models must generalize across organs, stains, scanner types, and institutions, and support tasks like rare cancer retrieval where labeled examples are extremely sparse.</li> </ul> </li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<ul> <li>Pretraining data (Mass\u2011340K): </li> <li>\u2248335,645 WSIs across 20 organ types.  </li> <li>182,862 human pathology reports at slide level.  </li> <li> <p>Diverse stains, tissue types, and scanners to maximize coverage of histopathology morphologies.</p> </li> <li> <p>Synthetic caption data: </p> </li> <li>423,122 synthetic ROI\u2011level captions generated from 8k\u00d78k pixel regions using PathChat, a multimodal pathology copilot.  </li> <li> <p>Each caption describes fine\u2011grained morphology within an ROI (e.g., cell types, tissue organization).</p> </li> <li> <p>Downstream benchmarks (representative): </p> </li> <li>Cancer subtyping and grading across multiple tumor types.  </li> <li>Molecular biomarker prediction (e.g., mutation status, molecular subtypes).  </li> <li>Survival prediction and outcome prognosis.  </li> <li>Rare cancer retrieval: retrieve similar slides for diagnostically challenging WSIs.  </li> <li>Cross\u2011modal retrieval (slide \u2194 report).  </li> <li> <p>Zero\u2011shot and few\u2011shot slide classification guided by textual prompts.</p> </li> <li> <p>Preprocessing / representation: </p> </li> <li>WSIs are divided into 512\u00d7512 patches at 20\u00d7 magnification, and each patch is encoded into a 768\u2011dimensional feature using a strong patch encoder (CONCH v1.5).  </li> <li>Patch features are arranged into a 2D grid reflecting spatial layout, then cropped into global and local views for SSL.  </li> <li>Pathology reports and synthetic captions are tokenized and embedded for vision\u2013language alignment.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<ul> <li>Model Type: </li> <li> <p>Slide\u2011level Vision Transformer (ViT) foundation model with multimodal vision\u2013language pretraining.</p> </li> <li> <p>Is it a new FM or an existing one? </p> </li> <li> <p>TITAN is a new whole\u2011slide foundation model, though it builds on existing patch encoders (e.g., CONCH) and SSL techniques (iBOT, CoCa\u2011style alignment).</p> </li> <li> <p>Key components and innovations:</p> </li> </ul> Aspect Details Backbone ViT\u2011style transformer operating on patch\u2011feature tokens Input tokens 2D grid of patch embeddings (from CONCH) plus [CLS] / slide tokens Vision\u2011only pretraining iBOT\u2011style masked prediction on WSI feature grids (TITANV) ROI\u2011level alignment Contrastive alignment with synthetic ROI captions (PathChat) Slide\u2011level alignment Contrastive / CoCa\u2011style alignment with pathology reports Positional encoding Long\u2011range encodings (e.g., ALiBi\u2011style) adapted to large 2D grids <ul> <li>Training setup (three stages):</li> <li>Stage 1 \u2013 Vision\u2011only SSL (TITANV): <ul> <li>Perform iBOT pretraining on region crops (16\u00d716 token grids) and their multi\u2011scale views (global 14\u00d714 and local 6\u00d76 crops).  </li> <li>Learns slide\u2011level representations that aggregate patch\u2011level morphologies.  </li> </ul> </li> <li>Stage 2 \u2013 ROI\u2011level vision\u2013language pretraining: <ul> <li>Align 423k ROI crops (8k\u00d78k regions) with synthetic captions from PathChat.  </li> <li>Encourages TITAN to associate specific morphological patterns with textual descriptions.  </li> </ul> </li> <li>Stage 3 \u2013 Slide\u2011level vision\u2013language pretraining: <ul> <li>Align 183k WSIs with their corresponding pathology reports, enabling slide\u2011level semantic understanding and cross\u2011modal retrieval.  </li> </ul> </li> <li>After pretraining, TITAN can be used for linear probing, few\u2011shot fine\u2011tuning, zero\u2011shot classification via text prompts, and report generation.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<ul> <li>Modalities integrated: </li> <li> <p>Histopathology WSIs and free\u2011text pathology reports, plus synthetic ROI\u2011level captions.</p> </li> <li> <p>How integration works: </p> </li> <li>Vision\u2011only backbone: <ul> <li>TITANV is trained with SSL on slide\u2011level patch features to learn rich visual embeddings.  </li> </ul> </li> <li>ROI\u2011level vision\u2013language alignment: <ul> <li>A contrastive or CoCa\u2011style loss aligns ROI embeddings with synthetic PathChat captions, injecting fine\u2011grained morphological semantics.  </li> </ul> </li> <li> <p>Slide\u2011level vision\u2013language alignment: </p> <ul> <li>Slide embeddings are aligned with pathology report text, enabling cross\u2011modal retrieval and zero\u2011shot, text\u2011prompted classification.  </li> </ul> </li> <li> <p>New capabilities enabled: </p> </li> <li>Zero\u2011shot and few\u2011shot slide classification using textual prompts describing subtypes or biomarkers.  </li> <li>Rare cancer retrieval: find clinically similar slides based on TITAN embeddings, even with minimal labeled examples.  </li> <li>Cross\u2011modal search: slide\u2011to\u2011report and report\u2011to\u2011slide retrieval for case exploration and education.  </li> <li>Report generation: generate slide\u2011level pathology reports conditioned on WSI embeddings and language decoders.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<ul> <li>Tasks / benchmarks: </li> <li>Slide\u2011level cancer subtyping and grading across multiple public and internal cohorts.  </li> <li>Molecular prediction (e.g., mutation and expression surrogates) from WSIs.  </li> <li>Survival prediction and risk stratification.  </li> <li>Rare cancer and challenging\u2011case retrieval.  </li> <li> <p>Cross\u2011modal slide\u2013report retrieval and zero\u2011shot classification using textual prompts.</p> </li> <li> <p>Baselines: </p> </li> <li>ROI\u2011based patch encoders combined with slide aggregators (MIL and attention\u2011pooling models).  </li> <li>Prior slide\u2011level foundation models trained with vision\u2011only SSL or smaller multimodal datasets.  </li> <li> <p>Task\u2011specific supervised models trained on individual cohorts.  </p> </li> <li> <p>Key findings (trends): </p> </li> <li>TITANV (vision\u2011only) already outperforms prior slide\u2011level models and ROI\u2011aggregator baselines on many slide classification and biomarker tasks.  </li> <li>Full TITAN (after multimodal pretraining) further improves performance, particularly in low\u2011data and few\u2011shot settings.  </li> <li>TITAN shows strong rare cancer retrieval performance, retrieving pathologically similar slides that can assist in challenging diagnoses.  </li> <li>Vision\u2013language pretraining with synthetic ROI captions and reports enables zero\u2011shot text\u2011guided classification and cross\u2011modal retrieval that prior slide FMs cannot match.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>First large\u2011scale multimodal whole\u2011slide foundation model that unifies vision\u2011only SSL and vision\u2013language alignment across ROI and slide levels.  </li> <li>Demonstrates that building on strong patch encoders and scaling to slide level yields state\u2011of\u2011the\u2011art performance across many pathology tasks.  </li> <li>Synthetic ROI captions from PathChat provide a practical way to incorporate fine\u2011grained morphological supervision at scale.  </li> <li>Extensive evaluations across tasks, organs, and settings (linear probing, few\u2011shot, zero\u2011shot) show TITAN\u2019s breadth and robustness.</li> </ul> <p>Limitations:</p> <ul> <li>Relies on a large, internal Mass\u2011340K dataset and synthetic captions that are not fully public, limiting reproducibility.  </li> <li>Synthetic captions, while powerful, may encode biases and failure modes of the PathChat generator.  </li> <li>Focuses primarily on histopathology WSIs; other modalities (radiology, multi\u2011omics, clinical text beyond reports) are not integrated.  </li> <li>Training at this scale requires substantial compute and storage, making it difficult for smaller groups to replicate or extend.</li> </ul> <p>Open Questions and Future Directions:</p> <ol> <li>How can TITAN\u2011style slide\u2011level FMs be extended to multimodal clinical contexts, integrating WSIs with genomics, radiology, and EHR data?  </li> <li>What are robust methods for validating and correcting synthetic captions, ensuring that vision\u2013language supervision does not propagate hallucinations into the slide encoder?  </li> <li>Can more efficient architectures (e.g., sparse attention, hierarchical transformers) reduce the cost of handling giga\u2011pixel WSIs without losing performance?  </li> <li>How should we design evaluation protocols and human\u2011in\u2011the\u2011loop workflows for rare cancer retrieval, where errors may have significant diagnostic consequences?  </li> <li>Could TITAN representations support interactive, region\u2011grounded explanations that show which slide regions drive predictions or retrieved cases?</li> </ol>"},{"location":"generated/kb_curated/papers-md/titan_2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<ul> <li>Position in the landscape: </li> <li>TITAN is to pathology WSIs what CLIP\u2011like and CoCa\u2011style models are to natural images and text: a general\u2011purpose slide representation that supports many downstream tasks and multimodal interactions.  </li> <li>It extends the trend of patch\u2011level pathology FMs to the slide level, helping bridge the gap between detailed morphology and patient\u2011level clinical endpoints.  </li> <li>Relation to well-known ideas: </li> <li>Combines iBOT\u2011style masked image modeling, patch\u2011encoder distillation, and vision\u2013language contrastive pretraining in a three\u2011stage pipeline.  </li> <li>Conceptually similar to GigaPath and other large\u2011scale pathology FMs but emphasizes multimodal, slide\u2011level pretraining and rare cancer retrieval.  </li> <li>Why this paper is a useful reference: </li> <li>Provides a detailed design for scaling from patch encoders to slide\u2011level transformers and for incorporating synthetic and real textual supervision.  </li> <li>For a grad student, TITAN is an archetype for building high\u2011capacity medical vision FMs and integrating them into multimodal medical AI systems.</li> </ul>"},{"location":"generated/kb_curated/papers-md/titan_2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<ul> <li>Problem: </li> <li> <p>Existing pathology FMs mostly work at the patch level and lack robust, multimodal slide\u2011level representations, limiting performance in patient\u2011level prediction and rare disease scenarios.  </p> </li> <li> <p>Method / model: </p> </li> <li>TITAN is a ViT\u2011based slide foundation model trained in three stages: large\u2011scale vision\u2011only SSL on WSI patch features, ROI\u2011level alignment with synthetic PathChat captions, and slide\u2011level alignment with pathology reports.  </li> <li> <p>Operates entirely in the patch\u2011embedding space, with long\u2011range positional encodings and multi\u2011scale cropping to handle giga\u2011pixel WSIs.  </p> </li> <li> <p>Results: </p> </li> <li>Outperforms ROI\u2011based and prior slide\u2011level FMs on cancer subtyping, biomarker prediction, prognosis, and retrieval, especially in low\u2011data and few\u2011shot regimes.  </li> <li> <p>Enables zero\u2011shot text\u2011guided classification, cross\u2011modal slide\u2013report retrieval, and pathology report generation.  </p> </li> <li> <p>Why it matters: </p> </li> <li>Establishes a strong template for multimodal slide\u2011level foundation models, bringing pathology closer to the capabilities seen in natural\u2011image FMs and VLMs.  </li> <li>Opens pathways for rare disease support, better education and retrieval tools, and future integration with other medical modalities.</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/","title":"Yoon BioKDD (2025)","text":""},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#classifying-major-depressive-disorder-with-exon-sequence-embeddings-from-dna-foundation-models","title":"Classifying Major Depressive Disorder with Exon Sequence Embeddings from DNA Foundation Models","text":"<p>Authors: Heesun Yoon, Eunji Lee, Heehwan Wang, Jiook Cha, Xin Dai, Shinjae Yoo, Yoonjung Yoonie Joo Year: 2025 Venue: BIOKDD '25 (24th International Workshop on Data Mining in Bioinformatics)</p>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#1-classification","title":"1. Classification","text":"<ul> <li>Domain Category: </li> <li> <p>Genomics FM + Application. This paper applies pretrained DNA foundation models (Caduceus, DNABERT-2) to psychiatric genomics, specifically using exon sequence embeddings to classify Major Depressive Disorder (MDD).</p> </li> <li> <p>FM Usage Type: </p> </li> <li> <p>Application of existing FMs. The study leverages two pretrained genomic foundation models\u2014Caduceus (Mamba-based) and DNABERT-2 (Transformer-based)\u2014to generate DNA sequence embeddings, then trains classical ML models on top of these embeddings for MDD classification.</p> </li> <li> <p>Key Modalities: </p> </li> <li>DNA sequences: Whole exome sequencing (WES) data from 38 MDD-associated genes</li> <li>Demographic/clinical features: Age, sex, household income, genetic principal components</li> <li>Derived representations: High-dimensional exon sequence embeddings from foundation models</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#2-executive-summary","title":"2. Executive Summary","text":"<p>This study introduces a novel framework for classifying Major Depressive Disorder (MDD) using raw DNA sequence embeddings extracted from the exonic regions of 38 MDD-associated genes. Rather than relying on traditional polygenic risk scores (PRS) that aggregate single nucleotide polymorphism (SNP) effects and typically explain only 2-3% of MDD variance, the authors leverage pretrained genomic foundation models\u2014Caduceus (based on the Mamba architecture) and DNABERT-2 (Transformer-based)\u2014to generate context-rich, high-dimensional representations of exon sequences from whole exome sequencing (WES) data. Using UK Biobank data with 10,307 MDD cases and 10,307 healthy controls, they systematically evaluate 15 machine learning pipelines combining three embedding aggregation strategies (PCA, max pooling, mean pooling) with five classifiers (logistic regression, random forest, CatBoost, MLP, 1D-CNN). The best-performing pipeline\u2014CatBoost with max pooling\u2014achieves a mean AUC of 0.851 and AUPRC of 0.812, representing a ~49-60% improvement over traditional PRS approaches. A leave-one-gene-out (LOGO) analysis reveals that the SOD2 gene contributes most significantly to classification performance despite lacking established GWAS hits in exonic regions, highlighting the model's ability to capture biological signals beyond SNP-level associations. Replication with DNABERT-2 embeddings yields identical top performance (AUC 0.851), confirming the framework's generalizability across different foundation model architectures. This work demonstrates that pretrained sequence models can effectively encode complex genomic context for psychiatric risk prediction, opening new avenues for applying foundation models to mental health genomics.</p>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#3-problem-setup-and-motivation","title":"3. Problem Setup and Motivation","text":"<p>Scientific / practical problem:</p> <ul> <li>Predicting MDD risk from genetic data:</li> <li>Major Depressive Disorder affects ~300 million people worldwide with substantial economic burden (~$6 trillion projected by 2030)</li> <li> <p>Individual risk prediction is challenging due to extreme polygenicity and high phenotypic heterogeneity</p> </li> <li> <p>Limitations of existing genetic approaches:</p> </li> <li>Polygenic Risk Scores (PRS): Aggregate SNP-level GWAS effects but explain only ~2-3% of MDD phenotypic variance (AUC ~0.53-0.57)</li> <li>Variant-level focus: Cannot capture broader sequence-level context, regulatory motifs, structural variants, or long-range dependencies</li> <li> <p>Exonic regions understudied: GWAS emphasizes non-coding regions, but functionally important protein-coding sequences may harbor disease-relevant information missed by SNP-based methods</p> </li> <li> <p>Opportunity with DNA foundation models:</p> </li> <li>Models like DNABERT, DNABERT-2, Caduceus, and Nucleotide Transformer pretrained on large genomic corpora can generate embeddings that capture sophisticated context beyond single variants</li> <li>These models have shown success in regulatory element classification and gene expression prediction but remain unexplored in psychiatric genomics</li> </ul> <p>Why this is hard:</p> <ul> <li>Polygenicity and small effects:</li> <li> <p>MDD involves thousands of loci with tiny individual contributions; signal-to-noise ratio is low</p> </li> <li> <p>Data modality mismatch:</p> </li> <li> <p>Whole exome sequencing (WES) captures exons cost-effectively but misses intronic/intergenic regulatory elements where many GWAS hits lie</p> </li> <li> <p>Embedding aggregation:</p> </li> <li> <p>38 genes produce 38 separate high-dimensional embeddings; need effective strategies to combine them without losing gene-specific signals or introducing noise</p> </li> <li> <p>Demographic confounding:</p> </li> <li> <p>MDD cases differ from controls in age, sex, and socioeconomic status; must control these covariates</p> </li> <li> <p>Model selection and overfitting:</p> </li> <li> <p>Many hyperparameters and modeling choices; need rigorous nested cross-validation to avoid overoptimistic results</p> </li> <li> <p>Interpretability:</p> </li> <li>Which genes or exons drive predictions? Black-box embeddings require LOGO-style analyses to identify key contributors</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#4-data-and-modalities","title":"4. Data and Modalities","text":"<p>Dataset:</p> <ul> <li>UK Biobank:</li> <li>10,307 MDD cases and 10,307 healthy controls (downsampled for balance)</li> <li>Whole Exome Sequencing (WES) data (Exome OQFE variant call files, interim 200k release)</li> <li> <p>Demographic and clinical features: age at recruitment, sex, household income, top 10 genetic principal components</p> </li> <li> <p>MDD case definition:</p> </li> <li>Participants meeting at least one of three depression criteria (Howard et al. 2018):<ul> <li>Broad depression</li> <li>Probable MDD</li> <li>ICD-coded MDD</li> </ul> </li> <li> <p>Cases: Single/recurrent probable major depression episodes</p> </li> <li> <p>Healthy control definition:</p> </li> <li>No bipolar disorder or major depression</li> <li>No history of psychiatric care on admission</li> <li>Never sought professional help for mental distress</li> </ul> <p>Modalities:</p> <ul> <li>Genomic sequences:</li> <li>FASTA sequences of exonic regions from 38 autosomal MDD-associated genes (genes reported \u22652 times in prior literature)</li> <li>Genome Reference Consortium Human Build 38 (GRCh38)</li> <li> <p>Individual-level sequences with reference alleles replaced by alternate alleles where applicable</p> </li> <li> <p>Demographics:</p> </li> <li>Age (mean: cases 55.4\u00b18.0 years, controls 57.1\u00b17.8 years; significant difference p&lt;0.001)</li> <li>Sex (cases 64.1% female, controls 46.9% female; p&lt;0.001)</li> <li>Household income (controls more likely in higher income brackets; p&lt;0.001)</li> <li>Genetic PCs (top 10 PCs to control population structure)</li> </ul> <p>Preprocessing / representation:</p> <ul> <li>Sequence embedding generation:</li> <li>Primary: Caduceus-PS (256 hidden dimensions, 16 MambaDNA layers)<ul> <li>Bi-directional Mamba with reverse-complement (RC) equivariance</li> <li>Hidden states averaged across forward and RC representations to obtain RC-invariant embeddings (512\u2192256 dimensions)</li> </ul> </li> <li>Replication: DNABERT-2 (Transformer-based genomic foundation model)</li> <li> <p>Each of 38 genes produces one 256-dimensional embedding per sample</p> </li> <li> <p>Embedding aggregation (three strategies tested):</p> </li> <li>PCA: Feature scaling + PCA to 256 principal components</li> <li>Max pooling: Element-wise maximum across all 38 gene embeddings</li> <li> <p>Mean pooling: Element-wise mean across all 38 gene embeddings</p> </li> <li> <p>Final input:</p> </li> <li>Aggregated gene embeddings (256 dimensions) concatenated with 14 demographic features (age, sex, income, 10 genetic PCs)</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#5-model-foundation-model","title":"5. Model / Foundation Model","text":"<p>Foundation Models Used:</p> Model Architecture Key Features Purpose Caduceus-PS Mamba (structured state space model) Bi-directional, RC-equivariant, linear complexity, long context (~131k) Primary embedding generator DNABERT-2 Transformer (BERT-like) Bidirectional attention, multi-species genome pretraining Replication/validation of framework generalizability <p>Caduceus Details:</p> <ul> <li>Architecture: BiMamba + MambaDNA layers with parameter sharing for RC equivariance</li> <li>Advantages:</li> <li>Linear computational complexity (vs quadratic for Transformers) \u2192 handles long sequences efficiently</li> <li>Bi-directionality captures upstream and downstream context</li> <li>RC equivariance ensures equivalent representations for forward and reverse-complement strands</li> <li>Configuration: 256 hidden dimensions, 16 layers, ~131k sequence length capacity</li> <li>Output: Per-sequence hidden states split in half and averaged to yield RC-invariant 256-d embeddings</li> </ul> <p>Downstream Classification Models (15 pipelines):</p> <p>Five classifiers \u00d7 three aggregation strategies:</p> <ol> <li>Logistic Regression (LR): Linear baseline</li> <li>Random Forest (RF): Ensemble of decision trees</li> <li>CatBoost (CB): Gradient boosting with categorical feature support</li> <li>Multilayer Perceptron (MLP): Feedforward neural network</li> <li>1D Convolutional Neural Network (1D-CNN): Convolutional layers for pattern detection</li> </ol> <p>Training Setup:</p> <ul> <li>Nested cross-validation:</li> <li>Outer loop: 10-fold stratified CV for unbiased model assessment</li> <li> <p>Inner loop: 3-fold stratified CV for hyperparameter tuning</p> </li> <li> <p>Hyperparameter optimization:</p> </li> <li>Framework: Optuna with Bayesian optimization</li> <li>Pruning strategies: HyperbandPruner for neural nets, MedianPruner for others</li> <li> <p>Trials: 150 for MLP, 100 for 1D-CNN, 300 for LR/RF/CB</p> </li> <li> <p>Early stopping:</p> </li> <li>CatBoost: 10 consecutive rounds without AUC improvement</li> <li>MLP/1D-CNN: 200 epochs with binary cross-entropy loss monitoring</li> <li> <p>10% holdout set for validation</p> </li> <li> <p>Evaluation metrics:</p> </li> <li>Primary: AUC (threshold-independent, consistent)</li> <li>Secondary: AUPRC (for tied AUC values), precision, recall, F1-score</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#6-multimodal-integration-aspects-if-applicable","title":"6. Multimodal / Integration Aspects (If Applicable)","text":"<p>This work is primarily unimodal (genomics) at the embedding level but incorporates late fusion of genetic embeddings with demographic/clinical features.</p> <p>Modalities integrated:</p> <ul> <li>Genomic: DNA sequence embeddings from foundation models (captures exonic sequence context)</li> <li>Demographic/clinical: Age, sex, household income, genetic PCs</li> </ul> <p>How they are integrated:</p> <ul> <li>Late fusion via feature concatenation:</li> <li>Gene embeddings aggregated into a 256-d vector</li> <li>Concatenated with 14 demographic features</li> <li>Combined feature vector fed to downstream classifiers</li> <li>This is analogous to late fusion in the integration baseline plan: modality-specific representations (gene sequences \u2192 embeddings, demographics as raw features) are combined at the feature level before final prediction</li> </ul> <p>Why this integration is useful:</p> <ul> <li>Demographics as confounders: Age, sex, and income differ significantly between MDD cases and controls; including them improves model accuracy and reduces spurious associations</li> <li>Genetic PCs control population structure: Prevent inflation due to ancestry-related confounding</li> <li>Complementary information: Genetic embeddings capture biological predisposition; demographics provide environmental/social context</li> </ul> <p>Relation to the integration baseline plan:</p> <ul> <li>Late fusion approach:</li> <li>Aligns with the plan's preference for preserving modality-specific signals before integration</li> <li> <p>Genetic embeddings are computed independently (via pretrained FMs), then concatenated with demographics</p> </li> <li> <p>Robustness and evaluation:</p> </li> <li>Nested CV with proper train/test splits mirrors the plan's emphasis on evaluation discipline</li> <li> <p>Leave-one-gene-out analysis with Wilcoxon signed-rank test + FDR correction aligns with plan's recommendation for attribution via LOGO and statistical rigor</p> </li> <li> <p>Genetics embedding hygiene:</p> </li> <li>Use of RC-equivariant Caduceus and RC-averaging reflects the plan's citation of Caduceus for proper genetics handling</li> <li> <p>Deterministic embedding generation ensures reproducibility</p> </li> <li> <p>Future escalation:</p> </li> <li>Current work uses simple concatenation; could extend to two-tower contrastive learning (genetic encoder vs demographic encoder) or attention-based fusion if performance plateaus</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#7-experiments-and-results","title":"7. Experiments and Results","text":"<p>Tasks / benchmarks:</p> <ul> <li>Binary classification: MDD cases (n=10,307) vs healthy controls (n=10,307) in UK Biobank</li> <li>Primary evaluation: 15 modeling pipelines (5 classifiers \u00d7 3 aggregation strategies)</li> <li>Replication: Same 15 pipelines with DNABERT-2 embeddings</li> <li>Interpretability: Leave-one-gene-out (LOGO) analysis to rank gene contributions</li> </ul> <p>Baselines:</p> <ul> <li>Traditional PRS: Historical MDD PRS models explain ~2-3% variance, AUC ~0.53-0.57</li> <li>Within-study baselines: All 15 pipeline combinations serve as internal comparisons</li> </ul> <p>Key findings:</p> <p>1. Primary Caduceus Results (Table 2):</p> <ul> <li>Best performance: CatBoost + max pooling</li> <li>Mean AUC: 0.851 \u00b1 0.009</li> <li> <p>Mean AUPRC: 0.812 \u00b1 0.013</p> </li> <li> <p>Tied second-best: Logistic Regression + max pooling</p> </li> <li>Mean AUC: 0.851 \u00b1 0.010</li> <li> <p>Mean AUPRC: 0.809 \u00b1 0.015 (slightly lower)</p> </li> <li> <p>Third: 1D-CNN + max pooling</p> </li> <li> <p>Mean AUC: 0.849 \u00b1 0.010</p> </li> <li> <p>Aggregation strategy impact:</p> </li> <li>Max pooling: Consistently best across all classifiers</li> <li>PCA: Intermediate performance</li> <li> <p>Mean pooling: Worst performance across all classifiers (e.g., 1D-CNN dropped to AUC 0.638 \u00b1 0.010)</p> </li> <li> <p>Performance gain over PRS:</p> </li> <li>~49-60% improvement in AUC relative to traditional PRS (0.851 vs 0.53-0.57)</li> </ul> <p>2. DNABERT-2 Replication:</p> <ul> <li>Best performance: CatBoost + max pooling</li> <li>Mean AUC: 0.851 (identical to Caduceus)</li> <li> <p>Confirms framework generalizability across different FM architectures (Mamba vs Transformer)</p> </li> <li> <p>Aggregation differences:</p> </li> <li>Mean pooling performed comparably to max pooling with DNABERT-2 (unlike Caduceus)</li> <li>Suggests optimal aggregation may be FM-specific</li> </ul> <p>3. LOGO Analysis (Figure 3):</p> <ul> <li>SOD2 gene:</li> <li>Highest contribution: Median \u0394AUC = 0.190 (IQR: 0.185-0.196)</li> <li>Only statistically significant gene after FDR correction (Wilcoxon W=0, p_FDR=0.038)</li> <li> <p>Removing SOD2 drastically reduces classification performance despite no established GWAS hits in its exonic regions</p> </li> <li> <p>SOD2 biological relevance:</p> </li> <li>Encodes superoxide dismutase 2 (mitochondrial enzyme)</li> <li>Involved in oxidative stress regulation</li> <li> <p>Oxidative stress implicated in MDD pathophysiology</p> </li> <li> <p>Demographics ablation:</p> </li> <li>Removing demographic features: \u0394AUC = 0.040 (0.037-0.045), significant (p_FDR=0.038)</li> <li>Confirms demographics contribute meaningfully but less than top genes</li> </ul> <p>4. Cohort Demographics:</p> <ul> <li>Age: Cases younger than controls (55.4 vs 57.1 years, p&lt;0.001)</li> <li>Sex: Cases more female (64.1% vs 46.9%, p&lt;0.001)</li> <li>Income: Controls more likely in higher income brackets (p&lt;0.001)</li> <li>All differences significant \u2192 justifies including as covariates</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#8-strengths-limitations-and-open-questions","title":"8. Strengths, Limitations, and Open Questions","text":"<p>Strengths:</p> <ul> <li>Substantial performance improvement: 49-60% AUC gain over traditional PRS demonstrates value of sequence-level context</li> <li>Novel application domain: First study to apply DNA foundation model embeddings to psychiatric disorder classification</li> <li>Rigorous methodology:</li> <li>Nested cross-validation prevents overfitting</li> <li>Multiple FM architectures tested (Caduceus + DNABERT-2) \u2192 validates generalizability</li> <li>Statistical testing (Wilcoxon + FDR correction) for LOGO analysis</li> <li>Biological interpretability: LOGO analysis identifies SOD2 as key gene, linking to known oxidative stress mechanisms in MDD</li> <li>Foundation model comparison: Shows Mamba and Transformer architectures yield similar top performance, suggesting robustness</li> <li>Embedding aggregation insights: Max pooling consistently superior for Caduceus; strategy choice matters</li> </ul> <p>Limitations:</p> <ul> <li>Exon-only analysis:</li> <li>Focus on protein-coding regions despite most GWAS hits lying in non-coding regulatory elements</li> <li> <p>Intronic and intergenic regions (much larger proportion of genome) not included</p> </li> <li> <p>Gene-level interpretation only:</p> </li> <li>LOGO analysis identifies contributing genes but not specific exons or base-pair regions</li> <li> <p>Finer-grained attribution (e.g., attention maps, saliency) not explored</p> </li> <li> <p>Single-ancestry cohort:</p> </li> <li>UK Biobank participants primarily of European ancestry</li> <li> <p>Transferability to other populations uncertain</p> </li> <li> <p>No external validation:</p> </li> <li>Framework not tested on independent cohorts outside UK Biobank</li> <li> <p>Generalization across datasets unknown</p> </li> <li> <p>Demographic differences:</p> </li> <li> <p>Cases and controls differ significantly in age, sex, income \u2192 potential residual confounding even after covariate adjustment</p> </li> <li> <p>Limited gene set:</p> </li> <li>Only 38 pre-selected MDD-associated genes; genome-wide exome analysis could yield further insights</li> </ul> <p>Open Questions and Future Directions:</p> <ul> <li>Expand genomic coverage:</li> <li>Include intronic and intergenic regions to capture regulatory elements where many GWAS hits reside</li> <li> <p>Test framework on whole-genome sequencing (WGS) data</p> </li> <li> <p>Multimodal integration:</p> </li> <li>Combine DNA embeddings with transcriptomics (RNA-seq), epigenomics (methylation), proteomics</li> <li> <p>Integrate with brain imaging (fMRI, sMRI) for brain-genomics multimodal models</p> </li> <li> <p>Fine-grained interpretability:</p> </li> <li>Exon-level or base-pair-level attribution via attention weights, integrated gradients, or saliency maps</li> <li> <p>Link specific sequence motifs to MDD risk</p> </li> <li> <p>External validation:</p> </li> <li>Test on independent cohorts (e.g., other biobanks, clinical samples)</li> <li> <p>Evaluate cross-ancestry performance and fairness</p> </li> <li> <p>Generative and design applications:</p> </li> <li>Can FMs be used to design therapeutic sequences or identify novel targets?</li> <li> <p>Generate synthetic exon variants and predict MDD risk changes</p> </li> <li> <p>Integration with PRS:</p> </li> <li>Combine exon embeddings with genome-wide PRS to capture both local and global genetic risk</li> <li>Test whether the two signals are complementary or redundant</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#9-context-and-broader-impact","title":"9. Context and Broader Impact","text":"<p>Position in the genomics FM landscape:</p> <ul> <li>This work sits at the intersection of genomic foundation models and psychiatric genomics</li> <li>Genomics FMs (DNABERT, Caduceus, Nucleotide Transformer, Evo) have primarily been applied to:</li> <li>Regulatory element classification (promoters, enhancers)</li> <li>Variant effect prediction (pathogenicity)</li> <li>Gene expression forecasting</li> <li>Psychiatric genomics has relied on:</li> <li>GWAS to identify risk loci</li> <li>PRS for individual-level risk aggregation</li> <li>Traditional ML (SVM, random forests) with handcrafted genomic features</li> </ul> <p>Relation to well-known ideas:</p> <ul> <li>Analogy: \"Like using BERT embeddings for text classification, but for DNA sequences predicting psychiatric risk\"</li> <li>Transfer learning paradigm: Pretrain large models on diverse genomic data, then fine-tune (or use embeddings) for specific downstream tasks</li> <li>Contrastive to PRS: PRS are linear sums of SNP effects (shallow, interpretable); FM embeddings are high-dimensional, context-rich representations (deep, powerful but less transparent)</li> </ul> <p>Connection to the integration baseline plan:</p> <ul> <li>Genetics embedding hygiene:</li> <li>Paper explicitly cites Caduceus for RC-equivariance, aligning with the plan's recommendation for proper DNA handling</li> <li> <p>RC-averaging and deterministic tokenization ensure stable embeddings</p> </li> <li> <p>Late fusion baseline:</p> </li> <li> <p>Concatenating gene embeddings with demographics mirrors the plan's preference for late fusion under heterogeneous semantics</p> </li> <li> <p>Evaluation rigor:</p> </li> <li> <p>Nested CV, LOGO with Wilcoxon + FDR, proper train/test splits align with plan's robustness discipline</p> </li> <li> <p>Future escalation:</p> </li> <li>If exon embeddings prove valuable, next steps could include:<ul> <li>Two-tower contrastive learning (genetic encoder vs demographic/clinical encoder)</li> <li>Attention-based fusion across genes (hub tokens \u00e0 la TAPE)</li> <li>Integration with brain FMs (BrainLM, Brain-JEPA) for brain-genomics multimodal models</li> </ul> </li> </ul> <p>Why this paper is a useful reference:</p> <ul> <li>First application: Demonstrates feasibility and value of applying DNA FMs to psychiatric disorders</li> <li>Methodological template: Provides reusable pipeline for embedding extraction \u2192 aggregation \u2192 ML classification</li> <li>Performance benchmark: Establishes AUC 0.851 as a target for future MDD genetic prediction studies</li> <li>Gene discovery: LOGO analysis identifies SOD2 despite absence of exonic GWAS hits, suggesting FM embeddings capture signals missed by SNP-level approaches</li> <li>Foundation for multimodal work: Genetic embeddings from this study could serve as one modality in larger brain-genomics integration projects</li> </ul>"},{"location":"generated/kb_curated/papers-md/yoon_biokdd2025/#10-key-takeaways-bullet-summary","title":"10. Key Takeaways (Bullet Summary)","text":"<p>Problem:</p> <ul> <li>Traditional polygenic risk scores (PRS) for MDD explain only 2-3% of variance (AUC ~0.53-0.57), limiting predictive utility</li> <li>PRS rely on SNP-level GWAS associations and miss broader sequence context, regulatory motifs, and long-range dependencies</li> <li>DNA foundation models (Caduceus, DNABERT-2) trained on large genomic corpora remain unexplored in psychiatric genomics</li> </ul> <p>Method / Model:</p> <ul> <li>Framework: Extract exon sequence embeddings from 38 MDD-associated genes using pretrained DNA foundation models</li> <li>Primary FM: Caduceus-PS (Mamba-based, bi-directional, RC-equivariant, 256-d embeddings per gene)</li> <li>Aggregation: Test PCA, max pooling, and mean pooling to combine 38 gene embeddings</li> <li>Downstream: Train 15 ML pipelines (5 classifiers \u00d7 3 aggregation strategies) with nested cross-validation</li> <li>Data: UK Biobank WES data, 10,307 MDD cases + 10,307 controls, adjusted for demographics and genetic PCs</li> <li>Replication: Validate framework generalizability with DNABERT-2 embeddings</li> </ul> <p>Results:</p> <ul> <li>Best performance: CatBoost + max pooling achieves AUC 0.851 \u00b1 0.009, AUPRC 0.812 \u00b1 0.013</li> <li>Improvement over PRS: ~49-60% AUC gain (0.851 vs 0.53-0.57)</li> <li>Replication: DNABERT-2 achieves identical top AUC (0.851), confirming robustness across FM architectures</li> <li>Key gene: LOGO analysis identifies SOD2 as most significant contributor (\u0394AUC 0.190), linking to oxidative stress pathways</li> <li>Aggregation matters: Max pooling consistently best for Caduceus; mean pooling severely degrades performance</li> </ul> <p>Why it matters:</p> <ul> <li>First psychiatric genomics application: Demonstrates DNA foundation models can effectively predict complex psychiatric disorders</li> <li>Paradigm shift: Moves beyond SNP-level associations to sequence-level context, capturing signals missed by GWAS</li> <li>Substantial performance gain: Large improvement over traditional PRS suggests promise for clinical risk prediction</li> <li>Methodological contribution: Provides reusable framework for applying genomic FMs to any polygenic disorder</li> <li>Foundation for integration: Exon embeddings could be combined with brain imaging, transcriptomics, or epigenomics in future multimodal models</li> </ul>"},{"location":"generated/templates/analysis_recipe_cca/","title":"Analysis Recipe: CCA + Permutation","text":""},{"location":"generated/templates/analysis_recipe_cca/#goal","title":"Goal","text":"<p>Quantify cross-modal associations (e.g., gene embeddings vs sMRI features) while controlling confounds and validating significance via permutation testing.</p>"},{"location":"generated/templates/analysis_recipe_cca/#inputs","title":"Inputs","text":"<ul> <li>Residualized &amp; z-scored feature matrices <code>X</code> (modalities A) and <code>Y</code> (modalities B) per fold.</li> <li>Covariate design matrices (age, sex, site/scanner, motion FD, SES, genetic PCs).</li> <li>Train/test splits (identical across modalities).</li> </ul>"},{"location":"generated/templates/analysis_recipe_cca/#preprocessing-checklist","title":"Preprocessing Checklist","text":"<ol> <li>Fit scaler + residualization models on train split only; apply to both train/test within the fold.</li> <li>Optional PCA/MLP projector to 512-D per modality; store fit parameters.</li> <li>Log confound regression coefficients for reproducibility.</li> </ol>"},{"location":"generated/templates/analysis_recipe_cca/#procedure","title":"Procedure","text":"<ol> <li>Fit CCA on train data: <code>cca = CCA(n_components=k, scale=False)</code> with shrinkage/regularization if needed.</li> <li>Transform: Obtain canonical variates for both train and test sets.</li> <li>Record metrics: Canonical correlations (\u03c1\u2081\u2026\u03c1_k), variance explained, loadings.</li> <li>Permutation test: Shuffle subject order in modality B, refit CCA <code>B</code> times (\u22651,000); build null distribution for each \u03c1.</li> <li>p-values: <code>p = (count(\u03c1_null \u2265 \u03c1_obs) + 1) / (B + 1)</code>.</li> <li>Confidence intervals (optional): Bootstrap subjects within folds.</li> <li>Partial correlations to outcomes: Regress canonical scores and clinical targets on covariates; correlate residuals or use covariate-adjusted regression.</li> </ol>"},{"location":"generated/templates/analysis_recipe_cca/#logging","title":"Logging","text":"<ul> <li>Save canonical correlations, permutation distributions, p-values, and loadings to <code>artifacts/generated/cca/&lt;experiment_id&gt;/</code>.</li> <li>Store config (modalities, projectors, covariates, seeds) alongside results.</li> </ul>"},{"location":"generated/templates/analysis_recipe_cca/#reporting-template","title":"Reporting Template","text":"<ul> <li>Table of top 3 \u03c1 with permutation p-values and 95% CI.</li> <li>Heatmap or bar chart of feature loadings (with sparse thresholding if needed).</li> <li>Partial correlation table linking canonical scores to clinical outcomes (effect size, p, FDR q).</li> </ul>"},{"location":"generated/templates/analysis_recipe_cca/#references","title":"References","text":"<ul> <li>EI &amp; oncology multimodal review for integration motivation.</li> <li>Classical CCA references and permutation testing guidelines.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/","title":"Analysis Recipe: Prediction Baselines (LR + GBDT)","text":""},{"location":"generated/templates/analysis_recipe_prediction/#goal","title":"Goal","text":"<p>Estimate classification performance for each modality (genes, sMRI, fMRI) and their late-fusion concatenation using strong, interpretable baselines with rigorous evaluation.</p>"},{"location":"generated/templates/analysis_recipe_prediction/#inputs","title":"Inputs","text":"<ul> <li>Fold-specific train/test splits shared across modalities.</li> <li>Residualized, z-scored, 512-D embeddings per modality (plus covariates for logging).</li> <li>Binary or multi-class clinical targets (e.g., MDD, eoMDD vs loMDD).</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#models","title":"Models","text":""},{"location":"generated/templates/analysis_recipe_prediction/#logistic-regression","title":"Logistic Regression","text":"<ul> <li>Solver: <code>saga</code> or <code>lbfgs</code>.</li> <li>Penalty: L2; tune <code>C</code> in <code>{0.01, 0.1, 1, 10}</code>.</li> <li><code>class_weight='balanced'</code>.</li> <li>Max iter \u2265 5,000; tolerance <code>1e-4</code>.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#lightgbm","title":"LightGBM","text":"<ul> <li><code>num_leaves=31</code>, <code>max_depth=-1</code>, <code>learning_rate=0.05</code>.</li> <li><code>n_estimators=1000</code> with early stopping (patience 50) on validation splits.</li> <li><code>feature_fraction=0.9</code>, <code>bagging_fraction=0.8</code>, <code>bagging_freq=5</code>.</li> <li><code>scale_pos_weight = N_neg / N_pos</code> if not using class weights.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#catboost-optional","title":"CatBoost (optional)","text":"<ul> <li><code>depth=6-8</code>, <code>learning_rate=0.05</code>, <code>iterations=2000</code>, <code>l2_leaf_reg=3</code>.</li> <li><code>loss_function='Logloss'</code>, <code>eval_metric='AUC'</code>, <code>auto_class_weights='Balanced'</code>.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#procedure","title":"Procedure","text":"<ol> <li>Fit models on train split per modality; tune hyperparameters via inner CV or validation fold.</li> <li>Evaluate on held-out fold; store probabilities and logits.</li> <li>Repeat for concatenated late-fusion features (stacked 512-D per modality).</li> <li>Record metrics: AUROC, AUPRC, accuracy, calibration (Brier/ECE optional).</li> <li>Compare modalities: run DeLong test (or bootstrap) on AUROC differences; report mean \u00b1 SD across folds.</li> <li>For LOGO analyses, compute \u0394AUC per fold and apply Wilcoxon + FDR.</li> </ol>"},{"location":"generated/templates/analysis_recipe_prediction/#logging","title":"Logging","text":"<ul> <li>Persist per-fold predictions in <code>artifacts/generated/predictions/&lt;experiment_id&gt;/</code>.</li> <li>Store trained hyperparameters, random seeds, and config YAML snapshot.</li> <li>Capture ROC/PR curves and metric tables; include class prevalence.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#reporting-template","title":"Reporting Template","text":"<ul> <li>Table with AUROC/AUPRC (mean \u00b1 SD) for Gene, Brain, Fusion.</li> <li>DeLong/Bootstrap \u0394AUROC with 95% CI and p-value.</li> <li>Calibration plot or summary if clinically relevant.</li> <li>Notes on covariates, residualization, and projector settings.</li> </ul>"},{"location":"generated/templates/analysis_recipe_prediction/#references","title":"References","text":"<ul> <li>Ensemble Integration paper (late fusion motivation).</li> <li>Oncology multimodal review (evaluation discipline).</li> </ul>"},{"location":"generated/templates/dataset_card/","title":"Dataset card","text":"<p>md. I\u2019ll also keep carrying the specific repository + license notice on every walkthrough page so each doc makes its licensing context explicit.</p>"},{"location":"generated/templates/dataset_card/#dataset-card-template","title":"Dataset Card Template","text":""},{"location":"generated/templates/dataset_card/#metadata","title":"Metadata","text":"<ul> <li>Dataset name: <code>&lt;UKB/ABCD/etc&gt;</code></li> <li>Source / accession: <code>&lt;URL or data agreement&gt;</code></li> <li>License / DUA: <code>&lt;e.g., UKB Research Analysis Tool terms&gt;</code></li> <li>Primary contact: <code>&lt;PI / analyst&gt;</code></li> <li>Related YAML: <code>kb/datasets/&lt;id&gt;.yaml</code></li> </ul>"},{"location":"generated/templates/dataset_card/#access-storage","title":"Access &amp; Storage","text":"<ul> <li>Paths to raw data (e.g., <code>/secure/ukb/raw</code>), processed derivatives, and sync locations.</li> <li>Encryption / PHI considerations; who has access.</li> </ul>"},{"location":"generated/templates/dataset_card/#hugging-face-shared-mirrors","title":"Hugging Face / Shared Mirrors","text":"<ul> <li>Dataset repo + subset names, record counts, checksum or snapshot tags.</li> <li>Notes on sync cadence between mirrors and secured copies.</li> </ul>"},{"location":"generated/templates/dataset_card/#cohort-summary","title":"Cohort Summary","text":"Split N subjects Notes Train <code>&lt;N&gt;</code> <code>&lt;filters&gt;</code> Val <code>&lt;N&gt;</code> Test <code>&lt;N&gt;</code> <p>Add stratification notes (sex balance, age range, sites).</p>"},{"location":"generated/templates/dataset_card/#modality-specific-schema","title":"Modality-Specific Schema","text":"<ul> <li>Column lists per modality (genes, ROIs, clinical tables) with links back to <code>docs/data/schemas.md</code>.</li> <li>Required covariates + units.</li> </ul>"},{"location":"generated/templates/dataset_card/#overlap-pairing-logic","title":"Overlap &amp; Pairing Logic","text":"<ul> <li>How subjects overlap with other modalities (e.g., gene \u00d7 sMRI intersection).</li> <li>Inclusion/exclusion rules, QC flags (motion, missing genes, etc.).</li> </ul>"},{"location":"generated/templates/dataset_card/#genomics-base-pair-stats","title":"Genomics / Base-Pair Stats","text":"<ul> <li>Total base pairs, mean/median sequence length, tokenizer context alignment details.</li> </ul>"},{"location":"generated/templates/dataset_card/#preprocessing-pipelines","title":"Preprocessing Pipelines","text":"<ul> <li>Imaging: software versions, atlases, smoothing, censoring thresholds.</li> <li>Genetics: variant calling, phasing, annotation, gene list definition.</li> <li>Clinical: diagnosis coding, questionnaire scoring.</li> </ul>"},{"location":"generated/templates/dataset_card/#available-features-covariates","title":"Available Features &amp; Covariates","text":"<ul> <li>Feature tables (e.g., FreeSurfer ROIs, FC z-maps, gene embeddings).</li> <li>Covariates recommended for residualization (age, sex, site, motion, PCs, SES).</li> <li>Missingness patterns and imputation strategy if required.</li> </ul>"},{"location":"generated/templates/dataset_card/#qc-harmonization","title":"QC &amp; Harmonization","text":"<ul> <li>Thresholds (FD &lt; 0.3 mm, min TR length).</li> <li>Site/scanner harmonization (e.g., ComBat) or reasons for not applying.</li> <li>Logs / reports location.</li> </ul>"},{"location":"generated/templates/dataset_card/#notes-risks","title":"Notes &amp; Risks","text":"<ul> <li>Any data use restrictions (no redistribution, publication review).</li> <li>Confounds or biases to monitor (ancestry imbalance, site skew).</li> <li>Planned updates or reprocessing tasks.</li> </ul>"},{"location":"generated/templates/dataset_card/#references","title":"References","text":"<ul> <li>Link to cohort publications / documentation.</li> <li>Internal tickets or notebooks for preprocessing runs.</li> </ul>"},{"location":"generated/templates/dataset_card/#linked-walkthroughs-external-repos","title":"Linked Walkthroughs &amp; External Repos","text":"<ul> <li>Walkthrough pages that describe preprocessing.</li> <li><code>external_repos/*</code> scripts or configs that produced the released tensors (commit/tag + entrypoint).</li> </ul>"},{"location":"generated/templates/integration_strategy/","title":"Integration Strategy Template","text":""},{"location":"generated/templates/integration_strategy/#metadata","title":"Metadata","text":"<ul> <li>Doc owner: <code>&lt;name&gt;</code></li> <li>Last updated: <code>&lt;YYYY-MM-DD&gt;</code></li> <li>Related decisions: <code>docs/decisions/&lt;file&gt;.md</code></li> <li>Model cards referenced: <code>&lt;comma-separated&gt;</code></li> </ul>"},{"location":"generated/templates/integration_strategy/#fusion-taxonomy-guidance","title":"Fusion Taxonomy &amp; Guidance","text":"Level When to use Risks / Notes Early fusion Homogeneous modalities with aligned semantics Modality collapse, confound bleed-through Intermediate fusion Shared latent required (e.g., cross-attn, hub tokens) Heavy engineering, risk of overfitting Late fusion Heterogeneous semantics (DNA vs brain) Requires good per-modality baselines <p>Summarize takeaways from the oncology multimodal review + EI paper; note that default stance is late integration until baselines show reliable gains.</p>"},{"location":"generated/templates/integration_strategy/#baseline-pipeline","title":"Baseline Pipeline","text":"<ol> <li>Preprocess per modality: z-score within train folds, residualize age/sex/site/scanner/motion/SES/genetic PCs.</li> <li>Project to 512-D: PCA (preferred) or tiny MLP (<code>Linear 1024\u2192512 \u2192 GELU \u2192 Dropout 0.1 \u2192 Linear 512\u2192512 \u2192 LayerNorm</code>).</li> <li>Association analysis: CCA + 1,000 permutations; report top canonical correlations, permutation p-values, loadings.</li> <li>Prediction: Logistic Regression (<code>class_weight='balanced'</code>) and LightGBM/CatBoost per modality + concatenated fusion; same CV folds; report AUROC/AUPRC \u00b1 SD.</li> <li>Statistical tests: DeLong or bootstrap for AUROC differences; Wilcoxon + FDR for LOGO \u0394AUC.</li> </ol>"},{"location":"generated/templates/integration_strategy/#confound-controls","title":"Confound Controls","text":"<ul> <li>Demographics: age, sex.</li> <li>Imaging: site/scanner, motion FD, TR group, SES if available.</li> <li>Genetics: top PCs (\u226510) or ancestry group.</li> <li>Technical: sequencing batch, acquisition protocol indicators.</li> <li>Residualize within fold; log design matrices in <code>artifacts/generated/confounds/</code>.</li> </ul>"},{"location":"generated/templates/integration_strategy/#evaluation-plan","title":"Evaluation Plan","text":"<ul> <li>CV design: Stratified K-fold (k=5 or 10) with group/site-aware splits if leakage risk.</li> <li>Metrics: AUROC, AUPRC, calibration (Brier/ECE optional), canonical correlations.</li> <li>Significance: DeLong for AUROC, bootstrap for AUPRC, permutation for CCA, Benjamini\u2013Hochberg for multiple tests.</li> <li>Logging: Store fold predictions, ROC/PR curves, permutation distributions under <code>artifacts/generated/metrics/&lt;experiment_id&gt;/</code>.</li> </ul>"},{"location":"generated/templates/integration_strategy/#extension-roadmap","title":"Extension Roadmap","text":"<ol> <li>Two-tower contrastive alignment: Freeze encoders, train 512-D projectors with InfoNCE; assess retrieval R@k and downstream AUROC.</li> <li>Ensemble Integration (stacking / ensemble selection): Train heterogeneous base learners per modality, stack with logistic meta-learner; record EI interpretation ranks.</li> <li>Joint latent models: Brain Harmony hub tokens / TAPE or cross-attention fusion, only after late-fusion + EI baselines saturate.</li> <li>Deployment hygiene: Missing-modality handling, calibration transfer, privacy considerations.</li> </ol>"},{"location":"generated/templates/integration_strategy/#references","title":"References","text":"<ul> <li>Ensemble Integration (Li et al., Bioinformatics Advances 2022)</li> <li>Multimodal oncology review (Waqas et al., 2024)</li> <li>BrainLM / Brain-JEPA / Brain Harmony primary papers</li> <li>Internal decisions (<code>docs/decisions/2025-11-integration-direction.md</code>)</li> </ul>"},{"location":"generated/templates/modality_features_fmri/","title":"Modality Features: Functional MRI (rs-fMRI)","text":""},{"location":"generated/templates/modality_features_fmri/#source-inputs","title":"Source Inputs","text":"<ul> <li>Preprocessed rs-fMRI timeseries (e.g., fMRIPrep outputs, custom pipelines).</li> <li>Atlas definitions (Schaefer-400, Gordon, volumetric grid).</li> <li>Motion / QC metrics (FD, DVARS, censoring masks).</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#option-a-functional-connectivity-fast-baseline","title":"Option A: Functional Connectivity (Fast Baseline)","text":"<ol> <li>Parcellate time series per subject.</li> <li>Compute Pearson correlations per ROI pair; apply Fisher z-transform.</li> <li>Flatten upper triangle; optionally reduce via PCA (100\u2013256 dims) before projecting to 512-D.</li> <li>Residualize covariates (age, sex, site/scanner, motion FD, TR, SES) within folds.</li> <li>Store embeddings + QC flags in <code>artifacts/generated/embeddings/fmri_fc/</code>.</li> </ol>"},{"location":"generated/templates/modality_features_fmri/#option-b-foundation-model-embeddings","title":"Option B: Foundation Model Embeddings","text":"<ul> <li>Supported encoders: BrainLM, Brain-JEPA, Brain Harmony, SwiFT, BrainMT.</li> <li>Preprocessing needs: TR normalization (Harmony TAPE), mask collators, gradient positional encodings.</li> <li>Pooling: CLS token, mean over spatial tokens, hub tokens; document choice.</li> <li>512-D projector (PCA/MLP) fit per fold; log checkpoint versions.</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#motion-site-handling","title":"Motion &amp; Site Handling","text":"<ul> <li>Include FD (mean, max) and number of censored volumes as covariates.</li> <li>Consider site-aware splits or mixed-effects modeling if imbalance severe.</li> <li>For heterogeneous TRs: align via resampling or Harmony\u2019s TAPE (PI-resize) before embedding.</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#covariate-residualization","title":"Covariate Residualization","text":"<ul> <li>Age, sex, site/scanner, motion FD, TR group, SES, acquisition batch.</li> <li>Document design matrices and residualization scripts.</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#integration-notes","title":"Integration Notes","text":"<ul> <li>Align subject IDs with genomics/sMRI intersections; record dropouts.</li> <li>Provide config references (<code>configs/projectors/fmri_pca512.yaml</code>, <code>kb/datasets/&lt;cohort&gt;.yaml</code>).</li> <li>Track version of preprocessing (e.g., fMRIPrep 23.2) and smoothing parameters.</li> </ul>"},{"location":"generated/templates/modality_features_fmri/#references","title":"References","text":"<ul> <li>BrainLM, Brain-JEPA, Brain Harmony, SwiFT, BrainMT papers.</li> <li>Internal notebooks for FC pipeline and FM embedding extraction.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/","title":"Modality Features: Genomics","text":""},{"location":"generated/templates/modality_features_genomics/#source-inputs","title":"Source Inputs","text":"<ul> <li>Candidate gene/exon list: <code>&lt;link to table or YAML&gt;</code></li> <li>Sequence extraction pipeline: <code>&lt;script / notebook&gt;</code></li> <li>Reference genome build: <code>&lt;GRCh38/...&gt;</code></li> </ul>"},{"location":"generated/templates/modality_features_genomics/#tokenization-rc-hygiene","title":"Tokenization &amp; RC Hygiene","text":"<ul> <li>Specify tokenizer per model (character, k-mer length, BPE vocab).</li> <li>For RC-equivariant models (Caduceus, Evo/Evo2): compute forward + reverse-complement embeddings and average.</li> <li>For k-mer/BPE models (DNABERT-2, GENERator): RC raw sequence first, then re-tokenize to keep alignment; log tokenizer version and casing.</li> <li>Enforce consistent padding/truncation rules; document how Ns/ambiguous bases handled.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/#embedding-procedure","title":"Embedding Procedure","text":"<ol> <li>Load pretrained checkpoints (paths under <code>external_repos/&lt;repo&gt;/checkpoints</code>).</li> <li>Apply pooling (mean or CLS) per exon/gene; annotate what constitutes a subject-level aggregation (e.g., mean across exons).</li> <li>Normalize embeddings with fold-specific scaler.</li> <li>Project to 512-D via PCA or small MLP; store projector weights.</li> </ol>"},{"location":"generated/templates/modality_features_genomics/#covariates-residualization","title":"Covariates &amp; Residualization","text":"<ul> <li>Always residualize against age, sex, ancestry PCs (\u226510), sequencing batch, and other study-specific covariates.</li> <li>Document covariate sources and residualization scripts; output to <code>artifacts/generated/confounds/</code>.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/#logo-attribution","title":"LOGO / Attribution","text":"<ul> <li>Outline Leave-One-Gene-Out loop (nested CV, \u0394AUC, Wilcoxon signed-rank, Benjamini\u2013Hochberg FDR).</li> <li>Capture tables of \u0394AUC (mean \u00b1 SD) with p/q-values.</li> <li>Store plots showing top contributing genes.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>Provide projector config reference (e.g., <code>projectors/genomics_pca512.yaml</code>).</li> <li>Note how to combine with brain embeddings for late fusion (concatenate 512-D vectors).</li> <li>Mention any pathways/enrichment analyses tied to CCA or LOGO outputs.</li> </ul>"},{"location":"generated/templates/modality_features_genomics/#references","title":"References","text":"<ul> <li>Caduceus, DNABERT-2, Evo2, GENERator papers.</li> <li>Internal notebooks for sequence extraction, tokenizer QA, RC sanity checks.</li> </ul>"},{"location":"generated/templates/modality_features_smri/","title":"Modality Features: Structural MRI (sMRI)","text":""},{"location":"generated/templates/modality_features_smri/#source-inputs","title":"Source Inputs","text":"<ul> <li>FreeSurfer / CIVET outputs path.</li> <li>Atlas / parcellation used (e.g., Desikan, Schaefer-400 volumetric maps).</li> <li>QC reports (Euler number thresholds, manual edits).</li> </ul>"},{"location":"generated/templates/modality_features_smri/#feature-extraction","title":"Feature Extraction","text":"<ul> <li>ROI metrics: cortical thickness, volume, surface area, subcortical volumes.</li> <li>Derived composites (asymmetry indices, global measures) if needed.</li> <li>File formats (<code>.tsv</code>, <code>.csv</code>, HDF5) and loader scripts.</li> </ul>"},{"location":"generated/templates/modality_features_smri/#preprocessing-harmonization","title":"Preprocessing &amp; Harmonization","text":"<ul> <li>Z-score within train folds; residualize covariates (age, sex, ICV, site/scanner, SES).</li> <li>Optionally apply ComBat or mixed-effects models; document rationale.</li> <li>Handle missing ROIs (impute, drop subject, or add mask).</li> </ul>"},{"location":"generated/templates/modality_features_smri/#embedding-projection","title":"Embedding / Projection","text":"<ul> <li>Stack ROI vectors per subject; optionally reduce via PCA to 512-D (fit per train fold).</li> <li>Alternative: use pretrained encoders (Brain Harmony hub tokens) when available; document pooling strategy.</li> <li>Persist embeddings + scalers in <code>artifacts/generated/embeddings/smri/</code>.</li> </ul>"},{"location":"generated/templates/modality_features_smri/#covariates","title":"Covariates","text":"<ul> <li>Minimum set: age, sex, intracranial volume, site/scanner, scanner software version.</li> <li>Additional: handedness, SES, acquisition batch.</li> </ul>"},{"location":"generated/templates/modality_features_smri/#integration-notes","title":"Integration Notes","text":"<ul> <li>Use same subject IDs as genomics/fMRI intersections; log any exclusions.</li> <li>Provide YAML hooks for experiments: <code>kb/datasets/&lt;cohort&gt;.yaml</code>, <code>configs/projectors/smri_pca512.yaml</code>.</li> <li>Mention reliability metrics (test-retest ICC if available).</li> </ul>"},{"location":"generated/templates/modality_features_smri/#references","title":"References","text":"<ul> <li>Documentation for FreeSurfer pipeline version.</li> <li>Papers motivating ROI selection / harmonization steps.</li> </ul>"},{"location":"generated/templates/model_card_template/","title":"Model Card Template","text":"<p>Reminder: Include a license note such as \u201cThis walkthrough references <code>&lt;repo&gt;</code> under <code>&lt;license&gt;</code>,\u201d plus links to the repo, latest tag/commit, the associated <code>kb/model_cards/*.yaml</code>, and the generated PDF export (stored under <code>artifacts/pdf/code_walkthroughs/</code> or released assets).</p>"},{"location":"generated/templates/model_card_template/#metadata","title":"Metadata","text":"<ul> <li>Model name: <code>&lt;Friendly alias&gt;</code></li> <li>External repo: <code>&lt;URL&gt;</code></li> <li>Latest tag / commit: <code>&lt;tag-or-sha&gt;</code></li> <li>License: <code>&lt;e.g., Apache-2.0&gt;</code></li> <li>Model card YAML: <code>kb/model_cards/&lt;id&gt;.yaml</code></li> <li>Download PDF: <code>&lt;artifact link&gt;</code></li> </ul>"},{"location":"generated/templates/model_card_template/#purpose-scope","title":"Purpose &amp; Scope","text":"<ul> <li>What the model is designed for (e.g., DNA sequence embeddings, rs-fMRI latents, multimodal fusion).</li> <li>Intended tasks / datasets; out-of-scope uses.</li> </ul>"},{"location":"generated/templates/model_card_template/#architecture-inductive-biases","title":"Architecture &amp; Inductive Biases","text":"<ul> <li>Brief bullets on backbone, depth/width, notable blocks (e.g., JEPA, MAE, Swin, RC-equivariant BiMamba).</li> <li>Any modality-specific design (hub tokens, TAPE, gradient positional encodings).</li> </ul>"},{"location":"generated/templates/model_card_template/#tokenization-input-constraints","title":"Tokenization &amp; Input Constraints","text":"<ul> <li>Tokenizer type (character, k-mer, BPE, voxel patches, ROI tensors).</li> <li>Context length / TR windows / voxel grids.</li> <li>Required preprocessing (sorting genes, TR normalization, motion censoring).</li> </ul>"},{"location":"generated/templates/model_card_template/#pooling-subject-level-embeddings","title":"Pooling &amp; Subject-Level Embeddings","text":"<ul> <li>How to pool token embeddings (mean, CLS, hub-token average).</li> <li>RC handling (average forward/RC) or TR alignment notes.</li> <li>Aggregation to subject/session level (e.g., exon \u2192 gene \u2192 subject).</li> </ul>"},{"location":"generated/templates/model_card_template/#training-data-checkpoints","title":"Training Data &amp; Checkpoints","text":"<ul> <li>Source datasets, sample counts, preprocessing assumptions.</li> <li>Checkpoint paths / download links; expected placement under <code>external_repos/&lt;repo&gt;/checkpoints</code>.</li> </ul>"},{"location":"generated/templates/model_card_template/#recommended-embedding-procedure","title":"Recommended Embedding Procedure","text":"<ol> <li>Preprocess inputs (tokenization, z-score, residualization).</li> <li>Run encoder with key flags (e.g., gradient checkpointing, mask configs).</li> <li>Pool + project to 512-D (PCA or tiny MLP) with fold-aware fitting.</li> <li>Persist embeddings + covariates for downstream analyses.</li> </ol>"},{"location":"generated/templates/model_card_template/#integration-hooks","title":"Integration Hooks","text":"<ul> <li>512-D projector config (<code>Linear \u2192 GELU \u2192 Dropout \u2192 Linear \u2192 LayerNorm</code>).</li> <li>Late-fusion guidance (LR/GBDT baselines, CCA requirements).</li> <li>LOGO / attribution tips (for gene models).</li> </ul>"},{"location":"generated/templates/model_card_template/#strengths-limitations","title":"Strengths &amp; Limitations","text":"<ul> <li>Where the model excels (e.g., long-context DNA, heterogeneous TRs).</li> <li>Known failure modes (site sensitivity, large VRAM needs, tokenizer quirks).</li> </ul>"},{"location":"generated/templates/model_card_template/#references-links","title":"References &amp; Links","text":"<ul> <li>Paper citation(s) with DOI/arXiv.</li> <li>Repo link, docs, issue tracker.</li> <li>Related KB cards (dataset, integration strategy, analysis recipes).</li> </ul>"},{"location":"guide/kb_overview/","title":"\ud83d\udcd6 Navigating the Neuro-Omics KB","text":"<p>Your guide to the knowledge base structure, YAML cards, and integration-critical metadata</p> <p>This page orients new readers to the structure of the knowledge base, how the YAML cards feed into the rendered docs, and where to find integration-critical metadata for gene\u2013brain\u2013behaviour foundation models.</p>"},{"location":"guide/kb_overview/#scope-evolution","title":"\ud83c\udfaf Scope &amp; Evolution","text":"<p>The KB began with adult UK Biobank\u2013centric gene\u2013brain FM alignment (genetics FM + MRI Brain FM outputs) and has expanded to include:</p> <ul> <li>\u2705 15 Model Cards: 13 foundation models (4 genetics + 5 brain + 4 multimodal/clinical) + 2 ARPA-H planning cards</li> <li>\u2705 20 Research Papers: Curated summaries with structured takeaways</li> <li>\u2705 Integration Guidance: 6 YAML registries (embedding strategies, harmonization, preprocessing, alignment) + 3 narrative integration cards (ensemble integration, oncology review, multimodal FM patterns)</li> <li>\u2705 Developmental/Neurodevelopmental Support: Longitudinal multimodal FMs spanning MRI/fMRI, EEG/EPhys, genetics, behavioral phenotypes</li> <li>\u2705 ARPA-H BOM Alignment: Documentation for escalating from late fusion \u2192 contrastive \u2192 unified architectures</li> </ul>"},{"location":"guide/kb_overview/#architecture-at-a-glance","title":"\ud83c\udfd7\ufe0f Architecture at a Glance","text":"<pre><code>flowchart LR\n    A[External repos &amp; datasets&lt;br/&gt;UKB \u00b7 HCP \u00b7 Gene FMs \u00b7 Dev cohorts] --&gt; B[Metadata curation&lt;br/&gt;kb/model_cards \u00b7 kb/datasets]\n    B --&gt; C[Integration strategy &amp; modality specs&lt;br/&gt;docs/integration/*]\n    C --&gt; D[Experiment configs&lt;br/&gt;configs/experiments/*.yaml]\n    D --&gt; E[Analysis outputs &amp; decisions&lt;br/&gt;docs/decisions \u00b7 docs/generated/*]\n    B --&gt; F[Embedding &amp; harmonization registries&lt;br/&gt;kb/integration_cards/*.yaml]\n    F --&gt; G[Semantic / hub alignments&lt;br/&gt;LLM \u00b7 VLM]\n    G --&gt; D</code></pre> <ul> <li>Source repos / datasets live under <code>external_repos/</code> (git-ignored) and upstream storage (UKB, HCP, Cha Hospital developmental cohorts, and future ARPA-H Brain-Omics Model (BOM) data sources).</li> <li>Metadata cards (<code>kb/**</code>) are the single source of truth for model/dataset specs, including adult and developmental gene\u2013brain\u2013behaviour datasets.</li> <li>Docs pages (this site) summarize how to use those cards for integration.</li> <li>Strategy registries (embedding/harmonization/preprocessing) connect raw exports to subject-level vectors.</li> <li>Semantic alignment / hub registries describe how modalities (gene, brain, EEG, behaviour) are aligned to LLM and VLM spaces in ARPA-H\u2013style Brain-Omics Model (BOM) projects.</li> <li>Experiment configs log the exact recipe IDs and folds before any analysis job starts.</li> </ul>"},{"location":"guide/kb_overview/#core-navigation-map","title":"\ud83d\uddfa\ufe0f Core Navigation Map","text":"<pre><code>**Foundation Models**\n\n- **Models \u2192 Genetics**: Caduceus, DNABERT-2, Evo2, GENERator \u2014 DNA sequence models with RC-equivariance\n- **Models \u2192 Brain**: BrainLM, Brain-JEPA, Brain Harmony, BrainMT, SwiFT \u2014 sMRI/fMRI neuroimaging models\n- **Models \u2192 Multimodal &amp; Clinical**: BAGEL, MoT, M3FM, Me-LLaMA, TITAN, FMS-Medical \u2014 clinical/multimodal FMs\n\n**Integration &amp; Strategy**\n\n- **Decisions \u2192 Integration plan (Nov 2025)**: Why we start with late fusion + CCA before escalating to Brain-Omics Model (BOM)\n- **Integration \u2192 Strategy**: Shared preprocessing doctrine and escalation triggers\n- **Integration \u2192 Integration cards**: Ensemble integration, oncology review, multimodal FM patterns\n- **Integration \u2192 Design patterns**: Late fusion, two-tower, MoT, BOM \u2014 reusable integration motifs\n- **Integration \u2192 Multimodal architectures**: Detailed architecture docs for BAGEL, MoT, M3FM, Me-LLaMA, TITAN\n- **Integration \u2192 Embedding policies**: Naming conventions and PCA dimensionality guidelines\n\n**Analysis &amp; Workflows**\n\n- **Integration \u2192 Analysis recipes**: CCA+permutation, prediction baselines, partial correlations\n- **Integration \u2192 Modality features**: Genomics, sMRI, fMRI feature prep with embedding strategy IDs\n- **Code walkthroughs**: Step-by-step guides for all 15 FMs\n\n**Data &amp; Metadata**\n\n- **Data**: UKB data map, schemas, subject keys, governance/QC logs\n- **KB Templates**: Model cards, dataset cards, integration cards, experiment configs\n- **Research Papers**: 18 curated paper summaries (genetics, brain, multimodal, integration methods)\n</code></pre>"},{"location":"guide/kb_overview/#metadata-you-must-log-per-run","title":"Metadata you must log per run","text":"<p>Use the CLI helpers to print the canonical recipes and copy their IDs into your run metadata:</p> <pre><code># Embedding (e.g., FreeSurfer PCA-512)\npython scripts/manage_kb.py ops strategy smri_free_surfer_pca512_v1\n\n# Harmonization (e.g., MURD for T1/T2)\npython scripts/manage_kb.py ops harmonization murd_t1_t2\n\n# Preprocessing stack (e.g., HCP-like rs-fMRI)\npython scripts/manage_kb.py ops strategy rsfmri_swift_segments_v1\n</code></pre> <p>Record:</p> <ol> <li><code>embedding_strategies.&lt;id&gt;</code> for every modality in the experiment.</li> <li><code>harmonization_methods.&lt;id&gt;</code> (even if it is <code>none_baseline</code>).</li> <li><code>rsfmri_preprocessing_pipelines.&lt;id&gt;</code> whenever an rs-fMRI FM is involved.</li> <li>CV scheme (<code>StratifiedGroupKFold</code>, seed, groups) and manifest used.</li> </ol>"},{"location":"guide/kb_overview/#how-content-stays-consistent","title":"How content stays consistent","text":"<ul> <li>Template-first editing. Every new card starts from the templates in <code>kb/templates/</code>.</li> <li>Docs \u2194 YAML parity. If a doc cites a field (e.g., embedding recipe level), the corresponding YAML must include it.</li> <li>Strict builds. <code>mkdocs build --strict</code> guards against broken navigation before publishing.</li> </ul>"},{"location":"guide/kb_overview/#suggested-reading-order","title":"Suggested reading order","text":""},{"location":"guide/kb_overview/#for-researchers-new-to-foundation-models","title":"For Researchers New to Foundation Models","text":"<ol> <li>KB overview (this page) \u2014 Understand the structure</li> <li>Models \u2192 Overview pages \u2014 Browse Genetics, Brain, or Multimodal overviews</li> <li>Integration \u2192 Strategy \u2014 Learn the late-fusion-first philosophy</li> <li>Code walkthroughs \u2014 Pick a model relevant to your modality</li> <li>Analysis recipes \u2014 Copy-ready runbooks for CCA, prediction, etc.</li> </ol>"},{"location":"guide/kb_overview/#for-integration-multimodal-work","title":"For Integration &amp; Multimodal Work","text":"<ol> <li>Integration \u2192 Design patterns \u2014 Late fusion \u2192 contrastive \u2192 unified architectures</li> <li>Integration \u2192 Integration cards \u2014 Study ensemble integration, oncology review, multimodal FM patterns</li> <li>Integration \u2192 Multimodal architectures \u2014 Deep dive into BAGEL, MoT, M3FM, Me-LLaMA, TITAN</li> <li>Decisions \u2192 Integration plan (Nov 2025) \u2014 See the escalation roadmap</li> <li>Integration \u2192 Embedding policies \u2014 Learn naming conventions for strategies</li> </ol>"},{"location":"guide/kb_overview/#for-adding-new-models-or-cards","title":"For Adding New Models or Cards","text":"<ol> <li>KB Templates \u2014 Start from templates in <code>kb/templates/</code> or <code>generated/templates/</code></li> <li>Existing model cards \u2014 Review <code>kb/model_cards/*.yaml</code> for patterns</li> <li>Docs \u2194 YAML parity \u2014 Ensure rendered docs cite fields that exist in YAML</li> <li>Validation \u2014 Run <code>python scripts/manage_kb.py validate models</code> before committing</li> </ol> <p>Use this map whenever you add new cards or plan analyses\u2014it keeps the documentation, YAML metadata, and experiment configs aligned.</p>"},{"location":"integration/","title":"Integration Hub","text":"<p>Everything in this section supports the phased escalation strategy documented in the Integration Plan (Nov 2025). Use it as the connective tissue between per-modality preprocessing, harmonization, and experiment execution.</p>"},{"location":"integration/#overview","title":"Overview","text":"<p>This hub provides end-to-end guidance for integrating genetics, brain, and behavioral data using foundation models.</p>"},{"location":"integration/#key-resources","title":"\ud83d\udccb Key Resources","text":"<ul> <li>Integration Strategy \u2014 High-level playbook: covariates to regress, projection dims, escalation triggers</li> <li>Design Patterns \u2014 Late fusion \u2192 two-tower \u2192 MoT \u2192 BOM escalation logic</li> <li>Multimodal Architectures \u2014 Detailed patterns from BAGEL, MoT, M3FM, Me-LLaMA, TITAN</li> <li>Embedding Policies \u2014 Naming conventions and PCA dimensionality guidelines</li> <li>Benchmarks \u2014 Prior benchmark targets to compare against</li> </ul>"},{"location":"integration/#analysis-recipes","title":"\ud83d\udd2c Analysis Recipes","text":"<p>Copy-ready runbooks for common integration tasks:</p> <ul> <li>CCA + Permutation \u2014 Test gene-brain associations before heavy fusion</li> <li>Prediction Baselines \u2014 Gene-only vs Brain-only vs Late fusion</li> <li>Partial Correlations \u2014 Control for covariates with logistic regression</li> </ul>"},{"location":"integration/#modality-features","title":"\ud83e\uddec\ud83e\udde0 Modality Features","text":"<p>Concrete instructions for extracting and harmonizing features:</p> <ul> <li>Genomics \u2014 Genetics embeddings, RC-equivariance, gene attribution</li> <li>sMRI \u2014 FreeSurfer ROIs, PCA compression, site harmonization</li> <li>fMRI \u2014 Functional connectivity, BrainLM/SwiFT embeddings, preprocessing</li> </ul>"},{"location":"integration/#integration-cards","title":"\ud83c\udfa8 Integration Cards","text":"<p>Comprehensive multimodal fusion guidance:</p> <ul> <li>Ensemble Integration \u2014 Model stacking, averaging, meta-learning</li> <li>Oncology Multimodal Review \u2014 Early/intermediate/late fusion taxonomy</li> <li>Multimodal FM Patterns \u2014 Architectural patterns from state-of-the-art FMs</li> </ul>"},{"location":"integration/#quick-start","title":"Quick Start","text":"<p>Before running any analysis, grab the relevant strategy IDs and log them with your experiment configs:</p> <pre><code># Show sMRI baseline recipe\npython scripts/manage_kb.py ops strategy smri_free_surfer_pca512_v1\n\n# Inspect harmonization metadata (e.g., MURD)\npython scripts/manage_kb.py ops harmonization murd_t1_t2\n\n# Show rs-fMRI preprocessing stack\npython scripts/manage_kb.py ops strategy rsfmri_swift_segments_v1\n</code></pre> <p>This keeps downstream reports auditable even when raw datasets (e.g., UKB) cannot be shared.</p>"},{"location":"integration/#integration-phases","title":"Integration Phases","text":"<p>We follow a phased escalation strategy to avoid premature complexity:</p> Phase Status Pattern Trigger Documentation Phase 1 \u2705 Active Late Fusion Baseline Integration Plan Phase 2 \ud83d\udea7 Prep Two-Tower Contrastive CCA p&lt;0.001, \u0394AUROC&gt;5% Integration Plan Phase 3 \u23f3 Future Unified Multimodal (MoT/BAGEL/LLM-Bridge) \u0394AUROC&gt;10%, cross-modal reasoning Integration Plan"},{"location":"integration/#navigation-guide","title":"Navigation Guide","text":""},{"location":"integration/#for-late-fusion-workflows-phase-1","title":"For Late Fusion Workflows (Phase 1)","text":"<ol> <li>Read Integration Strategy</li> <li>Pick analysis recipe: CCA, Prediction, or Partial Correlations</li> <li>Extract features: Genomics, sMRI, fMRI</li> <li>Review Ensemble Integration card for stacking strategies</li> <li>Run analysis with logged strategy IDs</li> </ol>"},{"location":"integration/#for-multimodal-architecture-design-phase-2","title":"For Multimodal Architecture Design (Phase 2+)","text":"<ol> <li>Read Design Patterns for escalation logic</li> <li>Study Multimodal Architectures for BAGEL/MoT/M3FM/Me-LLaMA/TITAN patterns</li> <li>Review Multimodal FM Patterns integration card</li> <li>Consult Oncology Multimodal Review for fusion taxonomy</li> <li>Check Integration Plan decision table for recommended pattern</li> </ol>"},{"location":"integration/#for-adding-new-integration-strategies","title":"For Adding New Integration Strategies","text":"<ol> <li>Start from Integration card template</li> <li>Review existing cards for structure and style</li> <li>Document mechanics, use cases, caveats, and BOM integration</li> <li>Add to <code>models/integrations/</code> directory</li> <li>Update <code>mkdocs.yml</code> navigation</li> </ol>"},{"location":"integration/#key-principles","title":"Key Principles","text":"<p>\u2705 Late fusion first \u2014 Preserve modality-specific signal under heterogeneous semantics \u2705 Unimodal baselines \u2014 Establish gene-only and brain-only performance before multimodal claims \u2705 Covariate control \u2014 Z-score + residualize vs age/sex/site before interpreting effects \u2705 Reproducibility \u2014 Log embedding strategy IDs, harmonization methods, CV folds \u2705 Phased escalation \u2014 Only escalate when data and compute justify the complexity  </p> <p>Read full integration plan \u2192</p>"},{"location":"integration/benchmarks/","title":"Integration benchmarks","text":""},{"location":"integration/benchmarks/#datasets","title":"Datasets","text":"<ul> <li>UK Biobank</li> <li>Targets:</li> <li>Primary: MDD diagnosis (binary; Howard et al. GWAS), PHQ-9 depression severity (continuous)</li> <li>Secondary cognitive: Fluid intelligence, reaction time, cognitive composite</li> <li>Stratifications: Early-onset MDD (age &lt; 21), late-onset MDD (age \u2265 21)</li> </ul>"},{"location":"integration/benchmarks/#splits","title":"Splits","text":"<ul> <li>GroupKFold by site</li> <li>Deterministic seeds with versioned <code>splits.json</code></li> </ul>"},{"location":"integration/benchmarks/#metrics-reporting","title":"Metrics &amp; reporting","text":"<ul> <li>AUC, PR-AUC, Brier score</li> <li>Calibration curves + subgroup reporting with confidence intervals</li> </ul>"},{"location":"integration/design_patterns/","title":"Integration Design Patterns","text":"<p>Default strategy: Late fusion via separate gene/brain heads (see Integration Strategy). The patterns below document escalation paths for when baseline fusion demonstrates significant gains, with risks and implementation guidance noted.</p>"},{"location":"integration/design_patterns/#overview","title":"Overview","text":"<p>This document catalogs integration architectures from simplest (late fusion) to most complex (unified multimodal transformers). Each pattern includes: - Description: How the pattern works - Use cases: When to apply it - Risks: What can go wrong - Examples: Reference models demonstrating the pattern - Escalation criteria: When to move to this pattern from simpler baselines</p>"},{"location":"integration/design_patterns/#pattern-1-late-fusion-baseline","title":"Pattern 1: Late Fusion (Baseline)","text":""},{"location":"integration/design_patterns/#description","title":"Description","text":"<p>Train separate encoders for each modality, extract fixed-size embeddings, concatenate, and pass to a simple classifier (LR, GBDT). No parameter sharing between modalities during training.</p>"},{"location":"integration/design_patterns/#architecture","title":"Architecture","text":"<pre><code>Genetics FM \u2192 gene_embed [512-D]  \u2500\u2500\u2510\n                                     \u251c\u2500\u2192 concat [1024-D] \u2192 LR/GBDT \u2192 prediction\nBrain FM    \u2192 brain_embed [512-D] \u2500\u2500\u2518\n</code></pre>"},{"location":"integration/design_patterns/#use-cases","title":"Use Cases","text":"<ul> <li>Initial baselines: Establish single-modality vs. fusion performance</li> <li>Heterogeneous modalities: Genetics (sequence) vs. brain (spatiotemporal) have different semantics</li> <li>Small sample sizes: Fewer parameters to tune than joint training</li> </ul>"},{"location":"integration/design_patterns/#risks","title":"Risks","text":"<ul> <li>Suboptimal if modalities have strong cross-modal dependencies</li> <li>No learned alignment between modality spaces</li> </ul>"},{"location":"integration/design_patterns/#implementation-current","title":"Implementation (Current)","text":"<ul> <li>Genetics: Caduceus/DNABERT-2 \u2192 gene-level embeddings \u2192 PCA-512</li> <li>Brain: BrainLM/Brain-JEPA \u2192 subject embeddings \u2192 PCA-512</li> <li>Fusion: <code>np.concatenate([gene_embed, brain_embed], axis=-1)</code></li> <li>Classifier: LogisticRegression (balanced, C=0.01) or LightGBM</li> </ul>"},{"location":"integration/design_patterns/#escalation-criteria","title":"Escalation Criteria","text":"<p>\u2705 Move to Pattern 2 when: Fusion significantly outperforms max(Gene, Brain) with p &lt; 0.05 (DeLong test)</p>"},{"location":"integration/design_patterns/#pattern-2-two-tower-contrastive-alignment","title":"Pattern 2: Two-Tower Contrastive Alignment","text":""},{"location":"integration/design_patterns/#description_1","title":"Description","text":"<p>Freeze pretrained modality encoders, add small learnable projectors, align via contrastive loss (InfoNCE). Creates shared embedding space without modifying foundation models.</p>"},{"location":"integration/design_patterns/#architecture_1","title":"Architecture","text":"<pre><code>Genetics FM (frozen) \u2192 gene_embed \u2192 projector_gene [256-D]  \u2500\u2500\u2510\n                                                               \u251c\u2500\u2192 InfoNCE loss\nBrain FM (frozen)    \u2192 brain_embed \u2192 projector_brain [256-D] \u2500\u2500\u2518\n                                           \u2193\n                               aligned_space [256-D] \u2192 classifier\n</code></pre>"},{"location":"integration/design_patterns/#use-cases_1","title":"Use Cases","text":"<ul> <li>Cross-modal retrieval: Find genes associated with brain patterns</li> <li>Zero-shot transfer: Align modalities on one cohort, transfer to another</li> <li>Foundation model preservation: Keep pretrained weights frozen</li> </ul>"},{"location":"integration/design_patterns/#risks_1","title":"Risks","text":"<ul> <li>Requires paired gene-brain samples (not all subjects have both modalities)</li> <li>Contrastive loss sensitive to negative sampling strategy</li> <li>May not capture complex non-linear interactions</li> </ul>"},{"location":"integration/design_patterns/#examples","title":"Examples","text":"<ul> <li>CLIP: Image-text contrastive alignment (OpenAI)</li> <li>M3FM: Medical image-text two-tower fusion (walkthrough)</li> </ul>"},{"location":"integration/design_patterns/#implementation-strategy","title":"Implementation Strategy","text":"<ol> <li>Freeze Caduceus and BrainLM checkpoints</li> <li>Add 512\u2192256 projectors (2-layer MLP with ReLU)</li> <li>Sample positive pairs: (gene_i, brain_i) from same subject</li> <li>Sample negatives: (gene_i, brain_j\u2260i) within batch</li> <li>Optimize InfoNCE loss on train split</li> <li>Extract aligned embeddings \u2192 downstream classifier</li> </ol>"},{"location":"integration/design_patterns/#escalation-criteria_1","title":"Escalation Criteria","text":"<p>\u2705 Move to Pattern 3 when: Cross-modal retrieval tasks emerge or need end-to-end joint training</p>"},{"location":"integration/design_patterns/#pattern-3-early-fusion-with-shared-encoder","title":"Pattern 3: Early Fusion with Shared Encoder","text":""},{"location":"integration/design_patterns/#description_2","title":"Description","text":"<p>Concatenate or interleave modality embeddings early, process through shared transformer layers. Enables cross-modal attention but requires careful preprocessing alignment.</p>"},{"location":"integration/design_patterns/#architecture_2","title":"Architecture","text":"<pre><code>gene_tokens [N_genes, D]  \u2500\u2500\u2510\n                            \u251c\u2500\u2192 concat \u2192 Shared Transformer \u2192 pooled_embed \u2192 classifier\nbrain_tokens [N_parcels, D] \u2500\u2518\n</code></pre>"},{"location":"integration/design_patterns/#use-cases_2","title":"Use Cases","text":"<ul> <li>Complex interactions: When modalities have intricate dependencies (e.g., gene regulatory networks affecting brain circuits)</li> <li>Multi-task learning: Share encoder across prediction tasks (MDD, cognitive scores)</li> <li>End-to-end optimization: Allow gradients to flow through all layers</li> </ul>"},{"location":"integration/design_patterns/#risks_2","title":"Risks","text":"<ul> <li>Modality imbalance: Dominant modality can suppress weaker one</li> <li>Overfitting: More parameters, higher risk with small N</li> <li>Preprocessing coupling: Requires consistent tokenization/normalization across modalities</li> </ul>"},{"location":"integration/design_patterns/#examples_1","title":"Examples","text":"<ul> <li>BAGEL: Unified decoder over text+image+video tokens (paper card)</li> <li>Brain Harmony: Joint sMRI+fMRI processing with hub tokens (model card)</li> </ul>"},{"location":"integration/design_patterns/#implementation-strategy_1","title":"Implementation Strategy","text":"<ol> <li>Tokenize both modalities to fixed dimensions</li> <li>Add modality-specific positional encodings</li> <li>Concatenate token sequences: <code>[gene_tok_1, ..., gene_tok_N, brain_tok_1, ..., brain_tok_M]</code></li> <li>Process through transformer layers with cross-modal attention</li> <li>Pool final layer \u2192 classification head</li> </ol>"},{"location":"integration/design_patterns/#escalation-criteria_2","title":"Escalation Criteria","text":"<p>\u2705 Move to Pattern 4 when: Need modality-specific parameter sets (avoid modality collapse)</p>"},{"location":"integration/design_patterns/#pattern-4-mixture-of-transformers-mot-sparse-fusion","title":"Pattern 4: Mixture-of-Transformers (MoT) Sparse Fusion","text":""},{"location":"integration/design_patterns/#description_3","title":"Description","text":"<p>Shared self-attention over all modality tokens, but separate FFNs and layer norms per modality. Balances cross-modal attention with modality-specific processing.</p>"},{"location":"integration/design_patterns/#architecture_3","title":"Architecture","text":"<pre><code>gene_tokens + brain_tokens + behavior_tokens\n    \u2193\nShared Self-Attention (all tokens interact)\n    \u2193\nModality-specific FFN branches:\n  \u251c\u2500 genetics_ffn\n  \u251c\u2500 brain_ffn\n  \u2514\u2500 behavior_ffn\n    \u2193\nPooled embedding \u2192 task heads\n</code></pre>"},{"location":"integration/design_patterns/#use-cases_3","title":"Use Cases","text":"<ul> <li>Compute efficiency: ~55% FLOPs vs. full dense multimodal transformer</li> <li>Modality preservation: Each modality retains specialized processing</li> <li>Scalable fusion: Handle 3+ modalities without parameter explosion</li> </ul>"},{"location":"integration/design_patterns/#risks_3","title":"Risks","text":"<ul> <li>More complex architecture than dense baseline</li> <li>Requires careful initialization of per-modality parameters</li> <li>May underperform dense if modalities highly correlated</li> </ul>"},{"location":"integration/design_patterns/#examples_2","title":"Examples","text":"<ul> <li>MoT paper: Sparse multimodal transformer (arXiv:2411.04996, card)</li> </ul>"},{"location":"integration/design_patterns/#implementation-strategy_2","title":"Implementation Strategy","text":"<ol> <li>Initialize shared attention layers (all modalities)</li> <li>Create separate FFN/norm modules per modality (genetics_ffn, brain_ffn, behavior_ffn)</li> <li>Forward pass: attention(all_tokens) \u2192 route_to_modality_ffn(token) \u2192 merge</li> <li>Train end-to-end with task-specific heads</li> </ol>"},{"location":"integration/design_patterns/#escalation-criteria_3","title":"Escalation Criteria","text":"<p>\u2705 Move to Pattern 5 when: Need generative capabilities (e.g., clinical report generation from gene-brain data)</p>"},{"location":"integration/design_patterns/#pattern-5-unified-brain-omics-model-bom","title":"Pattern 5: Unified Brain-Omics Model (BOM)","text":""},{"location":"integration/design_patterns/#description_4","title":"Description","text":"<p>Single decoder-only transformer with Mixture-of-Experts (MoE) processing all modalities as token sequences: genetics (nucleotide tokens), brain (parcel/voxel tokens), behavior (structured tokens), language (text tokens). Inspired by BAGEL/GPT-4o-style unified multimodal architectures.</p>"},{"location":"integration/design_patterns/#architecture_4","title":"Architecture","text":"<pre><code>Tokenize all modalities:\n  - Genetics: nucleotide sequences \u2192 tokens\n  - Brain MRI: 3D patches \u2192 tokens (ViT-style)\n  - fMRI: parcel time series \u2192 tokens\n  - EEG: channel \u00d7 time \u2192 tokens\n  - Behavior: structured data \u2192 embedding tokens\n  - Language: text \u2192 BPE tokens\n\n  \u2193\nUnified Decoder-Only Transformer (e.g., LLaMA-style)\n  - Mixture-of-Experts (understanding vs. generation)\n  - Cross-modal self-attention\n  - Next-token prediction objective\n\n  \u2193\nDownstream tasks:\n  - Gene-brain association discovery\n  - Clinical report generation\n  - Counterfactual reasoning (\"what if gene X was mutated?\")\n  - Cognitive decline prediction\n</code></pre>"},{"location":"integration/design_patterns/#use-cases_4","title":"Use Cases","text":"<ul> <li>ARPA-H Brain-Omics Model (BOM): Unified foundation model for neuro-omics</li> <li>LLM as semantic bridge: Language model embeddings as \"lingua franca\" for cross-modal reasoning</li> <li>Generative tasks: Report generation, sequence design, counterfactual prediction</li> <li>Unified pretraining: Single model handles all neuro-omics modalities</li> </ul>"},{"location":"integration/design_patterns/#risks_4","title":"Risks","text":"<ul> <li>Massive compute: Requires trillions of tokens, large-scale infrastructure</li> <li>Data curation: Need high-quality interleaved multimodal corpus</li> <li>Complexity: Hardest to debug, longest training time</li> <li>Evaluation: Requires diverse benchmarks across modalities</li> </ul>"},{"location":"integration/design_patterns/#examples_3","title":"Examples","text":"<ul> <li>BAGEL: Unified text+image+video+web model (walkthrough)</li> <li>Flamingo: Few-shot visual language model with Perceiver + gated cross-attention (paper)</li> <li>GPT-4o: Unified multimodal reasoning (proprietary; included here as a conceptual example, not a KB-tracked model)</li> <li>Chameleon: Text-image unified autoregressive model (dense unified baseline described in the MoT paper/card; used here as a reference pattern)^[See MoT summary: <code>docs/generated/kb_curated/papers-md/mot_2025.md</code>]</li> </ul>"},{"location":"integration/design_patterns/#implementation-strategy-long-term-vision","title":"Implementation Strategy (Long-term Vision)","text":"<p>Phase 1: Corpus Curation - Collect interleaved multimodal neuro-omics data:   - Genetic variants + brain scans + cognitive assessments + clinical notes   - Longitudinal trajectories (developmental, disease progression)   - Multimodal annotations (gene function descriptions, brain region labels, symptom text)</p> <p>Phase 2: Tokenization - Genetics: Nucleotide sequences (A/C/G/T) or k-mer tokens - Brain MRI: 3D patch tokens (ViT-style, 16\u00b3 patches) - fMRI: Parcel time series \u2192 temporal tokens - EEG: Channel-time matrices \u2192 spectral-spatial tokens - Behavior: Structured scores \u2192 learned embeddings - Language: Standard BPE/SentencePiece tokens</p> <p>Phase 3: Architecture - Decoder-only transformer (LLaMA/Qwen base) - Mixture-of-Experts: Understanding vs. generation experts - Modality-specific input embedders, shared transformer backbone - Task-specific heads (classification, generation, retrieval)</p> <p>Phase 4: Training - Next-token prediction across all modalities - Interleaved sequence objective (language \u2192 genetics \u2192 brain \u2192 language) - Multitask loss: prediction + generation + contrastive</p> <p>Phase 5: Evaluation - Gene-brain association discovery (AUC, correlation) - Clinical report generation (BLEU, METEOR, clinician ratings) - Cognitive prediction (AUROC on MDD, fluid intelligence) - Cross-modal retrieval (gene\u2192brain, brain\u2192phenotype) - Counterfactual reasoning (GPT-4-judge evaluation)</p>"},{"location":"integration/design_patterns/#escalation-criteria_4","title":"Escalation Criteria","text":"<p>\u2705 Implement BOM when: Phases 1-4 patterns exhausted, significant funding secured, compute infrastructure available</p>"},{"location":"integration/design_patterns/#arpa-h-integration-roadmap","title":"ARPA-H Integration Roadmap","text":""},{"location":"integration/design_patterns/#timeline","title":"Timeline","text":"Phase Pattern Status Target Completion Phase 1 Late fusion baselines In progress Nov 2025 Phase 2 Two-tower contrastive Pending Q1 2026 Phase 3 Early fusion / MoT Pending Q2 2026 Phase 4 Unified BOM (pilot) Planned Q3-Q4 2026 Phase 5 Scaled BOM deployment Vision 2027+"},{"location":"integration/design_patterns/#current-focus-nov-2025","title":"Current Focus (Nov 2025)","text":"<p>Active: - \u2705 Late fusion: Gene (Caduceus) + Brain (BrainLM) \u2192 LR/GBDT - \u2705 CCA + permutation: Assess cross-modal correlation structure - \u2705 LOGO attribution: Gene-level importance (Yoon et al. protocol)</p> <p>Next steps: 1. Complete late fusion baselines on UKB gene-brain data 2. Pilot two-tower contrastive alignment (frozen encoders) 3. Design interleaved corpus for Phase 3+ experiments</p>"},{"location":"integration/design_patterns/#reference-materials","title":"Reference Materials","text":"<p>Multimodal architecture examples: - Multimodal Architectures Overview \u2014 Detailed patterns from BAGEL, MoT, M3FM, Me-LLaMA, TITAN</p> <p>Integration strategies: - Integration Strategy \u2014 Preprocessing, harmonization, escalation criteria - CCA + Permutation Recipe \u2014 Statistical testing before fusion - Prediction Baselines \u2014 Late fusion implementation</p> <p>Model documentation: - Brain Models Overview - Genetics Models Overview</p> <p>Decision logs: - Integration Baseline Plan (Nov 2025) \u2014 Why late fusion first</p>"},{"location":"integration/embedding_policies/","title":"Embedding policies","text":""},{"location":"integration/embedding_policies/#embedding-naming-and-pca-policies","title":"Embedding naming and PCA policies","text":"<p>This page documents how we name embedding strategies in <code>kb/integration_cards/embedding_strategies.yaml</code> and how we choose PCA dimensionality before locking in a strategy ID.</p>"},{"location":"integration/embedding_policies/#naming-conventions","title":"Naming conventions","text":"<ul> <li>UKB sMRI PCA embeddings</li> <li><code>smri_ukb_pca32_v1</code>, <code>smri_ukb_pca64_v1</code>, <code>smri_ukb_pca128_v1</code>, <code>smri_ukb_pca256_v1</code></li> <li> <p>All refer to PCA-compressed FreeSurfer ROI features from <code>ukb_smri_freesurfer.yaml</code>, differing only in <code>dim</code>.</p> </li> <li> <p>CHA developmental sMRI PCA embeddings</p> </li> <li><code>smri_cha_dev_pca64_v1</code>, <code>smri_cha_dev_pca128_v1</code></li> <li> <p>Both wrap the same preprocessing pipeline (age/sex adjustment, pediatric QC) on <code>cha_dev_longitudinal.yaml</code>,     differing only in PCA <code>dim</code>.</p> </li> <li> <p>Genetics / Joo embeddings</p> </li> <li><code>genetics_gene_fm_pca512_v1</code>: generic gene-FM PCA embedding for adult UKB.</li> <li><code>genetics_joo_mdd_cog_v1</code>: Prof. Joo MDD + cognition gene embeddings (Yoon BIOKDD'25-style); dimension and FM     backbone to be filled once confirmed.</li> </ul> <p>In general:</p> <ul> <li>Start IDs with the modality (<code>smri_</code>, <code>rsfmri_</code>, <code>eeg_</code>, <code>genetics_</code>, <code>behaviour_</code>).</li> <li>Follow with the cohort or context (<code>ukb</code>, <code>cha_dev</code>, etc.).</li> <li>Then append the method and key hyperparameters (<code>pca64</code>, <code>pca128</code>, <code>pgs20traits</code>, etc.).</li> <li>End with a monotonically increasing version suffix (<code>_v1</code>, <code>_v2</code>, \u2026).</li> </ul>"},{"location":"integration/embedding_policies/#pca-dimensionality-policy","title":"PCA dimensionality policy","text":"<p>When choosing PCA dims for new strategies (especially sMRI):</p> <ol> <li>Upper bound </li> <li>Let (p) = number of input features (e.g., \u2248176 sMRI ROIs) and (n) = usable subjects.  </li> <li> <p>Do not set <code>dim &gt; p</code>, and keep <code>dim \u226a n</code> (e.g., \u2264 n/2) to avoid unstable components.</p> </li> <li> <p>Variance explained check (one-off) </p> </li> <li>On a representative training fold, fit PCA and inspect the cumulative variance curve.</li> <li> <p>Record dims where cumulative variance hits ~70, 80, 90% (e.g., 64, 128, 256).</p> </li> <li> <p>Grid search with nested CV </p> </li> <li>Define a small grid:<ul> <li>UKB sMRI: <code>[32, 64, 128, 256]</code> \u2192 registered as <code>smri_ukb_pca32_v1</code>\u2026<code>pca256_v1</code>.</li> <li>CHA sMRI: <code>[64, 128]</code> (given smaller N and higher heterogeneity).</li> </ul> </li> <li>Run nested CV for the actual downstream task (prediction or CCA+permutation) and compare dims by:<ul> <li>AUROC/AUPRC for classifiers, or</li> <li>significance and stability of canonical correlations for CCA.</li> </ul> </li> <li> <p>Choose the smallest dim within the 1 SE band of the best-performing dim, and record that as the default.</p> </li> <li> <p>Hardening the choice</p> </li> <li>Once a dim is selected, promote that strategy ID (e.g., <code>smri_ukb_pca128_v1</code>) as the default in downstream      experiment configs and document the decision (date, grid, metric) in results/metadata.</li> </ol>"},{"location":"integration/embedding_policies/#policy-summary","title":"Policy summary","text":"<ul> <li>Rule of thumb: start with <code>dim = min(128, p, n/2)</code> and adjust via nested CV.</li> <li>Never change the definition of an existing <code>*_v1</code> strategy silently; instead, create <code>_v2</code> and link to the decision.</li> <li>Always tie a strategy ID back to:</li> <li>a dataset card (<code>kb/datasets/*.yaml</code>),</li> <li>a preprocessing description (this page + modality-specific docs), and</li> <li>at least one experiment config that demonstrates its use.</li> </ul>"},{"location":"integration/integration_strategy/","title":"\ud83d\udd17 Integration Strategy","text":"<p>Late fusion first, then escalate to contrastive and unified architectures</p>"},{"location":"integration/integration_strategy/#overall-philosophy","title":"\ud83c\udfaf Overall Philosophy","text":"<p>Late fusion / integration first, then scale if we see gains.</p>"},{"location":"integration/integration_strategy/#why-this-applies-to-genes-brain","title":"\ud83e\uddec\ud83e\udde0 Why This Applies to Genes \u00d7 Brain","text":"<ul> <li>Heterogeneous semantics: Nucleic-acid sequence vs morphology/dynamics \u2192 maximize modality specificity before fusion</li> <li>Different confounds: Ancestry/batch vs site/motion/TR \u2192 deconfound independently</li> </ul>"},{"location":"integration/integration_strategy/#baselines","title":"\ud83d\udcca Baselines","text":"<ul> <li>Preprocess per modality</li> <li>Z-score features.</li> <li>Residualize against: age, sex, site/scanner, motion (FD), SES (if available), genetic PCs (PC1\u2013PC10).</li> <li>Dimensionality</li> <li>Project to 512 dims per modality (PCA or tiny MLP).</li> <li>CCA + permutation</li> <li>CCA on train folds; 1,000 shuffles; report \u03c11\u2013\u03c13 with p-values.</li> <li>Prediction</li> <li>LR (balanced) and LightGBM/CatBoost on Gene, Brain, Fusion; same CV folds; AUROC/AUPRC; DeLong/bootstrap for Fusion vs single-modality.</li> </ul>"},{"location":"integration/integration_strategy/#embedding-strategy-registry","title":"\ud83d\udccb Embedding Strategy Registry","text":"<p>Recipes live under <code>kb/integration_cards/embedding_strategies.yaml</code></p> <p>Print them via: <pre><code>python scripts/manage_kb.py ops strategy &lt;id&gt;\n</code></pre></p> <p>Available strategies: - sMRI (<code>smri_free_surfer_pca512_v1</code>). FreeSurfer ROI table (~176 features) \u2192 fold-wise z-score \u2192 residualize age/sex/site/ICV \u2192 PCA\u2192512. Future variants: FM encoders, diffusion MRI. Sources: <code>docs/integration/modality_features/smri.md</code>, FreeSurfer refs. - rs-fMRI baseline (<code>rsfmri_swift_segments_v1</code>). SwiFT exports per 20-frame segment \u2192 mean pool tokens \u2192 run mean \u2192 subject mean \u2192 residualize age/sex/site/motion \u2192 PCA\u2192512. Variants exist for BrainLM (<code>rsfmri_brainlm_segments_v1</code>), Brain-JEPA (<code>rsfmri_brainjepa_roi_v1</code>), and BrainMT (<code>rsfmri_brainmt_segments_v1</code>); each references the corresponding walkthrough/code. - Genetics (<code>genetics_gene_fm_pca512_v1</code>). RC-averaged gene FMs (Caduceus/DNABERT-2/Evo2/HyenaDNA/GENERaTOR) \u2192 exon \u2192 gene pooling \u2192 concatenate curated gene set \u2192 residualize age/sex/ancestry PCs/batch \u2192 PCA\u2192512. For non\u2013RC-equivariant encoders, follow RCCR/RC-averaging guidance in the genomics modality spec. - Fusion (<code>fusion_concat_gene_brain_1024_v1</code>). Concatenate the 512-D genetics vector with the chosen 512-D brain vector; z-score each block independently before concatenation.</p> <p>Experiments now reference these IDs (see <code>configs/experiments/*.yaml</code>) to keep per-subject embeddings traceable.</p>"},{"location":"integration/integration_strategy/#harmonization-site-effects","title":"\ud83d\udd27 Harmonization &amp; Site Effects","text":"<p>Cataloged in <code>kb/integration_cards/harmonization_methods.yaml</code></p> <p>Query via: <pre><code>python scripts/manage_kb.py ops harmonization &lt;id&gt;\n</code></pre></p> <p>Available harmonization methods: - Default (<code>none_baseline</code>). Feature-level z-score + covariate residualization; always record site/motion covariates. - Statistical (<code>combat_smri</code>). ROI-wise ComBat before PCA for sMRI (Fortin et al., 2018). Run the <code>02_harmonization_ablation_smri</code> config to benchmark vs. the baseline. - Deep (<code>murd_t1_t2</code>). Apply MURD (Liu &amp; Yap 2024) to T1/T2 volumes before FreeSurfer extraction; compare vs. ComBat and baseline to judge if image-space harmonization improves CCA/prediction.^[See MURD paper summary: <code>docs/generated/kb_curated/papers-md/murd_2024.md</code> and card <code>kb/paper_cards/murd_2024.yaml</code>.] - Representation unlearning (<code>site_unlearning_module</code>). Optional adversarial head that removes site labels from embedding space (Dinsdale et al., 2021); treat as experimental until harmonization ablations justify it.^[See site-unlearning paper summary: <code>docs/generated/kb_curated/papers-md/dinsdale_site_unlearning_2021.md</code> and card <code>kb/paper_cards/dinsdale_site_unlearning_2021.yaml</code>.]</p> <p>Record harmonization IDs (and preprocessing pipeline IDs such as <code>rsfmri_preprocessing_pipelines.hcp_like_minimal</code>) alongside embedding strategy IDs in every run.</p>"},{"location":"integration/integration_strategy/#escalation-criteria","title":"\ud83d\ude80 Escalation Criteria","text":"<p>If Fusion &gt; max(Gene, Brain) with p &lt; 0.05 (DeLong/bootstrap), consider:   - Two-tower contrastive alignment (frozen encoders; small projectors).   - EI stacking over per-modality models.   - Harmony-style hub tokens/TAPE if TR/site heterogeneity limits fMRI.</p>"},{"location":"integration/integration_strategy/#interpretability","title":"\ud83d\udd0d Interpretability","text":"<ul> <li>LOGO \u0394AUC with Wilcoxon + FDR for gene attribution</li> <li>CCA loadings: Partial correlations of axes with outcomes (covariate-adjusted)</li> </ul>"},{"location":"integration/integration_strategy/#tabular-fm-tabpfn-baseline","title":"Tabular FM (TabPFN) baseline","text":"<ul> <li>TabPFN (Nature 2024) is tracked under <code>kb/model_cards/tabpfn.yaml</code>. It is a predictor, not an fMRI encoder.</li> <li>Use TabPFN as a strong small-N tabular baseline on:</li> <li>Raw sMRI ROI tables (<code>smri_free_surfer_raw_176</code>).</li> <li>Genetics summary tables (<code>genetics_pgs_20traits</code>).</li> <li>Early-fusion tabular features (ROI + PGS).</li> <li>Compare TabPFN vs. LR/LightGBM in <code>configs/experiments/03_prediction_baselines_tabular.yaml</code> to quantify how much structured embeddings help beyond a tabular FM.</li> </ul> <p>Risks and mitigations</p> <ul> <li>Leakage: do scaling/residualization within train folds; apply transforms to test.</li> <li>Site imbalance: use group/site-aware CV when feasible.</li> <li>Overfitting at high dims: prefer 256\u2013512; regularize LR; early stopping for GBDT.</li> </ul>"},{"location":"integration/multimodal_architectures/","title":"Multimodal Architecture Patterns for Brain-Omics Models","text":"<p>This document catalogs architectural patterns from multimodal foundation models that inform the design of ARPA-H-style Brain-Omics Model (BOM) systems. These models demonstrate how to fuse heterogeneous modalities (vision, language, time series, structured data) at scale\u2014lessons directly applicable to gene\u2013brain\u2013behavior\u2013language integration.</p>"},{"location":"integration/multimodal_architectures/#overview","title":"Overview","text":"<p>Purpose: Extract design principles from state-of-the-art multimodal FMs to guide Neuro-Omics KB integration strategies as they escalate from late fusion \u2192 two-tower contrastive \u2192 unified multimodal architectures.</p> <p>Scope: Medical/clinical multimodal FMs, unified vision-language-speech models, and sparse multimodal transformers.</p>"},{"location":"integration/multimodal_architectures/#1-bagel-unified-multimodal-foundation-model","title":"1. BAGEL \u2014 Unified Multimodal Foundation Model","text":""},{"location":"integration/multimodal_architectures/#architecture-summary","title":"Architecture Summary","text":"<p>Model: BAGEL (Emerging Properties in Unified Multimodal Pretraining) Paper: arXiv:2505.14683 | Card: <code>kb/paper_cards/bagel_2025.yaml</code></p> <ul> <li>Backbone: Qwen2.5 decoder-only transformer (7B active, 14B total with MoT experts)</li> <li>Modalities: Text, images, video, web data</li> <li>Architecture: Mixture-of-Transformer-Experts (MoT) with separate experts for understanding vs. generation</li> <li>Visual encoding: SigLIP2-style ViT encoder for understanding</li> <li>Visual generation: FLUX VAE + rectified-flow diffusion conditioned on transformer states</li> <li>Training: Trillions of interleaved multimodal tokens with reasoning-oriented curation</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns","title":"Key Design Patterns","text":"<p>\u2705 Unified decoder-only architecture: Single transformer processes all modalities as token sequences \u2705 Mixture-of-experts (MoT): Separate experts for understanding (comprehension) vs. generation tasks \u2705 Interleaved data: Reasoning-oriented multimodal corpus with natural task diversity \u2705 Emergent capabilities: Complex reasoning, free-form manipulation, 3D understanding from unified pretraining</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Gene-brain-language unification: Treat genetics (nucleotide tokens), brain (parcel tokens), and behavior (structured tokens) as additional modalities alongside text - MoT for neuro-omics: Separate experts for discriminative (gene-brain association) vs. generative (report generation, counterfactual prediction) tasks - Interleaved corpus design: Create multimodal corpus pairing genetic variants + brain scans + cognitive assessments + clinical narratives</p> <p>Escalation path: 1. Late fusion baselines (current) 2. Two-tower contrastive (gene encoder \u2194 brain encoder) 3. MoT-style unified architecture where genetics/brain/behavior tokens share decoder with modality-specific experts</p> <p>Reference materials: - BAGEL walkthrough - BAGEL paper card</p>"},{"location":"integration/multimodal_architectures/#2-mot-mixture-of-transformers","title":"2. MoT \u2014 Mixture-of-Transformers","text":""},{"location":"integration/multimodal_architectures/#architecture-summary_1","title":"Architecture Summary","text":"<p>Model: Mixture-of-Transformers (Sparse and Scalable for Multi-Modal FMs) Paper: arXiv:2411.04996 | Card: <code>kb/paper_cards/mot_2025.yaml</code></p> <ul> <li>Backbone: Sparse multimodal transformer with modality-aware FFNs/attention</li> <li>Modalities: Text, images, speech</li> <li>Sparsity mechanism: Separate FFN/attention projections per modality; shared global self-attention</li> <li>Settings: Chameleon-style autoregressive + Transfusion-style diffusion</li> <li>Efficiency: ~55.8% FLOPs of dense baseline, similar or better performance</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns_1","title":"Key Design Patterns","text":"<p>\u2705 Modality-aware sparsity: Decouple non-embedding parameters by modality \u2705 Shared global attention: All tokens interact via self-attention (no routing) \u2705 Drop-in replacement: Compatible with existing dense transformer architectures \u2705 Stable scaling: Maintains performance across model sizes (1B \u2192 7B \u2192 30B)</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_1","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Per-modality FFNs: Separate feed-forward networks for genetics, brain MRI, fMRI, EEG, behavior tokens - Shared attention: Global self-attention over all modalities captures cross-modal dependencies - Compute efficiency: Critical for scaling to large cohorts (UK Biobank N=500k+)</p> <p>Integration with Neuro-Omics KB: - Implement modality-specific projectors (genetics_ffn, brain_ffn, behavior_ffn) - Retain shared attention over concatenated gene+brain+behavior tokens - Compare vs. learned MoE routing (simpler, more interpretable)</p> <p>Reference materials: - MoT walkthrough - MoT paper card</p>"},{"location":"integration/multimodal_architectures/#3-m3fm-multilingual-medical-model","title":"3. M3FM \u2014 Multilingual Medical Model","text":""},{"location":"integration/multimodal_architectures/#architecture-summary_2","title":"Architecture Summary","text":"<p>Model: M3FM (Multilingual Chest X-ray Report Generator) Repo: ai-in-health/M3FM | Card: <code>kb/model_cards/m3fm.yaml</code></p> <ul> <li>Backbone: Multilingual CLIP encoder + relational-memory Transformer decoder</li> <li>Modalities: Chest X-ray images, bilingual text (English/Chinese)</li> <li>Architecture: Two-tower (vision encoder + language decoder) with relational memory</li> <li>Decoder: Language selection via BOS token (1=English, 2=Chinese)</li> <li>Training: COV-CTR COVID-era CXR dataset with multilingual reports</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns_2","title":"Key Design Patterns","text":"<p>\u2705 Two-tower fusion: Vision encoder outputs \u2192 cross-attention in language decoder \u2705 Language-aware generation: Single decoder handles multiple languages via BOS conditioning \u2705 Relational memory: Augmented attention for capturing long-range report dependencies \u2705 Medical domain adaptation: CLIP text embeddings projected for medical terminology</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_2","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Brain-omics-to-language: Project brain/genetics embeddings into CLIP-like space \u2192 generate clinical narratives - Bilingual reporting: Extend to English/Korean for Cha Hospital developmental cohorts - Relational memory for clinical context: Track longitudinal patient history across visits</p> <p>Integration strategy: - Use M3FM-style two-tower for brain scan \u2192 clinical report generation - Adapt relational memory for multi-visit longitudinal modeling - Explore gene embedding \u2192 language generation (explain genetic risk in natural language)</p> <p>Reference materials: - M3FM walkthrough - M3FM model card</p>"},{"location":"integration/multimodal_architectures/#4-me-llama-medical-llm","title":"4. Me-LLaMA \u2014 Medical LLM","text":""},{"location":"integration/multimodal_architectures/#architecture-summary_3","title":"Architecture Summary","text":"<p>Model: Me-LLaMA (Medical LLaMA) Repo: BIDS-Xu-Lab/Me-LLaMA | Card: <code>kb/model_cards/me_llama.yaml</code></p> <ul> <li>Backbone: LLaMA-2/3 (13B/70B) with continual pretraining + LoRA instruction tuning</li> <li>Modality: Medical text (biomedical literature, clinical notes, guidelines)</li> <li>Pretraining ratio: 15:1:4 (biomedical : clinical : general)</li> <li>Training: 129B medical tokens + 214K instruction samples</li> <li>Evaluation: 12+ medical QA/NLP tasks with prompt templates</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns_3","title":"Key Design Patterns","text":"<p>\u2705 Continual pretraining: Adapt general LLM to medical domain with curated corpus \u2705 LoRA instruction tuning: Parameter-efficient adaptation for clinical reasoning \u2705 Prompt engineering: Modality-specific prompts for different clinical tasks \u2705 Evaluation harness: Structured benchmarking across medical NLP tasks</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_3","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Neuro-omics LLM: Continual pretrain LLaMA on neuroscience literature + genetics papers + clinical neurology notes - Instruction tuning for clinical tasks: Adapt for cognitive assessment interpretation, genetic counseling, neuroimaging report generation - Prompt templates: Create standardized prompts for gene-brain-behavior reasoning</p> <p>As semantic bridge in BOM: - Me-LLaMA-style medical LLM serves as semantic hub for Brain-Omics Model - Project genetics/brain/EEG embeddings into LLM token space for cross-modal reasoning - Enable natural language queries over multimodal neuro-omics data</p> <p>Reference materials: - Me-LLaMA walkthrough - Me-LLaMA model card</p>"},{"location":"integration/multimodal_architectures/#5-titan-whole-slide-image-fm","title":"5. TITAN \u2014 Whole-Slide Image FM","text":""},{"location":"integration/multimodal_architectures/#architecture-summary_4","title":"Architecture Summary","text":"<p>Model: TITAN (Transformer for Integrative Tissue Analysis) Repo: mahmoodlab/TITAN | Card: <code>kb/model_cards/titan.yaml</code></p> <ul> <li>Backbone: Slide-level transformer with multi-scale patch aggregation</li> <li>Modality: Whole-slide histopathology images</li> <li>Architecture: Hierarchical attention over gigapixel images (millions of patches)</li> <li>Applications: Cancer diagnosis, survival prediction, treatment response</li> </ul>"},{"location":"integration/multimodal_architectures/#key-design-patterns_4","title":"Key Design Patterns","text":"<p>\u2705 Multi-scale patch processing: Handle gigapixel images via hierarchical aggregation \u2705 Attention-based pooling: Learn to aggregate informative regions \u2705 Slide-level embeddings: Compress millions of patches \u2192 fixed-size vectors \u2705 Task-specific heads: Shared encoder for multiple downstream tasks</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_4","title":"Implications for Brain-Omics Models","text":"<p>Direct applications: - Brain MRI analogy: Whole-brain 3D volumes \u2192 hierarchical patch aggregation (similar to TITAN's slide processing) - Multi-scale fusion: Combine region-level (parcels) and voxel-level (fine-grained) brain features - Histology + genetics: If histopathology data available (e.g., brain tissue banks), TITAN-style processing + genetics fusion</p> <p>Integration with Neuro-Omics KB: - Adapt TITAN's multi-scale attention for 3D MRI volumes - Use TITAN-style patch aggregation for whole-brain sMRI + fMRI fusion - Explore cross-modal attention: pathology patches \u2194 genetic variants</p> <p>Reference materials: - TITAN walkthrough - TITAN model card</p>"},{"location":"integration/multimodal_architectures/#6-fms-medical-catalog","title":"6. FMS-Medical Catalog","text":""},{"location":"integration/multimodal_architectures/#resource-summary","title":"Resource Summary","text":"<p>Catalog: Awesome Foundation Models for Advancing Healthcare Repo: YutingHe-list/Awesome-Foundation-Models</p> <ul> <li>Scope: 200+ medical foundation models across modalities (text, vision, multimodal, protein, genomics, clinical time series)</li> <li>Organization: Bilingual (English/Chinese) with taxonomy by modality and task</li> <li>Usage: Reference catalog for discovering relevant medical FMs</li> </ul>"},{"location":"integration/multimodal_architectures/#key-resources","title":"Key Resources","text":"<p>\u2705 Medical vision FMs: CXR, CT, MRI, histopathology encoders \u2705 Medical LLMs: Clinical text understanding and generation models \u2705 Genomics/proteomics FMs: Sequence models for molecular biology \u2705 Multimodal FMs: Vision-language models for radiology, pathology reports</p>"},{"location":"integration/multimodal_architectures/#implications-for-brain-omics-models_5","title":"Implications for Brain-Omics Models","text":"<p>Discovery and benchmarking: - Identify relevant medical imaging FMs for brain scan processing - Find medical LLMs for clinical narrative generation - Discover multimodal architectures to adapt for gene-brain-behavior fusion</p> <p>Reference for ARPA-H integration: - Survey multimodal medical FMs to inform BOM architecture choices - Benchmark against medical FM baselines (e.g., CXR report generation \u2192 adapt for neuroimaging)</p> <p>Reference materials: - FMS-Medical walkthrough - FMS-Medical catalog YAML</p>"},{"location":"integration/multimodal_architectures/#integration-roadmap-for-neuro-omics-kb","title":"Integration Roadmap for Neuro-Omics KB","text":""},{"location":"integration/multimodal_architectures/#phase-1-late-fusion-baselines-current","title":"Phase 1: Late Fusion Baselines (Current)","text":"<ul> <li>Models: Separate encoders (Caduceus, BrainLM, FreeSurfer ROIs)</li> <li>Fusion: Concatenate embeddings \u2192 LR/GBDT prediction</li> <li>Evaluation: CCA + permutation, AUROC/AUPRC, DeLong tests</li> </ul>"},{"location":"integration/multimodal_architectures/#phase-2-two-tower-contrastive","title":"Phase 2: Two-Tower Contrastive","text":"<ul> <li>Architecture: Frozen gene encoder \u2194 frozen brain encoder with learnable projectors</li> <li>Loss: InfoNCE or similar contrastive objective</li> <li>Inspiration: CLIP-style alignment (M3FM two-tower paradigm)</li> </ul>"},{"location":"integration/multimodal_architectures/#phase-3-mot-style-sparse-integration","title":"Phase 3: MoT-Style Sparse Integration","text":"<ul> <li>Architecture: Shared self-attention over gene+brain+behavior tokens</li> <li>Sparsity: Modality-specific FFNs (genetics_ffn, brain_ffn, behavior_ffn)</li> <li>Inspiration: MoT paper (arXiv:2411.04996)</li> </ul>"},{"location":"integration/multimodal_architectures/#phase-4-unified-brain-omics-model-bom","title":"Phase 4: Unified Brain-Omics Model (BOM)","text":"<ul> <li>Architecture: BAGEL-style decoder-only with MoT experts</li> <li>Modalities: Genetics (nucleotide tokens) + brain (parcel/voxel tokens) + behavior (structured tokens) + language (text tokens)</li> <li>Semantic bridge: Me-LLaMA-style medical LLM as central hub</li> <li>Training: Interleaved multimodal corpus (genetic variants + brain scans + cognitive assessments + clinical notes)</li> </ul>"},{"location":"integration/multimodal_architectures/#next-steps","title":"Next Steps","text":"<ol> <li>Complete Phase 1 baselines (CCA + prediction on UKB gene-brain data)</li> <li>Pilot two-tower contrastive (gene-brain alignment with frozen encoders)</li> <li>Explore MoT-style sparsity (modality-specific FFNs vs. full early fusion)</li> <li>Design ARPA-H BOM architecture (unified multimodal transformer with neuro-omics tokens)</li> <li>Curate interleaved corpus (multimodal neuro-omics data for unified pretraining)</li> </ol>"},{"location":"integration/multimodal_architectures/#reference-index","title":"Reference Index","text":"<p>Walkthrough documents: - BAGEL walkthrough - MoT walkthrough - M3FM walkthrough - Me-LLaMA walkthrough - TITAN walkthrough - FMS-Medical walkthrough</p> <p>Paper/model cards: - <code>kb/paper_cards/bagel_2025.yaml</code> - <code>kb/paper_cards/mot_2025.yaml</code> - <code>kb/model_cards/m3fm.yaml</code> - <code>kb/model_cards/me_llama.yaml</code> - <code>kb/model_cards/titan.yaml</code> - <code>kb/datasets/fms_medical_catalog.yaml</code></p> <p>Integration recipes: - Integration Strategy - Design Patterns - CCA + Permutation</p>"},{"location":"integration/analysis_recipes/cca_permutation/","title":"CCA + Permutation","text":"<p>Inputs</p> <ul> <li>X_gene, X_brain: residualized and standardized matrices (N \u00d7 d_gene, N \u00d7 d_brain) projected to ~512 dims.</li> <li>Covariates: used upstream during residualization.</li> <li>Metadata: record <code>embedding_strategies.&lt;id&gt;</code>, <code>harmonization_methods.&lt;id&gt;</code>, and (for fMRI) <code>rsfmri_preprocessing_pipelines.&lt;id&gt;</code> to ensure results are traceable.</li> </ul> <p>Context in integration plan</p> <ul> <li>This recipe is part of the diagnostic / exploration layer of the integration stack.</li> <li>Run it after per-modality sanity checks but before heavier fusion models; it tells you whether there is cross-modal structure worth chasing.</li> <li>Treat it as a companion to the late-fusion-first baselines rather than a replacement for prediction experiments.</li> </ul> <p>Protocol</p>"},{"location":"integration/analysis_recipes/cca_permutation/#1-fold-discipline","title":"1. Fold discipline","text":"<ul> <li>Use K stratified folds (make them site-/scanner-aware when possible).</li> <li>For each train fold:</li> <li>Fit CCA on <code>(X_gene_train, X_brain_train)</code>.</li> <li>Transform the train and held-out fold to canonical scores so downstream metrics share the same projection space.</li> </ul>"},{"location":"integration/analysis_recipes/cca_permutation/#2-permutation-test","title":"2. Permutation test","text":"<ul> <li>Set <code>B = 1,000</code> (or at least 500 for quick scans).</li> <li>For each <code>b \u2208 {1 \u2026 B}</code> inside the training split:</li> <li>Permute subject IDs in one modality (genes or brain) while keeping covariates fixed.</li> <li>Refit CCA on the permuted pair.</li> <li>Store the first canonical correlation <code>\u03c11^(b)</code> to build a null.</li> <li>Compute the empirical p-value <code>p = ( # {\u03c11^(b) \u2265 \u03c11_obs} + 1 ) / (B + 1 )</code>.</li> </ul>"},{"location":"integration/analysis_recipes/cca_permutation/#3-reporting-interpretation","title":"3. Reporting &amp; interpretation","text":"<ul> <li>Report <code>\u03c11\u2013\u03c13</code> with their permutation p-values (per fold and averaged).</li> <li>Optionally bootstrap the canonical correlations for 95\u202f% CIs.</li> <li>Surface top loadings / feature contributions for both modalities to explain shared signal.</li> </ul> <p>Why pair CCA with permutations?</p> <ul> <li>CCA will always produce non-zero canonical correlations\u2014even when there is no shared structure\u2014because it can overfit high-dimensional spaces.</li> <li>The permutation loop builds a modality-shuffled null distribution so we can report p-values (or FDR-adjusted thresholds) and avoid over-interpreting noise.</li> <li>This statistical check is lightweight enough for \u201cquick tests\u201d while still respecting site/ancestry confounds.</li> </ul> <p>Pitfalls - Never fit CCA on all data. - Keep the same permutation protocol across folds for comparability.</p>"},{"location":"integration/analysis_recipes/partial_correlations/","title":"Partial Correlations","text":"<p>Goal - Associate canonical scores or PCs with outcomes controlling covariates.</p> <p>Context in integration plan</p> <ul> <li>Use this after you have stable embeddings and late-fusion baselines: it helps interpret axes (CCA components, PCs) rather than build new predictors.</li> <li>Treat it as an analysis layer sitting on top of the late-fusion-first stack, not as a standalone modeling approach.</li> <li>Prefer simple, regularized models here; if interpretation depends on heavy models, revisit whether the underlying embeddings/CCA steps are well-behaved.</li> </ul> <p>Continuous outcome (e.g., PHQ-9) - Residualize x and y on covariates within train folds \u2192 rx, ry. - Correlate rx, ry (Pearson/Spearman); aggregate across folds.</p> <p>Binary outcome (e.g., MDD) - Preferred: logistic regression y ~ x + covariates; report OR, CI, p. - Optional: approximate partial correlation via residuals y \u2212 p\u0302 from covariate-only logistic.</p> <p>Report - Per-axis coefficients/correlations with CIs; FDR across multiple tests if many axes.</p>"},{"location":"integration/analysis_recipes/prediction_baselines/","title":"Prediction Baselines","text":"<p>Inputs - Gene, Brain, and Fusion = [Gene | Brain], all 512-D after preprocessing (<code>embedding_strategies.genetics_gene_fm_pca512_v1</code>, <code>embedding_strategies.smri_free_surfer_pca512_v1</code>, <code>embedding_strategies.fusion_concat_gene_brain_1024_v1</code>). - Tabular mode: raw ROI tables (<code>smri_free_surfer_raw_176</code>) and genetics PGS (<code>genetics_pgs_20traits</code>) for TabPFN / LR / GBDT baselines.</p> <p>Context in integration plan</p> <ul> <li>This recipe is the primary late-fusion baseline: compare Gene-only, Brain-only, and simple Fusion ([Gene | Brain]) features using shallow models.</li> <li>Escalate beyond this (e.g., contrastive two-tower, EI stacking, hub tokens) only if Fusion clearly and consistently outperforms both single-modality baselines.</li> <li>Keep this runbook as the reference when deciding whether more complex integration architectures are justified.</li> </ul> <p>Models - Logistic Regression   - penalty=L2, C\u2208{0.5,1,2}, solver=saga/liblinear, class_weight=balanced, max_iter=5,000. - LightGBM   - num_leaves=31, learning_rate=0.05, n_estimators=1,000 with early stopping, scale_pos_weight \u2248 N_neg/N_pos. - CatBoost   - depth=6\u20138, learning_rate=0.05, iterations=2,000 with early stopping, loss_function=Logloss, auto class weights. - TabPFN (<code>kb/model_cards/tabpfn.yaml</code>)   - Max 10k samples and 500 features per forward pass; chunk folds if N is larger.   - Use for tabular baselines (raw ROI, PGS, ROI+PGS fusion) to benchmark whether representation learning beats a tabular FM.</p> <p>Evaluation - Same CV folds across modalities. - Metrics: AUROC, AUPRC; report mean \u00b1 SD across folds. - Significance: DeLong or bootstrap for Fusion vs each single-modality on held-out predictions.</p> <p>Outputs to save - Per-fold predictions and labels for later DeLong/bootstrap and calibration checks. - Embedding/harmonization IDs used to produce each feature set (copy from experiment config metadata).</p>"},{"location":"integration/modality_features/fmri/","title":"fMRI Features","text":""},{"location":"integration/modality_features/fmri/#reference-preprocessing-stacks","title":"Reference preprocessing stacks","text":"<ul> <li>Cataloged in <code>kb/integration_cards/rsfmri_preprocessing_pipelines.yaml</code>. Default: <code>hcp_like_minimal</code> (motion + distortion correction, nuisance regression, 0.01\u20130.1\u202fHz filter, Schaefer-400 parcellation).</li> <li>Document which pipeline ID you used per run; experiments reference it alongside the embedding strategy ID.</li> </ul>"},{"location":"integration/modality_features/fmri/#subject-level-embedding-strategies","title":"Subject-level embedding strategies","text":"<ul> <li><code>rsfmri_swift_segments_v1</code> (SwiFT):</li> <li>20-frame segments \u2192 mean-pool tokens from the last hidden state \u2192 mean over segments per run \u2192 mean over runs per subject.</li> <li>Fold-wise z-score + residualize(age, sex, site/scanner, mean FD, DVARS) \u2192 PCA\u2192512.</li> <li>Source references: <code>docs/code_walkthroughs/swift_walkthrough.md</code>.</li> <li><code>rsfmri_brainlm_segments_v1</code> (BrainLM ViT-MAE):</li> <li>32-frame windows with stride 16 \u2192 CLS pooling per window \u2192 attention pooling across windows with inverse-FD weights \u2192 mean over runs.</li> <li>Fold-wise z-score + residualize(age, sex, site, FD, DVARS) \u2192 PCA\u2192512.</li> <li><code>rsfmri_brainjepa_roi_v1</code> (Brain-JEPA):</li> <li>ROI tokens (Schaefer-400 + Tian-50) \u2192 mean pooling across unmasked tokens \u2192 option to average per functional network \u2192 subject mean.</li> <li>Residualize age/sex/site/motion (optionally GSR flag) \u2192 PCA\u2192512.</li> <li><code>rsfmri_brainmt_segments_v1</code> (BrainMT):</li> <li>32-frame conv/Mamba segments \u2192 mean patch tokens per segment \u2192 run mean \u2192 subject mean.</li> <li>Residualize age/sex/site/motion \u2192 PCA\u2192512.</li> </ul> <p>All recipes live in <code>kb/integration_cards/embedding_strategies.yaml</code>; call <code>python scripts/manage_kb.py ops strategy &lt;id&gt;</code> to print the full pipeline (including preprocessing notes and sources) before launching extraction.</p>"},{"location":"integration/modality_features/fmri/#classical-atlas-baseline-optional","title":"Classical atlas baseline (optional)","text":"<ul> <li>For quick sanity checks, you can still run atlas-based functional connectivity:</li> <li>Schaefer-400 time courses \u2192 Pearson FC matrix \u2192 Fisher z \u2192 vectorize upper triangle \u2192 z-score/residualize (include motion/site covariates) \u2192 PCA to 100\u2013256 \u2192 optionally pad to 512.</li> <li>Track this by creating its own <code>embedding_strategies</code> entry if you plan to use it beyond ad-hoc QA.</li> </ul>"},{"location":"integration/modality_features/genomics/","title":"Genomics Features","text":""},{"location":"integration/modality_features/genomics/#gene-fm-embedding-genetics_gene_fm_pca512_v1","title":"Gene FM embedding (<code>genetics_gene_fm_pca512_v1</code>)","text":"<ul> <li>Models: Caduceus, Evo\u202f2, HyenaDNA (conceptual), GENERaTOR, DNABERT-2 (see <code>kb/model_cards/</code>).</li> <li>RC hygiene:</li> <li>RC-equivariant encoders (e.g., Caduceus): verify equivariance with spot checks but no averaging required.</li> <li>Non-equivariant encoders (e.g., DNABERT-2, Evo\u202f2, HyenaDNA-style): run on forward and reverse-complement     sequences; average token embeddings before pooling, or apply RCCR-style consistency regularization when     fine-tuning.^See reverse-complement consistency paper</li> <li>Tokenization: maintain deterministic k-mer/BPE framing; avoid random masking for inference exports.</li> <li>Pooling hierarchy:</li> <li>Token \u2192 exon (mean or CLS).</li> <li>Exon \u2192 gene (mean, or attention if pathway-weighted).</li> <li>Gene set \u2192 subject vector (concatenate curated genes; align order with manifest).</li> <li>Covariates: residualize age, sex, ancestry PCs 1\u201310, sequencing batch.</li> <li>Dimensionality: PCA \u2192 512 (fit on train fold).</li> <li>Retrieve the latest recipe with <code>python scripts/manage_kb.py ops strategy genetics_gene_fm_pca512_v1</code>.</li> </ul>"},{"location":"integration/modality_features/genomics/#tabular-genetics-features-genetics_pgs_20traits","title":"Tabular genetics features (<code>genetics_pgs_20traits</code>)","text":"<ul> <li>20 curated UKB PGS + ancestry PCs.</li> <li>Preprocessing: mean-impute missing PGS, z-score each feature inside the train fold.</li> <li>Intended for tabular prediction baselines (including TabPFN) and for fusion with sMRI ROI tables.</li> </ul>"},{"location":"integration/modality_features/genomics/#attribution","title":"Attribution","text":"<ul> <li>Leave-one-gene-out (LOGO) \u0394AUC with Wilcoxon across folds + FDR control remains the recommended approach once embeddings feed prediction models.</li> </ul>"},{"location":"integration/modality_features/genomics/#long-context-genomic-fms-regulatory-windows","title":"Long-context genomic FMs (regulatory windows)","text":"<ul> <li>For exploratory regulatory-region embeddings (enhancers, promoters, long-range elements):</li> <li>Prefer Evo\u202f2 / StripedHyena-2\u2013style models for 100kb\u20131Mbp contexts.^See Evo\u202f2 paper summary and systems     note on multi-hybrid LMs.</li> <li>HyenaDNA provides architectural guidance for single-nucleotide, 1M-token contexts and motivates careful use     of sequence-length warm-up when experimenting with long genetic windows.</li> <li>Start with shorter windows (e.g., \u00b1100kb around TSS) before escalating to full 1Mbp context for cost reasons.</li> </ul>"},{"location":"integration/modality_features/smri/","title":"sMRI Features","text":""},{"location":"integration/modality_features/smri/#baseline-embedding-smri_free_surfer_pca512_v1","title":"Baseline embedding (<code>smri_free_surfer_pca512_v1</code>)","text":"<ul> <li>Input: FreeSurfer 7.x <code>aparc.stats</code> (thickness + volume) + <code>aseg.stats</code> (~176 ROIs).</li> <li>Fold discipline:</li> <li>Train-fold z-score per feature.</li> <li>Residualize covariates: age, sex, site/scanner, intracranial volume (\u00b1SES).</li> <li>Dimensionality: PCA \u2192 512-D subject vector (fit on train fold, apply to train/test).</li> <li>Harmonization hooks: default <code>none_baseline</code>; optional <code>combat_smri</code> or <code>murd_t1_t2</code> before FreeSurfer (see <code>kb/integration_cards/harmonization_methods.yaml</code>).</li> <li>Reference the recipe via <code>python scripts/manage_kb.py ops strategy smri_free_surfer_pca512_v1</code> and log the ID inside experiment configs.</li> </ul>"},{"location":"integration/modality_features/smri/#future-extensions-to-log-as-new-strategies","title":"Future extensions to log as new strategies","text":"<ul> <li>sMRI FM encoders. Whole-volume ViTs or hub-token encoders (Brain Harmony Stage\u202f0) that emit subject embeddings directly; register as <code>smri_fm_encoder_*</code>.</li> <li>Diffusion MRI. Tract-based spatial stats or tractography metrics \u2192 z-score \u2192 residualize \u2192 PCA\u2192512.</li> <li>Tabular predictors. When skipping PCA (raw 176-D features), reference <code>smri_free_surfer_raw_176</code> and evaluate TabPFN vs. LR/GBDT for tabular baselines.</li> </ul> <p>Always pair sMRI embeddings with consistent covariates and document which harmonization method (if any) preceded ROI extraction.</p>"},{"location":"kb/","title":"KB Cards","text":"<p>We maintain lightweight, reusable \u201ccards\u201d to capture:</p> <ul> <li>Integration principles (cross-domain guidance we adopt)</li> <li>Method families (e.g., Ensemble Integration)</li> <li>External model patterns (two-tower, JEPA/hub-tokens)</li> <li>Cross-domain evaluation</li> <li>Model and dataset cards (internal)</li> </ul> <p>Templates live in docs/kb/templates/.</p> <p>The actual cards (YAML) live under <code>kb/</code> at the repository root\u2014use the templates here as a starting point, then drop the finished card into <code>kb/model_cards</code>, <code>kb/datasets</code>, <code>kb/paper_cards</code>, or <code>kb/integration_cards</code> as appropriate.</p>"},{"location":"kb/templates/cross_domain_eval_card/","title":"Cross-domain eval card","text":"<p>title: \"Cross-Domain Evaluation\" status: draft updated: {{DATE}} tags: [evaluation]</p>"},{"location":"kb/templates/cross_domain_eval_card/#cross-domain-evaluation","title":"Cross-Domain Evaluation","text":"<p>Metrics - AUROC vs AUPRC; calibration (Brier/ECE)</p> <p>Statistical comparisons - DeLong, bootstrap, CIs</p> <p>CV/split strategy - Stratified, group/site-aware</p> <p>Leakage checklist</p> <p>Reporting template</p>"},{"location":"kb/templates/dataset_card/","title":"Dataset card","text":"<p>title: \"{{DATASET_NAME}} \u2014 Dataset Card\" status: draft updated: {{DATE}} tags: [dataset]</p>"},{"location":"kb/templates/dataset_card/#dataset_name","title":"{{DATASET_NAME}}","text":"<p>Paths and versions</p> <p>Access &amp; licensing (DUA, contacts, HF mirrors)</p> <p>Sample sizes, subset breakdowns, and inclusion criteria</p> <p>Base-pair / modality-specific statistics (e.g., total bp, TR windows)</p> <p>Modality column map (per-modality feature names + schema refs)</p> <p>Overlap logic and subject linking</p> <p>QC thresholds and preprocessing</p> <p>Linked assets - External repos used (e.g., <code>external_repos/caduceus</code>) - Walkthroughs covering extraction/validation</p> <p>Available covariates</p> <p>Notes and caveats</p>"},{"location":"kb/templates/experiment_config_stub/","title":"Experiment config stub","text":"<p>title: \"{{EXPERIMENT_NAME}} \u2014 Experiment Config\" status: draft updated: {{DATE}} tags: [experiment]</p>"},{"location":"kb/templates/experiment_config_stub/#experiment_name","title":"{{EXPERIMENT_NAME}}","text":"<p>Objective</p> <p>Datasets / splits</p> <p>Feature prep - Modality-specific transforms - Covariates/confounds handled</p> <p>Model config - Algorithms, key hyperparameters</p> <p>Evaluation - Metrics, statistical tests, logging</p> <p>Outputs to store</p> <p>Next decisions / escalation triggers</p>"},{"location":"kb/templates/external_model_pattern_card/","title":"External model pattern","text":"<p>title: \"{{PATTERN}} \u2014 External Model Pattern\" status: draft updated: {{DATE}} tags: [pattern, multimodal]</p>"},{"location":"kb/templates/external_model_pattern_card/#pattern","title":"{{PATTERN}}","text":"<p>Core idea</p> <p>Data assumptions</p> <p>Strengths</p> <p>Limitations</p> <p>Portability to our stack (now/later)</p> <p>Engineering cost</p> <p>Triggers to adopt</p> <p>References</p>"},{"location":"kb/templates/integration_principles_card/","title":"Integration principles","text":"<p>title: \"{{CARD_TITLE}} \u2014 Integration Principles\" status: draft updated: {{DATE}} tags: [integration, principles]</p>"},{"location":"kb/templates/integration_principles_card/#card_title","title":"{{CARD_TITLE}}","text":"<p>Citation</p> <p>One-line takeaway</p> <p>Taxonomy and trade-offs - Early / intermediate / late fusion (pros/cons)</p> <p>Cautions and failure modes - Heterogeneous semantics, alignment, missingness, over-smoothing, leakage</p> <p>Practices to adopt (bullets)</p> <p>Not adopting (for now)</p> <p>Implications for our project</p> <p>References/links</p>"},{"location":"kb/templates/method_family_card/","title":"Method family card","text":"<p>title: \"{{METHOD_FAMILY}} \u2014 Method Family\" status: draft updated: {{DATE}} tags: [method, ensembles]</p>"},{"location":"kb/templates/method_family_card/#method_family","title":"{{METHOD_FAMILY}}","text":"<p>What problem it solves</p> <p>How it works (brief) - Base learners - Stacking/meta-learner - Ensemble selection / interpretation</p> <p>When to use</p> <p>Implementation notes - Libraries - CV for stacking - Missingness handling</p> <p>Caveats</p> <p>Adoption plan</p>"},{"location":"kb/templates/model_card_template/","title":"Model card","text":"<p>title: \"{{MODEL_NAME}} \u2014 Model Card\" status: draft updated: {{DATE}} tags: [model, {{DOMAIN}}]</p>"},{"location":"kb/templates/model_card_template/#model_name","title":"{{MODEL_NAME}}","text":"<p>Purpose &amp; scope</p> <p>Architecture &amp; inductive biases</p> <p>Inputs, tokenization, constraints</p> <p>Pooling / subject-level embedding</p> <p>Typical dims and projector guidance</p> <p>Strengths / limitations</p> <p>Representative results (relevant to us)</p> <p>Implications for our pipeline</p> <p>References/links</p>"},{"location":"models/brain/","title":"\ud83e\udde0 Brain Foundation Models","text":"<p>Neuroimaging foundation models for brain representation learning</p> <p>This section documents the brain imaging foundation models that extract embeddings from structural MRI (sMRI), functional MRI (fMRI), and other brain imaging modalities for downstream integration with genomic data, behavioral phenotypes, and clinical outcomes.</p>"},{"location":"models/brain/#overview","title":"\ud83d\udccb Overview","text":"<p>All brain FMs documented here:</p> <ul> <li>Operate on neuroimaging data (volumetric MRI, parcel time series, or raw BOLD signals)</li> <li>Support subject-level embeddings via aggregation across spatial regions or temporal windows</li> <li>Are pretrained on large multi-site datasets (UK Biobank, HCP, ABCD, etc.)</li> <li>Enable cross-modal alignment with genomic and behavioral representations</li> </ul>"},{"location":"models/brain/#model-registry","title":"\ud83c\udfaf Model Registry","text":"Model Modality Architecture Key Feature Documentation \ud83e\udde0 BrainLM fMRI ViT-MAE Masked autoencoding; site-robust Walkthrough \ud83e\udde0 Brain-JEPA fMRI JEPA Joint-embedding prediction; lower-latency Walkthrough \ud83e\udde0 Brain Harmony sMRI + fMRI ViT + TAPE Multi-modal fusion via TAPE Walkthrough \ud83e\udde0 BrainMT sMRI + fMRI Mamba-Transformer Efficient long-range dependencies Walkthrough \ud83e\udde0 SwiFT fMRI Swin Transformer Hierarchical spatiotemporal modeling Walkthrough"},{"location":"models/brain/#usage-workflow","title":"\ud83d\udd04 Usage Workflow","text":""},{"location":"models/brain/#for-fmri-models-brainlm-brain-jepa-swift","title":"For fMRI models (BrainLM, Brain-JEPA, SwiFT)","text":"<ol> <li>Preprocess rs-fMRI: parcellation (Schaefer/AAL), bandpass filter, motion scrubbing</li> <li>Tokenize parcel time series (or 4D volumes for SwiFT)</li> <li>Embed via pretrained encoder</li> <li>Pool to subject-level representation (mean over tokens/time)</li> <li>Project to 512-D for cross-modal alignment</li> </ol>"},{"location":"models/brain/#for-smri-models-brainmt-brain-harmony","title":"For sMRI models (BrainMT, Brain Harmony)","text":"<ol> <li>Run FreeSurfer or FSL FAST for tissue segmentation</li> <li>Extract IDPs (cortical thickness, subcortical volumes) or feed raw T1w volumes</li> <li>Embed via pretrained encoder</li> <li>Pool to subject-level representation</li> <li>Project to 512-D for fusion</li> </ol>"},{"location":"models/brain/#key-considerations","title":"\ud83d\udd11 Key Considerations","text":""},{"location":"models/brain/#sitescanner-harmonization","title":"Site/scanner harmonization","text":"<p>Multi-site pretraining (e.g., BrainLM on UKB+HCP) improves site robustness, but residualize scanner/site effects before fusion:</p> <ul> <li>Regress site dummy variables from embeddings</li> <li>Use ComBat or similar harmonization if needed (see Integration Strategy)</li> </ul>"},{"location":"models/brain/#motion-artifacts","title":"Motion artifacts","text":"<p>fMRI embeddings are sensitive to head motion. Quality control:</p> <ul> <li>Exclude high-motion frames (FD &gt; 0.5 mm)</li> <li>Regress mean FD as confound in downstream prediction</li> <li>Report motion distributions stratified by diagnosis (e.g., ADHD vs TD)</li> </ul>"},{"location":"models/brain/#multimodal-fusion","title":"Multimodal fusion","text":"<p>Brain Harmony natively fuses sMRI and fMRI via TAPE (Target-Aware Projection Ensemble). For other models, use late fusion (concatenate embeddings) or two-tower contrastive alignment (see Design Patterns).</p>"},{"location":"models/brain/#integration-targets","title":"\ud83d\udd17 Integration Targets","text":"<p>Brain embeddings are integrated with:</p> <ul> <li>Genetics embeddings (Caduceus, DNABERT-2) for gene\u2013brain association discovery</li> <li>Behavioral phenotypes (cognitive scores, psychiatric diagnoses) via multimodal prediction</li> <li>Clinical data (longitudinal assessments, EHR records) for developmental trajectories</li> </ul> <p>Learn more: - Integration Strategy - Fusion protocols - Modality Features: sMRI - sMRI preprocessing - Modality Features: fMRI - fMRI preprocessing</p>"},{"location":"models/brain/#source-repositories","title":"\ud83d\udce6 Source Repositories","text":"Click to view all source repositories  All brain FM source code lives in `external_repos/`:  | Model | Local Path | GitHub Repository | |-------|------------|-------------------| | BrainLM | `external_repos/brainlm/` | [vandijklab/BrainLM](https://github.com/vandijklab/BrainLM) | | Brain-JEPA | `external_repos/brainjepa/` | [janklees/brainjepa](https://github.com/janklees/brainjepa) | | Brain Harmony | `external_repos/brainharmony/` | [hzlab/Brain-Harmony](https://github.com/hzlab/Brain-Harmony) | | BrainMT | `external_repos/brainmt/` | [arunkumar-kannan/brainmt-fmri](https://github.com/arunkumar-kannan/brainmt-fmri) | | SwiFT | `external_repos/swift/` | [Transconnectome/SwiFT](https://github.com/Transconnectome/SwiFT) |  Each model page includes: - \u2705 Detailed code walkthrough in `docs/code_walkthroughs/` - \u2705 Structured YAML card in `kb/model_cards/` - \u2705 Integration recipes and preprocessing specs"},{"location":"models/brain/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>\u2705 Validate brain embedding reproducibility across cohorts (UK Biobank, Cha Hospital developmental cohort)</li> <li>\u2705 Benchmark fMRI encoder stability across different parcellation schemes (Schaefer 100/200/400, AAL)</li> <li>\ud83d\udd2c Explore EEG/EPhys foundation models for pediatric/clinical settings (e.g., LaBraM, TBD)</li> <li>\ud83d\udd2c Integrate diffusion MRI embeddings for white matter microstructure (exploratory)</li> </ul>"},{"location":"models/brain/brainharmony/","title":"Brain Harmony","text":""},{"location":"models/brain/brainharmony/#overview","title":"Overview","text":"<p>Type: Multi-modal brain foundation model Architecture: ViT + TAPE (Temporal Adaptive Patch Embedding) Modalities: sMRI + fMRI (unified) Primary use: Cross-modal brain embeddings for heterogeneous cohorts</p>"},{"location":"models/brain/brainharmony/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Brain Harmony addresses a critical challenge in multi-site neuroimaging: heterogeneous TRs and scanning protocols. By introducing TAPE (Temporal Adaptive Patch Embedding), the model resizes temporal tokens to a fixed duration \u03c4, enabling unified processing of fMRI data with variable repetition times. Hub tokens fuse sMRI and fMRI modalities into a shared representation space.</p> <p>Key innovation: TAPE + hub tokens allow robust multimodal fusion even when different sites use different TR/scanner configurations.</p>"},{"location":"models/brain/brainharmony/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: Vision Transformer with TAPE for fMRI, standard patches for sMRI</li> <li>TAPE mechanism: Resizes temporal patches to fixed \u03c4 duration regardless of TR</li> <li>Hub tokens: Cross-modal attention for sMRI \u2194 fMRI fusion</li> <li>Input: T1w structural scans + parcel time series</li> <li>Output: Unified multimodal subject embeddings</li> </ul>"},{"location":"models/brain/brainharmony/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/brainharmony/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>multimodal_brain_harmony_v1</code> - Extract both sMRI and fMRI features through shared encoder - Hub tokens aggregate cross-modal information - Project to 512-D unified representation - Residualize: age, sex, site, scanner, ICV (sMRI), mean FD (fMRI)</p> <p>Fusion targets: - Gene-brain-behavior triangulation: Single unified brain vector + genomics - Multi-site robustness: Critical for UKB + Cha Hospital + ABCD combinations - Developmental trajectories: Handle TR changes across pediatric age ranges</p>"},{"location":"models/brain/brainharmony/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Brain Harmony exemplifies modality-adaptive fusion for Brain-Omics systems: - TAPE-style mechanisms can extend to other time-varying modalities (EEG, longitudinal behavior) - Hub tokens provide blueprint for cross-modal attention in gene-brain-language models - TR heterogeneity handling essential for federated Brain-Omics Model (BOM) deployment</p>"},{"location":"models/brain/brainharmony/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess sMRI \u2192 FreeSurfer / volumetric tensor\n# 2. Preprocess fMRI \u2192 parcellate + retain TR metadata\n# 3. Load pretrained Brain Harmony checkpoint\n# 4. Forward pass with TAPE temporal adaptation\n# 5. Extract hub token embeddings (not individual modality tokens)\n# 6. Project to 512-D if needed\n# 7. Log embedding_strategy ID + TR range in metadata\n</code></pre>"},{"location":"models/brain/brainharmony/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/brainharmony/#strengths","title":"Strengths","text":"<ul> <li>TR heterogeneity handling: TAPE critical for multi-site/longitudinal studies</li> <li>Multi-modal fusion: Native sMRI+fMRI joint embeddings</li> <li>Hub token architecture: Flexible attention mechanism for modality integration</li> <li>Practical engineering: Addresses real-world scanning protocol variations</li> </ul>"},{"location":"models/brain/brainharmony/#limitations","title":"Limitations","text":"<ul> <li>Higher complexity: TAPE + hub tokens increase training/inference cost</li> <li>Engineering overhead: More complex than single-modality encoders</li> <li>Limited public checkpoints: Newer model, fewer pretrained weights available</li> <li>Overkill for homogeneous cohorts: If TR is fixed, simpler models may suffice</li> </ul>"},{"location":"models/brain/brainharmony/#when-to-use-brain-harmony","title":"When to Use Brain Harmony","text":"<p>\u2705 Use when: - Combining UKB (TR=0.72s) + HCP (TR=0.72s) + Cha Hospital (TR=TBD) + ABCD (TR=0.8s) - Need both structural and functional information in single embedding - Site/scanner heterogeneity limits other approaches - Preparing for ARPA-H-style federated multimodal systems</p> <p>\u26a0\ufe0f Consider alternatives: - Late fusion (BrainLM + FreeSurfer): Simpler baseline if TR is homogeneous - BrainMT: If temporal modeling more critical than structural integration - SwiFT: For 4D volumetric approaches without explicit parcellation</p>"},{"location":"models/brain/brainharmony/#reference-materials","title":"Reference Materials","text":""},{"location":"models/brain/brainharmony/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): Brain Harmony (2025) - Code walkthrough: Brain Harmony walkthrough - Model card (YAML): <code>kb/model_cards/brainharmony.yaml</code> - Paper card (YAML): <code>kb/paper_cards/brainharmony_2025.yaml</code></p> <p>Integration recipes: - Modality Features: sMRI - Modality Features: fMRI - Integration Strategy</p>"},{"location":"models/brain/brainharmony/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/brainharmony/</code> - Official GitHub: hzlab/Brain-Harmony</p> <p>Original paper: - Title: \"Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens\" - Authors: Dong, Zijian; Li, Ruilin; et al. - Published: NeurIPS 2025 - Link: arXiv:2509.24693 - DOI: 10.48550/arXiv.2509.24693 - PDF (local): brainharmony_2025.pdf</p>"},{"location":"models/brain/brainharmony/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>TR profiling: Document TR distributions across UKB/Cha Hospital/ABCD</li> <li>Baseline comparison: Brain Harmony vs. late fusion of BrainLM+FreeSurfer</li> <li>Hub token analysis: Visualize what cross-modal patterns hub tokens capture</li> <li>Gene-multimodal-brain CCA: Test whether unified embeddings improve genetics alignment</li> <li>ARPA-H scalability: Evaluate TAPE mechanism for EEG time-varying modalities</li> </ol>"},{"location":"models/brain/brainjepa/","title":"Brain-JEPA","text":""},{"location":"models/brain/brainjepa/#overview","title":"Overview","text":"<p>Type: Joint-Embedding Predictive Architecture for fMRI Architecture: JEPA with functional gradient positioning Modality: Functional MRI (parcel time series) Primary use: Semantic-consistent subject embeddings for zero-shot and linear probing</p>"},{"location":"models/brain/brainjepa/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Brain-JEPA extends JEPA (Joint-Embedding Predictive Architecture) to fMRI by learning latent representations that predict masked brain regions without pixel-level reconstruction. The model emphasizes semantic consistency across brain states by using functional gradient positioning and spatiotemporal masking strategies (Cross-ROI, Cross-Time).</p> <p>Key innovation: Avoids reconstruction loss collapse; achieves better linear probe performance than MAE-based approaches on reported benchmarks.</p>"},{"location":"models/brain/brainjepa/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: JEPA encoder-predictor with functional gradient positional encoding</li> <li>Input: Parcel time series (ROI \u00d7 timepoints)</li> <li>Pretraining: Predict latent representations of masked regions/timeframes</li> <li>Masking: Cross-ROI (spatial) and Cross-Time (temporal) strategies</li> <li>Output: Token latents \u2192 pooled to compact subject vectors</li> </ul>"},{"location":"models/brain/brainjepa/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/brainjepa/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>rsfmri_brainjepa_roi_v1</code> - Extract token latents from pretrained encoder (no reconstruction decoder) - Pool latent tokens \u2192 subject-level embedding - Project to 512-D for downstream tasks - Residualize: age, sex, site, mean FD</p> <p>Fusion targets: - Gene-brain alignment: Late fusion with genomic embeddings (Caduceus, Evo2) - Behavioral prediction: Cognitive scores, psychiatric diagnoses - Zero-shot transfer: Leverage semantic consistency for unseen tasks</p>"},{"location":"models/brain/brainjepa/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Brain-JEPA provides lower-latency fMRI encoding compared to full autoencoding: - No reconstruction decoder \u2192 faster inference for large-scale screening - Semantic latents align well with language/vision embeddings in multimodal hubs - Functional gradient positioning preserves anatomical relationships for cross-modal reasoning</p>"},{"location":"models/brain/brainjepa/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess fMRI \u2192 parcellate (standard atlas)\n# 2. Load pretrained Brain-JEPA encoder (not predictor/decoder)\n# 3. Forward pass \u2192 extract token latents\n# 4. Pool (mean/attention) \u2192 subject embedding\n# 5. Optional: Apply harmonization before projection\n# 6. Log embedding_strategy ID: rsfmri_brainjepa_roi_v1\n</code></pre>"},{"location":"models/brain/brainjepa/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/brainjepa/#strengths","title":"Strengths","text":"<ul> <li>Better linear probing: Reported improvements over MAE on cognitive/behavioral tasks</li> <li>Lower inference cost: No reconstruction decoder needed at embedding extraction time</li> <li>Semantic consistency: Latent predictions enforce functional coherence</li> <li>Interpretability: Functional gradient positioning maintains anatomical structure</li> </ul>"},{"location":"models/brain/brainjepa/#limitations","title":"Limitations","text":"<ul> <li>Heavier engineering: JEPA training more complex than standard MAE</li> <li>Less mature ecosystem: Fewer public checkpoints vs. BrainLM</li> <li>Requires careful masking: Cross-ROI/Time strategies need domain expertise</li> <li>Limited long-context claims: Not explicitly designed for ultra-long temporal dependencies</li> </ul>"},{"location":"models/brain/brainjepa/#when-to-use-brain-jepa","title":"When to Use Brain-JEPA","text":"<p>\u2705 Use when: - Need semantic consistency for zero-shot/few-shot tasks - Want faster inference than full autoencoding models - Prioritize linear probe performance over reconstruction fidelity</p> <p>\u26a0\ufe0f Consider alternatives: - BrainLM: More mature, extensive benchmarks, simpler architecture - BrainMT: For long-range temporal modeling with Mamba blocks - Brain Harmony: Multi-modal sMRI+fMRI fusion - SwiFT: 4D volume input without parcellation</p>"},{"location":"models/brain/brainjepa/#reference-materials","title":"Reference Materials","text":""},{"location":"models/brain/brainjepa/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): Brain-JEPA (2024) - Code walkthrough: Brain-JEPA walkthrough - Model card (YAML): <code>kb/model_cards/brainjepa.yaml</code> - Paper card (YAML): <code>kb/paper_cards/brainjepa_2024.yaml</code></p> <p>Integration recipes: - Modality Features: fMRI - Integration Strategy - Design Patterns</p>"},{"location":"models/brain/brainjepa/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/brainjepa/</code> - Official GitHub: janklees/brainjepa</p> <p>Original paper: - Title: \"Brain-JEPA: Brain Dynamics Foundation Model with Joint-Embedding Predictive Architecture\" - Authors: Wang, Richard; et al. - Published: arXiv preprint, 2024 - Link: arXiv:2409.19407 - PDF (local): brainjepa_2024.pdf</p>"},{"location":"models/brain/brainjepa/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Benchmark vs. BrainLM: Compare linear probe performance on UKB cognitive tasks</li> <li>Latency profiling: Quantify inference speedup vs. full MAE reconstruction</li> <li>Gene-brain fusion: Test whether semantic latents improve CCA with genomic features</li> <li>Zero-shot evaluation: Assess transfer to Cha Hospital developmental cohort</li> <li>Multimodal alignment: Explore projection into shared LLM embedding space</li> </ol>"},{"location":"models/brain/brainlm/","title":"BrainLM","text":""},{"location":"models/brain/brainlm/#overview","title":"Overview","text":"<p>Type: Self-supervised foundation model for fMRI Architecture: Vision Transformer with Masked Autoencoding (ViT-MAE) Modality: Functional MRI (parcel time series) Primary use: Subject-level embeddings for downstream prediction tasks</p>"},{"location":"models/brain/brainlm/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>BrainLM applies masked autoencoding to fMRI parcel time series, learning site-invariant brain representations through large-scale multi-cohort pretraining (UK Biobank + HCP). The model reconstructs masked parcels across time, forcing the encoder to capture functional relationships and temporal dynamics without relying on task-specific supervision.</p> <p>Key innovation: Site-robust pretraining enables strong linear probe performance and generalization across diverse cohorts.</p>"},{"location":"models/brain/brainlm/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: ViT-MAE with spatial-temporal masking</li> <li>Input: Parcel time series (e.g., Schaefer-400 @ TR=0.72s)</li> <li>Pretraining: Mask random parcels/timepoints \u2192 reconstruct from latent tokens</li> <li>Output: Subject-level embeddings via mean pooling over latent tokens</li> </ul>"},{"location":"models/brain/brainlm/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/brainlm/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>rsfmri_brainlm_segments_v1</code> - Extract latent embeddings from pretrained encoder - Mean pool over time/tokens \u2192 subject vector - Project to 512-D for cross-modal alignment - Residualize: age, sex, site, mean FD, tSNR</p> <p>Fusion targets: - Gene-brain associations: Late fusion with Caduceus/DNABERT-2 embeddings - Behavioral prediction: MDD, fluid intelligence, cognitive composites - Developmental trajectories: Longitudinal cohorts (Cha Hospital, ABCD)</p>"},{"location":"models/brain/brainlm/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>BrainLM serves as a brain modality encoder in larger multimodal systems: - Embeddings can be projected into shared LLM/VLM spaces for cross-modal reasoning - Site-robust features critical for federated/multi-institution Brain-Omics Models - Natural baseline before escalating to multimodal encoders (Brain Harmony, BrainMT)</p>"},{"location":"models/brain/brainlm/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess fMRI \u2192 parcellate (Schaefer-400)\n# 2. Load pretrained BrainLM checkpoint\n# 3. Extract latent tokens (no masking during inference)\n# 4. Pool to subject vector\n# 5. Apply harmonization (ComBat/MURD) if needed\n# 6. Log embedding strategy ID in experiment config\n</code></pre>"},{"location":"models/brain/brainlm/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/brainlm/#strengths","title":"Strengths","text":"<ul> <li>Multi-site robustness: Pretraining on UKB+HCP reduces site effects</li> <li>Strong baselines: High linear probe accuracy on cognitive/behavioral tasks</li> <li>Computational efficiency: ViT inference faster than recurrent/SSM alternatives</li> <li>Well-documented: Extensive benchmarks vs. classical FC approaches</li> </ul>"},{"location":"models/brain/brainlm/#limitations","title":"Limitations","text":"<ul> <li>Requires parcellation: No raw 4D volume support (unlike SwiFT/BrainMT)</li> <li>Fixed TR assumption: Variable TR cohorts need TAPE-style adaptation</li> <li>Embedding interpretability: Latent space less directly tied to functional networks than FC matrices</li> </ul>"},{"location":"models/brain/brainlm/#when-to-use-brainlm","title":"When to Use BrainLM","text":"<p>\u2705 Use when: - Starting fMRI integration baselines (Option B in Nov 2025 plan) - Need site-robust features across UKB/HCP/developmental cohorts - Want efficient inference for large-N experiments</p> <p>\u26a0\ufe0f Consider alternatives: - Brain-JEPA: Lower latency, better semantic consistency claims - Brain Harmony: Multi-modal sMRI+fMRI fusion with TAPE for TR heterogeneity - BrainMT: Long-range temporal dependencies via Mamba blocks - SwiFT: 4D volume input without explicit parcellation</p>"},{"location":"models/brain/brainlm/#reference-materials","title":"Reference Materials","text":""},{"location":"models/brain/brainlm/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): BrainLM (2024) - Code walkthrough: BrainLM walkthrough - Model card (YAML): <code>kb/model_cards/brainlm.yaml</code> - Paper card (YAML): <code>kb/paper_cards/brainlm_2024.yaml</code></p> <p>Integration recipes: - Modality Features: fMRI - Integration Strategy - CCA + Permutation Recipe</p>"},{"location":"models/brain/brainlm/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/brainlm/</code> - Official GitHub: vandijklab/BrainLM</p> <p>Original paper: - Title: \"BrainLM: A foundation model for brain activity recordings\" - Authors: Talukder et al. - Published: 2024 - Link: bioRxiv/publication link - PDF (local): brainlm_2024.pdf</p>"},{"location":"models/brain/brainlm/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Validate extraction: Ensure consistent embeddings across UKB/Cha Hospital cohorts</li> <li>Benchmark stability: Test across different parcellation schemes (Schaefer 100/200/400)</li> <li>Gene-brain CCA: Align BrainLM embeddings with Caduceus gene vectors</li> <li>Fusion experiments: Compare late fusion vs. two-tower contrastive alignment</li> <li>Developmental extension: Adapt to pediatric fMRI (shorter scans, higher motion)</li> </ol>"},{"location":"models/brain/brainmt/","title":"BrainMT","text":""},{"location":"models/brain/brainmt/#overview","title":"Overview","text":"<p>Type: Hybrid State Space + Transformer for fMRI Architecture: Mamba blocks + Multi-Head Self-Attention (Hybrid SSM-Transformer) Modality: Functional MRI (3D volumes or parcels) Primary use: Long-range temporal dependency modeling with computational efficiency</p>"},{"location":"models/brain/brainmt/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>BrainMT fuses bidirectional Mamba blocks (State Space Models with temporal-first scanning) with Transformer attention to model long-range fMRI dependencies more efficiently than pure transformers. The architecture targets multitask learning across fluid intelligence regression, sex classification, and harmonization tasks on UKB/HCP cohorts.</p> <p>Key innovation: Mamba's sub-quadratic complexity enables processing longer temporal sequences (\u2265200 frames) without the memory explosion of full attention.</p>"},{"location":"models/brain/brainmt/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Hybrid blocks: Bidirectional Mamba (temporal scanning) + MHSA (global attention)</li> <li>Patch embedding: 3D Conv \u2192 flatten \u2192 linear projection</li> <li>Temporal modeling: Mamba handles sequence dependencies; attention captures global structure</li> <li>Multitask heads: Shared encoder \u2192 task-specific prediction heads</li> <li>Training: Requires fused CUDA kernels (Mamba-ssm library)</li> </ul>"},{"location":"models/brain/brainmt/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/brainmt/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>rsfmri_brainmt_segments_v1</code> - Extract embeddings from shared encoder (before task heads) - Mean pool over sequence length \u2192 subject vector - Project to 512-D for downstream fusion - Residualize: age, sex, site, mean FD - Metadata requirement: Log sequence length (BrainMT performance depends on context \u2265200)</p> <p>Fusion targets: - Long-context gene-brain alignment: When temporal dynamics critical (e.g., task fMRI) - Developmental trajectories: Pediatric longitudinal fMRI with evolving patterns - Multitask prediction: Joint cognitive + diagnostic tasks</p>"},{"location":"models/brain/brainmt/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>BrainMT demonstrates efficient long-context modeling for multimodal systems: - Mamba architecture adaptable to other sequential modalities (EEG, longitudinal assessments) - Hybrid SSM-Transformer paradigm balances efficiency vs. expressiveness - Multitask framework aligns with Brain-Omics Model (BOM) joint training over diverse phenotypes</p>"},{"location":"models/brain/brainmt/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess fMRI \u2192 3D volumes or parcels (\u2265200 frames preferred)\n# 2. Load pretrained BrainMT checkpoint\n# 3. Forward through encoder (Mamba blocks + MHSA layers)\n# 4. Extract pre-head embeddings (not task-specific outputs)\n# 5. Pool to subject-level vector\n# 6. Log: sequence_length, mamba_config, embedding_strategy_id\n</code></pre>"},{"location":"models/brain/brainmt/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/brainmt/#strengths","title":"Strengths","text":"<ul> <li>Efficient long-context: Mamba scales sub-quadratically vs. full attention</li> <li>Multitask learning: Single encoder serves multiple downstream tasks</li> <li>Hybrid architecture: Balances local temporal patterns (Mamba) + global structure (attention)</li> <li>Benchmarked on UKB/HCP: Published results on fluid intelligence and sex classification</li> </ul>"},{"location":"models/brain/brainmt/#limitations","title":"Limitations","text":"<ul> <li>Heavy dependencies: Requires Mamba-ssm CUDA kernels (custom build)</li> <li>Training complexity: Hybrid architecture harder to debug than pure ViT</li> <li>Checkpoint availability: Fewer public pretrained weights vs. BrainLM</li> <li>Overkill for short sequences: &lt;200 frames may not fully leverage Mamba's strengths</li> </ul>"},{"location":"models/brain/brainmt/#when-to-use-brainmt","title":"When to Use BrainMT","text":"<p>\u2705 Use when: - Need long-context modeling (task fMRI, naturalistic viewing) - Multitask setting with shared encoder across cognitive/diagnostic tasks - Want efficiency gains over pure Transformer for \u2265200 frame sequences - Exploring SSM architectures for neuro-omics applications</p> <p>\u26a0\ufe0f Defer until: - BrainLM/Brain-JEPA baselines exhausted (per Nov 2025 integration plan) - Engineering resources available for custom kernel setup - Sufficient GPU memory for hybrid block training/inference</p> <p>\u26a0\ufe0f Consider alternatives: - BrainLM: Simpler baseline, more mature ecosystem - Brain-JEPA: Faster inference, better for semantic consistency - SwiFT: 4D volumes without explicit sequence modeling - Brain Harmony: Multi-modal sMRI+fMRI fusion</p>"},{"location":"models/brain/brainmt/#reference-materials","title":"Reference Materials","text":""},{"location":"models/brain/brainmt/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): BrainMT (2025) - Code walkthrough: BrainMT walkthrough - Model card (YAML): <code>kb/model_cards/brainmt.yaml</code> - Paper card (YAML): <code>kb/paper_cards/brainmt_2025.yaml</code></p> <p>Integration recipes: - Modality Features: fMRI - Integration Strategy - Design Patterns</p>"},{"location":"models/brain/brainmt/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/brainmt/</code> - Official GitHub: arunkumar-kannan/brainmt-fmri</p> <p>Original paper: - Title: \"BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data\" - Authors: Kannan, Arunkumar; Lindquist, Martin A.; Caffo, Brian - Published: Conference proceedings (SpringerLink), September 2025, pp. 150-160 - Link: SpringerLink - PDF (local): brainmt_2025.pdf</p>"},{"location":"models/brain/brainmt/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Baseline comparisons: BrainMT vs. BrainLM on UKB cognitive tasks (same train/test splits)</li> <li>Sequence length ablation: Test performance vs. context length (100, 200, 400 frames)</li> <li>Gene-brain alignment: Evaluate whether long-context embeddings improve genetics CCA</li> <li>Developmental extension: Adapt to pediatric fMRI (Cha Hospital, ABCD)</li> <li>SSM exploration: Investigate Mamba-style architectures for EEG/EPhys modalities</li> </ol>"},{"location":"models/brain/brainmt/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Capture masking ratio and sequence length in metadata for reproducibility</li> <li>Multitask heads are task-specific; extract shared encoder embeddings for fusion</li> <li>When exporting weights, ensure Mamba kernel version compatibility across systems</li> </ul>"},{"location":"models/brain/swift/","title":"SwiFT","text":""},{"location":"models/brain/swift/#overview","title":"Overview","text":"<p>Type: Spatiotemporal foundation model for fMRI Architecture: Swin Transformer (hierarchical windows) Modality: Functional MRI (4D volumes) Primary use: Direct 4D volume encoding without explicit parcellation</p>"},{"location":"models/brain/swift/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>SwiFT (Swin Transformer for fMRI Time series) applies hierarchical windowed attention to 4D fMRI volumes, eliminating the need for explicit parcellation while capturing spatiotemporal patterns across multiple scales. The model processes raw BOLD signals through cascaded Swin blocks, enabling direct learning from volumetric data.</p> <p>Key innovation: Sequence-free 4D modeling with hierarchical attention windows preserves fine-grained spatial structure while capturing temporal dynamics.</p>"},{"location":"models/brain/swift/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: 4D Swin Transformer with shifted windows</li> <li>Input: Raw BOLD volumes (X \u00d7 Y \u00d7 Z \u00d7 T)</li> <li>Windowing: Hierarchical 4D patches with local/global attention</li> <li>No parcellation: Learns spatial structure end-to-end</li> <li>Output: Subject-level embeddings via global pooling or CLS token</li> </ul>"},{"location":"models/brain/swift/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/brain/swift/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>rsfmri_swift_segments_v1</code> - Process 4D volumes through Swin blocks (typically 20-frame segments) - Extract final layer representations - Pool across spatial-temporal dimensions \u2192 subject vector - Project to 512-D for cross-modal alignment - Residualize: age, sex, site, mean FD</p> <p>Fusion targets: - Gene-brain associations: When fine-grained spatial patterns matter - Atlasing-free analysis: Avoid parcellation scheme dependence - Multi-resolution modeling: Capture both local and global brain dynamics</p>"},{"location":"models/brain/swift/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>SwiFT's hierarchical 4D processing offers advantages for Brain-Omics systems: - No parcellation bias \u2192 better cross-site generalization - Multi-scale attention aligns with hierarchical biological organization - 4D paradigm extensible to other volumetric time series (perfusion imaging, DCE-MRI) - Can serve as blueprint for spatiotemporal EEG source reconstruction</p>"},{"location":"models/brain/swift/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Preprocess fMRI \u2192 motion correction, normalization (no parcellation)\n# 2. Segment into overlapping 4D windows (e.g., 20-frame chunks)\n# 3. Load pretrained SwiFT checkpoint\n# 4. Forward pass through Swin blocks\n# 5. Extract global representation (CLS token or spatial average)\n# 6. Aggregate across segments \u2192 subject embedding\n# 7. Log: window_size, stride, preprocessing_pipeline_id\n</code></pre>"},{"location":"models/brain/swift/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/brain/swift/#strengths","title":"Strengths","text":"<ul> <li>No parcellation required: Learns spatial structure end-to-end</li> <li>Multi-scale processing: Hierarchical windows capture local and global patterns</li> <li>Strong performance: Reported competitive results vs. parcellation-based methods</li> <li>Parcellation-agnostic: No bias from atlas choice (Schaefer vs. AAL vs. Gordon)</li> </ul>"},{"location":"models/brain/swift/#limitations","title":"Limitations","text":"<ul> <li>Computational cost: 4D convolutions and windowed attention memory-intensive</li> <li>Longer training: Hierarchical architecture requires more epochs to converge</li> <li>Preprocessing critical: Motion and spatial normalization quality directly impact performance</li> <li>GPU memory: Full 4D volumes with fine temporal resolution may exceed typical GPU limits</li> </ul>"},{"location":"models/brain/swift/#when-to-use-swift","title":"When to Use SwiFT","text":"<p>\u2705 Use when: - Want to avoid parcellation scheme dependence - Need fine-grained spatial analysis (subcortical structures, small nuclei) - Have sufficient compute for 4D volume processing - Exploring multi-resolution spatiotemporal patterns</p> <p>\u26a0\ufe0f Consider alternatives: - BrainLM/Brain-JEPA: If parcellation acceptable and want faster baselines - BrainMT: For longer temporal contexts with lower memory footprint - Brain Harmony: Multi-modal sMRI+fMRI fusion with TAPE</p>"},{"location":"models/brain/swift/#reference-materials","title":"Reference Materials","text":""},{"location":"models/brain/swift/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): SwiFT (2023) - Paper card (YAML): <code>kb/paper_cards/swift_2023.yaml</code> - Code walkthrough: SwiFT walkthrough - Model card (YAML): <code>kb/model_cards/swift.yaml</code></p> <p>Integration recipes: - Modality Features: fMRI - Integration Strategy - Preprocessing Pipelines</p>"},{"location":"models/brain/swift/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/swift/</code> - Official GitHub: Transconnectome/SwiFT</p> <p>Original paper: - Title: \"SwiFT: Swin 4D fMRI Transformer\" - Authors: Kim, Peter Yongho; Kwon, Junbeom; Joo, Sunghwan; Bae, Sangyoon; Lee, Donggyu; Jung, Yoonho; Yoo, Shinjae; Cha, Jiook; Moon, Taesup - Published: NeurIPS 2023 - Link: arXiv:2307.05916 - DOI: 10.48550/arXiv.2307.05916 - PDF (local): swift_2023.pdf</p>"},{"location":"models/brain/swift/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Parcellation comparison: SwiFT vs. BrainLM (Schaefer-400) on same UKB cognitive tasks</li> <li>Memory profiling: Document GPU requirements across different volume resolutions</li> <li>Preprocessing sensitivity: Test robustness to motion correction/spatial normalization choices</li> <li>Gene-brain fusion: Evaluate whether 4D embeddings improve genetics alignment</li> <li>Developmental adaptation: Assess performance on pediatric datasets with smaller brain volumes</li> </ol>"},{"location":"models/brain/swift/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Segment long scans into overlapping windows to fit GPU memory</li> <li>Log window size, stride, and overlap for reproducibility</li> <li>Spatial normalization quality critical \u2014 consider using MURD/ComBat preprocessing</li> <li>When comparing to parcellation-based models, ensure fair preprocessing parity</li> </ul>"},{"location":"models/genetics/","title":"\ud83e\uddec Genetics Foundation Models","text":"<p>DNA sequence foundation models for genomic representation learning</p> <p>This section documents the genetics foundation models used to extract gene-level embeddings from raw genomic sequences (DNA/RNA) for downstream integration with brain imaging, behavioral phenotypes, and clinical outcomes.</p>"},{"location":"models/genetics/#overview","title":"\ud83d\udccb Overview","text":"<p>All genetics FMs documented here:</p> <ul> <li>Operate on nucleotide sequences (A, C, G, T) rather than pre-computed variant calls or SNP arrays</li> <li>Support gene-level embeddings via forward/reverse-complement (RC) averaging and pooling strategies</li> <li>Enable interpretability through attribution methods like LOGO \u0394AUC</li> <li>Are pretrained on large genomic corpora (human reference genomes, multi-species datasets, or RefSeq)</li> </ul>"},{"location":"models/genetics/#model-registry","title":"\ud83c\udfaf Model Registry","text":"Model Architecture Key Feature Integration Role Documentation \ud83e\uddec Caduceus Mamba (BiMamba) + RC-equivariance Strand-robust, efficient long-context Primary gene encoder for UK Biobank WES Walkthrough \ud83e\uddec DNABERT-2 BERT (multi-species) BPE tokenization, cross-species transfer Alternative gene encoder; comparison baseline Walkthrough \ud83e\uddec Evo 2 StripedHyena (1M context) Ultra-long-range dependencies Exploratory; regulatory region capture Walkthrough \ud83e\uddec GENERator Generative 6-mer LM Generative modeling, sequence design Reference for generative vs discriminative Walkthrough \ud83e\uddec HyenaDNA Hyena implicit convolutions (1M context) Single-nucleotide, ultra-long genomic modeling Conceptual long-context genomics reference Architecture walkthrough"},{"location":"models/genetics/#usage-workflow","title":"\ud83d\udd04 Usage Workflow","text":"<ol> <li>Extract sequences from reference genome (hg38) for target genes</li> <li>Tokenize using model-specific vocabularies (k-mers, BPE, or single-nucleotide)</li> <li>Embed forward and reverse-complement sequences</li> <li>Pool to gene-level representation (mean/CLS depending on model)</li> <li>Project to 512-D for cross-modal alignment with brain embeddings</li> </ol>"},{"location":"models/genetics/#key-considerations","title":"\ud83d\udd11 Key Considerations","text":""},{"location":"models/genetics/#rc-equivariance","title":"RC-equivariance","text":"<p>DNA has no inherent directionality; models like Caduceus enforce BiMamba RC-equivariance to avoid strand bias. For non-equivariant models, manually average forward and RC embeddings.</p>"},{"location":"models/genetics/#variant-handling","title":"Variant handling","text":"<p>Foundation models operate on reference sequences by default. To incorporate subject-specific variants:</p> <ul> <li>Patch reference with VCF alleles</li> <li>Re-embed variant sequences</li> <li>Compare \u0394AUC between reference and variant embeddings (exploratory)</li> </ul>"},{"location":"models/genetics/#attribution","title":"Attribution","text":"<p>Use LOGO (Leave-One-Gene-Out) \u0394AUC to assess which genes contribute most to downstream prediction tasks (e.g., MDD risk, cognitive scores). See Yoon et al. BioKDD 2025 for protocol details.</p>"},{"location":"models/genetics/#integration-targets","title":"\ud83d\udd17 Integration Targets","text":"<p>Genetics embeddings are integrated with:</p> <ul> <li>sMRI IDPs (structural phenotypes) via CCA, late fusion, or contrastive alignment</li> <li>fMRI embeddings (e.g., BrainLM, Brain-JEPA) for gene\u2013brain\u2013behaviour triangulation</li> <li>Behavioral phenotypes (cognitive scores, psychiatric diagnoses) via multimodal prediction</li> </ul> <p>Learn more: - Integration Strategy - Fusion protocols - Modality Features: Genomics - Preprocessing specs</p>"},{"location":"models/genetics/#source-repositories","title":"\ud83d\udce6 Source Repositories","text":"Click to view all source repositories  All genetics FM source code lives in `external_repos/`:  | Model | Local Path | GitHub Repository | |-------|------------|-------------------| | Caduceus | `external_repos/caduceus/` | [kuleshov-group/caduceus](https://github.com/kuleshov-group/caduceus) | | DNABERT-2 | `external_repos/dnabert2/` | [Zhihan1996/DNABERT2](https://github.com/Zhihan1996/DNABERT2) | | Evo 2 | `external_repos/evo2/` | [ArcInstitute/evo2](https://github.com/ArcInstitute/evo2) | | GENERator | `external_repos/generator/` | [GenerTeam/GENERator](https://github.com/GenerTeam/GENERator) |  Each model page includes: - \u2705 Detailed code walkthrough in `docs/code_walkthroughs/` - \u2705 Structured YAML card in `kb/model_cards/` - \u2705 Integration recipes and preprocessing specs"},{"location":"models/genetics/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>\u2705 Validate gene embedding reproducibility across cohorts (UK Biobank WES, Cha Hospital panel sequencing)</li> <li>\u2705 Benchmark LOGO \u0394AUC stability under different embedding projection dimensions (256, 512, 1024)</li> <li>\ud83d\udd2c Explore regulatory region embeddings (enhancers, promoters) with long-context models like Evo 2</li> </ul>"},{"location":"models/genetics/caduceus/","title":"Caduceus","text":""},{"location":"models/genetics/caduceus/#overview","title":"Overview","text":"<p>Type: RC-equivariant DNA foundation model Architecture: BiMamba (bidirectional Mamba) + Hyena Modality: Nucleotide sequences (DNA/RNA) Primary use: Strand-robust gene-level embeddings for multimodal fusion</p>"},{"location":"models/genetics/caduceus/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Caduceus enforces reverse-complement (RC) equivariance through bidirectional Mamba/Hyena layers, ensuring embeddings are invariant to DNA strand orientation. This addresses a fundamental biological constraint: DNA has no inherent directionality, yet many language model architectures introduce strand bias. Caduceus learns sequence grammar while respecting this symmetry.</p> <p>Key innovation: RC-equivariance as architectural constraint (not post-hoc averaging) \u2192 more parameter-efficient and biologically principled.</p>"},{"location":"models/genetics/caduceus/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: BiMamba (Mamba blocks with bidirectional RC scanning) or RC-augmented Hyena</li> <li>RC enforcement: Built into attention/convolution layers</li> <li>Input: Raw nucleotide sequences (A, C, G, T) with k-mer or single-base tokenization</li> <li>Pretraining: Masked language modeling on large genomic corpora (human + multi-species)</li> <li>Output: Per-position embeddings \u2192 gene-level via mean pooling</li> </ul>"},{"location":"models/genetics/caduceus/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/caduceus/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>genetics_gene_fm_pca512_v1</code> (Caduceus variant) - Extract gene sequences from reference genome (hg38) - Tokenize with Caduceus vocabulary (typically 4-mer or 6-mer) - Forward pass \u2192 per-nucleotide embeddings - RC hygiene: Caduceus natively RC-equivariant, but verify with sanity check (forward == RC) - Mean pool over gene length \u2192 gene-level vector - Concatenate target gene set (e.g., 38 MDD genes from Yoon et al.) - Project to 512-D via PCA - Residualize: age, sex, ancestry PCs (1-10), batch</p> <p>Fusion targets: - Gene-brain CCA: Align with BrainLM/Brain-JEPA embeddings - LOGO attribution: Leave-one-gene-out \u0394AUC for gene importance (Yoon et al. protocol) - Variant impact: Compare reference vs. subject-specific sequences (exploratory)</p>"},{"location":"models/genetics/caduceus/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Caduceus provides strand-robust genetic representations for Brain-Omics systems: - RC-equivariance critical when sequences are sampled from forward or reverse strands - Gene embeddings can be projected into shared LLM/VLM spaces for cross-modal reasoning - Efficient Mamba architecture scales to whole-genome or regulatory region analysis - Natural encoder for \"genetic modality\" in unified multimodal Brain-Omics Model (BOM)</p>"},{"location":"models/genetics/caduceus/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Extract gene sequences from hg38 reference (GENCODE annotations)\n# 2. Tokenize with Caduceus vocabulary (e.g., 6-mer overlapping)\n# 3. Load pretrained Caduceus checkpoint\n# 4. Forward pass \u2192 per-position embeddings\n# 5. Verify RC equivariance (optional but recommended):\n#    embed(seq) \u2248 embed(reverse_complement(seq))\n# 6. Mean pool over gene \u2192 gene-level vector\n# 7. Concatenate gene set \u2192 subject genotype embedding\n# 8. Log: gene_list, reference_version, embedding_strategy_id\n</code></pre>"},{"location":"models/genetics/caduceus/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/genetics/caduceus/#strengths","title":"Strengths","text":"<ul> <li>RC-equivariant by design: No manual averaging needed</li> <li>Parameter efficient: Mamba/Hyena scale better than full attention for long sequences</li> <li>Strong benchmarks: Competitive performance on regulatory element prediction, variant effect</li> <li>Interpretable: Attention/conv patterns respect biological constraints</li> </ul>"},{"location":"models/genetics/caduceus/#limitations","title":"Limitations","text":"<ul> <li>Requires RC-aware tokenization: Some vocabularies break RC symmetry (use carefully)</li> <li>Limited to reference sequences: Variant handling requires re-embedding (computationally expensive)</li> <li>Checkpoint availability: Fewer pretrained scales vs. DNABERT-2 or ESM-style models</li> <li>K-mer choice matters: Different tokenizations yield different embedding quality</li> </ul>"},{"location":"models/genetics/caduceus/#when-to-use-caduceus","title":"When to Use Caduceus","text":"<p>\u2705 Use when: - Need strand-robust gene embeddings for UKB/Cha Hospital genetics - Prioritizing parameter efficiency for long sequences (&gt;10kb genes) - Want architectural RC-equivariance (not post-hoc correction) - Implementing LOGO attribution (Yoon et al. protocol)</p> <p>\u26a0\ufe0f Consider alternatives: - DNABERT-2: BPE tokenization, more public checkpoints, cross-species pretraining - Evo2: Ultra-long context (1M tokens) for regulatory regions - GENERator: Generative modeling if sequence design is goal</p>"},{"location":"models/genetics/caduceus/#reference-materials","title":"Reference Materials","text":""},{"location":"models/genetics/caduceus/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): Caduceus (2024) - Paper card (YAML): <code>kb/paper_cards/caduceus_2024.yaml</code> - Code walkthrough: Caduceus walkthrough - Model card (YAML): <code>kb/model_cards/caduceus.yaml</code></p> <p>Integration recipes: - Modality Features: Genomics - Integration Strategy - CCA + Permutation - LOGO Attribution (experiment config)</p>"},{"location":"models/genetics/caduceus/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/caduceus/</code> - Official GitHub: kuleshov-group/caduceus</p> <p>Original paper: - Title: \"Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling\" - Authors: Schiff et al. - Published: arXiv preprint, March 2024 - Link: arXiv:2403.03234 - PDF (local): caduceus_2024.pdf</p>"},{"location":"models/genetics/caduceus/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>UKB WES extraction: Embed 38 MDD genes + cognition-related genes from UK Biobank</li> <li>RC verification: Sanity check embed(seq) == embed(RC(seq)) on test genes</li> <li>Gene-brain CCA: Align Caduceus embeddings with BrainLM fMRI vectors</li> <li>LOGO protocol: Implement leave-one-gene-out \u0394AUC (Yoon et al. BioKDD'25)</li> <li>Variant exploration: Test impact of subject-specific SNPs on embeddings (pilot)</li> </ol>"},{"location":"models/genetics/caduceus/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Always log k-mer size and tokenization strategy in metadata</li> <li>Verify RC-equivariance on held-out genes before scaling to full cohort</li> <li>When comparing to DNABERT-2, use same gene set and reference version</li> <li>For attribution: LOGO requires nested CV to avoid leakage (see Yoon et al. protocol)</li> </ul>"},{"location":"models/genetics/dnabert2/","title":"DNABERT-2","text":""},{"location":"models/genetics/dnabert2/#overview","title":"Overview","text":"<p>Type: BERT-style DNA foundation model Architecture: BERT with BPE tokenization Modality: Nucleotide sequences (DNA/RNA) Primary use: Cross-species transfer and multi-task gene embeddings</p>"},{"location":"models/genetics/dnabert2/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>DNABERT-2 applies Byte-Pair Encoding (BPE) tokenization to DNA sequences, enabling flexible vocabulary that adapts to sequence statistics. Pretrained on multi-species genomic data, it excels at cross-species transfer and captures evolutionary conservation patterns. Unlike k-mer tokenizers, BPE can learn biologically meaningful subword units (e.g., regulatory motifs, repeat elements).</p> <p>Key innovation: BPE tokenization for genomics + multi-species pretraining \u2192 strong zero-shot transfer to understudied organisms.</p>"},{"location":"models/genetics/dnabert2/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: BERT encoder (bidirectional transformer)</li> <li>Tokenization: BPE vocabulary learned from multi-species corpus</li> <li>Pretraining: Masked language modeling across human + model organisms</li> <li>Context: Typically 512-1024 tokens (depends on checkpoint)</li> <li>Output: Per-token embeddings \u2192 aggregated to gene/region level</li> </ul>"},{"location":"models/genetics/dnabert2/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/dnabert2/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>genetics_gene_fm_pca512_v1</code> (DNABERT-2 variant) - Extract gene sequences from hg38 reference genome - Tokenize with BPE: Use pretrained DNABERT-2 tokenizer (maintain frame awareness) - Forward pass \u2192 per-token embeddings - RC handling: DNABERT-2 not RC-equivariant \u2192 manually average forward and RC embeddings - Pool to gene level (mean or CLS token, validate stability) - Concatenate target gene set - Project to 512-D via PCA - Residualize: age, sex, ancestry PCs, batch</p> <p>Fusion targets: - Gene-brain alignment: Late fusion with brain FM embeddings - Comparison baseline: DNABERT-2 vs. Caduceus RC-equivariance impact - Cross-species validation: Test on mouse/primate orthologs (exploratory)</p>"},{"location":"models/genetics/dnabert2/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>DNABERT-2 provides flexible tokenization for Brain-Omics systems: - BPE adapts to different genomic contexts (coding, regulatory, non-coding) - Multi-species pretraining enables cross-organism comparison (animal models \u2192 human) - Can serve as genetic encoder in unified multimodal architectures - BPE paradigm extensible to other biological sequences (proteins, chromatin states)</p>"},{"location":"models/genetics/dnabert2/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Extract gene sequences (hg38 reference, GENCODE annotations)\n# 2. Tokenize with DNABERT-2 BPE tokenizer\n# 3. Load pretrained checkpoint (e.g., zhihan1996/DNABERT-2-117M)\n# 4. Forward pass \u2192 extract token embeddings\n# 5. **RC correction:** Embed reverse-complement, average with forward\n# 6. Pool tokens \u2192 gene vector (test mean vs. CLS stability)\n# 7. Concatenate gene set \u2192 subject embedding\n# 8. Log: tokenizer_version, pooling_strategy, rc_averaged\n</code></pre>"},{"location":"models/genetics/dnabert2/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/genetics/dnabert2/#strengths","title":"Strengths","text":"<ul> <li>Adaptive tokenization: BPE learns biologically relevant subwords</li> <li>Cross-species transfer: Strong zero-shot performance on new organisms</li> <li>Public checkpoints: Well-supported on Hugging Face (zhihan1996/DNABERT-2-117M)</li> <li>Mature ecosystem: Compatible with transformers library, easy deployment</li> </ul>"},{"location":"models/genetics/dnabert2/#limitations","title":"Limitations","text":"<ul> <li>Not RC-equivariant: Requires manual forward/RC averaging (compute overhead)</li> <li>Tokenization complexity: BPE can introduce subtle biases if not carefully applied</li> <li>Frame shifts: BPE boundaries may not respect codon structure (issue for coding sequences)</li> <li>Longer inference: BERT attention quadratic in sequence length</li> </ul>"},{"location":"models/genetics/dnabert2/#when-to-use-dnabert-2","title":"When to Use DNABERT-2","text":"<p>\u2705 Use when: - Need comparison baseline vs. RC-equivariant models (Caduceus) - Want cross-species transfer capabilities - Prefer mature Hugging Face ecosystem - Exploring BPE tokenization for regulatory elements</p> <p>\u26a0\ufe0f Consider alternatives: - Caduceus: If RC-equivariance critical and want parameter efficiency - Evo2: For ultra-long regulatory contexts (&gt;10kb) - GENERator: If generative modeling is goal</p>"},{"location":"models/genetics/dnabert2/#reference-materials","title":"Reference Materials","text":""},{"location":"models/genetics/dnabert2/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): DNABERT-2 (2024) - Paper card (YAML): <code>kb/paper_cards/dnabert2_2024.yaml</code> (contains structured summary and metadata) - Code walkthrough: DNABERT-2 walkthrough - Model card (YAML): <code>kb/model_cards/dnabert2.yaml</code></p> <p>Integration recipes: - Modality Features: Genomics - Integration Strategy - CCA + Permutation</p>"},{"location":"models/genetics/dnabert2/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/dnabert2/</code> - Official GitHub: Zhihan1996/DNABERT2 - Hugging Face: zhihan1996/DNABERT-2-117M</p> <p>Original paper: - Title: \"DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome\" - Authors: Zhou et al. - Published: arXiv preprint, 2024 - Link: arXiv:2306.15006</p>"},{"location":"models/genetics/dnabert2/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>RC averaging stability: Test embed(forward) vs. mean(embed(forward), embed(RC))</li> <li>Pooling comparison: Mean vs. CLS token for gene-level embeddings</li> <li>Caduceus benchmark: Same gene set, same cohort, compare CCA/prediction performance</li> <li>BPE analysis: Visualize learned tokens, check for motif enrichment</li> <li>Cross-species pilot: If animal model data available, test zero-shot transfer</li> </ol>"},{"location":"models/genetics/dnabert2/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Always RC-average forward and reverse-complement embeddings (critical!)</li> <li>Log tokenizer version and BPE vocabulary size in metadata</li> <li>When comparing to Caduceus, ensure same gene list and reference genome version</li> <li>BPE tokenization is non-deterministic if vocab changes \u2192 freeze tokenizer for reproducibility</li> </ul>"},{"location":"models/genetics/evo2/","title":"Evo 2","text":""},{"location":"models/genetics/evo2/#overview","title":"Overview","text":"<p>Type: Ultra-long-context DNA foundation model Architecture: StripedHyena 2 (Hyena + attention) Modality: Nucleotide sequences (DNA/RNA) Primary use: Regulatory region embeddings with 1M token context</p>"},{"location":"models/genetics/evo2/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Evo2 extends DNA foundation models to 1 million token contexts using StripedHyena 2 architecture (hybrid Hyena operators + attention layers). This enables modeling entire genes with full regulatory context (promoters, enhancers, 3D loop anchors) in a single forward pass, capturing long-range genomic interactions that shorter-context models miss.</p> <p>Key innovation: 1M context via sub-quadratic Hyena operators \u2192 whole-locus modeling including distal regulatory elements.</p>"},{"location":"models/genetics/evo2/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: StripedHyena 2 (alternating Hyena convolution + multi-head attention)</li> <li>Context length: 1,048,576 tokens (~1 Mb of genomic sequence)</li> <li>Tokenization: Single-base or 2-mer/4-mer (preserves fine resolution)</li> <li>Pretraining: Masked LM on human + multi-species genomes</li> <li>Output: Per-position embeddings \u2192 region/gene pooling</li> </ul>"},{"location":"models/genetics/evo2/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/evo2/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>genetics_regulatory_evo2_v1</code> (exploratory) - Extract extended gene loci (gene + 100kb upstream/downstream for regulatory context) - Tokenize with Evo2 vocabulary (typically single-base or 2-mer) - Forward pass \u2192 per-position embeddings for full locus - RC handling: Evo2 not explicitly RC-equivariant \u2192 average forward/RC embeddings - Pool over gene CDS \u2192 gene embedding - Optionally extract regulatory region embeddings (promoter, enhancers) separately - Project to 512-D via PCA - Residualize: age, sex, ancestry PCs, batch</p> <p>Fusion targets: - Gene expression prediction: Regulatory context improves gene-phenotype links - Enhancer-gene mapping: Identify distal elements affecting brain-expressed genes - 3D genome modeling: Capture loop anchors and TAD boundaries (exploratory)</p>"},{"location":"models/genetics/evo2/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Evo2 enables whole-locus genetic representations for Brain-Omics systems: - 1M context captures regulatory grammar spanning hundreds of kilobases - Critical for brain-specific enhancers distant from target genes - Can embed entire pathways or multi-gene clusters in single pass - Blueprint for ultra-long-context multimodal architectures (e.g., long-range EEG patterns)</p>"},{"location":"models/genetics/evo2/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># 1. Extract extended loci (gene \u00b1 100kb from hg38)\n# 2. Tokenize with Evo2 single-base or k-mer vocabulary\n# 3. Load pretrained Evo2 checkpoint\n# 4. Forward pass (may require chunking if &gt;1M tokens)\n# 5. Extract embeddings for:\n#    - Gene CDS (coding sequence)\n#    - Promoter (-2kb to TSS)\n#    - Predicted enhancers (if annotated)\n# 6. RC-average forward + reverse-complement\n# 7. Pool each region \u2192 separate vectors or concatenate\n# 8. Log: context_length, regulatory_elements_included\n</code></pre>"},{"location":"models/genetics/evo2/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/genetics/evo2/#strengths","title":"Strengths","text":"<ul> <li>Ultra-long context: 1M tokens captures distal regulatory elements</li> <li>Whole-locus modeling: No need to manually select regulatory windows</li> <li>Sub-quadratic scaling: Hyena operators enable long context without full attention cost</li> <li>Regulatory grammar: Can learn enhancer-promoter interactions end-to-end</li> </ul>"},{"location":"models/genetics/evo2/#limitations","title":"Limitations","text":"<ul> <li>Massive memory footprint: 1M context requires high-memory GPUs (80GB+ A100/H100)</li> <li>Slower inference: Even with Hyena, 1M tokens slower than short-context models</li> <li>Overkill for coding sequences: Most genes &lt;10kb don't need 1M context</li> <li>Checkpoint availability: Fewer public weights vs. DNABERT-2/Caduceus</li> </ul>"},{"location":"models/genetics/evo2/#when-to-use-evo2","title":"When to Use Evo2","text":"<p>\u2705 Use when: - Need regulatory context for brain-specific gene expression - Studying long-range enhancer-promoter interactions - Have sufficient compute (80GB+ GPU, large batch sizes) - Exploring 3D genome structure embeddings</p> <p>\u26a0\ufe0f Defer until: - Caduceus/DNABERT-2 baselines complete - Regulatory element analysis becomes critical - GPU resources available for long-context experiments</p> <p>\u26a0\ufe0f Consider alternatives: - Caduceus: For coding sequences without regulatory context - DNABERT-2: For standard gene embeddings with manageable compute - GENERator: If generative modeling is priority</p>"},{"location":"models/genetics/evo2/#reference-materials","title":"Reference Materials","text":""},{"location":"models/genetics/evo2/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): Evo2 (2024) - Code walkthrough: Evo2 walkthrough - Model card (YAML): <code>kb/model_cards/evo2.yaml</code> - Paper card (YAML): <code>kb/paper_cards/evo2_2024.yaml</code></p> <p>Integration recipes: - Modality Features: Genomics - Integration Strategy</p>"},{"location":"models/genetics/evo2/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/evo2/</code> - Official GitHub: ArcInstitute/evo2</p> <p>Original paper: - Title: \"Genome modeling and design across all domains of life with Evo 2\" - Authors: Arc Institute Team - Published: bioRxiv preprint, February 2025 - Link: bioRxiv:2025.02.18.638918 - PDF (local): evo2_2024.pdf</p>"},{"location":"models/genetics/evo2/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Pilot study: Embed 5-10 brain-expressed genes with known distal enhancers</li> <li>Context ablation: Test 10kb vs. 100kb vs. 1M context for gene-brain CCA</li> <li>Memory profiling: Document GPU requirements and chunking strategies</li> <li>Enhancer-gene links: Compare Evo2 regulatory embeddings vs. eQTL databases</li> <li>ARPA-H vision: Explore Evo2-style long context for other modalities (EEG, longitudinal)</li> </ol>"},{"location":"models/genetics/evo2/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>GPU requirements: 80GB+ A100 or H100 for full 1M context</li> <li>Chunk long sequences if needed; aggregate chunk embeddings carefully</li> <li>Log context length used (may be &lt;1M for most genes)</li> <li>RC-averaging doubles compute; consider caching forward embeddings</li> <li>When comparing to short-context models, isolate regulatory contribution via ablation</li> </ul>"},{"location":"models/genetics/generator/","title":"GENERator","text":""},{"location":"models/genetics/generator/#overview","title":"Overview","text":"<p>Type: Generative DNA language model Architecture: 6-mer-based autoregressive transformer Modality: Nucleotide sequences (DNA) Primary use: Generative modeling and sequence design (with discriminative embedding extraction)</p>"},{"location":"models/genetics/generator/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>GENERator is a generative DNA language model trained on RefSeq and other genomic corpora using 6-mer tokenization. While primarily designed for sequence generation and design tasks (e.g., synthetic promoter optimization), its learned representations can be extracted for discriminative tasks like gene embedding and downstream fusion.</p> <p>Key innovation: 6-mer vocabulary balances computational tractability with sufficient resolution to capture regulatory motifs and codon structure.</p>"},{"location":"models/genetics/generator/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: Autoregressive transformer (GPT-style)</li> <li>Tokenization: 6-mer overlapping windows (4096-token vocabulary)</li> <li>Pretraining: Next-token prediction on human RefSeq + genomic corpora</li> <li>Generative objective: Likelihood maximization for sequence generation</li> <li>Output: Generative logits (design mode) or hidden states (embedding mode)</li> </ul>"},{"location":"models/genetics/generator/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/generator/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Embedding recipe: <code>genetics_gene_fm_pca512_v1</code> (GENERator variant) - Extract gene sequences from hg38 reference genome - Tokenize with 6-mer overlapping windows - Forward pass \u2192 extract hidden states (not generative logits) - RC handling: GENERator not RC-equivariant \u2192 average forward/RC embeddings - Mean pool over gene length \u2192 gene-level vector - Concatenate target gene set - Project to 512-D via PCA - Residualize: age, sex, ancestry PCs, batch</p> <p>Fusion targets: - Gene-brain alignment: Late fusion with brain FM embeddings - Generative vs. discriminative: Compare GENERator embeddings to Caduceus/DNABERT-2 - Sequence design (exploratory): Generate synthetic regulatory elements with desired properties</p>"},{"location":"models/genetics/generator/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>GENERator demonstrates generative modeling for biological sequences: - Hidden states from generative models can serve as discriminative features - Generative capability enables counterfactual analysis (\"what if this gene sequence changed?\") - 6-mer tokenization preserves codon structure for coding sequence analysis - Blueprint for generative components in multimodal Brain-Omics Model (BOM)</p>"},{"location":"models/genetics/generator/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<pre><code># Discriminative mode (embeddings)\n# 1. Extract gene sequences (hg38 reference)\n# 2. Tokenize with 6-mer overlapping windows\n# 3. Load pretrained GENERator checkpoint\n# 4. Forward pass \u2192 extract hidden states (not output logits)\n# 5. RC-average: embed(seq) and embed(reverse_complement(seq))\n# 6. Mean pool over tokens \u2192 gene embedding\n# 7. Log: token_vocabulary, pooling_layer (e.g., layer -1)\n\n# Generative mode (sequence design)\n# 1. Define target properties (e.g., GC content, expression level)\n# 2. Sample from GENERator with conditioning\n# 3. Validate generated sequences via wet-lab or in-silico assays\n</code></pre>"},{"location":"models/genetics/generator/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/genetics/generator/#strengths","title":"Strengths","text":"<ul> <li>Generative capability: Can design novel sequences (regulatory elements, synthetic genes)</li> <li>6-mer vocabulary: Preserves codon structure, captures motifs</li> <li>Hidden states useful: Discriminative embeddings competitive with specialized models</li> <li>Interpretable: Generative likelihoods inform sequence quality</li> </ul>"},{"location":"models/genetics/generator/#limitations","title":"Limitations","text":"<ul> <li>Not RC-equivariant: Requires manual forward/RC averaging</li> <li>Generative objective: Optimized for likelihood, not discriminative tasks</li> <li>Checkpoint availability: Fewer public weights vs. DNABERT-2</li> <li>6-mer limitations: May miss patterns spanning &gt;6 bases (compare to BPE or longer k-mers)</li> </ul>"},{"location":"models/genetics/generator/#when-to-use-generator","title":"When to Use GENERator","text":"<p>\u2705 Use when: - Interested in generative modeling and sequence design - Want to compare generative vs. discriminative embeddings - Need 6-mer vocabulary (codon-aware analysis) - Exploring counterfactual sequence perturbations</p> <p>\u26a0\ufe0f Consider alternatives: - Caduceus: For discriminative tasks with RC-equivariance - DNABERT-2: BPE tokenization, stronger discriminative benchmarks - Evo2: For ultra-long regulatory contexts</p>"},{"location":"models/genetics/generator/#reference-materials","title":"Reference Materials","text":""},{"location":"models/genetics/generator/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): GENERator (2024) - Code walkthrough: GENERator walkthrough - Model card (YAML): <code>kb/model_cards/generator.yaml</code> - Paper card (YAML): <code>kb/paper_cards/generator_2024.yaml</code></p> <p>Integration recipes: - Modality Features: Genomics - Integration Strategy</p>"},{"location":"models/genetics/generator/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/generator/</code> - Official GitHub: GenerTeam/GENERator</p> <p>Original paper: - Title: \"GENERator: A Long-Context Generative Genomic Foundation Model\" - Authors: Wu, Wei; Li, Qiuyi; et al. - Published: arXiv preprint, 2024 - Link: arXiv:2502.07272 - PDF (local): generator_2024.pdf</p>"},{"location":"models/genetics/generator/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Discriminative benchmark: Compare GENERator vs. Caduceus/DNABERT-2 on same gene set</li> <li>Generative pilot: Design synthetic promoters, test expression predictions</li> <li>Counterfactual analysis: Perturb gene sequences, measure embedding \u0394</li> <li>6-mer analysis: Visualize learned k-mer representations</li> <li>ARPA-H vision: Explore generative components for Brain-Omics Model (BOM)</li> </ol>"},{"location":"models/genetics/generator/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Extract hidden states, not logits for discriminative embeddings</li> <li>Always RC-average forward and reverse-complement embeddings</li> <li>Log layer used for extraction (typically last layer before output)</li> <li>6-mer tokenization is deterministic but frame-dependent (start position matters)</li> <li>When generating sequences, validate via independent predictors (avoid model collapse)</li> </ul>"},{"location":"models/genetics/hyenadna/","title":"HyenaDNA","text":""},{"location":"models/genetics/hyenadna/#overview","title":"Overview","text":"<p>Type: Long-context DNA foundation model Architecture: Decoder-only Hyena operators (implicit convolutions) Modality: Nucleotide sequences (DNA) Primary use (conceptual in KB): Reference architecture for 1M-token genomic modeling</p>"},{"location":"models/genetics/hyenadna/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>HyenaDNA demonstrates that sub-quadratic sequence operators can scale genomic language models to 1M-token contexts at single-nucleotide resolution, breaking the context-length barrier imposed by quadratic attention while preserving fine-grained variant information.^See arXiv:2306.15794 It is trained as a next-nucleotide predictor on the human reference genome and evaluated on standard regulatory element benchmarks, showing that carefully designed implicit convolutions can match or exceed attention-based DNA LMs with far fewer parameters and data.</p>"},{"location":"models/genetics/hyenadna/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Operators: Hyena implicit convolutions with data-controlled gating (no self-attention).</li> <li>Context length: Up to 1,000,000 tokens (1Mbp) with character-level tokenization.</li> <li>Training tricks: Sequence-length warm-up schedule, gradient checkpointing for ultralong   inputs, soft prompts for downstream adaptation.</li> <li>Outputs: Per-position logits/embeddings suitable for downstream pooling (gene, enhancer,   window-level features).</li> </ul> <p>HyenaDNA is not currently vendored as code in this KB; instead, the generic StripedHyena codebase in <code>external_repos/hyena/</code> is used for architectural walkthroughs.</p>"},{"location":"models/genetics/hyenadna/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/genetics/hyenadna/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>HyenaDNA is tracked as a long-context genomics reference:</p> <ul> <li>Informs the design of ultra-long-context pipelines built around Evo 2 (StripedHyena 2) for   regulatory-region and whole-locus embeddings.</li> <li>Motivates experimenting with 100kb\u20131Mbp windows when studying distal regulatory effects on   brain-related genes.</li> <li>Suggests that sequence-length warm-up and soft prompting should be standard recipes when   introducing Hyena/StripedHyena operators into neuro-omics models.</li> </ul> <p>Concrete embeddings in this KB currently use Caduceus, DNABERT-2, Evo 2, and GENERaTOR; HyenaDNA is kept as a design anchor and potential future encoder once public checkpoints and code stabilise.</p>"},{"location":"models/genetics/hyenadna/#reference-materials","title":"Reference Materials","text":""},{"location":"models/genetics/hyenadna/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<ul> <li>Paper summary: <code>docs/generated/kb_curated/papers-md/hyenadna_2023.md</code></li> <li>Paper card (YAML): <code>kb/paper_cards/hyenadna_2023.yaml</code></li> <li>Model card (YAML): <code>kb/model_cards/hyenadna.yaml</code></li> <li>Architecture walkthrough: <code>docs/code_walkthroughs/hyena_walkthrough.md</code> (StripedHyena core)</li> </ul>"},{"location":"models/genetics/hyenadna/#original-sources","title":"Original Sources","text":"<ul> <li>Paper: HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution   (NeurIPS 2023)^arXiv:2306.15794</li> <li>Hyena / StripedHyena code: see StripedHyena GitHub and   related Hyena project repositories referenced in the paper.</li> </ul>"},{"location":"models/integrations/ensemble_integration/","title":"Ensemble Integration (EI)","text":"<p>Source: Li et al. (2022), Bioinformatics Advances Type: Late fusion integration pattern Best for: Heterogeneous feature spaces, small-to-medium datasets, interpretable multimodal fusion</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#problem-it-solves","title":"Problem It Solves","text":"<p>Challenge: How to integrate heterogeneous biomedical data modalities (genetics, brain imaging, clinical data) that have very different structures, scales, and semantics without losing modality-specific signals.</p> <p>Solution: Ensemble Integration (EI) treats each modality as a first-class citizen by: 1. Training specialized models per modality with appropriate inductive biases 2. Combining modality predictions via heterogeneous ensembles (stacking, selection, averaging) 3. Providing interpretable feature rankings across all modalities</p> <p>Why traditional approaches fail: - Early integration (concatenate raw features) \u2192 loses modality-specific structure - Intermediate integration (shared embeddings) \u2192 emphasizes agreement, suppresses modality-unique signals - Single-model approaches \u2192 can't adapt architecture to each modality's semantics</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#core-mechanics","title":"Core Mechanics","text":"","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#1-modality-specific-model-training","title":"1. Modality-Specific Model Training","text":"<p>Train diverse base classifiers per modality using algorithms matched to data structure:</p> <pre><code># Genetics: sequence/graph data\ngene_models = {\n    'lr': LogisticRegression(C=0.01),\n    'rf': RandomForestClassifier(n_estimators=100),\n    'svm': SVC(kernel='rbf', probability=True),\n    'xgb': XGBClassifier(max_depth=5)\n}\n\n# Brain: spatial/temporal features\nbrain_models = {\n    'lr': LogisticRegression(C=0.1),\n    'gbdt': LightGBMClassifier(num_leaves=31),\n    'knn': KNeighborsClassifier(n_neighbors=5)\n}\n</code></pre> <p>Key insight: Different modalities benefit from different inductive biases (trees for genetics, neighbors for imaging).</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#2-late-fusion-strategies","title":"2. Late Fusion Strategies","text":"<p>Simple averaging: <pre><code>ensemble_pred = (gene_pred_proba + brain_pred_proba) / 2\n</code></pre></p> <p>Ensemble selection (Li et al. method): - Iteratively add models that improve validation performance - Greedy forward selection with replacement - Automatically weights models by contribution</p> <p>Stacking with meta-learner: <pre><code># Stack predictions from all base models\nmeta_features = np.hstack([gene_preds, brain_preds])\nmeta_model = LogisticRegression()\nmeta_model.fit(meta_features, y_train)\n</code></pre></p> <p>\u26a0\ufe0f Critical: Stacking must be fold-proper to avoid leakage\u2014train meta-learner only on out-of-fold base predictions.</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#3-interpretability-via-feature-ranking","title":"3. Interpretability via Feature Ranking","text":"<p>Cross-modality feature importance: 1. For each base model, extract feature importances (coefficients, SHAP values, permutation importance) 2. Aggregate via ensemble weights 3. Rank features across all modalities</p> <p>Result: Identify which genes AND which brain regions drive predictions, weighted by ensemble contribution.</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#when-to-use","title":"When to Use","text":"<p>\u2705 Use Ensemble Integration when: - Modalities have heterogeneous structures (sequences, images, graphs, tables) - Dataset size is small-to-medium (&lt; 10k samples) - Missing data is common (not all subjects have all modalities) - Interpretability is critical for clinical translation - Baseline comparisons needed (per-modality vs. fusion performance) - Computing resources are limited (no end-to-end training needed)</p> <p>\u2705 Particularly well-suited for: - Gene-brain-behavior prediction in neuro-omics - Multi-site cohort integration with batch effects - Clinical decision support requiring feature-level explanations - Research settings exploring which modalities contribute most</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#when-to-defer","title":"When to Defer","text":"<p>\u26a0\ufe0f Defer to more advanced methods when: - Modalities have strong cross-modal dependencies (e.g., paired image-text) - Large datasets available (&gt; 100k samples) enabling end-to-end joint training - Real-time deployment required (ensemble overhead too high) - Shared representations needed (e.g., cross-modal retrieval tasks)</p> <p>\u26a0\ufe0f Consider alternatives: - Two-tower contrastive if need aligned embedding space for retrieval - Early fusion if modalities naturally align (e.g., multi-view same subject) - Mixture-of-Experts if need learned routing by modality</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#adoption-in-our-neuro-omics-pipeline","title":"Adoption in Our Neuro-Omics Pipeline","text":"","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#current-implementation","title":"Current Implementation","text":"<p>Per-modality models: - Genetics: LR + LightGBM on Caduceus/DNABERT-2 embeddings (512-D) - Brain: LR + LightGBM on BrainLM/SwiFT embeddings (512-D) - Fusion: Ensemble selection or simple stacking with LR meta-learner</p> <p>Workflow: <pre><code># 1. Extract embeddings per modality\npython extract_gene_embeddings.py --model caduceus --out gene_emb.npy\npython extract_brain_embeddings.py --model brainlm --out brain_emb.npy\n\n# 2. Train per-modality models\npython train_per_modality.py --modality gene --models lr,gbdt\npython train_per_modality.py --modality brain --models lr,gbdt\n\n# 3. Ensemble integration\npython ensemble_fusion.py --strategy stacking --meta_model lr\n</code></pre></p> <p>Evaluation metrics: - Per-modality AUROC/AUPRC - Fusion AUROC/AUPRC - DeLong test: Fusion vs. max(Gene, Brain) - Feature importance rankings</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#integration-with-arpa-h-bom","title":"Integration with ARPA-H BOM","text":"<p>EI provides the baseline late fusion in our escalation strategy:</p> <pre><code>1. Ensemble Integration (baseline) \u2713 Current\n    \u2193 If fusion wins (p &lt; 0.05)\n2. Two-tower contrastive\n    \u2193 If gains plateau\n3. EI stacking with hub tokens\n    \u2193 Last resort\n4. Full early fusion (TAPE-style)\n</code></pre> <p>Why start with EI: - Establishes whether fusion helps at all before complex architectures - Provides interpretable baseline for regulatory/clinical validation - Enables per-modality ablations to identify which data types matter - Computationally cheap to iterate on cohort definitions and confounds</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#caveats-and-best-practices","title":"Caveats and Best Practices","text":"","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#leakage-prevention","title":"\u26a0\ufe0f Leakage Prevention","text":"<p>Problem: If meta-learner sees in-fold predictions, it overfits to noise.</p> <p>Solution: Always use out-of-fold predictions for stacking: <pre><code># WRONG: Train meta-learner on training predictions\nmeta_model.fit(base_preds_train, y_train)  # Leakage!\n\n# RIGHT: Train meta-learner on out-of-fold predictions\noof_preds = cross_val_predict(base_model, X_train, y_train, cv=5)\nmeta_model.fit(oof_preds, y_train)\n</code></pre></p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#confound-control","title":"\u26a0\ufe0f Confound Control","text":"<p>Problem: Batch effects (site, scanner) can dominate modality signals.</p> <p>Solution: Residualize before training base models: <pre><code># Per modality, residualize confounds\ngene_emb_residual = residualize(gene_emb, confounds=['age', 'sex', 'site', 'PCs'])\nbrain_emb_residual = residualize(brain_emb, confounds=['age', 'sex', 'site', 'FD'])\n</code></pre></p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#meta-learner-simplicity","title":"\u26a0\ufe0f Meta-Learner Simplicity","text":"<p>Problem: Complex meta-learners (deep NNs) can overfit ensemble predictions.</p> <p>Solution: Use simple meta-learners (LR, Ridge) unless &gt;10k samples: <pre><code># Prefer: Regularized logistic regression\nmeta_model = LogisticRegression(penalty='l2', C=1.0, max_iter=1000)\n\n# Avoid: Unless large N\nmeta_model = MLPClassifier(hidden_layers=(64, 32))  # Risk overfitting\n</code></pre></p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#missing-modality-handling","title":"\u26a0\ufe0f Missing Modality Handling","text":"<p>Problem: Not all subjects have both gene and brain data.</p> <p>Solution: Train modality-specific fallback models: <pre><code>if has_both_modalities(subject):\n    pred = ensemble_model.predict(gene_emb, brain_emb)\nelif has_gene_only(subject):\n    pred = gene_model.predict(gene_emb)\nelif has_brain_only(subject):\n    pred = brain_model.predict(brain_emb)\n</code></pre></p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#practical-implementation-guide","title":"Practical Implementation Guide","text":"","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#step-1-choose-base-models","title":"Step 1: Choose Base Models","text":"<p>Diversity is key \u2014 use algorithms with different inductive biases:</p> Modality Recommended Models Rationale Genetics (sequence) LR, XGBoost, SVM-RBF Linear + trees + kernels Brain (imaging) LR, LightGBM, k-NN Linear + trees + locality Behavior (tabular) LR, RandomForest, Ridge Simple + robust to correlation","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#step-2-train-with-proper-cv","title":"Step 2: Train with Proper CV","text":"<pre><code>from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Per-modality training with OOF predictions\nfor modality in ['gene', 'brain']:\n    oof_preds = []\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_val = X[val_idx]\n        \n        model.fit(X_train, y_train)\n        oof_preds.append(model.predict_proba(X_val)[:, 1])\n    \n    # Save OOF predictions for meta-learner\n    save_oof_predictions(modality, np.concatenate(oof_preds))\n</code></pre>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#step-3-meta-learner-training","title":"Step 3: Meta-Learner Training","text":"<pre><code># Load OOF predictions\ngene_oof = load_oof_predictions('gene')\nbrain_oof = load_oof_predictions('brain')\n\n# Stack into meta-features\nmeta_X = np.column_stack([gene_oof, brain_oof])\n\n# Train meta-learner\nmeta_model = LogisticRegression(C=1.0, max_iter=1000)\nmeta_model.fit(meta_X, y_train)\n\n# Evaluate on held-out test set\ntest_preds = np.column_stack([\n    gene_model.predict_proba(gene_test)[:, 1],\n    brain_model.predict_proba(brain_test)[:, 1]\n])\ntest_auc = roc_auc_score(y_test, meta_model.predict_proba(test_preds)[:, 1])\n</code></pre>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#step-4-feature-interpretation","title":"Step 4: Feature Interpretation","text":"<pre><code>import shap\n\n# Compute SHAP values for each base model\ngene_shap = shap.TreeExplainer(gene_model).shap_values(gene_emb)\nbrain_shap = shap.TreeExplainer(brain_model).shap_values(brain_emb)\n\n# Weight by ensemble contribution (meta-learner coefficients)\ngene_weight = np.abs(meta_model.coef_[0][0])\nbrain_weight = np.abs(meta_model.coef_[0][1])\n\n# Aggregate feature importance\nweighted_gene_importance = gene_shap.mean(axis=0) * gene_weight\nweighted_brain_importance = brain_shap.mean(axis=0) * brain_weight\n\n# Rank across all features\nall_importance = np.concatenate([weighted_gene_importance, weighted_brain_importance])\ntop_features = np.argsort(all_importance)[::-1][:20]\n</code></pre>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#reference-materials","title":"Reference Materials","text":"<p>Primary paper: - Ensemble Integration (Li 2022) \u2014 Full paper summary</p> <p>Related KB resources: - Integration Strategy \u2014 Overall fusion approach - Design Patterns \u2014 Pattern 1: Late Fusion - CCA + Permutation Recipe \u2014 Statistical testing - Prediction Baselines \u2014 Comparison protocol</p> <p>Integration cards: - Oncology Multimodal Review \u2014 Broader fusion taxonomy</p> <p>Model documentation: - Genetics Models \u2014 Gene embedding extraction - Brain Models \u2014 Brain embedding extraction</p> <p>Experiment configs: - <code>configs/experiments/02_prediction_baselines.yaml</code> \u2014 EI implementation template</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/ensemble_integration/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Baseline EI implementation \u2014 LR + GBDT per modality with stacking meta-learner</li> <li>Per-modality ablations \u2014 Which modality contributes most? (Gene vs. Brain vs. Fusion)</li> <li>Feature interpretation \u2014 Identify top predictive genes and brain regions</li> <li>Cohort extension \u2014 Test EI on Cha Hospital developmental cohort</li> <li>Escalation decision \u2014 If fusion wins significantly, move to two-tower contrastive</li> </ol> <p>Success criteria for escalation: - Fusion AUROC &gt; max(Gene, Brain) with p &lt; 0.05 (DeLong test) - Gains observed across multiple phenotypes (cognitive, diagnostic) - Stable across cross-validation folds (not driven by outliers)</p>","tags":["integration","late-fusion","ensembles","interpretability"]},{"location":"models/integrations/multimodal_fm_patterns/","title":"Multimodal Foundation Model Patterns for Brain-Omics","text":"<p>Source models: BAGEL, MoT, M3FM, Me-LLaMA, TITAN, FMS-Medical Catalog Type: Cross-model integration pattern synthesis Best for: Choosing multimodal architectures and fusion strategies that align with the ARPA-H Brain-Omics Model (BOM) vision.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#problem-it-solves","title":"Problem It Solves","text":"<p>Challenge: Given many powerful multimodal FMs (vision\u2013language models, unified MoT transformers, medical LLMs), how do we:</p> <ul> <li>Decide which architectural pattern (two-tower CLIP, unified MoT, hierarchical ViT, LLM-as-bridge) to use for gene\u2013brain\u2013behavior\u2013text integration?  </li> <li>Prioritize models and patterns that match ARPA-H BOM goals: zero-shot generalization, label efficiency, clinical interpretability, and scalability across sites and populations.  </li> <li>Avoid over-engineering (e.g., jumping to BAGEL-scale unified models) before late fusion and simpler patterns are exhausted.</li> </ul> <p>Solution (this card): Compare and contrast multimodal FMs and papers to extract three reusable integration patterns that can be slotted into the Neuro-Omics KB and BOM roadmap:</p> <ol> <li>Two-Tower CLIP-Style Alignment (M3FM, TITAN stage 2/3)  </li> <li>Unified MoT-Style Multimodal Transformer (BAGEL + MoT)  </li> <li>LLM-as-Semantic Bridge (Me-LLaMA + others)</li> </ol> <p>Each pattern is summarized below with strengths, benchmarks, and ARPA-H fit, then mapped to a recommended escalation path.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#core-multimodal-patterns","title":"Core Multimodal Patterns","text":"","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#pattern-1-two-tower-clip-style-alignment","title":"Pattern 1 \u2014 Two-Tower CLIP-Style Alignment","text":"<p>Representative models: - M3FM (medical vision\u2013language: CXR/CT + bilingual reports) - TITAN (histopathology slides + ROI captions + pathology reports)  </p> <p>Mechanism:</p> <pre><code>Image encoder (brain / CXR / WSI)  \u2192  visual_embedding\nText encoder (medical LLM / encoder) \u2192 text_embedding\n           \u2193                                   \u2193\n         CLIP-style contrastive loss in shared latent space\n           \u2193\n      Downstream heads (classification, retrieval, report generation)\n</code></pre> <p>Key properties (from M3FM/TITAN): - Label efficiency: Strong zero-shot / few-shot transfer once alignment is learned. - Modality decoupling: Vision and text encoders can be updated or swapped independently. - Multilingual extension: Language side can be extended (e.g., English \u2194 Korean) without retraining vision encoder. - Clinical relevance: Direct path from images \u2192 clinically meaningful text outputs.</p> <p>When it shines (benchmarks &amp; regimes): - Zero-shot report generation (M3FM) across CXR/CT and languages. - Cross-modal retrieval (TITAN) between slides and pathology reports. - Few-shot classification where paired data is available for pretraining.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#pattern-2-unified-mot-style-multimodal-transformer","title":"Pattern 2 \u2014 Unified MoT-Style Multimodal Transformer","text":"<p>Representative models: - BAGEL (unified text\u2013image\u2013video + generation) - MoT (modality-aware sparse transformers, Chameleon/Transfusion-style)</p> <p>Mechanism:</p> <pre><code>All modalities \u2192 token sequences \u2192 shared self-attention\n                           \u2193\n         Modality-aware FFNs / experts (MoT-style)\n                           \u2193\n                 Understanding + generation heads\n</code></pre> <p>Key properties (from BAGEL/MoT): - Unified reasoning: All modalities interact through a single attention backbone. - Modality-aware sparsity: MoT decouples FFNs per modality \u2192 ~40\u201360% FLOP savings. - Emergent capabilities: BAGEL-style models show free-form visual manipulation, world modeling. - Scalability: Works at billion-parameter scales with careful engineering.</p> <p>When it shines: - Rich cross-modal reasoning tasks (e.g., world navigation, complex multimodal Q&amp;A). - Scenarios where you want joint understanding + generation without bottlenecks. - High-resource settings where training unified models is computationally feasible.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#pattern-3-llm-as-semantic-bridge","title":"Pattern 3 \u2014 LLM-as-Semantic Bridge","text":"<p>Representative models: - Me-LLaMA (medical LLM with continual pretraining + instruction tuning) - Many general LLM + domain-adaptation pipelines</p> <p>Mechanism:</p> <pre><code>Modality encoders (genes, brain, behavior) \u2192 embeddings\n                                     \u2193\n                       Projection into LLM token space\n                                     \u2193\n                      Medical / neuro-omics LLM (Me-LLaMA-style)\n                                     \u2193\n             Clinical reasoning, report generation, question answering\n</code></pre> <p>Key properties (from Me-LLaMA + BOM vision): - Domain-knowledge injection: Continual pretraining on medical/neuroscience corpora. - Instruction-tuned reasoning: Multi-task prompts for QA, summarization, diagnosis. - Human interface: Natural language explanations for complex multimodal predictions.  </p> <p>When it shines: - Clinical reasoning and explanation tasks (e.g., \u201cWhy is this gene-brain pattern risky?\u201d). - Report generation that must mix imaging, genetics, and behavioral findings. - Scenarios where interpretability and human-AI collaboration are central.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#when-to-use-each-pattern-bom-centric-view","title":"When to Use Each Pattern (BOM-Centric View)","text":"","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#quick-decision-table","title":"Quick Decision Table","text":"Scenario Recommended Pattern Rationale Zero-shot imaging \u2192 report (brain + text) Two-Tower CLIP M3FM/TITAN show strong label-efficiency Scaling to many modalities with moderate compute MoT-style unified Modality-aware sparsity balances cost/performance Clinician-facing reasoning / explanations LLM-as-Bridge Me-LLaMA demonstrates strong clinical NLP Early BOM phases, limited data Two-Tower + LLM Bridge Leverage pretrained encoders &amp; LLMs; avoid full unification Later BOM phases, large paired multimodal datasets MoT-style unified + LLM Bridge Joint reasoning + language outputs","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#when-to-defer-these-patterns","title":"When to Defer These Patterns","text":"<p>\u26a0\ufe0f Defer heavy multimodal patterns when: - You haven\u2019t yet demonstrated that fusion beats strong single-modality baselines (per EI card). - You lack sufficient paired data to learn robust alignments (especially for two-tower and unified MoT). - Your primary goal is mechanistic interpretability, not raw predictive power. - Compute constraints make unified models impractical.</p> <p>\u26a0\ufe0f Prefer simpler approaches first: - Start with late fusion + Ensemble Integration (see EI card). - Use CCA + permutation to test for cross-modal structure before complex fusion. - Only escalate when fusion gains are statistically significant and stable.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#adoption-in-our-neuro-omics-arpa-h-bom-pipeline","title":"Adoption in Our Neuro-Omics / ARPA-H BOM Pipeline","text":"","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#phase-1-late-fusion-diagnostic-probes-current","title":"Phase 1 \u2014 Late Fusion + Diagnostic Probes (Current)","text":"<ul> <li>Use Ensemble Integration (EI) as in <code>ensemble_integration.md</code>.  </li> <li>Evaluate whether gene+brain fusion improves AUROC/AUPRC vs. best single modality.  </li> <li>Use CCA + permutation to detect cross-modal structure.</li> </ul>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#phase-2-two-tower-clip-style-alignment-near-term","title":"Phase 2 \u2014 Two-Tower CLIP-Style Alignment (Near Term)","text":"<p>Goal: Learn a shared brain \u2194 text space for clinical reporting.</p> <ul> <li>Vision side: SwiFT / BrainLM encoders for fMRI/sMRI.  </li> <li>Text side: Me-LLaMA-style medical LLM or encoder.  </li> <li>Training: Contrastive loss on paired brain scans + radiology/clinical notes (M3FM-style).  </li> <li>Outputs:  </li> <li>Zero-shot brain \u2192 report generation.  </li> <li>Cross-modal retrieval (find similar brains given text, or vice versa).</li> </ul> <p>Fit to ARPA-H BOM: - Directly supports clinical translation, zero-shot deployment, and multilingual extensions.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#phase-3-llm-as-bridge-for-genebrainbehavior","title":"Phase 3 \u2014 LLM-as-Bridge for Gene\u2013Brain\u2013Behavior","text":"<p>Goal: Use a Me-LLaMA-style LLM as the semantic hub for:</p> <pre><code>Genetics embeddings  \u2192 |\n                       | \u2192 LLM token space \u2192 clinical text\nBrain embeddings      \u2192 |\nBehavioral measures   \u2192 |\n</code></pre> <ul> <li>Continually pretrain LLaMA on neuroscience + genetics + clinical neurology corpora.  </li> <li>Instruction-tune for gene\u2013brain\u2013behavior reasoning tasks.  </li> <li>Use projections from gene/brain spaces into LLM embedding space for joint reasoning.</li> </ul>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#phase-4-unified-mot-style-multimodal-transformer-longer-term","title":"Phase 4 \u2014 Unified MoT-Style Multimodal Transformer (Longer Term)","text":"<p>Goal: BOM-scale unified model across genes, brain, behavior, and text.</p> <ul> <li>Treat all modalities as tokens in a shared transformer (BAGEL/MoT-style).  </li> <li>Use modality-aware FFNs (MoT) to control compute while preserving cross-modal attention.  </li> <li>Optionally couple with LLM-as-bridge for natural language interfaces and clinical reasoning.</li> </ul> <p>Prerequisites: - Large paired multimodal dataset (\u2265 50k subjects with gene+brain+behavior+text). - Demonstrated gains from Phase 2 &amp; 3 patterns. - Stable training infrastructure for 7B+ parameter models.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#caveats-and-best-practices","title":"Caveats and Best Practices","text":"","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#benchmark-mismatch","title":"\u26a0\ufe0f Benchmark Mismatch","text":"<p>Multimodal papers often report general benchmarks (e.g., VQA, CXR report BLEU) that don\u2019t map 1:1 to neuro-omics.</p> <p>Mitigation: - Define BOM-specific benchmarks: gene\u2013brain prediction, cognitive scores, clinical endpoints. - Use multimodal FMs as pattern references, not drop-in benchmarking baselines.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#domain-gap","title":"\u26a0\ufe0f Domain Gap","text":"<p>Most multimodal FMs are trained on radiology, pathology, or web data, not genetics/brain.</p> <p>Mitigation: - Reuse architectural patterns (two-tower, MoT, LLM-bridge) with domain-specific encoders (Caduceus, BrainLM, etc.). - Avoid directly applying off-the-shelf weights to neuro-omics without adaptation.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#compute-budget","title":"\u26a0\ufe0f Compute Budget","text":"<p>Unified models (BAGEL/MoT scale) are expensive to train and serve.</p> <p>Mitigation: - Start with two-tower + LLM-bridge using frozen encoders and adapters. - Use MoT-style sparsity if/when moving to unified architectures.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#practical-implementation-guide-pattern-1-example-two-tower-brain-text","title":"Practical Implementation Guide (Pattern 1 Example: Two-Tower Brain \u2194 Text)","text":"","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#step-1-choose-encoders","title":"Step 1 \u2014 Choose Encoders","text":"Component Choice Rationale Brain encoder BrainLM or SwiFT Strong fMRI/sMRI FMs in KB Text encoder Me-LLaMA or medical BERT Medical domain coverage Projection head 2\u20133 layer MLP Map to shared 256\u2013512D space","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#step-2-train-contrastive-alignment","title":"Step 2 \u2014 Train Contrastive Alignment","text":"<pre><code># Pseudo-code for InfoNCE over brain \u2194 text pairs\nfor brain_batch, text_batch in loader:\n    b_emb = brain_proj(brain_encoder(brain_batch))   # [B, d]\n    t_emb = text_proj(text_encoder(text_batch))      # [B, d]\n\n    b_emb = F.normalize(b_emb, dim=-1)\n    t_emb = F.normalize(t_emb, dim=-1)\n\n    logits = b_emb @ t_emb.T / tau   # cosine similarities\n    labels = torch.arange(len(brain_batch), device=logits.device)\n    loss = (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n</code></pre>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#step-3-downstream-tasks","title":"Step 3 \u2014 Downstream Tasks","text":"<ul> <li>Retrieval: nearest-neighbor in shared space.  </li> <li>Zero-shot classification: prompt-based thresholds in text space.  </li> <li>Report generation: condition LLM on aligned text embeddings.</li> </ul>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#reference-materials","title":"Reference Materials","text":"<p>Multimodal papers (summaries): - BAGEL (2025) \u2014 Unified MoT multimodal FM - MoT (2025) \u2014 Modality-aware sparse transformer - M3FM (2025) \u2014 Medical vision\u2013language with two-tower CLIP - Me-LLaMA (2024) \u2014 Medical LLM via continual pretraining - TITAN (2025) \u2014 Multi-scale pathology VLM - Multimodal FMs Survey (2025) \u2014 Broader architectural landscape</p> <p>Model documentation: - Multimodal Models \u2014 Model-level documentation - M3FM model card - Me-LLaMA model card - TITAN model card</p> <p>Integration guidance: - Integration Strategy \u2014 Overall fusion approach - Design Patterns \u2014 Escalation from late fusion \u2192 MoT - Multimodal Architecture Patterns \u2014 Detailed pattern catalog - Ensemble Integration (EI) \u2014 Late fusion baseline - Oncology Multimodal Principles \u2014 Fusion cautions &amp; taxonomy</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Catalog BOM requirements against these three patterns (two-tower, MoT, LLM-bridge).  </li> <li>Prototype two-tower brain \u2194 text alignment using BrainLM/SwiFT + Me-LLaMA on UKB radiology data.  </li> <li>Design neuro-omics LLM continual pretraining corpus (neuroscience + genetics + neurology).  </li> <li>Define data requirements for potential MoT-style unified BOM (subject counts, modalities, sites).  </li> <li>Update ARPA-H BOM roadmap with concrete pattern selection per phase.</li> </ol>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/multimodal_fm_patterns/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Two-tower CLIP-style alignment is the most immediately practical pattern for BOM: label-efficient, modular, clinically relevant.  </li> <li>MoT-style unified transformers are powerful but should be a Phase 3\u20134 goal once simpler fusion clearly helps and data is sufficient.  </li> <li>LLM-as-bridge is essential for clinical impact: it turns multimodal embeddings into reasoning and explanations.  </li> <li>Multimodal FM papers are best treated as pattern libraries, not plug-and-play models for neuro-omics.  </li> <li>ARPA-H BOM should escalate from late fusion \u2192 two-tower + LLM-bridge \u2192 MoT-style unification, always gated by evidence of fusion gains and data readiness.</li> </ol> <p>Bottom line: Use multimodal FMs to choose integration patterns, not just models\u2014starting with two-tower and LLM-bridge patterns that best match ARPA-H\u2019s emphasis on label-efficient, interpretable, clinically grounded brain-omics integration.</p>","tags":["integration","multimodal","clinical","bom","patterns"]},{"location":"models/integrations/oncology_multimodal_review/","title":"Oncology Multimodal Review \u2014 Integration Principles","text":"<p>Source: Waqas et al. (2024), Frontiers in Artificial Intelligence Type: Comprehensive review of multimodal fusion strategies Scope: Deep learning architectures (GNNs, Transformers) for cancer data integration Relevance: General principles applicable to neuro-omics gene-brain-behavior fusion</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#problem-context","title":"Problem Context","text":"<p>Domain: Cancer/oncology multimodal learning (imaging + omics + clinical) Relevance to Neuro-Omics: While focused on cancer, the review's taxonomy and cautions apply directly to gene-brain integration, where we face similar challenges:</p> Oncology Challenge Neuro-Omics Equivalent Radiology + histopathology + genomics fMRI + sMRI + WES/WGS Multi-site scanner heterogeneity UK Biobank + HCP + Cha Hospital sites Missing modalities per patient Incomplete genetic/imaging coverage Class imbalance (rare cancers) Rare neurological disorders Interpretability for clinicians Explainability for genetic counseling","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#key-taxonomy-fusion-strategies","title":"Key Taxonomy: Fusion Strategies","text":"<p>The review categorizes multimodal integration into three main patterns, with trade-offs for each:</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#1-early-fusion","title":"1. Early Fusion","text":"<p>Mechanism: Concatenate raw or lightly processed features from all modalities before feeding into a single model.</p> <pre><code>Gene sequences    \u2192 |\n                    | \u2192 Concatenate \u2192 Single Model \u2192 Prediction\nBrain volumes     \u2192 |\nClinical features \u2192 |\n</code></pre> <p>Advantages: - Simplest to implement - Single model to train and deploy - Can capture low-level cross-modal interactions</p> <p>Disadvantages: - Heterogeneous semantics: Hard to align sequences, images, and tables - Dimensionality explosion: Concatenating all features creates huge input spaces - Loss of modality structure: Throws away spatial (imaging) and sequential (genomics) structure - Dominance by one modality: High-dimensional modality can drown out others</p> <p>When to use: - Modalities are already aligned (e.g., multi-view same organ) - Small feature sets (&lt; 1k features total) - Quick prototyping to check if fusion helps at all</p> <p>When to avoid: - Large dimensional mismatch (e.g., 100 genes vs. 100k voxels) - Heterogeneous data types (sequences vs. images vs. tables) - Our default: Avoid for gene-brain fusion unless late fusion fails</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#2-intermediate-fusion-joint-embeddings","title":"2. Intermediate Fusion (Joint Embeddings)","text":"<p>Mechanism: Extract modality-specific embeddings, then fuse via learned projections or attention before final prediction.</p> <pre><code>Gene FM     \u2192 gene_embed [512-D]   \u2192 |\n                                     | \u2192 Fusion layer (attention/concat) \u2192 Classifier\nBrain FM    \u2192 brain_embed [512-D]  \u2192 |\n</code></pre> <p>Variants: - Concatenation + MLP: <code>concat(gene_emb, brain_emb) \u2192 MLP \u2192 pred</code> - Cross-attention: Query one modality with keys/values from another - Gated fusion: Learn modality weights dynamically per sample - Graph fusion (GNN): Model relationships among genes, brain regions, patients</p> <p>Advantages: - Preserves modality-specific structure in embeddings - Can learn cross-modal interactions - Balances early vs. late fusion</p> <p>Disadvantages: - Over-smoothing (GNNs): Deep GNN layers blur modality boundaries - Alignment challenges: Requires paired data for joint training - Hyperparameter sensitivity: Fusion architecture choices critical</p> <p>When to use: - Have large paired dataset (&gt;10k samples) - Need learned cross-modal interactions - Foundation models provide good embeddings already</p> <p>Our escalation path: Move here if late fusion (EI) demonstrates significant gains.</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#3-late-fusion-decision-level","title":"3. Late Fusion (Decision-Level)","text":"<p>Mechanism: Train separate models per modality, combine predictions via ensembles or simple averaging.</p> <pre><code>Gene model  \u2192 gene_pred [proba]  \u2192 |\n                                    | \u2192 Ensemble (avg/stack) \u2192 Final pred\nBrain model \u2192 brain_pred [proba] \u2192 |\n</code></pre> <p>Advantages: - Respects modality-specific semantics: Each model uses appropriate architecture - Handles missing data gracefully: Can fall back to single-modality predictions - Interpretable: Can isolate which modality contributes most - Computationally efficient: No joint training overhead</p> <p>Disadvantages: - Limited cross-modal learning: Models trained independently - Suboptimal if strong dependencies: May miss synergistic interactions</p> <p>When to use: - Baseline comparisons: Always start here - Heterogeneous modalities with different structures - Small-to-medium datasets (&lt; 10k samples) - Interpretability critical for clinical translation</p> <p>Our default: This is Pattern 1 in our Design Patterns.</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#practical-cautions-for-neuro-omics","title":"Practical Cautions for Neuro-Omics","text":"<p>The review highlights six major pitfalls\u2014all directly applicable to our gene-brain integration:</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#1-heterogeneous-semantics","title":"1. Heterogeneous Semantics","text":"<p>Problem: Genetics (sequences, graphs), brain imaging (spatial volumes), behavior (tables) have fundamentally different structures.</p> <p>Oncology example: Mixing WSI pixels with RNA-seq counts Neuro-omics risk: Na\u00efvely concatenating gene embeddings with fMRI voxels</p> <p>Mitigation: - Use modality-specific foundation models (Caduceus for genes, BrainLM for fMRI) - Project to common dimensionality (512-D) before fusion - Normalize scales (z-score per modality)</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#2-alignment-across-scales","title":"2. Alignment Across Scales","text":"<p>Problem: Modalities capture biology at different resolutions (molecular \u2192 cellular \u2192 tissue \u2192 organ).</p> <p>Oncology example: Aligning genomic variants (bp-level) with CT scans (mm-level) Neuro-omics risk: Gene-level variants vs. voxel-level BOLD signal</p> <p>Mitigation: - Aggregate to common level: Gene-level embeddings \u2194 Subject-level brain embeddings - Hierarchical fusion: Match genomic pathways to brain networks - Avoid pixel-level alignment: Use pretrained FMs to handle within-modality structure</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#3-missing-modalities","title":"3. Missing Modalities","text":"<p>Problem: Not all subjects have complete data (incomplete genetic sequencing, missing scans).</p> <p>Oncology example: Some patients lack genomic profiling due to sample quality Neuro-omics risk: UKB has imaging for subset; HCP lacks genetics</p> <p>Mitigation: - Late fusion with fallbacks: Train per-modality models that work independently - Imputation: Use modality-specific imputation (e.g., gene expression imputation from SNPs) - Multi-task learning: Share representations where data overlaps - Avoid requiring all modalities: Don't force early fusion that drops incomplete samples</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#4-over-smoothing-gnns","title":"4. Over-Smoothing (GNNs)","text":"<p>Problem: Deep graph neural networks blur distinctions between node types, losing modality-specific signals.</p> <p>Oncology example: Patient-gene-image GNN collapses to uniform representations Neuro-omics risk: Gene-brain-subject heterogeneous graph loses modality boundaries</p> <p>Mitigation: - Limit GNN depth: Use 2-3 layers maximum - Modality-aware message passing: Separate parameters for gene-gene vs. gene-brain edges - Prefer late fusion: Avoid GNNs unless strong relational structure justifies complexity</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#5-confounds-and-batch-effects","title":"5. Confounds and Batch Effects","text":"<p>Problem: Site, scanner, sequencing platform, demographics can dominate biological signal.</p> <p>Oncology example: Multi-site TCGA data has strong batch effects Neuro-omics risk: UK Biobank imaging sites, genetic ancestry PCs, scanner upgrades</p> <p>Mitigation: - Residualize before fusion: Remove age, sex, site, motion (FD), genetic PCs per modality - Harmonization: ComBat for imaging, surrogate variable analysis for genomics - Stratified CV: Ensure train/val/test splits balance sites - Site-unlearning: Adversarial debiasing if residualization insufficient</p> <p>\u26a0\ufe0f Critical: Always residualize before extracting embeddings from FMs.</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#6-data-leakage","title":"6. Data Leakage","text":"<p>Problem: Information from test set bleeds into training, inflating performance estimates.</p> <p>Oncology example: Normalizing across train+test before split Neuro-omics risks: - Fitting PCA on full dataset before CV split - Selecting features based on full-cohort statistics - Using in-fold predictions for stacking meta-learner</p> <p>Mitigation: - Fit preprocessing only on training folds: z-score, PCA, harmonization per fold - Out-of-fold predictions for stacking: Use <code>cross_val_predict</code> for meta-learner - Strict fold boundaries: No subject overlap between train/val/test - Validation gates: Use <code>scripts/manage_kb.py</code> to check for leakage</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#practices-we-adopt","title":"Practices We Adopt","text":"<p>Based on the review's recommendations and our neuro-omics context:</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#default-late-fusion-pattern-1","title":"\u2705 Default Late Fusion (Pattern 1)","text":"<p>Rationale: Heterogeneous gene-brain modalities, small-to-medium cohorts (UKB ~40k imaging, HCP ~1k)</p> <p>Implementation: - Per-modality FMs: Caduceus/DNABERT-2 (genetics), BrainLM/SwiFT (brain) - Project to 512-D per modality - Ensemble Integration (see EI card) - Compare: Gene-only vs. Brain-only vs. Fusion</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#rigorous-confound-control","title":"\u2705 Rigorous Confound Control","text":"<p>Per modality, residualize: - Genetics: Age, sex, genetic PCs (ancestry), cohort - Brain: Age, sex, site/scanner, motion (mean FD), intracranial volume (ICV) - Behavior: Age, sex, SES, education</p> <p>Harmonization when needed: - Brain imaging: ComBat for site effects, MURD for T1/T2 - Genetics: Surrogate variable analysis for batch effects</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#stratified-cross-validation","title":"\u2705 Stratified Cross-Validation","text":"<ul> <li>5-fold StratifiedKFold: Preserve outcome class balance</li> <li>Site stratification: Ensure each fold has all sites represented</li> <li>Subject-level split: No leakage via related individuals (kinship matrix)</li> </ul>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#proper-significance-testing","title":"\u2705 Proper Significance Testing","text":"<ul> <li>DeLong test: Compare Fusion AUROC vs. max(Gene, Brain) AUROC</li> <li>Permutation tests: Null distributions for CCA canonical correlations</li> <li>Bootstrap CIs: 95% confidence intervals on performance metrics</li> <li>Bonferroni correction: Adjust for multiple phenotype tests</li> </ul>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#interpretability-first","title":"\u2705 Interpretability-First","text":"<ul> <li>Feature importance: SHAP values per modality, aggregated by ensemble weights</li> <li>Ablation studies: Which modality contributes most? (Gene vs. Brain)</li> <li>Biological validation: Top genes/regions mapped to known pathways/networks</li> </ul>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#what-we-defer-and-why","title":"What We Defer (and Why)","text":"<p>The review documents advanced architectures (deep GNNs, Transformers, foundation models), but we defer these until baselines justify the complexity:</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#early-fusion-immediate-concatenation","title":"\ud83d\udeab Early Fusion (Immediate Concatenation)","text":"<p>Why defer: Heterogeneous semantics (sequences vs. images), dimensionality mismatch When reconsider: If late fusion completely fails (unlikely)</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#deep-graph-neural-networks","title":"\ud83d\udeab Deep Graph Neural Networks","text":"<p>Why defer: Over-smoothing risk, limited interpretability, requires careful graph construction When reconsider: If explicit gene-brain relationship modeling justified (e.g., gene-pathway-brain-network hierarchies)</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#end-to-end-joint-training","title":"\ud83d\udeab End-to-End Joint Training","text":"<p>Why defer: Requires large paired datasets (&gt;50k), computationally expensive, risk of overfitting When reconsider: If move to two-tower contrastive (Pattern 2) and have sufficient data</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#foundation-model-fine-tuning","title":"\ud83d\udeab Foundation Model Fine-Tuning","text":"<p>Why defer: Gene/brain FMs pretrained on massive corpora; fine-tuning risks losing generalization When reconsider: If task-specific adaptation needed (e.g., pediatric-specific brain FM for Cha cohort)</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#direct-implications-for-our-project","title":"Direct Implications for Our Project","text":"","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#phase-1-late-fusion-baselines-current","title":"Phase 1: Late Fusion Baselines (Current)","text":"<p>Goal: Establish whether gene-brain fusion helps at all</p> <p>Methods: - Ensemble Integration (EI) with LR + GBDT per modality - CCA + permutation tests for cross-modal structure - Per-modality vs. fusion AUROC comparisons</p> <p>Success criteria: Fusion significantly outperforms max(Gene, Brain) on cognitive/diagnostic tasks</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#phase-2-escalation-to-intermediate-fusion-if-phase-1-succeeds","title":"Phase 2: Escalation to Intermediate Fusion (If Phase 1 succeeds)","text":"<p>Goal: Learn cross-modal interactions if late fusion wins</p> <p>Methods: - Two-tower contrastive alignment (Pattern 2) - Cross-attention fusion - EI stacking with learned modality weights</p> <p>Trigger: Fusion AUROC &gt; max(Gene, Brain) + 0.03, p &lt; 0.05 across multiple phenotypes</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#phase-3-advanced-architectures-long-term","title":"Phase 3: Advanced Architectures (Long-term)","text":"<p>Goal: Unified Brain-Omics Model (BOM) for ARPA-H vision</p> <p>Methods: - Mixture-of-Transformers (MoT) with modality-specific experts - Hub tokens / TAPE-style early fusion - LLM as semantic bridge for gene-brain-behavior reasoning</p> <p>Trigger: Intermediate fusion demonstrates consistent gains, large paired dataset (&gt;50k) available</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#architecture-decision-tree","title":"Architecture Decision Tree","text":"<pre><code>Start: Gene + Brain data\n    \u2193\nQ1: Does fusion help at all?\n    \u2192 Run EI baseline (LR + GBDT per modality, stacking)\n    \u2192 Compare Fusion vs. max(Gene, Brain) AUROC\n    \n    If NO (Fusion \u2264 max + 0.01):\n        \u2192 Stop: Use best single-modality model\n        \u2192 Document: Modalities independent for this phenotype\n    \n    If YES (Fusion &gt; max + 0.03, p &lt; 0.05):\n        \u2193\nQ2: Do we have large paired dataset (&gt;10k)?\n    \n    If NO:\n        \u2192 Continue with EI\n        \u2192 Add interpretability: Which modality contributes?\n    \n    If YES:\n        \u2193\nQ3: Are modalities strongly coupled?\n        \u2192 Run CCA + permutation: Significant canonical correlations?\n        \n        If NO:\n            \u2192 Continue with EI (late fusion optimal)\n        \n        If YES:\n            \u2193\n            \u2192 Escalate to Two-Tower Contrastive (Pattern 2)\n            \u2192 If gains plateau, consider EI stacking with hub tokens\n            \u2192 If still improving, consider early fusion (Pattern 5)\n</code></pre>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#reference-materials","title":"Reference Materials","text":"<p>Primary paper: - Oncology Multimodal Review (Waqas 2024) \u2014 Full paper summary</p> <p>Related integration cards: - Ensemble Integration (EI) \u2014 Late fusion implementation details</p> <p>KB integration guides: - Integration Strategy \u2014 Overall fusion approach - Design Patterns \u2014 5 patterns with escalation criteria - Multimodal Architectures \u2014 Clinical/multimodal model patterns</p> <p>Analysis recipes: - CCA + Permutation \u2014 Cross-modal structure testing - Prediction Baselines \u2014 Fusion vs. single-modality comparison - Partial Correlations \u2014 Confound-aware associations</p> <p>Data governance: - Governance &amp; QC \u2014 Quality control protocols - UKB Data Map \u2014 Cohort definitions and confounds</p> <p>Model documentation: - Genetics Models \u2014 Foundation models for gene embeddings - Brain Models \u2014 Foundation models for brain embeddings - Multimodal Models \u2014 Examples of fusion architectures</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/oncology_multimodal_review/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Start simple: Late fusion (EI) is the principled baseline\u2014establish whether fusion helps before complexity</li> <li>Control confounds: Residualize age, sex, site, motion before fusion\u2014batch effects dominate biology</li> <li>Respect heterogeneity: Don't force early fusion on sequences + images\u2014use modality-specific FMs</li> <li>Handle missing data: Late fusion supports per-modality fallbacks\u2014don't drop incomplete samples</li> <li>Test significance: DeLong tests and permutations quantify fusion gains\u2014avoid over-interpreting noise</li> <li>Prioritize interpretability: Feature rankings and ablations guide biological discovery and clinical trust</li> <li>Escalate conditionally: Only move to intermediate/early fusion if late fusion wins and data supports it</li> </ol> <p>Bottom line: The oncology review validates our default late fusion strategy and provides clear criteria for when (and when not) to escalate to more complex architectures.</p>","tags":["integration","principles","review","fusion-taxonomy","deep-learning"]},{"location":"models/integrations/template/","title":"Integration card template","text":"<p>title: \"{{CARD_TITLE}} \u2014 Integration Guidance Card\" status: draft updated: {{DATE}} tags: [integration, {{TAGS}}]</p>"},{"location":"models/integrations/template/#card_title","title":"{{CARD_TITLE}}","text":"<p>Source: {{AUTHORS}} ({{YEAR}}), {{VENUE}} Type: {{INTEGRATION_PATTERN_TYPE}} Best for: {{PRIMARY_USE_CASES}}</p>"},{"location":"models/integrations/template/#problem-it-solves","title":"Problem It Solves","text":"<p>Challenge: [Describe the integration challenge this approach addresses]</p> <p>Solution: [High-level description of the solution approach]</p> <p>Why traditional approaches fail: [Key limitations of naive or simpler methods]</p>"},{"location":"models/integrations/template/#core-mechanics","title":"Core Mechanics","text":""},{"location":"models/integrations/template/#1-primary-mechanism-name","title":"1. [Primary Mechanism Name]","text":"<p>[Detailed description of how the integration works]</p> <pre><code># Code example showing key implementation pattern\n</code></pre> <p>Key insight: [Main technical or conceptual contribution]</p>"},{"location":"models/integrations/template/#2-secondary-mechanisms-if-applicable","title":"2. [Secondary Mechanisms if applicable]","text":"<p>[Additional details on variants, extensions, or related techniques]</p>"},{"location":"models/integrations/template/#when-to-use","title":"When to Use","text":"<p>\u2705 Use this approach when: - [Condition 1: data characteristics] - [Condition 2: dataset size requirements] - [Condition 3: interpretability needs] - [Condition 4: computational constraints]</p> <p>\u2705 Particularly well-suited for: - [Specific application 1] - [Specific application 2] - [Specific application 3]</p>"},{"location":"models/integrations/template/#when-to-defer","title":"When to Defer","text":"<p>\u26a0\ufe0f Defer to other methods when: - [Condition where this approach is suboptimal] - [Alternative scenario] - [Computational or data constraints]</p> <p>\u26a0\ufe0f Consider alternatives: - [Alternative 1]: [When to use instead] - [Alternative 2]: [When to use instead]</p>"},{"location":"models/integrations/template/#adoption-in-our-neuro-omics-pipeline","title":"Adoption in Our Neuro-Omics Pipeline","text":""},{"location":"models/integrations/template/#current-implementation","title":"Current Implementation","text":"<p>Per-modality setup: - Genetics: [FM choice, embedding dimensions, preprocessing] - Brain: [FM choice, embedding dimensions, preprocessing] - Fusion: [How modalities are combined]</p> <p>Workflow: <pre><code># Step-by-step commands for implementation\n</code></pre></p> <p>Evaluation metrics: - [Metric 1 with rationale] - [Metric 2 with rationale] - [Statistical test for fusion gain]</p>"},{"location":"models/integrations/template/#integration-with-arpa-h-bom","title":"Integration with ARPA-H BOM","text":"<p>[How this approach fits into the Brain-Omics Model escalation strategy]</p> <pre><code>[Escalation diagram showing where this fits]\n</code></pre> <p>Why [start with / escalate to] this approach: - [Rationale 1] - [Rationale 2] - [Rationale 3]</p>"},{"location":"models/integrations/template/#caveats-and-best-practices","title":"Caveats and Best Practices","text":""},{"location":"models/integrations/template/#caveat-1-name","title":"\u26a0\ufe0f [Caveat 1 Name]","text":"<p>Problem: [Description of what can go wrong]</p> <p>Solution: [How to avoid or mitigate the issue] <pre><code># Code example showing correct vs. incorrect approach\n</code></pre></p>"},{"location":"models/integrations/template/#caveat-2-name","title":"\u26a0\ufe0f [Caveat 2 Name]","text":"<p>Problem: [Description]</p> <p>Solution: [Mitigation]</p> <p>[Repeat for additional caveats]</p>"},{"location":"models/integrations/template/#practical-implementation-guide","title":"Practical Implementation Guide","text":""},{"location":"models/integrations/template/#step-1-setup-phase","title":"Step 1: [Setup Phase]","text":"<p>[Detailed instructions for initial setup]</p> Component Configuration Rationale [Item 1] [Config] [Why] [Item 2] [Config] [Why]"},{"location":"models/integrations/template/#step-2-trainingintegration-phase","title":"Step 2: [Training/Integration Phase]","text":"<pre><code># Detailed code example for core integration step\n</code></pre>"},{"location":"models/integrations/template/#step-3-evaluation-phase","title":"Step 3: [Evaluation Phase]","text":"<pre><code># Code for evaluating integration performance\n</code></pre> <p>[Repeat for additional steps as needed]</p>"},{"location":"models/integrations/template/#reference-materials","title":"Reference Materials","text":"<p>Primary paper: - Paper Title (Authors Year) \u2014 see <code>../../generated/kb_curated/papers-md/{{PAPER_SLUG}}.md</code></p> <p>Related integration cards: - {{RELATED_CARD_1}} \u2014 Brief description (link once created) - {{RELATED_CARD_2}} \u2014 Brief description (link once created)</p> <p>KB integration guides: - Integration Strategy \u2014 Overall fusion approach - Design Patterns \u2014 Pattern taxonomy - Multimodal Architectures \u2014 Model examples</p> <p>Analysis recipes: - Reference recipe: <code>../../integration/analysis_recipes/{{RECIPE}}.md</code> - Optional second recipe: <code>../../integration/analysis_recipes/{{RECIPE}}.md</code></p> <p>Model documentation: - Genetics Models \u2014 Gene embedding extraction - Brain Models \u2014 Brain embedding extraction - Multimodal Models \u2014 Fusion architectures</p>"},{"location":"models/integrations/template/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>[Phase 1] \u2014 [Description and deliverable]</li> <li>[Phase 2] \u2014 [Description and deliverable]</li> <li>[Phase 3] \u2014 [Description and deliverable]</li> <li>[Phase 4] \u2014 [Description and deliverable]</li> <li>[Phase 5] \u2014 [Description and deliverable]</li> </ol> <p>Success criteria for escalation: - [Quantitative criterion 1] - [Quantitative criterion 2] - [Qualitative criterion]</p>"},{"location":"models/integrations/template/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>[Takeaway 1] \u2014 [Explanation]</li> <li>[Takeaway 2] \u2014 [Explanation]</li> <li>[Takeaway 3] \u2014 [Explanation]</li> <li>[Takeaway 4] \u2014 [Explanation]</li> <li>[Takeaway 5] \u2014 [Explanation]</li> </ol> <p>Bottom line: [One-sentence summary of when and why to use this integration approach]</p>"},{"location":"models/multimodal/","title":"\ud83c\udfe5 Multimodal &amp; Clinical Foundation Models","text":"<p>Unified multimodal architectures informing gene-brain-behavior integration</p>"},{"location":"models/multimodal/#overview","title":"\ud83d\udccb Overview","text":"<p>This section covers multimodal and clinical foundation models that integrate multiple modalities beyond genetics and neuroimaging, including medical imaging, text, video, and clinical data. These models represent the state-of-the-art in unified multimodal AI for healthcare and general-purpose vision-language understanding.</p>"},{"location":"models/multimodal/#models-in-this-section","title":"\ud83c\udfaf Models in this Section","text":""},{"location":"models/multimodal/#general-multimodal-models","title":"General Multimodal Models","text":"Model Architecture Key Innovation Parameters Documentation \ud83d\udd17 Flamingo Perceiver + gated cross-attention Few-shot multimodal VLM via frozen encoders 3B / 4B / 9B / 80B Walkthrough \ud83d\udd17 BAGEL MoT decoder + SigLIP + VAE Unified understanding + generation 7B active / 14B total Walkthrough \ud83d\udd17 MoT Sparse transformer Modality-aware FFNs (~55% FLOPs) Scales to 7B+ Walkthrough"},{"location":"models/multimodal/#medical-multimodal-models","title":"Medical Multimodal Models","text":"Model Architecture Clinical Focus Languages Documentation \ud83c\udfe5 M3FM CLIP + medical LLM CXR + CT report generation EN + CN Walkthrough \ud83c\udfe5 Me-LLaMA Continual pretrained LLaMA Medical knowledge integration English Walkthrough \ud83c\udfe5 TITAN Vision transformer Whole-slide pathology (gigapixel) English Walkthrough"},{"location":"models/multimodal/#medical-data-catalog","title":"Medical Data Catalog","text":"Resource Coverage Use Case Documentation \ud83d\udcda FMS-Medical 100+ medical datasets Dataset discovery + benchmarking Walkthrough"},{"location":"models/multimodal/#why-multimodal-models-matter-for-neuro-omics","title":"Why Multimodal Models Matter for Neuro-Omics","text":"<p>While the Neuro-Omics KB focuses primarily on genetics and brain foundation models, understanding multimodal integration patterns is critical for:</p> <ol> <li>Integration Strategy Design</li> <li>BAGEL and MoT demonstrate successful architectures for combining diverse modalities</li> <li> <p>Medical models show how to handle domain-specific data with limited labels</p> </li> <li> <p>Zero-Shot Transfer Learning</p> </li> <li>Medical models excel at cross-domain and cross-language generalization</li> <li> <p>These patterns inform how to transfer gene-brain models to new cohorts</p> </li> <li> <p>Clinical Translation</p> </li> <li>Medical VLMs provide templates for integrating brain imaging with clinical text</li> <li> <p>Pathology models show how to scale vision transformers to gigapixel inputs</p> </li> <li> <p>LLM Integration</p> </li> <li>Me-LLaMA demonstrates medical knowledge injection into general LLMs</li> <li>This approach extends to neuro-omics applications (e.g., genetics literature + brain phenotypes)</li> </ol>"},{"location":"models/multimodal/#integration-with-arpa-h-brain-omics-model-bom","title":"Integration with ARPA-H Brain-Omics Model (BOM)","text":"<p>The BOM vision includes multimodal integration beyond gene-brain fusion:</p> <pre><code>Gene embeddings \u2192 |\n                  | \u2192 Brain-Omics Model (BOM) \u2192 Clinical predictions\nBrain embeddings \u2192|                             \u2193\n                  |                         Multimodal LLM\nClinical text    \u2192|                         (reasoning + reports)\n</code></pre> <p>Multimodal models inform the BOM design in three ways:</p>"},{"location":"models/multimodal/#1-architecture-patterns","title":"1. Architecture Patterns","text":"<ul> <li>BAGEL/MoT: Show how to build unified models with understanding + generation</li> <li>M3FM: Demonstrates two-tower CLIP-style alignment for medical domains</li> <li>TITAN: Provides hierarchical vision transformer patterns for multi-scale data</li> </ul>"},{"location":"models/multimodal/#2-training-strategies","title":"2. Training Strategies","text":"<ul> <li>Zero-shot capabilities: Critical for rare diseases and new cohorts</li> <li>Multilingual support: Extends models to diverse global populations</li> <li>Continual pretraining: Me-LLaMA shows how to inject domain knowledge post-hoc</li> </ul>"},{"location":"models/multimodal/#3-clinical-workflows","title":"3. Clinical Workflows","text":"<ul> <li>Report generation: Automated clinical summaries from multimodal inputs</li> <li>Diagnosis support: Combining embeddings for downstream classification</li> <li>Few-shot adaptation: Rapid deployment with minimal labeled data</li> </ul>"},{"location":"models/multimodal/#model-selection-guide","title":"Model Selection Guide","text":"<p>Choose multimodal models based on your integration goals:</p>"},{"location":"models/multimodal/#for-architecture-design","title":"For Architecture Design","text":"<ul> <li>If building unified understanding + generation:</li> <li>Start with BAGEL or MoT architectures</li> <li>These show how to handle multiple modalities in one model</li> </ul>"},{"location":"models/multimodal/#for-medical-applications","title":"For Medical Applications","text":"<ul> <li>If working with medical imaging + text:</li> <li>Use M3FM for CLIP-style alignment</li> <li> <p>Consider TITAN for pathology/high-resolution imaging</p> </li> <li> <p>If integrating medical knowledge with LLMs:</p> </li> <li>Study Me-LLaMA for continual pretraining approaches</li> <li>See FMS-Medical for dataset selection</li> </ul>"},{"location":"models/multimodal/#for-zero-shot-transfer","title":"For Zero-Shot Transfer","text":"<ul> <li>If targeting low-resource settings:</li> <li>All medical models demonstrate strong zero-shot capabilities</li> <li>M3FM is particularly strong for cross-language transfer</li> </ul>"},{"location":"models/multimodal/#next-steps","title":"Next Steps","text":"<ol> <li>Read model pages:</li> <li> <p>Each model page includes architecture details, integration strategies, and reference materials</p> </li> <li> <p>Review integration patterns:</p> </li> <li>See Design Patterns for fusion architectures</li> <li> <p>Check Multimodal Architectures for detailed integration guides</p> </li> <li> <p>Explore code walkthroughs:</p> </li> <li> <p>Practical implementation details in Code Walkthroughs</p> </li> <li> <p>Study paper summaries:</p> </li> <li>Full paper notes available in Research Papers section (see site navigation)</li> </ol>"},{"location":"models/multimodal/#reference-materials","title":"Reference Materials","text":"<ul> <li>Integration strategy: integration/integration_strategy.md</li> <li>Design patterns: integration/design_patterns.md</li> <li>Multimodal architectures: integration/multimodal_architectures.md</li> <li>ARPA-H vision: See integration plan (Nov 2025) in decisions folder</li> </ul>"},{"location":"models/multimodal/bagel/","title":"BAGEL","text":""},{"location":"models/multimodal/bagel/#overview","title":"Overview","text":"<p>Type: Unified Multimodal Foundation Model Architecture: Qwen2 MoT decoder + SigLIP-NaViT encoder + FLUX VAE Modality: Text, images, video, web content (interleaved sequences) Primary use: Unified multimodal understanding and generation with emergent reasoning capabilities</p>"},{"location":"models/multimodal/bagel/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>BAGEL (Bottleneck-free Architecture for Generation and Education-rich Learning) is an open-source unified multimodal foundation model that performs both understanding and generation across text, images, and video within a single architecture. Built on a Qwen2.5-derived decoder with Mixture-of-Transformers (MoT) experts\u2014one for understanding, one for generation\u2014BAGEL processes trillions of interleaved tokens to achieve emergent capabilities like free-form visual manipulation, 3D understanding, and world navigation.</p> <p>Key innovation: Unlike models with separate understanding/generation modules, BAGEL uses shared self-attention across a unified token sequence, allowing tight coupling between reasoning and generation without architectural bottlenecks.</p>"},{"location":"models/multimodal/bagel/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>MoT structure: Two experts (understanding + generation) share the same token sequence via common self-attention</li> <li>Visual encoders: </li> <li>SigLIP2-so400m/14 with NaViT for understanding (native aspect ratios)</li> <li>FLUX VAE for generation (latent tokens, 8\u00d7 downsampled, 16 channels)</li> <li>Backbone: Qwen2.5 decoder with RMSNorm, SwiGLU, RoPE, GQA, QK-Norm</li> <li>Training objective: Next-token prediction for text; rectified flow diffusion for visual tokens</li> <li>Scale: 7B active parameters, 14B total; trained on trillions of interleaved multimodal tokens</li> </ul>"},{"location":"models/multimodal/bagel/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/multimodal/bagel/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>BAGEL provides architectural templates for multimodal integration:</p> <p>Key lessons for gene-brain-behavior fusion: - Unified sequences: How to process heterogeneous modalities (genes, brain scans, behavior) in one forward pass - Expert specialization: MoT pattern adaptable to \"genomics expert\" + \"brain expert\" + shared attention - Interleaved data: Training on mixed sequences improves cross-modal reasoning</p> <p>Not directly used in KB pipeline (no neuroscience pretraining), but informs: - Design patterns for late-stage multimodal fusion (see Design Patterns) - LLM-as-semantic-bridge architectures for ARPA-H BOM - Evaluation strategies for emergent multimodal capabilities</p>"},{"location":"models/multimodal/bagel/#for-arpa-h-brain-omics-model-bom","title":"For ARPA-H Brain-Omics Model (BOM)","text":"<p>BAGEL demonstrates how to build bottleneck-free unified models:</p> <pre><code>Gene embeddings    \u2192 |\n                     | Shared self-attention over unified sequence\nBrain embeddings   \u2192 |     \u2193\n                     | Expert routing (MoT-style)\nClinical text      \u2192 |     \u2193\n                     | Understanding + generation heads\nBehavioral data    \u2192 |\n</code></pre> <p>Transfer insights: - Emergent reasoning: BAGEL shows that scaling interleaved data produces complex reasoning\u2014applicable to gene-brain-behavior associations - CFG for generation: Classifier-free guidance patterns transferable to conditional brain image synthesis - Long-context modeling: NaiveCache streaming inference applicable to longitudinal neuroimaging sequences</p>"},{"location":"models/multimodal/bagel/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<p>BAGEL is not used for embedding extraction in the Neuro-Omics KB (domain mismatch), but if adapting for clinical imaging + reports:</p> <pre><code># 1. Prepare interleaved sequences (image patches + text tokens)\n# 2. Load pretrained BAGEL checkpoint\n# 3. Forward through shared self-attention + MoT experts\n# 4. Extract pre-head embeddings (not task-specific outputs)\n# 5. Pool to subject-level vectors for downstream fusion\n</code></pre> <p>For clinical extension: See M3FM for medical imaging integration patterns.</p>"},{"location":"models/multimodal/bagel/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/multimodal/bagel/#strengths","title":"Strengths","text":"<ul> <li>Unified architecture: Single model for understanding + generation without bottlenecks</li> <li>Emergent capabilities: Free-form manipulation, multiview synthesis, world navigation</li> <li>Open-source: Full code, checkpoints, and quantized inference (NF4, INT8)</li> <li>Scalable: FSDP training, packed sequences, MFU telemetry for large-scale runs</li> </ul>"},{"location":"models/multimodal/bagel/#limitations","title":"Limitations","text":"<ul> <li>Compute intensive: Training requires substantial resources (trillions of tokens)</li> <li>General domain: Not specialized for neuroscience or genomics</li> <li>Deployment costs: 7B\u201314B parameters require high-memory GPUs (12\u201380 GB)</li> <li>Data requirements: Interleaved multimodal corpora hard to curate for domain-specific tasks</li> </ul>"},{"location":"models/multimodal/bagel/#when-to-use-bagel","title":"When to Use BAGEL","text":"<p>\u2705 Use as reference when: - Designing unified multimodal architectures for neuro-omics - Exploring MoT-style expert routing for gene + brain modalities - Building LLM-guided clinical report generation from brain imaging</p> <p>\u26a0\ufe0f Do not use directly for: - Neuroimaging embedding extraction (use BrainLM, SwiFT, etc.) - Genetic sequence modeling (use Caduceus, Evo2, etc.) - Production clinical workflows (general model, not clinically validated)</p> <p>\u26a0\ufe0f Consider alternatives: - M3FM: For medical imaging + text with CLIP-style alignment - BrainMT: For neuroimaging with efficient long-context modeling - Caduceus + BrainLM fusion: For gene-brain integration with domain-specific FMs</p>"},{"location":"models/multimodal/bagel/#reference-materials","title":"Reference Materials","text":""},{"location":"models/multimodal/bagel/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): BAGEL (2025) - Code walkthrough: BAGEL walkthrough - Model card (YAML): <code>kb/model_cards/bagel.yaml</code> (if exists) - Paper card (YAML): <code>kb/paper_cards/bagel_2025.yaml</code></p> <p>Integration recipes: - Multimodal Architectures - Design Patterns - Integration Strategy</p>"},{"location":"models/multimodal/bagel/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/bagel/</code> - Official GitHub: ChaofanTao/BAGEL</p> <p>Original paper: - Title: \"Emerging Properties in Unified Multimodal Pretraining\" - Authors: Deng, Chaorui; Zhu, Deyao; Li, Kunchang; Gou, Chenhui; Li, Feng; Wang, Zeyu; Zhong, Shu; Yu, Weihao; Nie, Xiaonan; Song, Ziang; Shi, Guang; Fan, Haoqi - Published: arXiv preprint, 2025 - Link: arXiv:2505.14683 - PDF (local): bagel_2025.pdf</p>"},{"location":"models/multimodal/bagel/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Architecture study: Extract MoT patterns for potential gene-brain expert routing</li> <li>Interleaved data design: Inform how to structure mixed gene + brain + behavior sequences</li> <li>LLM integration: Study CFG and generation strategies for clinical report synthesis</li> <li>Evaluation framework: Adapt IntelligentBench patterns for neuro-omics emergent capabilities</li> <li>Clinical extension: Combine BAGEL insights with M3FM for brain imaging + clinical text</li> </ol>"},{"location":"models/multimodal/bagel/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>BAGEL uses packed sequences with modality-specific indices\u2014applicable to gene + brain token mixing</li> <li>CFG contexts (text/image guidance) are plain dicts\u2014easy to extend to clinical conditioning</li> <li>Quantization (NF4, INT8) provides deployment patterns for resource-constrained clinical settings</li> <li>FSDP + EMA training pipeline applicable to large-scale neuro-omics model training</li> </ul>"},{"location":"models/multimodal/flamingo/","title":"Flamingo","text":""},{"location":"models/multimodal/flamingo/#overview","title":"Overview","text":"<p>Type: Visual language model (VLM) Architecture: Perceiver-augmented vision encoder + gated cross-attention over a causal LM Modality: Images, videos, text (interleaved) Primary use: Few-shot multimodal understanding and text generation</p>"},{"location":"models/multimodal/flamingo/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Flamingo extends large language models to the visual domain by bridging frozen vision and language backbones with a lightweight Perceiver Resampler and gated cross-attention layers. Instead of fine-tuning on each downstream task, Flamingo is trained once on large-scale web multimodal data and then adapted via in-context examples, mirroring GPT-3-style few-shot prompting for text.^See arXiv:2204.14198</p> <p>Key idea: keep powerful pretrained vision and language models intact, and add minimal, well- behaved connectors that enable multimodal reasoning without catastrophic forgetting.</p>"},{"location":"models/multimodal/flamingo/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Vision encoder: Pretrained CLIP/OpenCLIP-style ViT (e.g., ViT-L/14) for images or video frames.</li> <li>Perceiver Resampler: Converts variable-resolution feature maps into a fixed set of visual tokens   via cross-attention from learnable latent queries.</li> <li>Language model: Pretrained causal LM (e.g., Chinchilla/MPT/RedPajama), largely frozen.</li> <li>GATED XATTN-DENSE layers: Inserted between LM blocks to cross-attend from text tokens to visual   tokens, with tanh-gated residuals for stable training.</li> <li>Interleaved inputs: Sequences of images/videos and text with <code>&lt;image&gt;</code> and <code>&lt;|endofchunk|&gt;</code>   markers; image-causal masking ensures each text span only sees its associated images.</li> </ul> <p>For implementation details, see the OpenFlamingo factory and Flamingo wrapper in the code walkthrough.</p>"},{"location":"models/multimodal/flamingo/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/multimodal/flamingo/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Flamingo is not a primary model in current neuro-omics experiments but serves as a design reference for:</p> <ul> <li>Scan-conditioned report generation: Replace the CLIP encoder with brain encoders (BrainLM,   BrainMT, Brain Harmony) so fMRI/sMRI tokens play the role of image tokens.</li> <li>Multimodal adapters: Reuse the Perceiver Resampler concept for compressing high-dimensional   brain features into a fixed number of tokens.</li> <li>LLM semantic bridge: Use Flamingo-style gated cross-attention to inject brain/genetics   embeddings into language models (see <code>kb/model_cards/llm_semantic_bridge.yaml</code>).</li> </ul>"},{"location":"models/multimodal/flamingo/#for-arpa-h-brain-omics-models","title":"For ARPA-H Brain-Omics Models","text":"<p>Flamingo illustrates how to:</p> <ul> <li>Keep foundation encoders frozen while adding small multimodal connectors.</li> <li>Structure interleaved multimodal sequences that include context examples followed by a query.</li> <li>Build few-shot-capable architectures without task-specific heads for every benchmark.</li> </ul> <p>These patterns carry over to Brain\u2013Omics\u2013LLM stacks that must reason jointly over genetics, brain imaging, and clinical text.</p>"},{"location":"models/multimodal/flamingo/#reference-materials","title":"Reference Materials","text":""},{"location":"models/multimodal/flamingo/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<ul> <li>Paper summary: <code>docs/generated/kb_curated/papers-md/flamingo_2022.md</code></li> <li>Paper card (YAML): <code>kb/paper_cards/flamingo_2022.yaml</code></li> <li>Code walkthrough: <code>docs/code_walkthroughs/flamingo_walkthrough.md</code></li> <li>Model card (YAML): <code>kb/model_cards/flamingo.yaml</code></li> </ul>"},{"location":"models/multimodal/flamingo/#original-sources","title":"Original Sources","text":"<ul> <li>Official implementation: OpenFlamingo GitHub</li> <li>Paper: Flamingo: a Visual Language Model for Few-Shot Learning (NeurIPS 2022)^arXiv:2204.14198</li> </ul>"},{"location":"models/multimodal/fms_medical/","title":"FMS-Medical: Foundation Models for Advancing Healthcare (Catalog)","text":""},{"location":"models/multimodal/fms_medical/#overview","title":"Overview","text":"<p>Type: Knowledge Base / Survey Resource Format: Curated repository of medical foundation models + datasets Coverage: Language (LFM), Vision (VFM), Bioinformatics (BFM), Multimodal (MFM) Primary use: Model and dataset discovery, benchmarking reference, literature review</p>"},{"location":"models/multimodal/fms_medical/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>FMS-Medical is an \"awesome list\" style knowledge base tracking foundation model research across healthcare modalities. Maintained as a GitHub repository with bilingual documentation (English + Chinese), it provides structured references to 100+ medical FM papers, datasets, tutorials, and related resources organized by modality and year. The resource is anchored by an IEEE Reviews in Biomedical Engineering survey paper and serves as a comprehensive entry point for medical AI research.</p> <p>Key value: Centralized, actively maintained catalog of medical FMs and datasets\u2014ideal for systematic literature review and baseline selection.</p>"},{"location":"models/multimodal/fms_medical/#catalog-structure","title":"Catalog Structure","text":""},{"location":"models/multimodal/fms_medical/#model-taxonomies","title":"Model Taxonomies","text":"Category Coverage Use Case LFM (Language) Medical LLMs, clinical NLP Text understanding, report generation, QA VFM (Vision) Medical image encoders, segmentation Radiology, pathology, ultrasound analysis BFM (Bioinformatics) Genomics, proteomics, drug discovery Sequence modeling, variant interpretation MFM (Multimodal) Vision-language, integrated models Unified diagnosis, multimodal reasoning"},{"location":"models/multimodal/fms_medical/#dataset-catalogs","title":"Dataset Catalogs","text":"<ul> <li>Text datasets: Clinical notes, radiology reports, biomedical literature</li> <li>Imaging datasets: CXR, CT, MRI, pathology, ultrasound, retinal</li> <li>Omics datasets: Genomics, transcriptomics, proteomics</li> <li>Multimodal datasets: Image-text pairs, integrated EHR + imaging</li> </ul>"},{"location":"models/multimodal/fms_medical/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/multimodal/fms_medical/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>FMS-Medical provides dataset discovery and model benchmarking:</p> <p>Key uses: - Literature review: Identify related work in medical AI for neuro-omics - Dataset selection: Find imaging and genomics datasets for validation - Baseline comparison: Track state-of-the-art methods for benchmarking - Survey reference: Cite comprehensive medical FM survey</p> <p>Application to KB pipeline: <pre><code>FMS-Medical catalog\n    \u2193\nExtract relevant entries:\n    - Brain imaging models\n    - Genomics foundation models\n    - Multimodal medical datasets\n    \u2193\nPopulate KB model/dataset cards\n    \u2193\nBenchmark neuro-omics methods against medical FM baselines\n</code></pre></p>"},{"location":"models/multimodal/fms_medical/#for-arpa-h-brain-omics-model-bom","title":"For ARPA-H Brain-Omics Model (BOM)","text":"<p>FMS-Medical informs medical AI landscape understanding:</p> <pre><code>Survey medical FMs\n    \u2193\nIdentify integration patterns:\n    - CLIP-style alignment (M3FM, TITAN)\n    - MoT/MoE architectures\n    - LLM continual pretraining (Me-LLaMA)\n    \u2193\nApply to BOM design\n</code></pre> <p>Transfer insights: - Comprehensive coverage: Survey spans all medical FM modalities\u2014identify gaps for neuro-omics - Dataset catalogs: Find publicly available datasets for cross-validation - Benchmark references: Track medical FM performance for comparison - Bilingual support: Chinese documentation aids international collaboration</p>"},{"location":"models/multimodal/fms_medical/#catalog-access-workflow","title":"Catalog Access Workflow","text":"<pre><code># 1. Clone or sync FMS-Medical repository\ngit clone https://github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare\n\n# 2. Review README for model/dataset tables\n\n# 3. Extract entries relevant to neuro-omics:\n#    - VFM: Brain imaging models\n#    - BFM: Genomics models\n#    - MFM: Multimodal integration patterns\n\n# 4. Populate KB YAML cards from extracted entries\n\n# 5. Track updates via GitHub (actively maintained)\n</code></pre> <p>For KB automation: - Parse README tables to generate candidate model/dataset cards - Link to survey PDFs for detailed descriptions - Monitor repository updates for new medical FMs</p>"},{"location":"models/multimodal/fms_medical/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/multimodal/fms_medical/#strengths","title":"Strengths","text":"<ul> <li>Comprehensive coverage: 100+ medical FMs across all modalities</li> <li>Actively maintained: Regular updates with publication news</li> <li>Bilingual: English + Chinese documentation</li> <li>Well-organized: Structured by modality, year, and venue</li> <li>Survey anchor: Peer-reviewed IEEE Reviews paper provides synthesis</li> </ul>"},{"location":"models/multimodal/fms_medical/#limitations","title":"Limitations","text":"<ul> <li>Documentation-only: No executable code or model weights</li> <li>General medical focus: Limited neuro-omics specific content</li> <li>No unified schema: Markdown tables, not structured YAML/JSON</li> <li>Citation lag: New models may take time to appear in catalog</li> </ul>"},{"location":"models/multimodal/fms_medical/#when-to-use-fms-medical","title":"When to Use FMS-Medical","text":"<p>\u2705 Use when: - Starting literature review on medical FMs - Selecting baseline models for benchmarking - Finding publicly available medical datasets - Identifying integration patterns from related work - Citing comprehensive medical FM surveys</p> <p>\u26a0\ufe0f Not a substitute for: - Model implementation code (links to external repos) - Pretrained model weights (links to paper/HuggingFace) - Executable benchmarking pipelines (reference only)</p> <p>\u26a0\ufe0f Complement with: - Papers With Code: For benchmark leaderboards - HuggingFace Model Hub: For pretrained weights - Model-specific repos: For implementation details</p>"},{"location":"models/multimodal/fms_medical/#reference-materials","title":"Reference Materials","text":""},{"location":"models/multimodal/fms_medical/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Code walkthrough: FMS-Medical walkthrough - Dataset card (YAML): <code>kb/datasets/fms_medical_catalog.yaml</code></p> <p>Integration recipes: - Integration Strategy \u2014 Dataset selection - KB Overview \u2014 Catalog integration patterns</p>"},{"location":"models/multimodal/fms_medical/#original-sources","title":"Original Sources","text":"<p>Source repositories: - Local copy: <code>external_repos/fms-medical/</code> - Official GitHub: YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare</p> <p>Original survey paper: - Title: \"Foundation Models for Healthcare\" - Authors: Yuting He, et al. - Published: IEEE Reviews in Biomedical Engineering, 2024 - Link (IEEE): IEEE Xplore: 10750441 - Link (arXiv): arXiv:2404.03264</p>"},{"location":"models/multimodal/fms_medical/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Systematic extraction: Parse FMS-Medical tables to populate KB model/dataset cards</li> <li>Gap analysis: Identify medical FM capabilities missing from neuro-omics KB</li> <li>Benchmark selection: Choose medical FM baselines for comparison experiments</li> <li>Dataset discovery: Find publicly available datasets for cross-validation</li> <li>Literature tracking: Monitor repository updates for new medical FM methods</li> </ol>"},{"location":"models/multimodal/fms_medical/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>FMS-Medical is pure documentation\u2014no code dependencies</li> <li>Bilingual PDFs in <code>files/</code> directory useful for international teams</li> <li>Citation metadata in tables ready for automated KB card generation</li> <li>Active maintenance\u2014check GitHub for recent updates before citing</li> <li>Survey paper provides narrative synthesis complementing tabular catalog</li> </ul>"},{"location":"models/multimodal/m3fm/","title":"M3FM (Multimodal, Multidomain, Multilingual Medical Foundation Model)","text":""},{"location":"models/multimodal/m3fm/#overview","title":"Overview","text":"<p>Type: Medical Vision-Language Foundation Model Architecture: MultiMedCLIP (CLIP-style) + MultiMedLM (medical LLM) Modality: Chest X-ray, CT, radiology reports (English + Chinese) Primary use: Zero-shot medical report generation and disease diagnosis across domains and languages</p>"},{"location":"models/multimodal/m3fm/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>M3FM is a medical foundation model designed for zero-shot radiology report generation and diagnosis across imaging modalities (CXR, CT) and languages (English, Chinese). The model learns a shared vision-language embedding space through contrastive learning (MultiMedCLIP), then trains a multilingual medical LLM (MultiMedLM) to generate reports and support diagnosis without labeled data in the target domain or language.</p> <p>Key innovation: Single model handles multiple imaging modalities and languages through CLIP-style alignment + medical LLM, enabling deployment where labeled data is scarce.</p>"},{"location":"models/multimodal/m3fm/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Two-stage training:</li> <li>Stage 1: MultiMedCLIP aligns images (CXR, CT) with English text, and English-Chinese text pairs</li> <li>Stage 2: MultiMedLM trained on multilingual corpora for report generation</li> <li>Vision encoder: CNN/ViT encoding CXR and CT to visual embeddings</li> <li>Text encoder/decoder: Transformer-based for English and Chinese reports</li> <li>Alignment: CLIP-like contrastive loss creates shared embedding space</li> <li>Inference: Zero-shot report generation via visual \u2192 text decoding through aligned space</li> </ul>"},{"location":"models/multimodal/m3fm/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/multimodal/m3fm/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>M3FM provides medical imaging integration patterns:</p> <p>Key lessons for brain imaging + clinical text: - Two-tower alignment: Separate brain imaging encoder + clinical text encoder with contrastive loss - Zero-shot transfer: Applicable to new cohorts (e.g., Cha Hospital) without labeled data - Multilingual support: Extend brain-behavior models to non-English populations - Report generation: Automate clinical summaries from neuroimaging</p> <p>Potential adaptation: <pre><code>Brain MRI/fMRI \u2192 Vision encoder (SwiFT/BrainLM) \u2192 |\n                                                   | Contrastive alignment\nClinical notes \u2192 Text encoder (medical LLM)     \u2192 |\n                                                   \u2193\n                                           Shared latent space\n                                                   \u2193\n                                           Report generation LLM\n</code></pre></p>"},{"location":"models/multimodal/m3fm/#for-arpa-h-brain-omics-model-bom","title":"For ARPA-H Brain-Omics Model (BOM)","text":"<p>M3FM demonstrates clinical translation patterns:</p> <pre><code>Brain embeddings \u2192 |\n                   | Two-tower contrastive alignment\nClinical text    \u2192 |     \u2193\n                   | Shared embedding space\nGene annotations \u2192 |     \u2193\n                   | Medical LLM for report generation\n</code></pre> <p>Transfer insights: - Zero-shot diagnosis: Critical for rare neurological disorders with limited training data - Cross-domain generalization: M3FM's CXR\u2192CT transfer informs MRI\u2192fMRI\u2192CT transfers - Multilingual clinical AI: Extend neuro-omics models to global cohorts - Few-shot learning: Strong performance with minimal downstream labels</p>"},{"location":"models/multimodal/m3fm/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<p>If adapting M3FM for brain imaging:</p> <pre><code># 1. Train CLIP-style alignment on brain scans + clinical notes\n# 2. Load pretrained brain FM (SwiFT, BrainLM) as vision encoder\n# 3. Load medical LLM (Me-LLaMA, etc.) as text encoder\n# 4. Contrastive training on paired brain-text data\n# 5. Extract embeddings from shared space for downstream tasks\n</code></pre> <p>For neuro-omics: - Vision encoder: SwiFT (fMRI) or BrainLM (3D volumes) - Text encoder: Medical LLM pretrained on neurology literature - Alignment data: Brain scans + radiology reports from UKB, HCP</p>"},{"location":"models/multimodal/m3fm/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/multimodal/m3fm/#strengths","title":"Strengths","text":"<ul> <li>Genuine zero-shot: Generates reports without labeled downstream data</li> <li>Cross-domain + cross-language: Single model handles CXR, CT, English, Chinese</li> <li>Clinical validation: Evaluated on 9 downstream datasets (COVID-19, TB, etc.)</li> <li>Practical: Leverages machine translation to bootstrap multilingual capabilities</li> </ul>"},{"location":"models/multimodal/m3fm/#limitations","title":"Limitations","text":"<ul> <li>Machine translation artifacts: Reliance on MT for Chinese may introduce biases</li> <li>Modality coverage: Only CXR and CT\u2014no MRI, ultrasound, pathology</li> <li>Compute intensive: Requires substantial resources for two-stage training</li> <li>Evaluation gaps: Standard metrics may not capture clinical safety</li> </ul>"},{"location":"models/multimodal/m3fm/#when-to-use-m3fm","title":"When to Use M3FM","text":"<p>\u2705 Use as reference when: - Building brain imaging + clinical text models - Designing zero-shot transfer for new cohorts - Implementing CLIP-style alignment for neuro-omics - Supporting multilingual neuroimaging research</p> <p>\u26a0\ufe0f Do not use directly for: - Neuroimaging (trained on CXR/CT, not brain scans) - Production clinical diagnosis (requires validation) - Non-imaging modalities (no genetics support)</p> <p>\u26a0\ufe0f Consider alternatives: - BAGEL/MoT: For unified understanding + generation - TITAN: For high-resolution pathology imaging - Me-LLaMA: For medical LLM without imaging</p>"},{"location":"models/multimodal/m3fm/#reference-materials","title":"Reference Materials","text":""},{"location":"models/multimodal/m3fm/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): M3FM (2025) - Code walkthrough: M3FM walkthrough - Model card (YAML): <code>kb/model_cards/m3fm.yaml</code> - Paper card (YAML): <code>kb/paper_cards/m3fm_2025.yaml</code></p> <p>Integration recipes: - Multimodal Architectures - Design Patterns \u2014 Two-tower contrastive section - Integration Strategy</p>"},{"location":"models/multimodal/m3fm/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/M3FM/</code> - Official GitHub: ai-in-health/M3FM</p> <p>Original paper: - Title: \"M3FM: A Multimodal, Multidomain, Multilingual Medical Foundation Model for Zero\u2011Shot Clinical Diagnosis\" - Authors: Liu, Fenglin; Li, Zheng; Yin, Qingyu; Huang, Jinfa; Luo, Jiebo; Thakur, Anshul; Branson, Kim; Schwab, Patrick; Yin, Bing; Wu, Xian; Zheng, Yefeng; Clifton, David A. - Published: npj Digital Medicine, 2025 - Link: Nature: s41746-024-01339-7 - DOI: 10.1038/s41746-024-01339-7 - PDF (local): m3fm_2025.pdf</p>"},{"location":"models/multimodal/m3fm/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>CLIP adaptation: Implement brain imaging + clinical text contrastive learning</li> <li>Zero-shot evaluation: Test on new cohorts (Cha Hospital) without fine-tuning</li> <li>Multilingual extension: Adapt to Korean clinical notes for Cha pediatric cohort</li> <li>Report generation: Automate neuroimaging report synthesis from embeddings</li> <li>Diagnostic support: Combine M3FM patterns with gene-brain fusion for clinical predictions</li> </ol>"},{"location":"models/multimodal/m3fm/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>M3FM's two-stage training separates alignment from generation\u2014applicable to neuro-omics</li> <li>Contrastive learning requires paired data\u2014use UKB radiology reports + imaging</li> <li>Machine translation can bootstrap multilingual capabilities before human-labeled data available</li> <li>Zero-shot evaluation critical for rare neurological disorders with &lt;100 cases</li> </ul>"},{"location":"models/multimodal/me_llama/","title":"Me-LLaMA (Medical Large Language Model)","text":""},{"location":"models/multimodal/me_llama/#overview","title":"Overview","text":"<p>Type: Medical Foundation Large Language Model Architecture: LLaMA-2 with continual pretraining + instruction tuning Modality: Text (biomedical literature + clinical notes) Primary use: Medical text understanding, generation, and clinical reasoning</p>"},{"location":"models/multimodal/me_llama/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Me-LLaMA is a family of open-source medical foundation LLMs (13B and 70B parameters) built by continually pretraining LLaMA-2 on 129 billion tokens of biomedical literature and clinical notes, then instruction-tuning on 214k medical task examples. The model targets comprehensive medical text analysis across question answering, named entity recognition, relation extraction, classification, summarization, natural language inference, and complex clinical case reasoning.</p> <p>Key innovation: Large-scale continual pretraining on diverse medical corpora (literature + clinical notes + general text) enables Me-LLaMA to match or exceed GPT-4 on several medical benchmarks while remaining fully open-source.</p>"},{"location":"models/multimodal/me_llama/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Backbone: LLaMA-2 decoder-only transformers (13B and 70B parameters)</li> <li>Continual pretraining: 129B tokens from biomedical literature + clinical notes + general text</li> <li>Instruction tuning: 214k multi-task medical instructions covering 6+ task families</li> <li>Model family: Base models (Me-LLaMA-13B/70B) and chat models (Me-LLaMA-13B/70B-chat)</li> <li>Evaluation: 12 benchmarks + clinical case diagnosis vs open-source and commercial LLMs</li> </ul>"},{"location":"models/multimodal/me_llama/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/multimodal/me_llama/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>Me-LLaMA provides medical LLM integration patterns:</p> <p>Key lessons for neuro-omics text integration: - Continual pretraining: How to inject domain knowledge into general LLMs - Clinical + literature mix: Balance research articles with real-world clinical language - Instruction tuning: Multi-task learning across diverse neuro-omics NLP tasks - Zero-shot transfer: Applicable to new clinical scenarios without labeled data</p> <p>Potential adaptation for neuro-omics: <pre><code>General LLM (LLaMA-2) \n    \u2193\nContinual pretrain on:\n    - Neuroscience literature (PubMed)\n    - Genetics literature (dbGaP, ClinVar annotations)\n    - Clinical neurology notes\n    \u2193\nInstruction tune on:\n    - Gene-disease QA\n    - Brain phenotype description\n    - Genetic counseling dialogs\n    \u2193\nNeuro-Omics LLM\n</code></pre></p>"},{"location":"models/multimodal/me_llama/#for-arpa-h-brain-omics-model-bom","title":"For ARPA-H Brain-Omics Model (BOM)","text":"<p>Me-LLaMA demonstrates LLM as semantic bridge:</p> <pre><code>Gene embeddings   \u2192 |\n                    | Feature extraction\nBrain embeddings  \u2192 |     \u2193\n                    | Medical LLM (Me-LLaMA-style)\nClinical notes    \u2192 |     \u2193\n                    | Unified reasoning + report generation\n</code></pre> <p>Transfer insights: - Knowledge injection: Add neuroscience + genetics knowledge to general LLMs - Clinical reasoning: Complex case-based diagnosis applicable to neurological disorders - Multimodal bridge: LLM connects structured embeddings (gene, brain) with unstructured text - Report generation: Automate clinical summaries from multimodal neuro-omics inputs</p>"},{"location":"models/multimodal/me_llama/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<p>If adapting Me-LLaMA for neuro-omics:</p> <pre><code># 1. Collect neuroscience + genetics text corpora\n#    - PubMed Central (neuroscience + genetics papers)\n#    - Clinical neurology notes (de-identified)\n#    - Genetic variant annotations (ClinVar, dbGaP)\n# 2. Continual pretrain LLaMA-2 on domain corpora\n# 3. Curate instruction-tuning dataset\n#    - Gene-disease QA\n#    - Brain phenotype classification\n#    - Clinical case reasoning\n# 4. Instruction tune and evaluate on medical NLP tasks\n# 5. Use as semantic bridge for gene-brain-text integration\n</code></pre> <p>For neuro-omics KB: - Text encoder: Extract embeddings from Me-LLaMA for clinical notes - Semantic alignment: Align gene/brain embeddings with text embeddings - Report generation: Generate clinical summaries from gene-brain predictions</p>"},{"location":"models/multimodal/me_llama/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/multimodal/me_llama/#strengths","title":"Strengths","text":"<ul> <li>Large-scale continual pretraining: 129B tokens from diverse medical sources</li> <li>Open-source: Fully released models, data, and code</li> <li>Comprehensive evaluation: 12 benchmarks + clinical case diagnosis</li> <li>Competitive performance: Matches or exceeds GPT-4 on several medical tasks</li> </ul>"},{"location":"models/multimodal/me_llama/#limitations","title":"Limitations","text":"<ul> <li>Text-only: No vision or multimodal capabilities (unlike M3FM, TITAN)</li> <li>Compute intensive: Training requires &gt;100k GPU hours</li> <li>Clinical validation: Strong benchmarks but limited real-world deployment data</li> <li>Data access: Clinical notes require institutional access and IRB approval</li> </ul>"},{"location":"models/multimodal/me_llama/#when-to-use-me-llama","title":"When to Use Me-LLaMA","text":"<p>\u2705 Use as reference when: - Building neuro-omics text understanding models - Designing continual pretraining strategies for domain LLMs - Creating instruction-tuning datasets for medical NLP - Integrating LLMs as semantic bridges in multimodal systems</p> <p>\u26a0\ufe0f Do not use directly for: - Multimodal gene-brain integration (text-only model) - Vision-language tasks (no image encoder) - Production clinical diagnosis (requires validation)</p> <p>\u26a0\ufe0f Consider alternatives: - M3FM: For medical imaging + text with CLIP-style alignment - BAGEL: For unified understanding + generation with vision - TITAN: For whole-slide pathology with vision-language alignment</p>"},{"location":"models/multimodal/me_llama/#reference-materials","title":"Reference Materials","text":""},{"location":"models/multimodal/me_llama/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): Me-LLaMA (2024) - Code walkthrough: Me-LLaMA walkthrough - Model card (YAML): <code>kb/model_cards/me_llama.yaml</code> - Paper card (YAML): <code>kb/paper_cards/me_llama_2024.yaml</code></p> <p>Integration recipes: - Multimodal Architectures - Design Patterns \u2014 LLM as semantic bridge - Integration Strategy</p>"},{"location":"models/multimodal/me_llama/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/me-lamma/</code> - Official GitHub: BIDS-Xu-Lab/Me-LLaMA</p> <p>Original paper: - Title: \"Me-LLaMA: Medical Foundation Large Language Models for Comprehensive Text Analysis and Clinical Reasoning\" - Authors: Xie, Qianqian; Chen, Qingyu; Chen, Aokun; Peng, Cheng; Hu, Yan; Lin, Fongci; Peng, Xueqing; Huang, Jimin; Zhang, Jeffrey; Keloth, Vipina; Zhou, Xinyu; Qian, Lingfei; He, Huan; Shung, Dennis; Ohno\u2011Machado, Lucila; Wu, Yonghui; Xu, Hua; Bian, Jiang - Published: Preprint, 2024 - Link: arXiv:2404.05416 - PDF (local): me_llama_2024.pdf</p>"},{"location":"models/multimodal/me_llama/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Domain corpus curation: Collect neuroscience + genetics literature for continual pretraining</li> <li>Instruction dataset design: Create neuro-omics QA, NER, RE task datasets</li> <li>Continual pretraining: Adapt LLaMA-2 to neuroscience + genetics domains</li> <li>Semantic bridge integration: Connect gene/brain embeddings with LLM text space</li> <li>Clinical report generation: Automate neuroimaging + genetics summaries</li> </ol>"},{"location":"models/multimodal/me_llama/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>Me-LLaMA's mixture weighting (general + biomedical + clinical) preserves broad language competence</li> <li>Instruction tuning on 214k examples spans 6 task families\u2014applicable to neuro-omics NLP</li> <li>Clinical case reasoning evaluation critical for validating complex diagnostic capabilities</li> <li>Open-source release includes base and chat models\u2014study instruction tuning strategies</li> </ul>"},{"location":"models/multimodal/mot/","title":"Mixture-of-Transformers (MoT)","text":""},{"location":"models/multimodal/mot/#overview","title":"Overview","text":"<p>Type: Sparse Multimodal Transformer Architecture Architecture: Modality-aware sparse transformer with global self-attention Modality: Text, images, speech (unified token sequences) Primary use: Compute-efficient multimodal foundation models with 40\u201360% FLOP savings</p>"},{"location":"models/multimodal/mot/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>Mixture-of-Transformers (MoT) introduces modality-aware sparsity to make large multimodal foundation models dramatically more efficient. Instead of a single dense transformer over all modalities, MoT decouples all non-embedding parameters (FFNs, attention projections, layer norms) by modality while keeping global self-attention over the full sequence. This structured sparsity matches dense baseline performance while using only 40\u201360% of pretraining FLOPs and significantly reduces wall-clock training time.</p> <p>Key innovation: Rule-based routing by modality (not learned MoE routing) provides stability and simplicity while achieving substantial compute savings.</p>"},{"location":"models/multimodal/mot/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Sparsity mechanism: Modality-aware parameter decoupling (separate FFNs, attention matrices, layer norms per modality)</li> <li>Shared attention: Full self-attention over mixed sequences\u2014no routing-based attention sparsity</li> <li>Parameter selection: Token modality tag determines which parameter set to use</li> <li>Compatibility: Drop-in replacement for dense transformers in Chameleon and Transfusion architectures</li> <li>Scaling: Evaluated from 37M to 7B parameters across multiple settings</li> </ul>"},{"location":"models/multimodal/mot/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/multimodal/mot/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>MoT provides efficiency patterns for gene-brain-behavior integration:</p> <p>Key lessons: - Modality-specific processing: Separate genomics FFN + brain FFN + shared attention over joint sequences - Compute savings: 40\u201360% FLOP reduction applicable to large-scale neuro-omics pretraining - Stable training: No MoE routing instability\u2014deterministic modality selection - Implementation simplicity: Easy to implement vs. complex load-balancing in MoE</p> <p>Application to KB pipeline: <pre><code># Pseudocode for neuro-omics MoT\nfor token in sequence:\n    if token.modality == \"gene\":\n        ffn_output = gene_ffn(attention_output)\n    elif token.modality == \"brain\":\n        ffn_output = brain_ffn(attention_output)\n    elif token.modality == \"behavior\":\n        ffn_output = behavior_ffn(attention_output)\n    # Shared self-attention across all modalities\n</code></pre></p>"},{"location":"models/multimodal/mot/#for-arpa-h-brain-omics-model-bom","title":"For ARPA-H Brain-Omics Model (BOM)","text":"<p>MoT demonstrates scalable multimodal architectures:</p> <pre><code>Gene tokens   \u2192 |\n                | Global self-attention (dense)\nBrain tokens  \u2192 |     \u2193\n                | Modality-aware FFNs (sparse)\nText tokens   \u2192 |     \u2193\n                | Prediction heads\n</code></pre> <p>Transfer insights: - Efficiency-first design: Critical for scaling to population-level datasets (UK Biobank, HCP) - Leave-one-modality-out: MoT evaluation patterns inform ablation studies for gene-brain fusion - Hybrid models: Combining MoT (modality sparsity) with MoE (expert routing) for complementary benefits - Systems optimization: Wall-clock profiling applicable to neuro-omics training runs</p>"},{"location":"models/multimodal/mot/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<p>MoT is an architectural pattern, not a standalone model, but if implementing for neuro-omics:</p> <pre><code># 1. Tag tokens by modality (gene / brain / behavior)\n# 2. Build MoT transformer with modality-specific FFNs\n# 3. Forward through model (global attention + modality FFNs)\n# 4. Extract embeddings before task-specific heads\n# 5. Use for downstream fusion tasks\n</code></pre> <p>For implementation: See MoT paper code repository and adapt to neuro-omics modalities.</p>"},{"location":"models/multimodal/mot/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/multimodal/mot/#strengths","title":"Strengths","text":"<ul> <li>Dramatic compute savings: 40\u201360% FLOP reduction with matched performance</li> <li>Training stability: No MoE routing instability or load-balancing overhead</li> <li>Implementation simplicity: Rule-based routing easier than learned expert selection</li> <li>Extensive evaluation: Multiple settings (Chameleon, Transfusion), scales (37M\u20137B), and system profiling</li> </ul>"},{"location":"models/multimodal/mot/#limitations","title":"Limitations","text":"<ul> <li>Modality labels required: Tokens must be pre-tagged by modality</li> <li>Limited to tested modalities: Text, images, speech\u2014no structured data (tables, graphs, sequences)</li> <li>No within-modality routing: Single FFN per modality\u2014no fine-grained specialization</li> <li>Infrastructure-specific: Results tied to specific training setups (AWS p4de, A100s)</li> </ul>"},{"location":"models/multimodal/mot/#when-to-use-mot","title":"When to Use MoT","text":"<p>\u2705 Use when: - Building large-scale multimodal models with limited compute budgets - Want structured sparsity without MoE training complexity - Need stable, deterministic routing by modality - Scaling neuro-omics models to population-level datasets</p> <p>\u26a0\ufe0f Defer until: - Dense baselines established (per Nov 2025 integration plan) - Modality boundaries clear (e.g., which brain features are \"brain\" vs \"behavior\") - Engineering resources available for custom MoT implementation</p> <p>\u26a0\ufe0f Consider alternatives: - Dense fusion: Simpler baseline for initial gene-brain experiments - MoE architectures: If need learned task-specific routing - Late fusion: If modalities processed independently before combination</p>"},{"location":"models/multimodal/mot/#reference-materials","title":"Reference Materials","text":""},{"location":"models/multimodal/mot/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): MoT (2025) - Code walkthrough: MoT walkthrough - Model card (YAML): <code>kb/model_cards/mot.yaml</code> (if exists) - Paper card (YAML): <code>kb/paper_cards/mot_2025.yaml</code></p> <p>Integration recipes: - Multimodal Architectures - Design Patterns - Integration Strategy</p>"},{"location":"models/multimodal/mot/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/MoT/</code> - Official GitHub: Meta Mixture-of-Transformers</p> <p>Original paper: - Title: \"Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models\" - Authors: Liang, Weixin; Yu, Lili; Luo, Liang; Iyer, Srinivasan; Dong, Ning; Zhou, Chunting; Ghosh, Gargi; Lewis, Mike; Yih, Wen-tau; Zettlemoyer, Luke; Lin, Xi Victoria - Published: Transactions on Machine Learning Research (TMLR), 2025 - Link: arXiv:2411.04996 - DOI: 10.48550/arXiv.2411.04996 - PDF (local): mot_2025.pdf</p>"},{"location":"models/multimodal/mot/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Architecture adaptation: Design gene-brain-behavior MoT variant</li> <li>Efficiency benchmarking: Compare MoT vs dense fusion on UKB cognitive tasks</li> <li>Ablation studies: Implement leave-one-modality-out for gene-brain analysis</li> <li>Hybrid exploration: Test MoT + MoE combination for neuro-omics</li> <li>Systems profiling: Measure wall-clock and FLOP savings on KB training runs</li> </ol>"},{"location":"models/multimodal/mot/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>MoT FLOPs match dense models with same parameter budget\u2014key for fair comparison</li> <li>Modality separation analysis in paper informs how to design gene/brain/behavior boundaries</li> <li>Hybrid MoT+MoE results suggest complementary benefits for future neuro-omics architectures</li> <li>Transfusion compatibility shows MoT works with mixed objectives (autoregressive + diffusion)</li> </ul>"},{"location":"models/multimodal/titan/","title":"TITAN (Transformer-based Image and Text Alignment Network)","text":""},{"location":"models/multimodal/titan/#overview","title":"Overview","text":"<p>Type: Whole-Slide Pathology Foundation Model Architecture: Slide-level Vision Transformer with vision-language alignment Modality: Whole-slide histopathology images (WSIs) + pathology reports Primary use: Slide-level feature extraction for diagnosis, prognosis, retrieval, and report generation</p>"},{"location":"models/multimodal/titan/#purpose-design-philosophy","title":"Purpose &amp; Design Philosophy","text":"<p>TITAN is a slide-level foundation model for digital pathology that transforms gigapixel whole-slide images into general-purpose feature representations supporting diagnosis, biomarker prediction, survival analysis, rare disease retrieval, and report generation. Instead of operating on raw pixels, TITAN builds on pre-extracted patch embeddings and scales self-supervised learning to entire slides using vision transformers with long-context positional encodings. The model is pretrained in three stages: vision-only SSL, ROI-level caption alignment, and slide-level report alignment.</p> <p>Key innovation: Multi-scale hierarchical architecture processes gigapixel pathology images end-to-end with vision-language alignment, achieving strong zero-shot and few-shot performance on rare diseases.</p>"},{"location":"models/multimodal/titan/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Three-stage pretraining:</li> <li>Stage 1 (TITANV): Vision-only SSL on WSI feature grids with iBOT-style masked prediction</li> <li>Stage 2: ROI-level contrastive alignment with synthetic captions (PathChat)</li> <li>Stage 3: Slide-level alignment with pathology reports via CoCa-style objectives</li> <li>Input representation: 2D grid of patch embeddings (from CONCH v1.5 encoder) + [CLS] token</li> <li>Positional encoding: Long-range encodings adapted to large 2D grids (&gt;10\u2074 patches)</li> <li>Scale: Pretrained on 335k WSIs across 20 organ types + 182k pathology reports</li> <li>Tasks: Subtyping, biomarker prediction, survival, retrieval, zero-shot classification, report generation</li> </ul>"},{"location":"models/multimodal/titan/#integration-strategy","title":"Integration Strategy","text":""},{"location":"models/multimodal/titan/#for-neuro-omics-kb","title":"For Neuro-Omics KB","text":"<p>TITAN provides hierarchical vision-language patterns:</p> <p>Key lessons for brain imaging: - Multi-scale processing: Hierarchical approach applicable to multi-resolution brain imaging (T1, T2, fMRI) - Patch-to-whole aggregation: TITAN's patch \u2192 slide pipeline informs voxel \u2192 brain \u2192 subject aggregation - Vision-language alignment: Contrastive learning patterns transferable to brain scans + radiology reports - Zero-shot rare disease: Critical for uncommon neurological phenotypes with &lt;100 cases</p> <p>Potential adaptation for neuroimaging: <pre><code>Brain MRI voxels \u2192 Patch embeddings (BrainLM, SwiFT)\n                \u2192 3D grid of features\n                \u2192 Vision transformer with long-context encoding\n                \u2192 Contrastive alignment with radiology reports\n                \u2192 Zero-shot diagnosis + report generation\n</code></pre></p>"},{"location":"models/multimodal/titan/#for-arpa-h-brain-omics-model-bom","title":"For ARPA-H Brain-Omics Model (BOM)","text":"<p>TITAN demonstrates whole-system feature extraction:</p> <pre><code>Gene variants \u2192 Regional embeddings\n               \u2193\nBrain volumes \u2192 Multi-scale features (voxel \u2192 region \u2192 whole-brain)\n               \u2193\n               Vision-language alignment\n               \u2193\nClinical predictions + report generation\n</code></pre> <p>Transfer insights: - Long-context modeling: Process entire brain volumes without cropping/downsampling - Rare phenotype retrieval: TITAN's retrieval success informs rare genetic disorder diagnosis - Few-shot learning: Strong performance with minimal labels\u2014critical for rare neurological conditions - Synthetic caption generation: PathChat patterns applicable to brain ROI descriptions</p>"},{"location":"models/multimodal/titan/#embedding-extraction-workflow","title":"Embedding Extraction Workflow","text":"<p>If adapting TITAN for neuroimaging:</p> <pre><code># 1. Extract patch-level features from brain scans\n#    - Use BrainLM or SwiFT as patch encoder (analogous to CONCH)\n# 2. Arrange patches into 3D grid preserving spatial layout\n# 3. Apply vision transformer with long-context positional encoding\n# 4. Stage 1: Self-supervised pretraining on brain volumes\n# 5. Stage 2: ROI-level alignment with synthetic captions\n# 6. Stage 3: Whole-scan alignment with radiology reports\n# 7. Extract embeddings for downstream tasks\n</code></pre> <p>For neuro-omics KB: - Hierarchical features: Multi-scale brain representations - Report alignment: Connect brain scans with clinical text - Zero-shot transfer: Apply to new cohorts without labeled data</p>"},{"location":"models/multimodal/titan/#strengths-limitations","title":"Strengths &amp; Limitations","text":""},{"location":"models/multimodal/titan/#strengths","title":"Strengths","text":"<ul> <li>Gigapixel-scale processing: Handles entire WSIs (&gt;10\u2074 patches) end-to-end</li> <li>Vision-language alignment: Supports zero-shot classification and report generation</li> <li>Strong few-shot performance: Excels with limited labeled data</li> <li>Rare disease retrieval: Validated on diagnostically challenging cases</li> <li>Multi-scale pretraining: Vision-only + ROI-level + slide-level stages</li> </ul>"},{"location":"models/multimodal/titan/#limitations","title":"Limitations","text":"<ul> <li>Pathology-specific: Trained on histopathology, not neuroimaging</li> <li>Requires powerful patch encoder: Depends on CONCH v1.5 quality</li> <li>Compute intensive: Large-scale WSI pretraining expensive</li> <li>Limited to 2D spatial context: Does not natively handle 3D/4D neuroimaging sequences</li> </ul>"},{"location":"models/multimodal/titan/#when-to-use-titan","title":"When to Use TITAN","text":"<p>\u2705 Use as reference when: - Designing hierarchical vision models for brain imaging - Building vision-language alignment for medical imaging + reports - Implementing zero-shot rare disease classification - Scaling models to gigapixel/high-resolution inputs</p> <p>\u26a0\ufe0f Do not use directly for: - Neuroimaging (trained on pathology, not brain scans) - 3D/4D temporal sequences (designed for 2D spatial grids) - Production diagnosis (requires clinical validation)</p> <p>\u26a0\ufe0f Consider alternatives: - BrainLM/SwiFT: For neuroimaging-specific feature extraction - M3FM: For CLIP-style alignment with medical reports - BAGEL: For unified understanding + generation across modalities</p>"},{"location":"models/multimodal/titan/#reference-materials","title":"Reference Materials","text":""},{"location":"models/multimodal/titan/#knowledge-base-resources","title":"Knowledge Base Resources","text":"<p>Curated materials in this KB: - Paper summary &amp; notes (PDF): TITAN (2025) - Code walkthrough: TITAN walkthrough - Model card (YAML): <code>kb/model_cards/titan.yaml</code> - Paper card (YAML): <code>kb/paper_cards/titan_2025.yaml</code></p> <p>Integration recipes: - Multimodal Architectures - Design Patterns \u2014 Hierarchical vision-language - Integration Strategy</p>"},{"location":"models/multimodal/titan/#original-sources","title":"Original Sources","text":"<p>Source code repositories: - Local copy: <code>external_repos/titan/</code> - Official GitHub: mahmoodlab/TITAN</p> <p>Original paper: - Title: \"TITAN: A Multimodal Whole-Slide Foundation Model for Computational Pathology\" - Authors: Ding, Tong; Wagner, Sophia J.; Song, Andrew H.; Chen, Richard J.; Lu, Ming Y.; Zhang, Andrew; Vaidya, Anurag J.; Jaume, Guillaume; Shaban, Muhammad; Kim, Ahrong; Williamson, Drew F. K.; Robertson, Harry; Chen, Bowen; Almagro-P\u00e9rez, Cristina; Doucet, Paul; Sahai, Sharifa; Chen, Chengkuan; Chen, Christina S.; Komura, Daisuke; Kawabe, Akihiro; Ochi, Mieko; Sato, Shinya; Yokose, Tomoyuki; Miyagi, Yohei; Ishikawa, Shumpei; Gerber, Georg; Peng, Tingying; Le, Long Phi; Mahmood, Faisal - Published: Nature Medicine, 2025 - Link: Nature: s41591-024-03235-7 - PDF (local): titan_2025.pdf</p>"},{"location":"models/multimodal/titan/#next-steps-in-our-pipeline","title":"Next Steps in Our Pipeline","text":"<ol> <li>Hierarchical architecture study: Extract multi-scale patterns for brain imaging</li> <li>Vision-language adaptation: Implement brain scan + report contrastive learning</li> <li>Zero-shot rare phenotypes: Evaluate on uncommon neurological disorders</li> <li>3D/4D extension: Adapt long-context encoding to temporal fMRI sequences</li> <li>Few-shot learning: Test with limited labels on Cha Hospital pediatric cohorts</li> </ol>"},{"location":"models/multimodal/titan/#engineering-notes","title":"Engineering Notes","text":"<ul> <li>TITAN's three-stage pretraining (vision \u2192 ROI captions \u2192 reports) provides a clear template</li> <li>Long-context positional encodings critical for processing entire brain volumes</li> <li>PathChat synthetic captions demonstrate value of synthetic data for vision-language alignment</li> <li>Rare disease retrieval evaluation pattern applicable to rare genetic neurological disorders</li> </ul>"}]}