
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Code-first knowledge base for genetics and brain foundation models and multimodal integration">
      
      
      
        <link rel="canonical" href="https://allison-eunse.github.io/neuro-omics-kb/generated/kb_curated/papers-md/brainlm_2024/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>Brainlm 2024 - Neuro-Omics KB</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#brainlm-a-foundation-model-for-brain-activity-recordings" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Neuro-Omics KB" class="md-header__button md-logo" aria-label="Neuro-Omics KB" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Neuro-Omics KB
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Brainlm 2024
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="light-blue" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/allison-eunse/neuro-omics-kb" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../guide/kb_overview/" class="md-tabs__link">
          
  
  
  Guides

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../models/genetics/" class="md-tabs__link">
          
  
  
  Models

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../integration/" class="md-tabs__link">
          
  
  
  Integration

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../data/ukb_data_map/" class="md-tabs__link">
          
  
  
  Data

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../code_walkthroughs/" class="md-tabs__link">
          
  
  
  Code walkthroughs

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../decisions/2025-11-integration-plan/" class="md-tabs__link">
          
  
  
  Decisions

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Neuro-Omics KB" class="md-nav__button md-logo" aria-label="Neuro-Omics KB" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Neuro-Omics KB
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/allison-eunse/neuro-omics-kb" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Guides
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Guides
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../guide/kb_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    KB overview
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Models
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Models
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Genetics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Genetics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/genetics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/genetics/caduceus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Caduceus
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/genetics/dnabert2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DNABERT-2
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/genetics/evo2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Evo 2
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/genetics/generator/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GENERator
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Brain
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Brain
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/brain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/brain/brainlm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    BrainLM
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/brain/brainjepa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Brain-JEPA
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/brain/brainharmony/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Brain Harmony
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/brain/brainmt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    BrainMT
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../models/brain/swift/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    SwiFT
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Integration
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Integration
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/integration_strategy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Strategy
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--draft"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
        
          
          <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Analysis recipes
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Analysis recipes
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/analysis_recipes/cca_permutation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CCA + permutation
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--ready"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/analysis_recipes/prediction_baselines/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Prediction baselines
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--ready"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/analysis_recipes/partial_correlations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Partial correlations
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--ready"></span>
  

  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
        
          
          <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Modality features
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Modality features
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/modality_features/genomics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Genomics
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--ready"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/modality_features/smri/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    sMRI
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--ready"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/modality_features/fmri/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    fMRI
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--ready"></span>
  

  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/design_patterns/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Design patterns
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../integration/benchmarks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Benchmarks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Data
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Data
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../data/ukb_data_map/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    UKB data map
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../data/schemas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Schemas
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../data/governance_qc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Governance & QC
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Code walkthroughs
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Code walkthroughs
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/caduceus_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Caduceus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/dnabert2_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DNABERT-2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/evo2_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Evo 2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/generator_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GENERator
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/brainlm_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    BrainLM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/brainjepa_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Brain-JEPA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/brainharmony_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Brain Harmony
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/brainmt_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    BrainMT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../code_walkthroughs/swift_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    SwiFT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Decisions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Decisions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../decisions/2025-11-integration-plan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Integration plan (Nov 2025)
  

    
  </span>
  
  
    
  
  
    <span class="md-status md-status--active"></span>
  

  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/allison-eunse/neuro-omics-kb/edit/master/docs/generated/kb_curated/papers-md/brainlm_2024.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


  <h1>Brainlm 2024</h1>

<hr />
<h2 id="brainlm-a-foundation-model-for-brain-activity-recordings">BrainLM: A Foundation Model For Brain Activity Recordings<a class="headerlink" href="#brainlm-a-foundation-model-for-brain-activity-recordings" title="Permanent link">&para;</a></h2>
<p><strong>Authors:</strong> Josue Ortega Caro, Antonio H. de O. Fonseca, Syed A. Rizvi, Matteo Rosati, Christopher Averill, James L. Cross, Prateek Mittal, Emanuele Zappala, Rahul M. Dhodapkar, Chadi G. Abdallah, David van Dijk, et al.<br />
<strong>Year:</strong> 2024<br />
<strong>Venue:</strong> ICLR (International Conference on Learning Representations)</p>
<hr />
<h2 id="1-classification">1. Classification<a class="headerlink" href="#1-classification" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Domain Category:</strong>  </li>
<li>
<p><strong>Brain FM.</strong> The paper develops a large foundation model specifically for functional MRI (fMRI) recordings, learning spatiotemporal representations of brain activity dynamics across the whole brain.</p>
</li>
<li>
<p><strong>FM Usage Type:</strong>  </p>
</li>
<li>
<p><strong>Core FM development.</strong> BrainLM is introduced as a new foundation model architecture and pretraining scheme for fMRI, with subsequent fine-tuning and zero-shot applications.</p>
</li>
<li>
<p><strong>Key Modalities:</strong>  </p>
</li>
<li>Task-based and resting-state <strong>fMRI</strong> (BOLD time series) from large population cohorts (UK Biobank and Human Connectome Project).</li>
</ul>
<hr />
<h2 id="2-executive-summary">2. Executive Summary<a class="headerlink" href="#2-executive-summary" title="Permanent link">&para;</a></h2>
<p>This paper introduces <strong>BrainLM</strong>, a large transformer-based foundation model trained on <strong>6,700 hours of fMRI recordings</strong> from over 77,000 scans. Instead of training separate models for each narrow decoding task, BrainLM is pretrained in a <strong>self-supervised masked-reconstruction</strong> fashion to learn general-purpose representations of whole-brain activity over time. After pretraining, the same model can be fine-tuned to predict <strong>clinical variables</strong> (age, neuroticism, PTSD, anxiety), <strong>forecast future brain states</strong>, and perform <strong>zero-shot inference</strong> such as discovering functional brain networks directly from attention patterns. The authors show that BrainLM <strong>generalizes across datasets</strong>, performing well both on held-out UK Biobank scans and on the independent Human Connectome Project cohort. They also demonstrate interpretable attention maps that align with known brain networks and clinical differences (e.g., depression severity). For a new grad student, this paper is a key example of how <strong>foundation model ideas from language and vision</strong> (e.g., masked autoencoders) can be adapted to <strong>neuroimaging</strong>, creating a versatile model that unifies many downstream tasks on fMRI data.</p>
<hr />
<h2 id="3-problem-setup-and-motivation">3. Problem Setup and Motivation<a class="headerlink" href="#3-problem-setup-and-motivation" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Scientific / practical problem</strong></li>
<li>Build a <strong>single, general-purpose model</strong> of brain activity dynamics that can support many downstream tasks: predicting clinical variables, modeling brain networks, and forecasting future activity.</li>
<li>Learn <strong>unsupervised representations</strong> from large-scale fMRI repositories rather than training separate, task-specific models on small datasets.</li>
<li>
<p>Capture the <strong>full spatiotemporal structure</strong> of fMRI signals across the brain, not just limited regions like the visual cortex.</p>
</li>
<li>
<p><strong>Why this is hard</strong></p>
</li>
<li><strong>High dimensionality and complexity:</strong> fMRI produces thousands of voxel or parcel time series, with complex dependencies across brain regions and time.</li>
<li><strong>Indirect and noisy signal:</strong> BOLD signals are an indirect measure of neural activity and can be hard to interpret.</li>
<li><strong>Limited labels:</strong> Many large fMRI datasets have rich time series but relatively few labels for specific tasks, making supervised training challenging.</li>
<li><strong>Task-specific models do not generalize well:</strong> Traditional supervised models (e.g., SVMs, small neural nets) are tuned to narrow tasks and do not transfer well to new datasets or objectives.</li>
<li><strong>Need for scalable training:</strong> To benefit from large repositories like UK Biobank and HCP, models must handle massive data and learn representations that scale with model size and data size.</li>
</ul>
<hr />
<h2 id="4-data-and-modalities">4. Data and Modalities<a class="headerlink" href="#4-data-and-modalities" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Datasets used</strong></li>
<li><strong>UK Biobank (UKB):</strong><ul>
<li>~<strong>76,296</strong> task-based and resting-state fMRI recordings with associated medical records.</li>
<li>Ages approximately 40–69; scanned on a Siemens 3T scanner at ~0.735 s temporal resolution.</li>
<li><strong>80% (61,038 recordings)</strong> used for training; 20% held out for testing.</li>
</ul>
</li>
<li><strong>Human Connectome Project (HCP):</strong><ul>
<li><strong>1,002</strong> high-quality fMRI recordings from healthy adults.</li>
<li>~0.72 s temporal resolution; used entirely as an external evaluation cohort.</li>
</ul>
</li>
<li>
<p>In total, the training corpus spans <strong>77,298 recordings</strong> and <strong>6,700 hours</strong> of preprocessed fMRI.</p>
</li>
<li>
<p><strong>Modalities</strong></p>
</li>
<li>Single modality: <strong>functional MRI (fMRI)</strong>, representing whole-brain BOLD time series.</li>
<li>
<p>Both <strong>task</strong> and <strong>resting-state</strong> recordings are included.</p>
</li>
<li>
<p><strong>Preprocessing / representation</strong></p>
</li>
<li>Standard preprocessing: <strong>motion correction</strong>, <strong>normalization</strong>, <strong>temporal filtering</strong>, and <strong>ICA-based denoising</strong>.</li>
<li>Brain parcellation into <strong>424 regions (AAL-424 atlas)</strong>, yielding 424-dimensional time series per scan.</li>
<li>Time series sampled at ~1 Hz after preprocessing.</li>
<li><strong>Robust scaling</strong>: per-parcel median subtraction and division by interquartile range across subjects.</li>
<li>
<p>For model input:</p>
<ul>
<li>Random <strong>200-timestep subsequences</strong> are extracted from each recording.</li>
<li>Each parcel’s 200-timestep sequence is split into <strong>patches of 20 time points</strong>, giving 10 patches per parcel.</li>
<li>The resulting patches (conceptually 424 × 10) are treated as "tokens" via a <strong>learnable linear projection</strong> into 512-dimensional embeddings.</li>
<li>The 424 × 200 window is also viewed as a <strong>2D image</strong> (parcels × time) with parcels ordered by Y-coordinate to preserve spatial locality.</li>
</ul>
</li>
<li>
<p><strong>Missing details</strong></p>
</li>
<li>Exact number of subjects, hardware details beyond scanner type, and some hyperparameters are referenced but not fully spelled out in the main extracted text (likely given in supplementary material).</li>
</ul>
<hr />
<h2 id="5-model-foundation-model">5. Model / Foundation Model<a class="headerlink" href="#5-model-foundation-model" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Model Type</strong></li>
<li><strong>Masked autoencoder (MAE) based on a Transformer</strong> architecture.</li>
<li>
<p>Encoder–decoder structure with <strong>self-attention blocks</strong> that operate on spatiotemporal tokens derived from parcel-time patches.</p>
</li>
<li>
<p><strong>Is it a new FM or an existing one?</strong></p>
</li>
<li>
<p><strong>New foundation model.</strong> The authors design BrainLM specifically for fMRI data, inspired by <strong>BERT</strong> and <strong>Vision Transformer (ViT)</strong> style masked modeling but adapted to 2D parcel×time structure.</p>
</li>
<li>
<p><strong>Key components and innovations</strong></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tokenization of fMRI patches</td>
<td>200-timestep windows split into 20-timestep patches per parcel; patches projected into 512-d embeddings.</td>
</tr>
<tr>
<td>Spatiotemporal masking</td>
<td>Random and future-timepoint masking at rates of <strong>20%, 75%, or 90%</strong>, making the model reconstruct masked tokens.</td>
</tr>
<tr>
<td>2D "image-like" formulation</td>
<td>Treats the 424-parcel × 200-timestep window as a 2D grid; parcels ordered by Y-coordinate to preserve spatial adjacency, enabling multi-parcel tokens and scalable encoding.</td>
</tr>
<tr>
<td>Transformer encoder</td>
<td>Processes <strong>only unmasked tokens</strong>, with <strong>4 self-attention layers</strong> and <strong>4 attention heads</strong>.</td>
</tr>
<tr>
<td>Transformer decoder</td>
<td><strong>2-layer decoder</strong> that takes both encoded visible tokens and masked tokens, then reconstructs the full input.</td>
</tr>
<tr>
<td>Positional embeddings</td>
<td>Learnable <strong>spatial and temporal embeddings</strong> added to token representations to encode parcel location and time.</td>
</tr>
<tr>
<td>Latent CLS token</td>
<td>A special token summarizing each sequence, later used for clinical variable prediction and visualization.</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Training setup</strong></li>
<li><strong>Objective:</strong> Minimize <strong>mean squared error (MSE)</strong> between original and reconstructed fMRI signals for masked patches (self-supervised reconstruction).</li>
<li><strong>Pretraining data:</strong> 6,700 hours of fMRI from UKB and HCP, using random 200-timestep subsequences.</li>
<li><strong>Optimization:</strong> <strong>Adam</strong> optimizer; <strong>100 epochs</strong>; batch size <strong>512</strong>.</li>
<li><strong>Scaling:</strong> Multiple model sizes (e.g., <strong>13M, 111M, 650M parameters</strong>), with performance improving as both model size and dataset size increase.</li>
<li><strong>Downstream adaptation:</strong><ul>
<li>Add a <strong>3-layer MLP head</strong> to the pretrained encoder for regression of clinical variables.</li>
<li>Fine-tune on subsets of UKB data withheld from pretraining.</li>
<li>For future state prediction, fine-tune the model to forecast the next 20 timesteps given 180 observed timesteps.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="6-multimodal-integration-aspects-if-applicable">6. Multimodal / Integration Aspects (If Applicable)<a class="headerlink" href="#6-multimodal-integration-aspects-if-applicable" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Is the paper multimodal?</strong></li>
<li><strong>Not in the main experiments.</strong> BrainLM is trained and evaluated on <strong>single-modality fMRI data</strong> (task and rest). The core contribution is a <strong>unimodal</strong> foundation model for brain activity recordings.</li>
<li>
<p>However, the <strong>discussion explicitly points to multimodal extensions</strong> as future work, suggesting integration with EEG, MEG, and other brain-wise or even genomic information.</p>
</li>
<li>
<p><strong>Relation to integration baseline plan</strong></p>
</li>
<li>The paper itself does <strong>not</strong> implement late fusion, CCA, or contrastive cross-modal alignment. It is focused on learning a strong <strong>single-modality encoder</strong> for fMRI.</li>
<li>In terms of the integration baseline plan:<ul>
<li>BrainLM can be seen as a <strong>per-modality encoder</strong> that could feed into a <strong>late fusion</strong> or <strong>stacking</strong> approach when combined with other modalities (e.g., structural MRI, genetics, clinical variables).</li>
<li>Its robust self-supervised representations align with the plan’s emphasis on <strong>preserving modality-specific signal</strong> before fusion.</li>
<li>The evaluation on multiple tasks and datasets is compatible with the plan’s emphasis on <strong>robustness and disciplined evaluation</strong>, although the paper does not explicitly follow the full AUROC/AUPRC + confidence-interval protocol discussed in the plan.</li>
</ul>
</li>
<li>
<p>Future multimodal systems could:</p>
<ul>
<li>Use BrainLM’s embeddings as one tower in a <strong>two-tower contrastive model</strong>, with another tower encoding, for example, genetics or behavioral data.</li>
<li>Perform <strong>late fusion</strong> of BrainLM features with those from other FMs (e.g., for combined clinical prediction).</li>
</ul>
</li>
<li>
<p><strong>Summary</strong></p>
</li>
<li>For now, BrainLM is best viewed as a <strong>strong building block for multimodal integration</strong>, rather than a multimodal FM itself.</li>
</ul>
<hr />
<h2 id="7-experiments-and-results">7. Experiments and Results<a class="headerlink" href="#7-experiments-and-results" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Tasks / benchmarks</strong></li>
<li><strong>Masked reconstruction / generalization:</strong><ul>
<li>Evaluate reconstruction accuracy (e.g., R² on masked patches) on held-out <strong>UKB test data</strong> and independent <strong>HCP</strong> data.</li>
</ul>
</li>
<li><strong>Clinical variable prediction:</strong><ul>
<li>Fine-tune BrainLM to regress <strong>age</strong>, <strong>neuroticism</strong>, <strong>PTSD (PCL-5)</strong>, and <strong>general anxiety (GAD-7)</strong> scores from fMRI recordings.</li>
</ul>
</li>
<li><strong>Future brain state prediction:</strong><ul>
<li>Given 180 observed timesteps, predict the next 20 timesteps of parcel activity, evaluated on UKB and HCP.</li>
</ul>
</li>
<li><strong>Interpretability via attention analysis:</strong><ul>
<li>Analyze self-attention weights (especially from the CLS token) to study how attention changes across tasks and clinical groups (e.g., depression severity).</li>
</ul>
</li>
<li>
<p><strong>Functional network prediction (zero-shot-like):</strong></p>
<ul>
<li>Use attention-derived features to classify parcels into <strong>7 intrinsic functional networks</strong> without network-specific supervision.</li>
</ul>
</li>
<li>
<p><strong>Baselines</strong></p>
</li>
<li>For clinical variable regression:<ul>
<li><strong>SVR</strong> and <strong>MLP</strong> on correlation matrices.</li>
<li><strong>LSTM</strong> and <strong>GCN</strong> models that directly use fMRI recordings.</li>
<li>Comparisons both to models trained on <strong>raw data</strong> and models on <strong>pretrained embeddings</strong>.</li>
</ul>
</li>
<li>For future state prediction:<ul>
<li><strong>LSTM</strong>.</li>
<li><strong>Neural ODE</strong> and <strong>Latent ODE</strong> models.</li>
<li>A <strong>Transformer model without pretraining</strong> (same architecture but trained only on the forecasting task).</li>
</ul>
</li>
<li>
<p>For functional network identification:</p>
<ul>
<li>k-NN classifiers using:</li>
<li>Raw parcel time series,</li>
<li>Variational Autoencoder (VAE) embeddings,</li>
<li>GCN embeddings,</li>
<li>BrainLM attention weights.</li>
</ul>
</li>
<li>
<p><strong>Key findings</strong></p>
</li>
<li><strong>Generalization and reconstruction:</strong><ul>
<li>BrainLM achieves strong R² on UKB test data and <strong>generalizes well to HCP</strong>, despite domain differences, showing that pretraining learns robust, dataset-agnostic representations.</li>
<li>Performance improves with <strong>larger models and more data</strong>, demonstrating <strong>scaling laws</strong> similar to those seen in language and vision FMs.</li>
</ul>
</li>
<li><strong>Clinical variable prediction:</strong><ul>
<li>BrainLM-based regressors achieve <strong>lower mean squared error</strong> than baselines (SVR, MLP, LSTM, GCN, raw data) across age, PTSD, anxiety, and neuroticism.</li>
<li>Fine-tuning further improves over using frozen embeddings, indicating that pretrained representations are <strong>rich but still adaptable</strong>.</li>
<li>Even <strong>zero-shot regression</strong> (no fine-tuning) shows non-trivial predictive power, and performance scales with model size.</li>
</ul>
</li>
<li><strong>Future brain state prediction:</strong><ul>
<li>Fine-tuned BrainLM <strong>significantly outperforms LSTM, Neural ODE, Latent ODE, and the non-pretrained Transformer</strong> on both UKB and HCP.</li>
<li>The benefit of pretraining is clear: the model without pretraining performs noticeably worse.</li>
<li>Larger BrainLM variants maintain better forecasting performance over multiple timesteps.</li>
</ul>
</li>
<li><strong>Interpretability and networks:</strong><ul>
<li>Attention maps distinguish <strong>task vs rest</strong> (e.g., stronger attention to visual cortex during task states).</li>
<li>Differences in attention for <strong>high vs low depression</strong> emphasize frontal and limbic regions, consistent with clinical literature.</li>
<li>Using attention-based features, BrainLM achieves <strong>~58.8% accuracy</strong> in classifying parcels into 7 functional networks, outperforming VAE, GCN, and raw data baselines.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="8-strengths-limitations-and-open-questions">8. Strengths, Limitations, and Open Questions<a class="headerlink" href="#8-strengths-limitations-and-open-questions" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Strengths</strong></li>
<li>Introduces a <strong>true foundation model for fMRI</strong>, trained at a scale (6,700 hours, 77k recordings) much larger than prior work.</li>
<li>Uses <strong>self-supervised masked modeling</strong> to efficiently exploit unlabeled fMRI data, enabling versatile downstream applications.</li>
<li>Demonstrates <strong>strong generalization</strong> across cohorts (UKB → HCP) and across diverse tasks (reconstruction, forecasting, clinical prediction, network identification).</li>
<li>Provides <strong>interpretable attention maps</strong> that align with known brain networks and clinical patterns, offering neuroscientific insights.</li>
<li>
<p>Shows clear <strong>scaling behavior</strong>, suggesting that larger models and datasets can further improve performance.</p>
</li>
<li>
<p><strong>Limitations</strong></p>
</li>
<li>Currently <strong>unimodal</strong>: only fMRI is modeled; multimodal integration with EEG, structural MRI, genetics, and behavior is left for future work.</li>
<li>Despite interpretability via attention, the latent representations are still <strong>complex</strong>, and a full mechanistic understanding of what is encoded remains challenging.</li>
<li>The approach is <strong>computationally heavy</strong>, requiring large-scale pretraining with transformers on big imaging datasets.</li>
<li>Some preprocessing choices (e.g., parcellation scheme, scaling, window length) may influence results, but not all ablations are detailed in the main text.</li>
<li>
<p>Real-world clinical deployment requires careful validation, robustness checks, and fairness analyses that go beyond the current experiments.</p>
</li>
<li>
<p><strong>Open Questions and Future Directions:</strong></p>
</li>
<li>How does BrainLM compare to <strong>alternative pretraining objectives</strong> (contrastive, masked prediction on different views, generative modeling) for fMRI?</li>
<li>Can BrainLM embeddings be effectively <strong>combined with other modalities</strong> (EEG, MEG, structural MRI, genetics) using late fusion or contrastive two-tower setups, and does this improve clinical prediction?</li>
<li>What <strong>neuroscientific structure</strong> is captured in the CLS token and internal layers—can we relate specific attention patterns or latent dimensions to known circuits or cognitive processes?</li>
<li>How robust is BrainLM to <strong>distribution shifts</strong> such as different scanners, acquisition protocols, or clinical populations (e.g., pediatric, elderly, or specific disorders)?</li>
<li>Can smaller, <strong>distilled versions</strong> of BrainLM retain most performance while being practical for clinical or real-time applications?</li>
</ul>
<hr />
<h2 id="9-context-and-broader-impact">9. Context and Broader Impact<a class="headerlink" href="#9-context-and-broader-impact" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Position in the FM landscape</strong></li>
<li>BrainLM is to <strong>fMRI</strong> what large language models (like GPT) are to text: a <strong>general, pretrained backbone</strong> that can be adapted to many tasks rather than a task-specific model.</li>
<li>Within brain/neuro FMs, it extends prior work that focused on <strong>visual cortex or small datasets</strong> to <strong>whole-brain modeling at population scale</strong>.</li>
<li>
<p>It shows that the <strong>masked autoencoder paradigm</strong> from vision and language transfers well to <strong>spatiotemporal brain data</strong>.</p>
</li>
<li>
<p><strong>Relation to well-known ideas</strong></p>
</li>
<li>Conceptually, BrainLM behaves like a <strong>BERT-style or ViT-style masked model</strong> applied to a 2D grid where one dimension is space (brain parcels) and the other is time (fMRI timesteps).</li>
<li>The attention-based interpretability parallels how we interpret attention maps in NLP and vision, but here the "tokens" are <strong>brain parcels over time</strong>.</li>
<li>
<p>The clinical prediction and network discovery tasks illustrate how <strong>foundation models can support both prediction and scientific discovery</strong>.</p>
</li>
<li>
<p><strong>Why it matters and how it links to integration plans</strong></p>
</li>
<li>For a grad student interested in <strong>computational neuroscience</strong>, BrainLM is a blueprint for building large, reusable models of brain activity.</li>
<li>It provides a <strong>ready-made encoder</strong> that can plug into multimodal integration pipelines—consistent with the integration baseline plan’s idea of learning strong, modality-specific representations before fusion.</li>
<li>The work signals a general trend: <strong>foundation models are moving into neuroimaging</strong>, opening paths to richer multimodal systems that combine brain signals with genetic, behavioral, and clinical data.</li>
</ul>
<hr />
<h2 id="10-key-takeaways-bullet-summary">10. Key Takeaways (Bullet Summary)<a class="headerlink" href="#10-key-takeaways-bullet-summary" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Problem</strong></li>
<li>There is a need for a <strong>single, scalable model</strong> that can learn from massive fMRI repositories and support many downstream neuroscience and clinical tasks.</li>
<li>
<p>Traditional task-specific models struggle with <strong>generalization, data scale, and transfer across cohorts</strong>.</p>
</li>
<li>
<p><strong>Method / model</strong></p>
</li>
<li>BrainLM is a <strong>transformer-based masked autoencoder</strong> that treats fMRI parcel×time windows as a 2D grid of tokens.</li>
<li>It uses <strong>spatiotemporal masking and reconstruction</strong> to learn representations from 6,700 hours of fMRI without task labels.</li>
<li>The architecture includes <strong>4-layer encoder and 2-layer decoder</strong> transformers, with learned spatial and temporal embeddings and a summary <strong>CLS token</strong>.</li>
<li>
<p>Multiple model sizes (13M–650M parameters) are trained, showing <strong>improved performance with scale</strong>.</p>
</li>
<li>
<p><strong>Results</strong></p>
</li>
<li>BrainLM shows strong <strong>reconstruction and generalization</strong> performance on both UKB and HCP datasets.</li>
<li>It <strong>outperforms baselines</strong> (SVR, MLP, LSTM, GCN, Neural ODE, non-pretrained Transformer) in clinical variable prediction and future brain state forecasting.</li>
<li>
<p>Attention-based analyses reveal <strong>meaningful functional networks and clinical differences</strong>, and attention-derived features outperform other representations in classifying parcels into known networks.</p>
</li>
<li>
<p><strong>Why it matters</strong></p>
</li>
<li>BrainLM establishes a <strong>foundation model paradigm for fMRI</strong>, demonstrating that large, self-supervised models can unify diverse tasks in brain dynamics modeling.</li>
<li>It provides a <strong>flexible, interpretable, and extensible backbone</strong> that future work can extend to multimodal settings and more ambitious clinical and neuroscientific applications.</li>
</ul>
<hr />












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../../..", "features": ["navigation.tabs", "navigation.sections", "navigation.instant", "navigation.top", "content.code.copy", "content.tabs.link", "toc.integrate", "toc.follow", "content.action.edit"], "search": "../../../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
    
  </body>
</html>